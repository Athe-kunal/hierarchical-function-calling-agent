{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import scrape_pandas_website\n",
    "\n",
    "scrape_pandas_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import build_no_summary_graph, build_graph\n",
    "pandas_no_summary_graph = build_no_summary_graph()\n",
    "# pandas_graph = build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.summarize_dspy_agent import get_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Input/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Input/output: 100%|██████████| 1/1 [00:02<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for General functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For General functions: 100%|██████████| 1/1 [00:02<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Series: 100%|██████████| 2/2 [00:25<00:00, 12.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for DataFrame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For DataFrame: 100%|██████████| 1/1 [00:23<00:00, 23.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for pandas arrays, scalars, and data types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For pandas arrays, scalars, and data types: 100%|██████████| 1/1 [00:23<00:00, 23.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Index objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Index objects: 100%|██████████| 1/1 [00:22<00:00, 22.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Date offsets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Date offsets: 100%|██████████| 4/4 [01:12<00:00, 18.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Window\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Window: 100%|██████████| 1/1 [00:23<00:00, 23.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for GroupBy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For GroupBy: 100%|██████████| 1/1 [00:25<00:00, 25.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Resampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Resampling: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Style\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Style: 100%|██████████| 1/1 [00:24<00:00, 24.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Plotting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Plotting: 100%|██████████| 1/1 [00:22<00:00, 22.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Options and settings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Options and settings: 100%|██████████| 1/1 [00:22<00:00, 22.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Extensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Extensions: 100%|██████████| 1/1 [00:21<00:00, 21.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Testing: 100%|██████████| 1/1 [00:24<00:00, 24.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Missing values: 100%|██████████| 1/1 [00:21<00:00, 21.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries saved to data/parent_summary_dict.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Input/output': 'The pandas library provides a wide range of functions for reading and writing data in various formats. These functions include loading and saving pickled objects, reading and writing CSV files, Excel files, JSON data, HTML tables, XML documents, LaTeX tables, and more. Additionally, there are functions for working with HDFStore, Feather, parquet, ORC, SAS, SPSS, SQL databases, Google BigQuery, and Stata files. The library also offers functions for handling data labels and exporting data to different formats. Overall, pandas provides a comprehensive set of tools for efficiently managing and manipulating data across different file formats and data sources. ',\n",
       " 'General functions': 'The functions provided cover a wide range of data manipulation tasks in pandas. These include unpivoting a DataFrame from wide to long format, creating pivot tables, binning values, merging DataFrames, converting categorical variables, reshaping data, computing cross tabulations, discretizing data based on quantiles, concatenating pandas objects, detecting missing or non-missing values, converting data types, working with datetime indexes and periods, inferring frequencies, working with timedeltas, evaluating Python expressions, guessing datetime formats, generating deterministic integers, hashing data, and building DataFrames from compatible sources. These functions provide a comprehensive toolkit for data analysis and manipulation in pandas. ',\n",
       " 'Series': 'The pandas library provides a wide range of functions for working with one-dimensional ndarrays with axis labels, including time series. These functions allow for operations such as returning the Series as an ndarray, checking for NaN values, accessing specific elements, performing arithmetic operations, rounding values, comparing values, computing dot products, updating null elements, and more. Additionally, there are functions for aggregation, mapping values, rolling window calculations, exponentially weighted calculations, expanding window calculations, and applying functions to values. Descriptive statistics such as mean, minimum, maximum, median, mode, variance, standard deviation, and unique values can also be computed. Other functions include computing autocorrelation, trimming values, counting non-null observations, generating ranks, skew, and kurtosis, and encoding variables as categorical. The functions provided cover a wide range of operations that can be performed on Series and DataFrames in pandas. These operations include returning boolean values for monotonically increasing data, counting unique values, aligning objects, removing duplicate values, testing for equality, selecting rows based on criteria, setting axis names, sampling data, truncating data, replacing values, suffixing/prefixing labels, filling missing values, detecting missing values, sorting, transforming, shifting, converting time series data, localizing time zones, resampling, accessing datetime properties, working with sparse data, and manipulating strings. These functions provide a comprehensive toolkit for data manipulation and analysis in pandas. ',\n",
       " 'DataFrame': 'The pandas library provides a powerful tool for working with two-dimensional, size-mutable, potentially heterogeneous tabular data structures called DataFrames. DataFrames allow for easy manipulation and analysis of data, with functions such as returning the data types in the DataFrame, accessing subsets of columns based on data types, and providing memory usage information. Other functions include printing concise summaries of DataFrames, converting DataFrames to NumPy arrays, and accessing data based on integer positions or labels. Operations such as addition, subtraction, division, and multiplication can be performed element-wise on DataFrames, as well as more complex operations like matrix multiplication and rolling window calculations. Data can be aggregated, grouped, and analyzed using functions that provide descriptive statistics, compute correlations and covariances, and calculate cumulative sums and products. Additionally, functions for handling missing values, applying functions element-wise, and aligning data on different axes are available. Overall, pandas offers a comprehensive set of tools for data manipulation and analysis in Python. ',\n",
       " 'pandas arrays, scalars, and data types': 'The functions provided cover a wide range of functionalities related to handling dates, times, durations, and intervals in Pandas. These functions include creating arrays, working with different data types such as float64, boolean, and integer data, handling time zones, converting between different time representations, and extracting various date and time components such as day of the week, day of the year, number of days in a month, and more. Additionally, there are functions for working with periods, intervals, categorical data, sparse data, and timedelta data. The functions also provide capabilities for checking leap years, first and last days of months, quarters, and years, as well as formatting timestamps and durations in ISO 8601 format. Overall, these functions offer comprehensive support for date and time manipulation within the Pandas library. ',\n",
       " 'Index objects': 'An Index is an immutable sequence used for indexing and alignment in pandas. It can return an array representing the data, check for duplicate values, return the data type, shape, and dimensions of the underlying data, as well as the memory usage. It can also check for truthy elements, find the position of the smallest and largest values, make copies, insert or delete items, encode objects, and more. Additionally, there are functions specific to RangeIndex, CategoricalIndex, and IntervalIndex, which provide functionalities related to integer ranges, categorical data, and intervals, respectively. MultiIndex is a hierarchical index object that allows for multiple levels of indexing. ',\n",
       " 'Date offsets': 'The functions described are related to date increments and offsets. They include functions for determining the frequency of a date range, checking if a timestamp occurs on specific dates within a month, quarter, or year, returning extra parameters for the offset, and checking if the frequency is a unit frequency. There are also functions for business days, business hours, custom business days, and increments between the last business day of the month. Each function returns a string representing the frequency, the base frequency, and a copy of the frequency. Additionally, they provide information on whether a timestamp intersects with the frequency and occurs at the start or end of a month, quarter, or year. The functions described include returning a dictionary of extra parameters for the offset, determining if the frequency is a unit frequency, checking if a timestamp occurs on the start or end of a month, quarter, or year, representing custom business months, checking if a timestamp intersects with a specific frequency, describing monthly dates based on week and day of the week, weekly offsets, and date offsets between business days of each quarter. These functions provide a comprehensive set of tools for working with date frequencies and offsets in a variety of scenarios. The functions provided describe various aspects related to frequencies, timestamps, and offsets. They cover returning a string representing the base frequency, copying the frequency, checking if a timestamp intersects with a frequency, determining if a timestamp occurs on the month, quarter, or year end or start, providing extra parameters for the offset, checking if the frequency is a unit frequency, and calculating date offsets between different points in time such as quarter end dates, quarter start dates, the last business day of the year, the first business day of the year, calendar year end dates, calendar year begin dates, and business quarter dates for a 52-53 week fiscal year. Additionally, there is a description of a 52-53 week fiscal year known as a 4-4-5 calendar. The functions provided in the list are related to determining whether a timestamp occurs on specific dates such as month end, quarter end, year end, month start, quarter start, and year start. Additionally, there are functions for calculating the DateOffset for the Easter holiday, checking the frequency of timestamps, and providing extra parameters for the offset. The functions also involve returning the total number of nanoseconds, copying the frequency, and checking if a timestamp intersects with a specific frequency. Each function returns a boolean value or a string representing the frequency or base frequency. Some functions involve offsetting by a certain number of days, hours, minutes, seconds, milliseconds, microseconds, or nanoseconds. ',\n",
       " 'Window': 'The functions provided are used for calculating various rolling and expanding statistics on a dataset. These include calculating rolling count, mean, variance, minimum, correlation, unbiased skewness, custom aggregation function, quantile, rank, sum, median, standard deviation, maximum, sample covariance, Fisher’s definition of kurtosis without bias, standard error of mean, weighted window mean, variance, sum, standard deviation, expanding count, mean, variance, minimum, correlation, unbiased skewness, custom aggregation function, quantile, rank, sum, median, standard deviation, maximum, sample covariance, Fisher’s definition of kurtosis without bias, standard error of mean, ewm (exponential weighted moment) mean, standard deviation, sample correlation, sum, variance, sample covariance, as well as creating window boundaries for fixed-length windows. These functions provide a comprehensive set of tools for analyzing and summarizing data over rolling and expanding windows. ',\n",
       " 'GroupBy': \"The Groupby iterator allows the user to specify a groupby instruction for an object, providing various functionalities such as constructing a DataFrame from a group with a provided name, applying functions group-wise, aggregating using one or more operations over a specified axis, calling functions producing same-indexed Series or DataFrame on each group, filtering elements from groups that don't satisfy a criterion, computing descriptive statistics, filling NA/NaN values within groups, returning the first or last n rows of each group, computing cumulative max, min, sum, and product for each group, providing ranks of values within each group, returning a rolling grouper for rolling functionality per group, computing standard error of the mean, standard deviation, and variance of groups, returning elements at given positional indices in each group, checking if any or all values in a group are truthful, computing pairwise correlation and covariance of columns, computing the first or last entry of each column within each group, returning the index of the first occurrence of maximum or minimum over a specified axis, computing mean or median of groups, numbering items within each group, returning counts of unique elements in each position, calculating pct_change of each value to the previous entry in a group, returning group values at a given quantile, providing resampling with TimeGrouper, returning a random sample of items from each group, shifting each group by a specified number of observations, returning unbiased skew within groups, computing correlation and covariance with other Series, generating descriptive statistics, making box plots, histograms, and other plots of Series or DataFrame data. \",\n",
       " 'Resampling': 'The functions provided allow for grouping data by a specified criteria, constructing DataFrames from these groups, and performing various operations such as aggregation, applying functions, resampling, filling missing values, interpolating values, computing statistics like count, max, median, mean, min, standard deviation, variance, unique elements, product, standard error of the mean, sum, and quantiles within each group. These functions provide a comprehensive set of tools for analyzing and summarizing grouped data efficiently. ',\n",
       " 'Style': 'The Styler class in pandas helps style a DataFrame or Series using HTML and CSS. It allows for applying CSS-styling functions column-wise, row-wise, or table-wise. Additionally, it can be used to format text display values, relabel index or column headers, combine multiple Stylers, set table styles, generate tooltips, add CSS for scrolling frames, and more. The Styler class also provides functions for elementwise styling, hiding index or column headers, setting HTML element attributes, adding captions, defining CSS properties, resetting styles, highlighting missing values, minimum, maximum, quantiles, defined ranges, and more. It can also color text and backgrounds in gradient styles, draw bar charts in cell backgrounds, export styles, write to files or strings, and set styles on the current Styler. ',\n",
       " 'Plotting': 'The functions include generating matplotlib plots for visualizing clusters of multivariate data, autocorrelation plots for time series, bootstrap plots on mean, median, and mid-range statistics, creating box plots from DataFrame columns, removing pandas formatters and converters, lag plots for time series, parallel coordinates plotting, storing pandas plotting options, plotting multidimensional datasets in 2D, registering pandas formatters and converters with matplotlib, drawing a matrix of scatter plots, and converting DataFrame and Series to matplotlib tables using a helper function. ',\n",
       " 'Options and settings': \"The functions provided include:\\n- Printing the description for one or more registered options\\n- Retrieving the value of a specified option\\n- Acting as a context manager to temporarily set options within a 'with' statement context\\n- Resetting one or more options to their default values\\n- Setting the value of a specified option\\n- Formatting float representation in a DataFrame with SI notation\\n\\nThese functions collectively allow for managing and manipulating options, as well as formatting data within a DataFrame. \",\n",
       " 'Extensions': 'The functions include registering an ExtensionType with pandas as a class decorator, registering custom accessors on DataFrame, Series, and Index objects, creating a custom data type to be paired with an ExtensionArray, defining an abstract base class for custom 1-D array types, creating a pandas ExtensionArray for NumPy data, and checking if an indexer is a valid array indexer for an array. ',\n",
       " 'Testing': 'The functions in the list cover a wide range of error handling, warning messages, and exceptions that can occur when working with pandas. These include checking for equality between DataFrames, Indexes, Series, and ExtensionArrays, raising errors for bad syntax in SQL operations, encountering empty data or headers in CSV files, mismatched dimensions during indexing, invalid comparisons, unsupported operations on np.ndarrays, unsupported engine routines, timedelta values that cannot be represented, conflicts in index attributes, setting on copied slices, ill-specified functions in aggregation, slicing MultiIndexes that are not lexsorted, non-string values in category columns, incompatible criteria in HDF5 files, invalid index keys, merging data, null frequencies, using invalid variable names, datetime ranges outside representation, parsing errors in file contents, performance impacts, values outside int64 in to_stata, unsupported numpy functions, and unsupported clipboard functionality. These functions provide detailed error messages, warnings, and exceptions to help users troubleshoot issues and provide useful information for bug reports. ',\n",
       " 'Missing values': 'The functions described include an alias of <NA> and an alias of NaT. These functions likely involve creating aliases for missing or not-a-time values in a dataset. '}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_summaries(pandas_graph=pandas_no_summary_graph,MAX_WORDS=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'function_desc': 'Load pickled pandas object (or any object) from file.', 'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_pickle.html#pandas.read_pickle', 'trail': 'Input/output', 'type': 'function_node', 'function_name': 'pandas.read_pickle', 'function_calling': '{\\'name\\': \\'pandas#read_pickle\\', \\'descriptions\\': \\'Load pickled pandas object (or any object) from file.\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'filepath_or_buffer\\': {\\'type\\': \\'string\\', \\'description\\': \\'String, path object (implementing os.PathLike[str]), or file-like\\\\nobject implementing a binary readlines() function.\\\\nAlso accepts URL. URL is not limited to S3 and GCS.\\\\n\\'}, \\'compression\\': {\\'type\\': \\'string\\', \\'description\\': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is\\\\npath-like, then detect compression from the following extensions: ‘.gz’,\\\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\\\n(otherwise no compression).\\\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\\\nSet to None for no decompression.\\\\nCan also be a dict with key \\'method\\' set\\\\nto one of {\\'zip\\', \\'gzip\\', \\'bz2\\', \\'zstd\\', \\'xz\\', \\'tar\\'} and\\\\nother key-value pairs are forwarded to\\\\nzipfile.ZipFile, gzip.GzipFile,\\\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\\\ntarfile.TarFile, respectively.\\\\nAs an example, the following could be passed for Zstandard decompression using a\\\\ncustom compression dictionary:\\\\ncompression={\\'method\\': \\'zstd\\', \\'dict_data\\': my_compression_dict}.\\\\n\\\\nNew in version 1.5.0: Added support for .tar files.\\\\n\\\\n\\\\nChanged in version 1.4.0: Zstandard support.\\\\n\\\\n\"}, \\'storage_options\\': {\\'type\\': \\'dictionary\\', \\'description\\': \\'Extra options that make sense for a particular storage connection, e.g.\\\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\\\nare forwarded to urllib.request.Request as header options. For other\\\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\\\ndetails, and for more examples on storage options refer here.\\\\n\\'}}, \\'required\\': []}}', 'parameter_names_desc': '[{\\'param_name\\': \\'filepath_or_buffer\\', \\'param_type\\': \\'str, path object, or file-like object\\', \\'param_desc\\': \\'String, path object (implementing os.PathLike[str]), or file-like\\\\nobject implementing a binary readlines() function.\\\\nAlso accepts URL. URL is not limited to S3 and GCS.\\\\n\\'}, {\\'param_name\\': \\'compression\\', \\'param_type\\': \\'str or dict, default ‘infer’\\', \\'param_desc\\': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is\\\\npath-like, then detect compression from the following extensions: ‘.gz’,\\\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\\\n(otherwise no compression).\\\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\\\nSet to None for no decompression.\\\\nCan also be a dict with key \\'method\\' set\\\\nto one of {\\'zip\\', \\'gzip\\', \\'bz2\\', \\'zstd\\', \\'xz\\', \\'tar\\'} and\\\\nother key-value pairs are forwarded to\\\\nzipfile.ZipFile, gzip.GzipFile,\\\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\\\ntarfile.TarFile, respectively.\\\\nAs an example, the following could be passed for Zstandard decompression using a\\\\ncustom compression dictionary:\\\\ncompression={\\'method\\': \\'zstd\\', \\'dict_data\\': my_compression_dict}.\\\\n\\\\nNew in version 1.5.0: Added support for .tar files.\\\\n\\\\n\\\\nChanged in version 1.4.0: Zstandard support.\\\\n\\\\n\"}, {\\'param_name\\': \\'storage_options\\', \\'param_type\\': \\'dict, optional\\', \\'param_desc\\': \\'Extra options that make sense for a particular storage connection, e.g.\\\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\\\nare forwarded to urllib.request.Request as header options. For other\\\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\\\ndetails, and for more examples on storage options refer here.\\\\n\\'}]'}\n"
     ]
    }
   ],
   "source": [
    "for node,attr in pandas_no_summary_graph.nodes(data=True):\n",
    "    if 'trail' in attr:\n",
    "        print(attr)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.utils import build_graph\n",
    "pandas_graph = build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.database import build_database, build_docs_metadata\n",
    "\n",
    "docs,metadata = build_docs_metadata(pandas_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The pandas library provides a wide range of functions for reading and writing data in various formats. These functions include loading and saving pickled objects, reading and writing CSV files, Excel files, JSON data, HTML tables, XML documents, LaTeX tables, and more. Additionally, there are functions for working with HDFStore, Feather, parquet, ORC, SAS, SPSS, SQL databases, Google BigQuery, and Stata files. The library also offers functions for handling data labels and exporting data to different formats. Overall, pandas provides a comprehensive set of tools for efficiently managing and manipulating data across different file formats and data sources. ',\n",
       " {'url': 'https://pandas.pydata.org/docs/reference/io.html',\n",
       "  'type': 'parent_node',\n",
       "  'node_description': 'The pandas library provides a wide range of functions for reading and writing data in various formats. These functions include loading and saving pickled objects, reading and writing CSV files, Excel files, JSON data, HTML tables, XML documents, LaTeX tables, and more. Additionally, there are functions for working with HDFStore, Feather, parquet, ORC, SAS, SPSS, SQL databases, Google BigQuery, and Stata files. The library also offers functions for handling data labels and exporting data to different formats. Overall, pandas provides a comprehensive set of tools for efficiently managing and manipulating data across different file formats and data sources. ',\n",
       "  'name': 'Input/output'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0],metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1851, 1851)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs),len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(name=pandas_docs)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "build_database(docs,metadata,api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import load_database\n",
    "import os\n",
    "pandas_collection = load_database(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import PandasAgentBM25, PandasAgentChroma\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_chroma_agent = PandasAgentChroma(pandas_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General functions: The functions provided cover a wide range of data manipulation tasks in pandas. These include unpivoting a DataFrame from wide to long format, creating pivot tables, binning values, merging DataFrames, converting categorical variables, reshaping data, computing cross tabulations, discretizing data based on quantiles, concatenating pandas objects, detecting missing or non-missing values, converting data types, working with datetime indexes and periods, inferring frequencies, working with timedeltas, evaluating Python expressions, guessing datetime formats, generating deterministic integers, hashing data, and building DataFrames from compatible sources. These functions provide a comprehensive toolkit for data analysis and manipulation in pandas. \n",
      "\n",
      "DataFrame: The pandas library provides a powerful tool for working with two-dimensional, size-mutable, potentially heterogeneous tabular data structures called DataFrames. DataFrames allow for easy manipulation and analysis of data, with functions such as returning the data types in the DataFrame, accessing subsets of columns based on data types, and providing memory usage information. Other functions include printing concise summaries of DataFrames, converting DataFrames to NumPy arrays, and accessing data based on integer positions or labels. Operations such as addition, subtraction, division, and multiplication can be performed element-wise on DataFrames, as well as more complex operations like matrix multiplication and rolling window calculations. Data can be aggregated, grouped, and analyzed using functions that provide descriptive statistics, compute correlations and covariances, and calculate cumulative sums and products. Additionally, functions for handling missing values, applying functions element-wise, and aligning data on different axes are available. Overall, pandas offers a comprehensive set of tools for data manipulation and analysis in Python. \n",
      "\n",
      "Series: The pandas library provides a wide range of functions for working with one-dimensional ndarrays with axis labels, including time series. These functions allow for operations such as returning the Series as an ndarray, checking for NaN values, accessing specific elements, performing arithmetic operations, rounding values, comparing values, computing dot products, updating null elements, and more. Additionally, there are functions for aggregation, mapping values, rolling window calculations, exponentially weighted calculations, expanding window calculations, and applying functions to values. Descriptive statistics such as mean, minimum, maximum, median, mode, variance, standard deviation, and unique values can also be computed. Other functions include computing autocorrelation, trimming values, counting non-null observations, generating ranks, skew, and kurtosis, and encoding variables as categorical. The functions provided cover a wide range of operations that can be performed on Series and DataFrames in pandas. These operations include returning boolean values for monotonically increasing data, counting unique values, aligning objects, removing duplicate values, testing for equality, selecting rows based on criteria, setting axis names, sampling data, truncating data, replacing values, suffixing/prefixing labels, filling missing values, detecting missing values, sorting, transforming, shifting, converting time series data, localizing time zones, resampling, accessing datetime properties, working with sparse data, and manipulating strings. These functions provide a comprehensive toolkit for data manipulation and analysis in pandas. \n",
      "\n",
      " DataFrame; General functions; Series\n",
      "pandas.DataFrame.pivot_table: Create a spreadsheet-style pivot table as a DataFrame.\n",
      "\n",
      "pandas.DataFrame.melt: Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.\n",
      "\n",
      "pandas.DataFrame.apply: Apply a function along an axis of the DataFrame.\n",
      "\n",
      "pandas.DataFrame.T: The transpose of the DataFrame.\n",
      "\n",
      "pandas.DataFrame.unstack: Pivot a level of the (necessarily hierarchical) index labels.\n",
      "\n",
      "\n",
      "{'$or': [{'function_name': {'$eq': 'pandas.DataFrame.pivot_table'}}, {'function_name': {'$eq': ' pandas.DataFrame.melt'}}, {'function_name': {'$eq': ' pandas.DataFrame.unstack'}}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': ['id591'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'function_calling': \"{'name': 'pandas#DataFrame#pivot_table', 'descriptions': 'Create a spreadsheet-style pivot table as a DataFrame.', 'parameters': {'type': 'object', 'properties': {'values': {'type': 'list-like or scalar, optional', 'description': 'Column or columns to aggregate.\\\\n'}, 'index': {'type': 'column, Grouper, array, or list of the previous', 'description': 'Keys to group by on the pivot table index. If a list is passed,\\\\nit can contain any of the other types (except list). If an array is\\\\npassed, it must be the same length as the data and will be used in\\\\nthe same manner as column values.\\\\n'}, 'columns': {'type': 'column, Grouper, array, or list of the previous', 'description': 'Keys to group by on the pivot table column. If a list is passed,\\\\nit can contain any of the other types (except list). If an array is\\\\npassed, it must be the same length as the data and will be used in\\\\nthe same manner as column values.\\\\n'}, 'aggfunc': {'type': 'dictionary', 'description': 'If a list of functions is passed, the resulting pivot table will have\\\\nhierarchical columns whose top level are the function names\\\\n(inferred from the function objects themselves).\\\\nIf a dict is passed, the key is column to aggregate and the value is\\\\nfunction or list of functions. If margin=True, aggfunc will be\\\\nused to calculate the partial aggregates.\\\\n'}, 'fill_value': {'type': 'scalar, default None', 'description': 'Value to replace missing values with (in the resulting pivot table,\\\\nafter aggregation).\\\\n'}, 'margins': {'type': 'boolean', 'description': 'If margins=True, special All columns and rows\\\\nwill be added with partial group aggregates across the categories\\\\non the rows and columns.\\\\n'}, 'dropna': {'type': 'boolean', 'description': 'Do not include columns whose entries are all NaN. If True,\\\\nrows with a NaN value in any column will be omitted before\\\\ncomputing margins.\\\\n'}, 'margins_name': {'type': 'string', 'description': 'Name of the row / column that will contain the totals\\\\nwhen margins is True.\\\\n'}, 'observed': {'type': 'boolean', 'description': 'This only applies if any of the groupers are Categoricals.\\\\nIf True: only show observed values for categorical groupers.\\\\nIf False: show all values for categorical groupers.\\\\n\\\\nDeprecated since version 2.2.0: The default value of False is deprecated and will change to\\\\nTrue in a future version of pandas.\\\\n\\\\n'}, 'sort': {'type': 'boolean', 'description': 'Specifies if the result should be sorted.\\\\n\\\\nNew in version 1.3.0.\\\\n\\\\n'}}, 'required': []}}\",\n",
       "   'function_desc': 'Create a spreadsheet-style pivot table as a DataFrame.',\n",
       "   'function_name': 'pandas.DataFrame.pivot_table',\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot_table.html#pandas.DataFrame.pivot_table',\n",
       "   'parameter_names_desc': \"[{'param_name': 'values', 'param_type': 'list-like or scalar, optional', 'param_desc': 'Column or columns to aggregate.\\\\n'}, {'param_name': 'index', 'param_type': 'column, Grouper, array, or list of the previous', 'param_desc': 'Keys to group by on the pivot table index. If a list is passed,\\\\nit can contain any of the other types (except list). If an array is\\\\npassed, it must be the same length as the data and will be used in\\\\nthe same manner as column values.\\\\n'}, {'param_name': 'columns', 'param_type': 'column, Grouper, array, or list of the previous', 'param_desc': 'Keys to group by on the pivot table column. If a list is passed,\\\\nit can contain any of the other types (except list). If an array is\\\\npassed, it must be the same length as the data and will be used in\\\\nthe same manner as column values.\\\\n'}, {'param_name': 'aggfunc', 'param_type': 'function, list of functions, dict, default “mean”', 'param_desc': 'If a list of functions is passed, the resulting pivot table will have\\\\nhierarchical columns whose top level are the function names\\\\n(inferred from the function objects themselves).\\\\nIf a dict is passed, the key is column to aggregate and the value is\\\\nfunction or list of functions. If margin=True, aggfunc will be\\\\nused to calculate the partial aggregates.\\\\n'}, {'param_name': 'fill_value', 'param_type': 'scalar, default None', 'param_desc': 'Value to replace missing values with (in the resulting pivot table,\\\\nafter aggregation).\\\\n'}, {'param_name': 'margins', 'param_type': 'bool, default False', 'param_desc': 'If margins=True, special All columns and rows\\\\nwill be added with partial group aggregates across the categories\\\\non the rows and columns.\\\\n'}, {'param_name': 'dropna', 'param_type': 'bool, default True', 'param_desc': 'Do not include columns whose entries are all NaN. If True,\\\\nrows with a NaN value in any column will be omitted before\\\\ncomputing margins.\\\\n'}, {'param_name': 'margins_name', 'param_type': 'str, default ‘All’', 'param_desc': 'Name of the row / column that will contain the totals\\\\nwhen margins is True.\\\\n'}, {'param_name': 'observed', 'param_type': 'bool, default False', 'param_desc': 'This only applies if any of the groupers are Categoricals.\\\\nIf True: only show observed values for categorical groupers.\\\\nIf False: show all values for categorical groupers.\\\\n\\\\nDeprecated since version 2.2.0: The default value of False is deprecated and will change to\\\\nTrue in a future version of pandas.\\\\n\\\\n'}, {'param_name': 'sort', 'param_type': 'bool, default True', 'param_desc': 'Specifies if the result should be sorted.\\\\n\\\\nNew in version 1.3.0.\\\\n\\\\n'}]\",\n",
       "   'trail': 'DataFrame',\n",
       "   'type': 'function_node'}],\n",
       " 'documents': ['Create a spreadsheet-style pivot table as a DataFrame.'],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_chroma_agent(\"How to pivot a dataframe?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input/output: The pandas library provides a wide range of functions for reading and writing data in various formats. These functions include loading and saving pickled objects, reading and writing CSV files, Excel files, JSON data, HTML tables, XML documents, LaTeX tables, and more. Additionally, there are functions for working with HDFStore, Feather, parquet, ORC, SAS, SPSS, SQL databases, Google BigQuery, and Stata files. The library also offers functions for handling data labels and exporting data to different formats. Overall, pandas provides a comprehensive set of tools for efficiently managing and manipulating data across different file formats and data sources. \n",
      "\n",
      "DataFrame: The pandas library provides a powerful tool for working with two-dimensional, size-mutable, potentially heterogeneous tabular data structures called DataFrames. DataFrames allow for easy manipulation and analysis of data, with functions such as returning the data types in the DataFrame, accessing subsets of columns based on data types, and providing memory usage information. Other functions include printing concise summaries of DataFrames, converting DataFrames to NumPy arrays, and accessing data based on integer positions or labels. Operations such as addition, subtraction, division, and multiplication can be performed element-wise on DataFrames, as well as more complex operations like matrix multiplication and rolling window calculations. Data can be aggregated, grouped, and analyzed using functions that provide descriptive statistics, compute correlations and covariances, and calculate cumulative sums and products. Additionally, functions for handling missing values, applying functions element-wise, and aligning data on different axes are available. Overall, pandas offers a comprehensive set of tools for data manipulation and analysis in Python. \n",
      "\n",
      "Plotting: The functions include generating matplotlib plots for visualizing clusters of multivariate data, autocorrelation plots for time series, bootstrap plots on mean, median, and mid-range statistics, creating box plots from DataFrame columns, removing pandas formatters and converters, lag plots for time series, parallel coordinates plotting, storing pandas plotting options, plotting multidimensional datasets in 2D, registering pandas formatters and converters with matplotlib, drawing a matrix of scatter plots, and converting DataFrame and Series to matplotlib tables using a helper function. \n",
      "\n",
      " Input/output; DataFrame; Plotting\n",
      "pandas.read_parquet: Load a parquet object from the file path, returning a DataFrame.\n",
      "\n",
      "pandas.DataFrame.to_parquet: Write a DataFrame to the binary parquet format.\n",
      "\n",
      "pandas.read_feather: Load a feather-format object from the file path.\n",
      "\n",
      "pandas.read_table: Read general delimited file into DataFrame.\n",
      "\n",
      "pandas.HDFStore.get: Retrieve pandas object stored in file.\n",
      "\n",
      "\n",
      "{'$or': [{'function_name': {'$eq': 'pandas.read_parquet'}}, {'function_name': {'$eq': ' pandas.DataFrame.to_parquet'}}, {'function_name': {'$eq': ' pandas.read_feather'}}, {'function_name': {'$eq': ' pandas.read_table'}}, {'function_name': {'$eq': ' pandas.HDFStore.get'}}]}\n"
     ]
    }
   ],
   "source": [
    "func = pandas_chroma_agent(\"How to read parquet files?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['id52'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'function_calling': '{\\'name\\': \\'pandas#read_parquet\\', \\'descriptions\\': \\'Load a parquet object from the file path, returning a DataFrame.\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'path\\': {\\'type\\': \\'string\\', \\'description\\': \\'String, path object (implementing os.PathLike[str]), or file-like\\\\nobject implementing a binary read() function.\\\\nThe string could be a URL. Valid URL schemes include http, ftp, s3,\\\\ngs, and file. For file URLs, a host is expected. A local file could be:\\\\nfile://localhost/path/to/table.parquet.\\\\nA file URL can also be a path to a directory that contains multiple\\\\npartitioned parquet files. Both pyarrow and fastparquet support\\\\npaths to directories as well as file URLs. A directory path could be:\\\\nfile://localhost/path/to/tables or s3://bucket/partition_dir.\\\\n\\'}, \\'engine\\': {\\'type\\': \\'string\\', \\'enum\\': [\\'auto\\', \\' pyarrow\\', \\' fastparquet\\'], \\'description\\': \"Parquet library to use. If ‘auto’, then the option\\\\nio.parquet.engine is used. The default io.parquet.engine\\\\nbehavior is to try ‘pyarrow’, falling back to ‘fastparquet’ if\\\\n‘pyarrow’ is unavailable.\\\\nWhen using the \\'pyarrow\\' engine and no storage options are provided\\\\nand a filesystem is implemented by both pyarrow.fs and fsspec\\\\n(e.g. “s3://”), then the pyarrow.fs filesystem is attempted first.\\\\nUse the filesystem keyword with an instantiated fsspec filesystem\\\\nif you wish to use its implementation.\\\\n\"}, \\'columns\\': {\\'type\\': \\'list, default=None\\', \\'description\\': \\'If not None, only these columns will be read from the file.\\\\n\\'}, \\'storage_options\\': {\\'type\\': \\'dictionary\\', \\'description\\': \\'Extra options that make sense for a particular storage connection, e.g.\\\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\\\nare forwarded to urllib.request.Request as header options. For other\\\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\\\ndetails, and for more examples on storage options refer here.\\\\n\\\\nNew in version 1.3.0.\\\\n\\\\n\\'}, \\'use_nullable_dtypes\\': {\\'type\\': \\'boolean\\', \\'description\\': \\'If True, use dtypes that use pd.NA as missing value indicator\\\\nfor the resulting DataFrame. (only applicable for the pyarrow\\\\nengine)\\\\nAs new dtypes are added that support pd.NA in the future, the\\\\noutput with this option will change to use those dtypes.\\\\nNote: this is an experimental option, and behaviour (e.g. additional\\\\nsupport dtypes) may change without notice.\\\\n\\\\nDeprecated since version 2.0.\\\\n\\\\n\\'}, \\'dtype_backend\\': {\\'type\\': \\'string\\', \\'enum\\': [\\'numpy_nullable\\', \\' pyarrow\\'], \\'description\\': \\'Back-end data type applied to the resultant DataFrame\\\\n(still experimental). Behaviour is as follows:\\\\n\\\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\\\n(default).\\\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\\\nDataFrame.\\\\n\\\\n\\\\nNew in version 2.0.\\\\n\\\\n\\'}, \\'filesystem\\': {\\'type\\': \\'fsspec or pyarrow filesystem, default None\\', \\'description\\': \\'Filesystem object to use when reading the parquet file. Only implemented\\\\nfor engine=\"pyarrow\".\\\\n\\\\nNew in version 2.1.0.\\\\n\\\\n\\'}, \\'filters\\': {\\'type\\': \\'List[Tuple] or List[List[Tuple]], default None\\', \\'description\\': \\'To filter out data.\\\\nFilter syntax: [[(column, op, val), …],…]\\\\nwhere op is [==, =, >, >=, <, <=, !=, in, not in]\\\\nThe innermost tuples are transposed into a set of filters applied\\\\nthrough an AND operation.\\\\nThe outer list combines these sets of filters through an OR\\\\noperation.\\\\nA single list of tuples can also be used, meaning that no OR\\\\noperation between set of filters is to be conducted.\\\\nUsing this argument will NOT result in row-wise filtering of the final\\\\npartitions unless engine=\"pyarrow\" is also specified. For\\\\nother engines, filtering is only performed at the partition level, that is,\\\\nto prevent the loading of some row-groups and/or files.\\\\n\\\\nNew in version 2.1.0.\\\\n\\\\n\\'}}, \\'required\\': []}}',\n",
       "   'function_desc': 'Load a parquet object from the file path, returning a DataFrame.',\n",
       "   'function_name': 'pandas.read_parquet',\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html#pandas.read_parquet',\n",
       "   'parameter_names_desc': '[{\\'param_name\\': \\'path\\', \\'param_type\\': \\'str, path object or file-like object\\', \\'param_desc\\': \\'String, path object (implementing os.PathLike[str]), or file-like\\\\nobject implementing a binary read() function.\\\\nThe string could be a URL. Valid URL schemes include http, ftp, s3,\\\\ngs, and file. For file URLs, a host is expected. A local file could be:\\\\nfile://localhost/path/to/table.parquet.\\\\nA file URL can also be a path to a directory that contains multiple\\\\npartitioned parquet files. Both pyarrow and fastparquet support\\\\npaths to directories as well as file URLs. A directory path could be:\\\\nfile://localhost/path/to/tables or s3://bucket/partition_dir.\\\\n\\'}, {\\'param_name\\': \\'engine\\', \\'param_type\\': \\'{‘auto’, ‘pyarrow’, ‘fastparquet’}, default ‘auto’\\', \\'param_desc\\': \"Parquet library to use. If ‘auto’, then the option\\\\nio.parquet.engine is used. The default io.parquet.engine\\\\nbehavior is to try ‘pyarrow’, falling back to ‘fastparquet’ if\\\\n‘pyarrow’ is unavailable.\\\\nWhen using the \\'pyarrow\\' engine and no storage options are provided\\\\nand a filesystem is implemented by both pyarrow.fs and fsspec\\\\n(e.g. “s3://”), then the pyarrow.fs filesystem is attempted first.\\\\nUse the filesystem keyword with an instantiated fsspec filesystem\\\\nif you wish to use its implementation.\\\\n\"}, {\\'param_name\\': \\'columns\\', \\'param_type\\': \\'list, default=None\\', \\'param_desc\\': \\'If not None, only these columns will be read from the file.\\\\n\\'}, {\\'param_name\\': \\'storage_options\\', \\'param_type\\': \\'dict, optional\\', \\'param_desc\\': \\'Extra options that make sense for a particular storage connection, e.g.\\\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\\\nare forwarded to urllib.request.Request as header options. For other\\\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\\\ndetails, and for more examples on storage options refer here.\\\\n\\\\nNew in version 1.3.0.\\\\n\\\\n\\'}, {\\'param_name\\': \\'use_nullable_dtypes\\', \\'param_type\\': \\'bool, default False\\', \\'param_desc\\': \\'If True, use dtypes that use pd.NA as missing value indicator\\\\nfor the resulting DataFrame. (only applicable for the pyarrow\\\\nengine)\\\\nAs new dtypes are added that support pd.NA in the future, the\\\\noutput with this option will change to use those dtypes.\\\\nNote: this is an experimental option, and behaviour (e.g. additional\\\\nsupport dtypes) may change without notice.\\\\n\\\\nDeprecated since version 2.0.\\\\n\\\\n\\'}, {\\'param_name\\': \\'dtype_backend\\', \\'param_type\\': \\'{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’\\', \\'param_desc\\': \\'Back-end data type applied to the resultant DataFrame\\\\n(still experimental). Behaviour is as follows:\\\\n\\\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\\\n(default).\\\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\\\nDataFrame.\\\\n\\\\n\\\\nNew in version 2.0.\\\\n\\\\n\\'}, {\\'param_name\\': \\'filesystem\\', \\'param_type\\': \\'fsspec or pyarrow filesystem, default None\\', \\'param_desc\\': \\'Filesystem object to use when reading the parquet file. Only implemented\\\\nfor engine=\"pyarrow\".\\\\n\\\\nNew in version 2.1.0.\\\\n\\\\n\\'}, {\\'param_name\\': \\'filters\\', \\'param_type\\': \\'List[Tuple] or List[List[Tuple]], default None\\', \\'param_desc\\': \\'To filter out data.\\\\nFilter syntax: [[(column, op, val), …],…]\\\\nwhere op is [==, =, >, >=, <, <=, !=, in, not in]\\\\nThe innermost tuples are transposed into a set of filters applied\\\\nthrough an AND operation.\\\\nThe outer list combines these sets of filters through an OR\\\\noperation.\\\\nA single list of tuples can also be used, meaning that no OR\\\\noperation between set of filters is to be conducted.\\\\nUsing this argument will NOT result in row-wise filtering of the final\\\\npartitions unless engine=\"pyarrow\" is also specified. For\\\\nother engines, filtering is only performed at the partition level, that is,\\\\nto prevent the loading of some row-groups and/or files.\\\\n\\\\nNew in version 2.1.0.\\\\n\\\\n\\'}]',\n",
       "   'trail': 'Input/output',\n",
       "   'type': 'function_node'}],\n",
       " 'documents': ['Load a parquet object from the file path, returning a DataFrame.'],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import ast\n",
    "\n",
    "\n",
    "def run_function_calling(fcs, question: str):\n",
    "    \n",
    "    for meta in fcs[\"metadatas\"]:\n",
    "        function_call = ast.literal_eval(meta[\"function_calling\"])\n",
    "        break\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"human\", \"{input}\")])\n",
    "    function_name = \"_\".join(function_call[\"name\"].split(\"#\"))\n",
    "    print(function_name)\n",
    "    model = ChatOpenAI(temperature=0).bind(\n",
    "        functions=[function_call], function_call={\"name\": function_name}\n",
    "    )\n",
    "    runnable = prompt | model\n",
    "    resp = runnable.invoke({\"input\": question})\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas_read_parquet\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid 'functions[0].name': string does not match pattern. Expected a string that matches the pattern '^[a-zA-Z0-9_-]+$'.\", 'type': 'invalid_request_error', 'param': 'functions[0].name', 'code': 'invalid_value'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function_calling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow to read parquet files?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m, in \u001b[0;36mrun_function_calling\u001b[0;34m(fcs, question)\u001b[0m\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mbind(\n\u001b[1;32m     15\u001b[0m     functions\u001b[38;5;241m=\u001b[39m[function_call], function_call\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_name}\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m runnable \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m model\n\u001b[0;32m---> 18\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mrunnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/runnables/base.py:4511\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4507\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4508\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4509\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4510\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4512\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4513\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4514\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:154\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    150\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    151\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    153\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    164\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:556\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    550\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    554\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    555\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:417\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    416\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 417\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    418\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    419\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    421\u001b[0m ]\n\u001b[1;32m    422\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:407\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 407\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m         )\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:626\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 626\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_community/chat_models/openai.py:441\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    436\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    440\u001b[0m }\n\u001b[0;32m--> 441\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_community/chat_models/openai.py:356\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(\u001b[38;5;28mself\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/resources/chat/completions.py:667\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:1233\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1221\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1228\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1229\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1230\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1231\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1232\u001b[0m     )\n\u001b[0;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:922\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    915\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    920\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    921\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1012\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1013\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1016\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1017\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1021\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid 'functions[0].name': string does not match pattern. Expected a string that matches the pattern '^[a-zA-Z0-9_-]+$'.\", 'type': 'invalid_request_error', 'param': 'functions[0].name', 'code': 'invalid_value'}}"
     ]
    }
   ],
   "source": [
    "resp = run_function_calling(func,\"How to read parquet files?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas.DataFrame.eq: Get Equal to of dataframe and other, element-wise (binary operator eq).pandas.DataFrame.quantile: Return values at the given quantile over requested axis.pandas.DataFrame.to_gbq: Write a DataFrame to a Google BigQuery table.pandas.DataFrame.skew: Return unbiased skew over requested axis.pandas.DataFrame.query: Query the columns of a DataFrame with a boolean expression.\n",
      "{'function_name': {'$eq': 'pandas.DataFrame.to_gbq'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': ['id648'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'function_calling': '{\\'name\\': \\'pandas#DataFrame#to_gbq\\', \\'descriptions\\': \\'Write a DataFrame to a Google BigQuery table.\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'destination_table\\': {\\'type\\': \\'string\\', \\'description\\': \\'Name of table to be written, in the form dataset.tablename.\\\\n\\'}, \\'project_id\\': {\\'type\\': \\'string\\', \\'description\\': \\'Number of rows to be inserted in each chunk from the dataframe.\\\\nSet to None to load the whole dataframe at once.\\\\n\\'}, \\'chunksize\\': {\\'type\\': \\'string\\', \\'description\\': \\'Number of rows to be inserted in each chunk from the dataframe.\\\\nSet to None to load the whole dataframe at once.\\\\n\\'}, \\'reauth\\': {\\'type\\': \\'boolean\\', \\'description\\': \\'Force Google BigQuery to re-authenticate the user. This is useful\\\\nif multiple accounts are used.\\\\n\\'}, \\'if_exists\\': {\\'type\\': \\'string\\', \\'description\\': \"Behavior when the destination table exists. Value can be one of:\\\\n\\\\n\\'fail\\'If table exists raise pandas_gbq.gbq.TableCreationError.\\\\n\\\\n\\'replace\\'If table exists, drop it, recreate it, and insert data.\\\\n\\\\n\\'append\\'If table exists, insert data. Create if does not exist.\\\\n\\\\n\\\\n\"}, \\'auth_local_webserver\\': {\\'type\\': \\'boolean\\', \\'description\\': \\'Use the local webserver flow instead of the console flow\\\\nwhen getting user credentials.\\\\nNew in version 0.2.0 of pandas-gbq.\\\\n\\\\nChanged in version 1.5.0: Default value is changed to True. Google has deprecated the\\\\nauth_local_webserver = False “out of band” (copy-paste)\\\\nflow.\\\\n\\\\n\\'}, \\'table_schema\\': {\\'type\\': \\'dictionary\\', \\'description\\': \"List of BigQuery table fields to which according DataFrame\\\\ncolumns conform to, e.g. [{\\'name\\': \\'col1\\', \\'type\\':\\\\n\\'STRING\\'},...]. If schema is not provided, it will be\\\\ngenerated according to dtypes of DataFrame columns. See\\\\nBigQuery API documentation on available names of a field.\\\\nNew in version 0.3.1 of pandas-gbq.\\\\n\"}, \\'location\\': {\\'type\\': \\'string\\', \\'description\\': \\'Location where the load job should run. See the BigQuery locations\\\\ndocumentation for a\\\\nlist of available locations. The location must match that of the\\\\ntarget dataset.\\\\nNew in version 0.5.0 of pandas-gbq.\\\\n\\'}, \\'progress_bar\\': {\\'type\\': \\'boolean\\', \\'description\\': \\'Use the library tqdm to show the progress bar for the upload,\\\\nchunk by chunk.\\\\nNew in version 0.5.0 of pandas-gbq.\\\\n\\'}, \\'credentials\\': {\\'type\\': \\'google.auth.credentials.Credentials, optional\\', \\'description\\': \\'Credentials for accessing Google APIs. Use this parameter to\\\\noverride default credentials, such as to use Compute Engine\\\\ngoogle.auth.compute_engine.Credentials or Service\\\\nAccount google.oauth2.service_account.Credentials\\\\ndirectly.\\\\nNew in version 0.8.0 of pandas-gbq.\\\\n\\'}}, \\'required\\': [\\'destination_table\\']}}',\n",
       "   'function_desc': 'Write a DataFrame to a Google BigQuery table.',\n",
       "   'function_name': 'pandas.DataFrame.to_gbq',\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_gbq.html#pandas.DataFrame.to_gbq',\n",
       "   'parameter_names_desc': '[{\\'param_name\\': \\'destination_table\\', \\'param_type\\': \\'str\\', \\'param_desc\\': \\'Name of table to be written, in the form dataset.tablename.\\\\n\\'}, {\\'param_name\\': \\'project_id\\', \\'param_type\\': \\'str, optional\\', \\'param_desc\\': \\'Google BigQuery Account project ID. Optional when available from\\\\nthe environment.\\\\n\\'}, {\\'param_name\\': \\'chunksize\\', \\'param_type\\': \\'int, optional\\', \\'param_desc\\': \\'Number of rows to be inserted in each chunk from the dataframe.\\\\nSet to None to load the whole dataframe at once.\\\\n\\'}, {\\'param_name\\': \\'reauth\\', \\'param_type\\': \\'bool, default False\\', \\'param_desc\\': \\'Force Google BigQuery to re-authenticate the user. This is useful\\\\nif multiple accounts are used.\\\\n\\'}, {\\'param_name\\': \\'if_exists\\', \\'param_type\\': \\'str, default ‘fail’\\', \\'param_desc\\': \"Behavior when the destination table exists. Value can be one of:\\\\n\\\\n\\'fail\\'If table exists raise pandas_gbq.gbq.TableCreationError.\\\\n\\\\n\\'replace\\'If table exists, drop it, recreate it, and insert data.\\\\n\\\\n\\'append\\'If table exists, insert data. Create if does not exist.\\\\n\\\\n\\\\n\"}, {\\'param_name\\': \\'auth_local_webserver\\', \\'param_type\\': \\'bool, default True\\', \\'param_desc\\': \\'Use the local webserver flow instead of the console flow\\\\nwhen getting user credentials.\\\\nNew in version 0.2.0 of pandas-gbq.\\\\n\\\\nChanged in version 1.5.0: Default value is changed to True. Google has deprecated the\\\\nauth_local_webserver = False “out of band” (copy-paste)\\\\nflow.\\\\n\\\\n\\'}, {\\'param_name\\': \\'table_schema\\', \\'param_type\\': \\'list of dicts, optional\\', \\'param_desc\\': \"List of BigQuery table fields to which according DataFrame\\\\ncolumns conform to, e.g. [{\\'name\\': \\'col1\\', \\'type\\':\\\\n\\'STRING\\'},...]. If schema is not provided, it will be\\\\ngenerated according to dtypes of DataFrame columns. See\\\\nBigQuery API documentation on available names of a field.\\\\nNew in version 0.3.1 of pandas-gbq.\\\\n\"}, {\\'param_name\\': \\'location\\', \\'param_type\\': \\'str, optional\\', \\'param_desc\\': \\'Location where the load job should run. See the BigQuery locations\\\\ndocumentation for a\\\\nlist of available locations. The location must match that of the\\\\ntarget dataset.\\\\nNew in version 0.5.0 of pandas-gbq.\\\\n\\'}, {\\'param_name\\': \\'progress_bar\\', \\'param_type\\': \\'bool, default True\\', \\'param_desc\\': \\'Use the library tqdm to show the progress bar for the upload,\\\\nchunk by chunk.\\\\nNew in version 0.5.0 of pandas-gbq.\\\\n\\'}, {\\'param_name\\': \\'credentials\\', \\'param_type\\': \\'google.auth.credentials.Credentials, optional\\', \\'param_desc\\': \\'Credentials for accessing Google APIs. Use this parameter to\\\\noverride default credentials, such as to use Compute Engine\\\\ngoogle.auth.compute_engine.Credentials or Service\\\\nAccount google.oauth2.service_account.Credentials\\\\ndirectly.\\\\nNew in version 0.8.0 of pandas-gbq.\\\\n\\'}]',\n",
       "   'trail': 'DataFrame',\n",
       "   'type': 'function_node'}],\n",
       " 'documents': ['Write a DataFrame to a Google BigQuery table.'],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_bm25_agent = PandasAgentBM25(pandas_collection)\n",
    "pandas_bm25_agent(\"How to read parquet files?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
