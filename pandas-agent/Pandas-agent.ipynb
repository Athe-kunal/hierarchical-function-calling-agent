{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = \"pandas_function_openai.json\"\n",
    "\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        # Load the JSON data from the file\n",
    "        data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"parent_summary_dict.json\"\n",
    "\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        # Load the JSON data from the file\n",
    "        parent_summary_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "pandas_graph = nx.DiGraph()\n",
    "\n",
    "parent_names = [(data[d]['name'],{\"url\":data[d]['url'],\"type\":\"parent_node\",\"node_description\":parent_summary_dict[data[d]['name']]}) for d in data]\n",
    "pandas_graph.add_nodes_from(parent_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parent in data:\n",
    "    parent_name = data[parent]['name']\n",
    "    for sub_level in data[parent]['functions']:\n",
    "        for func in sub_level['function_definitions']:\n",
    "            func_name = func['function_name']\n",
    "            # parent_trail = parent.split(\".\")[0]\n",
    "            # print(func['parameter_names_desc'])\n",
    "            # print(func.keys())\n",
    "            pandas_graph.add_nodes_from([(func_name,{'function_desc':func['function_text'],'function_url':func['function_url'],'trail':parent_name,\"type\":\"function_node\",\"function_name\":func['function_name'],'function_calling':str(func['openai_function']),'parameter_names_desc':str(func['parameter_names_desc'])})])\n",
    "            pandas_graph.add_edge(parent_name,func_name)\n",
    "        # break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import os\n",
    "from chromadb.utils.batch_utils import create_batches\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = []\n",
    "metadata = []\n",
    "\n",
    "for node, attributes in pandas_graph.nodes(data=True):\n",
    "    if attributes['type'] == 'function_node':\n",
    "        docs.append(attributes['function_desc'])\n",
    "        # metadata.append({\"metadata\":str(attributes)})\n",
    "        metadata.append(attributes)\n",
    "\n",
    "    elif attributes['type'] == 'parent_node':\n",
    "        docs.append(attributes['node_description'])\n",
    "        attributes['name'] = node\n",
    "        # metadata.append({\"metadata\":str(attributes)})\n",
    "        metadata.append(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# pp = pprint.PrettyPrinter(depth=4)\n",
    "# pp.pprint(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1851, 1851)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs),len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_desc': 'Return the dataframe interchange object implementing the interchange protocol.',\n",
       " 'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.__dataframe__.html#pandas.DataFrame.__dataframe__',\n",
       " 'trail': 'DataFrame',\n",
       " 'type': 'function_node',\n",
       " 'function_name': 'pandas.DataFrame.__dataframe__',\n",
       " 'function_calling': \"{'name': 'pandas#DataFrame#__dataframe__', 'descriptions': 'Return the dataframe interchange object implementing the interchange protocol.', 'parameters': {'type': 'object', 'properties': {'nan_as_null': {'type': 'boolean', 'description': 'nan_as_null is DEPRECATED and has no effect. Please avoid using\\\\nit; it will be removed in a future release.\\\\n'}, 'allow_copy': {'type': 'boolean', 'description': ''}}, 'required': []}}\",\n",
       " 'parameter_names_desc': \"[{'param_name': 'nan_as_null', 'param_type': 'bool, default False', 'param_desc': 'nan_as_null is DEPRECATED and has no effect. Please avoid using\\\\nit; it will be removed in a future release.\\\\n'}, {'param_name': 'allow_copy', 'param_type': 'bool, default True', 'param_desc': 'Whether to allow memory copying when exporting. If set to False\\\\nit would cause non-zero-copy exports to fail.\\\\n'}]\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[-1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_database(docs, metadata, api_key):\n",
    "    database_path = \"PANDAS\"\n",
    "    collection_name = \"pandas_docs\"\n",
    "    load_dotenv(override=True)\n",
    "    emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=api_key, model_name=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    client = chromadb.PersistentClient(path=database_path)\n",
    "    pandas_collection = client.create_collection(\n",
    "        name=collection_name, embedding_function=emb_fn\n",
    "    )\n",
    "\n",
    "    openbb_ids = [f\"id{i}\" for i in range(len(docs))]\n",
    "    batches = create_batches(\n",
    "        api=client, ids=openbb_ids, documents=docs, metadatas=metadata\n",
    "    )\n",
    "    for batch in batches:\n",
    "        pandas_collection.add(ids=batch[0], documents=batch[3], metadatas=batch[2])\n",
    "    return pandas_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(name=pandas_docs)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_database(docs,metadata,os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_database(api_key):\n",
    "    database_path = \"PANDAS\"\n",
    "    collection_name = \"pandas_docs\"\n",
    "    emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=api_key, model_name=\"text-embedding-3-small\"\n",
    "    )\n",
    "    client = chromadb.PersistentClient(path=database_path)\n",
    "    pandas_collection = client.get_collection(\n",
    "        name=collection_name, embedding_function=emb_fn\n",
    "    )\n",
    "    return pandas_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_collection = load_database(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(list1, list2):\n",
    "    pairs = []\n",
    "    for l1 in list1:\n",
    "        for l2 in list2:\n",
    "            curr_trail = l1\n",
    "            curr_trail += f\"-->{l2}\"\n",
    "            pairs.append(curr_trail)\n",
    "    return [pairs]\n",
    "\n",
    "\n",
    "def generate_pairs_recursive(trail_list):\n",
    "    if len(trail_list) == 1:\n",
    "        return trail_list[0]\n",
    "    curr_pairs = generate_pairs(trail_list[-2], trail_list[-1])\n",
    "    modified_trail_list = trail_list[:-2] + curr_pairs\n",
    "    return generate_pairs_recursive(modified_trail_list)\n",
    "\n",
    "\n",
    "def get_trail_list_pairs(trail_list_pairs,metadata_name=\"trail\"):\n",
    "    if len(trail_list_pairs) == 1:\n",
    "        trail_where_clause = {metadata_name: {\"$eq\": trail_list_pairs[0]}}\n",
    "    elif len(trail_list_pairs) > 1:\n",
    "        trail_where_clause = {\"$or\": [{metadata_name: {\"$eq\": t}} for t in trail_list_pairs]}\n",
    "    return trail_where_clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "class FirstSecondLevel(dspy.Signature):\n",
    "    \"You are given a list of keys and values separated by semicolon.\"\n",
    "    \"Based on the query, you have to output the key that is most relevant to the question separated by semicolon.\"\n",
    "    \"Be precise and output only the relevant key or keys from the provided keys only.\"\n",
    "    \"Don't include any other information\"\n",
    "\n",
    "    query = dspy.InputField(prefix=\"Query which you need to classify: \", format=str)\n",
    "    keys_values = dspy.InputField(prefix=\"Keys and Values: \", format=str)\n",
    "    output = dspy.OutputField(\n",
    "        prefix=\"Relevant Key(s): \", format=str, desc=\"relevant keys separated by semicolon\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"], model_name=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "llm = dspy.OpenAI()\n",
    "dspy.settings.configure(lm=llm)\n",
    "class PandasAgentChroma(dspy.Module):\n",
    "    def __init__(self,collection):\n",
    "        super(PandasAgentChroma, self).__init__()\n",
    "        self.collection = collection\n",
    "        self.firstSecondLevel = dspy.Predict(FirstSecondLevel)\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return super().__call__(*args, **kwargs)\n",
    "    \n",
    "    def forward(self,query:str):\n",
    "        query_emb = emb_fn([query])[0]\n",
    "\n",
    "        # Parent level querying\n",
    "        parent_level = self.collection.query(\n",
    "                query_embeddings=query_emb,\n",
    "                where={\n",
    "                    \"type\": {\"$eq\": \"parent_node\"},\n",
    "                },\n",
    "                n_results=3,\n",
    "            )\n",
    "        parent_level_str = \"\"\n",
    "        for parent_level_metadata in parent_level['metadatas'][0]:\n",
    "            parent_level_str += f\"{parent_level_metadata['name']}: {parent_level_metadata['node_description']}\\n\\n\"\n",
    "        \n",
    "        parent_level_answer = self.firstSecondLevel(\n",
    "            query=query, keys_values=parent_level_str\n",
    "        ).output\n",
    "        print(parent_level_str,parent_level_answer)\n",
    "        trail_list = [parent_level_answer.split(\";\")]\n",
    "        trail_list_pairs = generate_pairs_recursive(trail_list)\n",
    "        \n",
    "        trail_where_clause = get_trail_list_pairs(trail_list_pairs)\n",
    "\n",
    "        function_level = self.collection.query(\n",
    "            query_embeddings=query_emb,\n",
    "            where={\n",
    "                \"$and\":[\n",
    "                    trail_where_clause,\n",
    "                    {\"type\": {\"$eq\": \"function_node\"}},\n",
    "                ]\n",
    "            },\n",
    "            n_results=5,\n",
    "        )\n",
    "\n",
    "        function_level_str = \"\"\n",
    "        for function_level_metadata in function_level['metadatas'][0]:\n",
    "            function_level_str += f\"{function_level_metadata['function_name']}: {function_level_metadata['function_desc']}\\n\\n\"\n",
    "        print(function_level_str)\n",
    "        function_level_answer = self.firstSecondLevel(\n",
    "            query=query, keys_values=function_level_str\n",
    "        ).output\n",
    "        function_list = generate_pairs_recursive([function_level_answer.split(\";\")])\n",
    "        function_where_clause = get_trail_list_pairs(function_list,\"function_name\")\n",
    "        print(function_where_clause)\n",
    "        functions = self.collection.get(\n",
    "            where = {\n",
    "                \"$and\":[\n",
    "                    function_where_clause,\n",
    "                    {\"type\": {\"$eq\": \"function_node\"}},\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        return functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General functions: The functions provided cover a wide range of data manipulation tasks in pandas. These include unpivoting a DataFrame from wide to long format, creating pivot tables, binning values, merging DataFrames, converting categorical variables, reshaping data, computing cross tabulations, discretizing data based on quantiles, concatenating pandas objects, detecting missing or non-missing values, converting data types, working with datetime data, inferring frequencies, evaluating expressions, guessing datetime formats, generating deterministic integers, hashing data, and building DataFrames from compatible sources. These functions provide a comprehensive toolkit for data analysis and manipulation in pandas. \n",
      "\n",
      "DataFrame: The pandas library provides a wide range of functions for working with tabular data. These functions include serialization to various file formats such as CSV, Excel, JSON, Feather, parquet, ORC, SQL databases, and Stata dta format. Additionally, there are functions for rendering data as HTML tables, LaTeX tables, and nested tables. The library also offers functions for manipulating data within DataFrames, such as accessing specific rows and columns, inferring data types, and performing mathematical operations like addition, subtraction, division, and exponentiation. Other functions include memory usage analysis, copying data, querying, and replacing values based on conditions. Overall, pandas provides a comprehensive set of tools for data manipulation and analysis. The functions available for DataFrame manipulation include getting the modulo, less than, less than or equal to, not equal to, addition, multiplication, floating division, matrix multiplication, subtraction, integer division, exponential power, greater than, greater than or equal to, equal to, updating null elements, applying functions along an axis, aggregating using operations, rolling window calculations, exponentially weighted calculations, absolute numeric values, checking if any element is True, computing pairwise correlation, counting non-NA cells, cumulative maximum, cumulative product, generating descriptive statistics, evaluating string operations, unbiased kurtosis, mean, minimum, fractional change, product of values, numerical data ranks, unbiased standard error of the mean, sum of values, unbiased variance, frequency of distinct rows, checking if all elements are True, trimming values, computing pairwise covariance, cumulative minimum, cumulative sum, discrete difference. The functions provided cover a wide range of operations that can be performed on a DataFrame in Python. These include returning the maximum, median, mode, product, and quantile values over a specified axis, as well as rounding values to a specific number of decimal places. Other functions involve calculating skew, standard deviation, and counting distinct elements. There are also functions for aligning, selecting, and removing duplicate rows, as well as testing for equality between objects. Time-based functions allow for selecting values at specific times of the day, while index-related functions involve setting, resetting, or renaming indices. Missing values can be handled by filling with the next valid observation, interpolation, or specified method. Additionally, there are functions for detecting, removing, and replacing missing values. Pivot tables, sorting, and reshaping operations are also available, along with functions for transposing, squeezing, and swapping levels in a MultiIndex. \n",
      "\n",
      "Style: The Styler function provides a wide range of capabilities for styling DataFrames or Series in Python. It allows for styling with HTML and CSS, LaTeX, and text formats. Users can apply CSS-styling functions column-wise, row-wise, or table-wise, as well as to index or column headers. The function also enables users to format text display values, relabel index or column header keys, and combine multiple Stylers into a single table. Additional features include setting table styles, generating tooltips, adding CSS permanently, and applying styles elementwise. Users can also highlight missing values, minimum and maximum values, defined ranges, and values based on quantiles. The function supports gradient coloring for text and background, drawing bar charts in cell backgrounds, and exporting styles. Additionally, users can reset styles, set class attributes, table attributes, captions, and CSS properties for specific subsets. \n",
      "\n",
      " DataFrame; pivot tables; unpivoting; wide to long format; merging; reshaping; concatenating; missing values; datetime data; inferring frequencies; guessing datetime formats; generating deterministic integers; hashing data; building DataFrames; serialization; file formats; rendering data; accessing rows and columns; inferring data types; mathematical operations; memory usage analysis; copying data; querying; replacing values; modulo; addition; multiplication; subtraction; exponential power; updating null elements; applying functions; aggregating; rolling window calculations; absolute numeric values; pairwise correlation; cumulative maximum; cumulative product; descriptive statistics; string operations; unbiased kurtosis; mean; minimum; product of values; numerical data ranks; unbiased standard error of the mean; sum of\n",
      "pandas.DataFrame.pivot_table: Create a spreadsheet-style pivot table as a DataFrame.\n",
      "\n",
      "pandas.DataFrame.melt: Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.\n",
      "\n",
      "pandas.DataFrame.apply: Apply a function along an axis of the DataFrame.\n",
      "\n",
      "pandas.DataFrame.T: The transpose of the DataFrame.\n",
      "\n",
      "pandas.DataFrame.unstack: Pivot a level of the (necessarily hierarchical) index labels.\n",
      "\n",
      "\n",
      "{'$or': [{'function_name': {'$eq': 'pandas.DataFrame.pivot_table'}}, {'function_name': {'$eq': ' pandas.DataFrame.melt'}}, {'function_name': {'$eq': ' pandas.DataFrame.apply'}}, {'function_name': {'$eq': ' pandas.DataFrame.T'}}, {'function_name': {'$eq': ' pandas.DataFrame.unstack'}}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': ['id591'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'function_calling': \"{'name': 'pandas#DataFrame#pivot_table', 'descriptions': 'Create a spreadsheet-style pivot table as a DataFrame.', 'parameters': {'type': 'object', 'properties': {'values': {'type': 'list-like or scalar, optional', 'description': 'Column or columns to aggregate.\\\\n'}, 'index': {'type': 'column, Grouper, array, or list of the previous', 'description': 'Keys to group by on the pivot table index. If a list is passed,\\\\nit can contain any of the other types (except list). If an array is\\\\npassed, it must be the same length as the data and will be used in\\\\nthe same manner as column values.\\\\n'}, 'columns': {'type': 'column, Grouper, array, or list of the previous', 'description': 'Keys to group by on the pivot table column. If a list is passed,\\\\nit can contain any of the other types (except list). If an array is\\\\npassed, it must be the same length as the data and will be used in\\\\nthe same manner as column values.\\\\n'}, 'aggfunc': {'type': 'dictionary', 'description': 'If a list of functions is passed, the resulting pivot table will have\\\\nhierarchical columns whose top level are the function names\\\\n(inferred from the function objects themselves).\\\\nIf a dict is passed, the key is column to aggregate and the value is\\\\nfunction or list of functions. If margin=True, aggfunc will be\\\\nused to calculate the partial aggregates.\\\\n'}, 'fill_value': {'type': 'scalar, default None', 'description': 'Value to replace missing values with (in the resulting pivot table,\\\\nafter aggregation).\\\\n'}, 'margins': {'type': 'boolean', 'description': 'If margins=True, special All columns and rows\\\\nwill be added with partial group aggregates across the categories\\\\non the rows and columns.\\\\n'}, 'dropna': {'type': 'boolean', 'description': 'Do not include columns whose entries are all NaN. If True,\\\\nrows with a NaN value in any column will be omitted before\\\\ncomputing margins.\\\\n'}, 'margins_name': {'type': 'string', 'description': 'Name of the row / column that will contain the totals\\\\nwhen margins is True.\\\\n'}, 'observed': {'type': 'boolean', 'description': 'This only applies if any of the groupers are Categoricals.\\\\nIf True: only show observed values for categorical groupers.\\\\nIf False: show all values for categorical groupers.\\\\n\\\\nDeprecated since version 2.2.0: The default value of False is deprecated and will change to\\\\nTrue in a future version of pandas.\\\\n\\\\n'}, 'sort': {'type': 'boolean', 'description': 'Specifies if the result should be sorted.\\\\n\\\\nNew in version 1.3.0.\\\\n\\\\n'}}, 'required': []}}\",\n",
       "   'function_desc': 'Create a spreadsheet-style pivot table as a DataFrame.',\n",
       "   'function_name': 'pandas.DataFrame.pivot_table',\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot_table.html#pandas.DataFrame.pivot_table',\n",
       "   'parameter_names_desc': \"[{'param_name': 'values', 'param_type': 'list-like or scalar, optional', 'param_desc': 'Column or columns to aggregate.\\\\n'}, {'param_name': 'index', 'param_type': 'column, Grouper, array, or list of the previous', 'param_desc': 'Keys to group by on the pivot table index. If a list is passed,\\\\nit can contain any of the other types (except list). If an array is\\\\npassed, it must be the same length as the data and will be used in\\\\nthe same manner as column values.\\\\n'}, {'param_name': 'columns', 'param_type': 'column, Grouper, array, or list of the previous', 'param_desc': 'Keys to group by on the pivot table column. If a list is passed,\\\\nit can contain any of the other types (except list). If an array is\\\\npassed, it must be the same length as the data and will be used in\\\\nthe same manner as column values.\\\\n'}, {'param_name': 'aggfunc', 'param_type': 'function, list of functions, dict, default “mean”', 'param_desc': 'If a list of functions is passed, the resulting pivot table will have\\\\nhierarchical columns whose top level are the function names\\\\n(inferred from the function objects themselves).\\\\nIf a dict is passed, the key is column to aggregate and the value is\\\\nfunction or list of functions. If margin=True, aggfunc will be\\\\nused to calculate the partial aggregates.\\\\n'}, {'param_name': 'fill_value', 'param_type': 'scalar, default None', 'param_desc': 'Value to replace missing values with (in the resulting pivot table,\\\\nafter aggregation).\\\\n'}, {'param_name': 'margins', 'param_type': 'bool, default False', 'param_desc': 'If margins=True, special All columns and rows\\\\nwill be added with partial group aggregates across the categories\\\\non the rows and columns.\\\\n'}, {'param_name': 'dropna', 'param_type': 'bool, default True', 'param_desc': 'Do not include columns whose entries are all NaN. If True,\\\\nrows with a NaN value in any column will be omitted before\\\\ncomputing margins.\\\\n'}, {'param_name': 'margins_name', 'param_type': 'str, default ‘All’', 'param_desc': 'Name of the row / column that will contain the totals\\\\nwhen margins is True.\\\\n'}, {'param_name': 'observed', 'param_type': 'bool, default False', 'param_desc': 'This only applies if any of the groupers are Categoricals.\\\\nIf True: only show observed values for categorical groupers.\\\\nIf False: show all values for categorical groupers.\\\\n\\\\nDeprecated since version 2.2.0: The default value of False is deprecated and will change to\\\\nTrue in a future version of pandas.\\\\n\\\\n'}, {'param_name': 'sort', 'param_type': 'bool, default True', 'param_desc': 'Specifies if the result should be sorted.\\\\n\\\\nNew in version 1.3.0.\\\\n\\\\n'}]\",\n",
       "   'trail': 'DataFrame',\n",
       "   'type': 'function_node'}],\n",
       " 'documents': ['Create a spreadsheet-style pivot table as a DataFrame.'],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd = PandasAgentChroma(pandas_collection)\n",
    "ans = pd(\"How to pivot a dataframe\")\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'name': 'pandas#DataFrame#pivot_table', 'descriptions': 'Create a spreadsheet-style pivot table as a DataFrame.', 'parameters': {'type': 'object', 'properties': {'values': {'type': 'list-like or scalar, optional', 'description': 'Column or columns to aggregate.\\\\n'}, 'index': {'type': 'column, Grouper, array, or list of the previous', 'description': 'Keys to group by on the pivot table index. If a list is passed,\\\\nit can contain any of the other types (except list). If an array is\\\\npassed, it must be the same length as the data and will be used in\\\\nthe same manner as column values.\\\\n'}, 'columns': {'type': 'column, Grouper, array, or list of the previous', 'description': 'Keys to group by on the pivot table column. If a list is passed,\\\\nit can contain any of the other types (except list). If an array is\\\\npassed, it must be the same length as the data and will be used in\\\\nthe same manner as column values.\\\\n'}, 'aggfunc': {'type': 'dictionary', 'description': 'If a list of functions is passed, the resulting pivot table will have\\\\nhierarchical columns whose top level are the function names\\\\n(inferred from the function objects themselves).\\\\nIf a dict is passed, the key is column to aggregate and the value is\\\\nfunction or list of functions. If margin=True, aggfunc will be\\\\nused to calculate the partial aggregates.\\\\n'}, 'fill_value': {'type': 'scalar, default None', 'description': 'Value to replace missing values with (in the resulting pivot table,\\\\nafter aggregation).\\\\n'}, 'margins': {'type': 'boolean', 'description': 'If margins=True, special All columns and rows\\\\nwill be added with partial group aggregates across the categories\\\\non the rows and columns.\\\\n'}, 'dropna': {'type': 'boolean', 'description': 'Do not include columns whose entries are all NaN. If True,\\\\nrows with a NaN value in any column will be omitted before\\\\ncomputing margins.\\\\n'}, 'margins_name': {'type': 'string', 'description': 'Name of the row / column that will contain the totals\\\\nwhen margins is True.\\\\n'}, 'observed': {'type': 'boolean', 'description': 'This only applies if any of the groupers are Categoricals.\\\\nIf True: only show observed values for categorical groupers.\\\\nIf False: show all values for categorical groupers.\\\\n\\\\nDeprecated since version 2.2.0: The default value of False is deprecated and will change to\\\\nTrue in a future version of pandas.\\\\n\\\\n'}, 'sort': {'type': 'boolean', 'description': 'Specifies if the result should be sorted.\\\\n\\\\nNew in version 1.3.0.\\\\n\\\\n'}}, 'required': []}}\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans['metadatas'][0]['function_calling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
