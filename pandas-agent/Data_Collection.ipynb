{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://pandas.pydata.org/docs/reference/index.html\"\n",
    "response = requests.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "navbar = soup.find_all(attrs={\"class\": \"reference internal\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://pandas.pydata.org/docs/reference/io.html'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_url = \"https://pandas.pydata.org/docs/reference/\"\n",
    "ref_url+ navbar[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Style'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "navbar[10].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level = {}\n",
    "for nbar in navbar:\n",
    "    text_val = nbar.text\n",
    "    href = nbar['href']\n",
    "    full_url = ref_url+ href\n",
    "    if  \"#\" not in href:\n",
    "        first_level.update({href:{\"functions\":[],\"name\":text_val,\"url\":full_url}})\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nbar in navbar:\n",
    "    text_val = nbar.text\n",
    "    href = nbar['href']\n",
    "    full_url = ref_url+ href\n",
    "    if \"#\" in href:\n",
    "        parent_html = href.split(\"#\")[0]\n",
    "        if parent_html in first_level:\n",
    "            first_level[parent_html]['functions'].append({\"name\":text_val,\"url\":full_url})\n",
    "        else:\n",
    "            print(text_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'DateOffset',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#dateoffset'},\n",
       " {'name': 'BusinessDay',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#businessday'},\n",
       " {'name': 'BusinessHour',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#businesshour'},\n",
       " {'name': 'CustomBusinessDay',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#custombusinessday'},\n",
       " {'name': 'CustomBusinessHour',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#custombusinesshour'},\n",
       " {'name': 'MonthEnd',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#monthend'},\n",
       " {'name': 'MonthBegin',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#monthbegin'},\n",
       " {'name': 'BusinessMonthEnd',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#businessmonthend'},\n",
       " {'name': 'BusinessMonthBegin',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#businessmonthbegin'},\n",
       " {'name': 'CustomBusinessMonthEnd',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#custombusinessmonthend'},\n",
       " {'name': 'CustomBusinessMonthBegin',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#custombusinessmonthbegin'},\n",
       " {'name': 'SemiMonthEnd',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#semimonthend'},\n",
       " {'name': 'SemiMonthBegin',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#semimonthbegin'},\n",
       " {'name': 'Week',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#week'},\n",
       " {'name': 'WeekOfMonth',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#weekofmonth'},\n",
       " {'name': 'LastWeekOfMonth',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#lastweekofmonth'},\n",
       " {'name': 'BQuarterEnd',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#bquarterend'},\n",
       " {'name': 'BQuarterBegin',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#bquarterbegin'},\n",
       " {'name': 'QuarterEnd',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#quarterend'},\n",
       " {'name': 'QuarterBegin',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#quarterbegin'},\n",
       " {'name': 'BYearEnd',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#byearend'},\n",
       " {'name': 'BYearBegin',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#byearbegin'},\n",
       " {'name': 'YearEnd',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#yearend'},\n",
       " {'name': 'YearBegin',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#yearbegin'},\n",
       " {'name': 'FY5253',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#fy5253'},\n",
       " {'name': 'FY5253Quarter',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#fy5253quarter'},\n",
       " {'name': 'Easter',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#easter'},\n",
       " {'name': 'Tick',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#tick'},\n",
       " {'name': 'Day',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#day'},\n",
       " {'name': 'Hour',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#hour'},\n",
       " {'name': 'Minute',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#minute'},\n",
       " {'name': 'Second',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#second'},\n",
       " {'name': 'Milli',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#milli'},\n",
       " {'name': 'Micro',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#micro'},\n",
       " {'name': 'Nano',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#nano'},\n",
       " {'name': 'Frequencies',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/offset_frequency.html#frequencies'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_level['offset_frequency.html']['functions']\n",
    "# https://pandas.pydata.org/docs/reference/io.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACTING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent_url = first_level['io.html']['url']\n",
    "\n",
    "# parent_soup = BeautifulSoup(requests.get(parent_url).text,'lxml')\n",
    "# url_id = \"pickling\"\n",
    "# s1 = parent_soup.find(attrs={\"id\":url_id})\n",
    "# s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_level['io.html']['functions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object is not subscriptable\n",
      "series.html\n",
      "<tr class=\"row-odd\"><th class=\"head\"><p>Data Type</p></th>\n",
      "<th class=\"head\"><p>Accessor</p></th>\n",
      "</tr>\n",
      "accessors\n",
      "----------------------------------------------------------------------------------------------------\n",
      "'NoneType' object is not subscriptable\n",
      "arrays.html\n",
      "<tr class=\"row-odd\"><th class=\"head\"><p>Kind of Data</p></th>\n",
      "<th class=\"head\"><p>pandas Data Type</p></th>\n",
      "<th class=\"head\"><p>Scalar</p></th>\n",
      "<th class=\"head\"><p>Array</p></th>\n",
      "</tr>\n",
      "objects\n",
      "----------------------------------------------------------------------------------------------------\n",
      "'NoneType' object is not subscriptable\n",
      "arrays.html\n",
      "<tr class=\"row-odd\"><th class=\"head\"><p>PyArrow type</p></th>\n",
      "<th class=\"head\"><p>pandas extension type</p></th>\n",
      "<th class=\"head\"><p>NumPy type</p></th>\n",
      "</tr>\n",
      "objects\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_links(id_elem,base_func_url,class_name,first_level_name,url_id):\n",
    "    curr_urls = []\n",
    "    try:\n",
    "        func_urls = id_elem.find_all(attrs={\"class\":class_name})\n",
    "        for odd_url in func_urls:\n",
    "            try:\n",
    "                func_url = odd_url.find('a')['href']\n",
    "                curr_urls.append(base_func_url+func_url)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(first_level_name)\n",
    "                print(odd_url)\n",
    "                print(url_id)\n",
    "                print('-'*100)\n",
    "        return curr_urls\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        func_urls = id_elem.find(attrs={\"class\":class_name}).find('a')['href']\n",
    "        curr_urls.append(base_func_url+func_url)\n",
    "        return curr_urls\n",
    "    finally:\n",
    "        return curr_urls\n",
    "    \n",
    "base_func_url = \"https://pandas.pydata.org/docs/reference/\"\n",
    "for first_level_name in first_level:\n",
    "    parent_url = first_level[first_level_name]['url']\n",
    "    parent_soup = BeautifulSoup(requests.get(parent_url).text,'lxml')\n",
    "    for idx,func in enumerate(first_level[first_level_name]['functions']):\n",
    "        url_id = func['url'].split(\"#\")[-1]\n",
    "        id_elem = parent_soup.find(attrs={\"id\":url_id})\n",
    "        odd_urls = get_links(id_elem,base_func_url,\"row-odd\",first_level_name,url_id)     \n",
    "        even_urls = get_links(id_elem,base_func_url,\"row-even\",first_level_name,url_id)     \n",
    "        # all_urls.extend(odd_urls)\n",
    "        # all_urls.extend(even_urls)\n",
    "        first_level[first_level_name]['functions'][idx]['function_urls'] = odd_urls + even_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_level['io.html']['functions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.51it/s]\n",
      "100%|██████████| 41/41 [00:17<00:00,  2.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "logname = \"pandas.log\"\n",
    "# logging.basicConfig(filename=logname,\n",
    "#                     filemode='a',\n",
    "#                     format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "#                     datefmt='%H:%M:%S',\n",
    "#                     level=logging.DEBUG)\n",
    "\n",
    "def get_param_data(first_level):\n",
    "    not_worked = []\n",
    "    for parent in first_level:\n",
    "        parent_dict = first_level[parent]['functions']\n",
    "        for idx,sub_level in enumerate(parent_dict):\n",
    "            parent_dict[idx].update({\"function_definitions\":[]})\n",
    "            for func_url in tqdm(sub_level['function_urls']):\n",
    "                func_response = requests.get(func_url)\n",
    "                func_soup = BeautifulSoup(func_response.content, \"lxml\",from_encoding=\"utf-8\")\n",
    "                \n",
    "                func_name = func_soup.find(\"h1\").text.replace(\"#\",\"\") #remove #\n",
    "                elem = func_soup.find(attrs={\"class\":\"sig sig-object py\"})\n",
    "                try:\n",
    "                    full_function = elem.text.replace(\"[source]#\",\"\").replace(\"\\n\",\"\")\n",
    "                    func_text = func_soup.find(\"dd\").find('p').text\n",
    "                    curr_dict = {\"function_name\":func_name,\"full_function\":full_function,\"function_text\":func_text,\"parameter_names_desc\":[],\"function_url\":func_url}\n",
    "                    em = func_soup.find_all(attrs={\"class\":\"field-odd\"})\n",
    "                    if em[0].text =='Parameters:':\n",
    "                        param_names = em[-1].find_all(\"dt\")\n",
    "                        desc_list = em[-1].find_all(\"dd\")\n",
    "\n",
    "                        # if len(param_names)!=len(desc_list):\n",
    "                        #     print(func_name)\n",
    "                        #     print(f\"param_names={len(param_names)} and desc_list={len(desc_list)}\")\n",
    "                        # first_level[parent]['function_definitions'].append()\n",
    "                        for pn,dn in zip(param_names,desc_list):\n",
    "                            try:\n",
    "                                param_name = pn.strong.text\n",
    "                                param_type = pn.find(attrs={\"class\":\"classifier\"}).text\n",
    "                                if param_name == \"**kwargs\": \n",
    "                                    continue\n",
    "                                param_desc = dn.text\n",
    "                                curr_dict['parameter_names_desc'].append({\"param_name\":param_name,\"param_type\":param_type,\"param_desc\":param_desc})\n",
    "                            except Exception as e:\n",
    "                                print(e,pn.text)\n",
    "                    # else:\n",
    "                    #     parent_dict[idx]['function_definitions'].append(curr_dict)\n",
    "                    #     continue\n",
    "                except Exception as e:\n",
    "                    # print(e)\n",
    "                    not_worked.append((func_name,func_text,e,func_url))\n",
    "                    # logging.debug(\"With error {e} for {func_name} and {func_text}\")\n",
    "                finally:\n",
    "                    parent_dict[idx]['function_definitions'].append(curr_dict)\n",
    "        clear_output(wait=True)\n",
    "    return first_level,not_worked\n",
    "\n",
    "function_def_dict,not_worked = get_param_data(first_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pandas.tseries.offsets.CustomBusinessMonthEnd.freqstr',\n",
       " 'Return a string representing the frequency.',\n",
       " IndexError('list index out of range'),\n",
       " 'https://pandas.pydata.org/docs/reference/api/pandas.tseries.offsets.CustomBusinessMonthEnd.freqstr.html#pandas.tseries.offsets.CustomBusinessMonthEnd.freqstr')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_worked[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('not_worked.txt', 'w') as file:\n",
    "    for item in not_worked:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to pandas_function_v3.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "filename = \"pandas_function_v3.json\"\n",
    "\n",
    "# Serialize the dictionary to a JSON string\n",
    "json_data = json.dumps(function_def_dict, ensure_ascii=False)\n",
    "\n",
    "# Write the JSON string to a file with UTF-8 encoding\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "\n",
    "print(\"Data has been successfully written to\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = \"pandas_function_v2.json\"\n",
    "\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        # Load the JSON data from the file\n",
    "        data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_url = 'https://pandas.pydata.org/docs/reference/api/pandas.read_clipboard.html'\n",
    "# func_url = 'https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.nanoseconds.html'\n",
    "func_response = requests.get(func_url)\n",
    "func_soup = BeautifulSoup(func_response.content, \"lxml\",from_encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_name = func_soup.find(\"h1\").text.replace(\"#\",\"\") #remove #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pandas.read_clipboard'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem = func_soup.find(attrs={\"class\":\"sig sig-object py\"})\n",
    "full_function = elem.text.replace(\"[source]#\",\"\").replace(\"\\n\",\"\")\n",
    "func_text = func_soup.find(\"dd\").find('p').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Read text from clipboard and pass to read_csv().'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = func_soup.find(attrs={\"class\":\"field-list\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = param_names.find_all(attrs={\"class\":\"field-odd\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsepstr, default ‘\\\\s+’A string or regex delimiter. The default of \\'\\\\\\\\s+\\' denotes\\none or more whitespace characters.\\n\\ndtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n\\n**kwargsSee read_csv() for the full argument list.\\n\\n\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em[-1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<dt><strong>extra_args</strong><span class=\"classifier\">list[str], default None</span></dt>,\n",
       " <dt><strong>run_doctests</strong><span class=\"classifier\">bool, default False</span></dt>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(em[-1].find_all('dt'))\n",
    "em[-1].find_all('dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"classifier\">str, default â\\s+â</span>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_soup.find(attrs={\"class\":\"field-list\"}).find(attrs={\"class\":\"classifier\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = func_soup.find_all(attrs={\"class\":\"field-odd\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<dt><strong>sep</strong><span class=\"classifier\">str, default â\\s+â</span></dt>,\n",
       " <dt><strong>dtype_backend</strong><span class=\"classifier\">{ânumpy_nullableâ, âpyarrowâ}, default ânumpy_nullableâ</span></dt>,\n",
       " <dt><strong>**kwargs</strong></dt>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em[-1].find_all(\"dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A string or regex delimiter. The default of '\\\\s+' denotes\n",
      "one or more whitespace characters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Back-end data type applied to the resultant DataFrame\n",
      "(still experimental). Behaviour is as follows:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\"numpy_nullable\": returns nullable-dtype-backed DataFrame\n",
      "(default).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\n",
      "DataFrame.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "See read_csv() for the full argument list.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for p in em[-1].find_all(\"p\"):\n",
    "\n",
    "    if p.find(attrs={\"class\":\"versionmodified added\"}) is None and p.find(attrs={\"class\":\"versionmodified changed\"}) is None:\n",
    "        print(p.text)\n",
    "        print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = \"pandas_function.json\"\n",
    "\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        # Load the JSON data from the file\n",
    "        data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['io.html']['function_definitions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_name': 'pandas.read_pickle',\n",
       " 'full_function': \"pandas.read_pickle(filepath_or_buffer, compression='infer', storage_options=None)\",\n",
       " 'function_text': 'Load pickled pandas object (or any object) from file.',\n",
       " 'parameter_names_desc': [{'param_name': 'filepath_or_bufferstr, path object, or file-like object',\n",
       "   'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary readlines() function.\\nAlso accepts URL. URL is not limited to S3 and GCS.\\n'},\n",
       "  {'param_name': 'compressionstr or dict, default ‘infer’',\n",
       "   'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "  {'param_name': 'storage_optionsdict, optional',\n",
       "   'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['io.html']['function_definitions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'functions': [{'name': 'Pickling',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#pickling'},\n",
       "  {'name': 'Flat file',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#flat-file'},\n",
       "  {'name': 'Clipboard',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#clipboard'},\n",
       "  {'name': 'Excel',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#excel'},\n",
       "  {'name': 'JSON',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#json'},\n",
       "  {'name': 'HTML',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#html'},\n",
       "  {'name': 'XML',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#xml'},\n",
       "  {'name': 'Latex',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#latex'},\n",
       "  {'name': 'HDFStore: PyTables (HDF5)',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#hdfstore-pytables-hdf5'},\n",
       "  {'name': 'Feather',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#feather'},\n",
       "  {'name': 'Parquet',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#parquet'},\n",
       "  {'name': 'ORC',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#orc'},\n",
       "  {'name': 'SAS',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#sas'},\n",
       "  {'name': 'SPSS',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#spss'},\n",
       "  {'name': 'SQL',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#sql'},\n",
       "  {'name': 'Google BigQuery',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#google-bigquery'},\n",
       "  {'name': 'STATA',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#stata'}],\n",
       " 'name': 'Input/output',\n",
       " 'url': 'https://pandas.pydata.org/docs/reference/io.html',\n",
       " 'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_pickle.html#pandas.read_pickle',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html#pandas.DataFrame.to_pickle',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_table.html#pandas.read_table',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#pandas.read_csv',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_fwf.html#pandas.read_fwf',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_clipboard.html#pandas.read_clipboard',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_clipboard.html#pandas.DataFrame.to_clipboard',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html#pandas.read_excel',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.html#pandas.ExcelFile',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.sheet_names.html#pandas.ExcelFile.sheet_names',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.to_excel.html#pandas.io.formats.style.Styler.to_excel',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.ExcelWriter.html#pandas.ExcelWriter',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.book.html#pandas.ExcelFile.book',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.parse.html#pandas.ExcelFile.parse',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_json.html#pandas.read_json',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html#pandas.DataFrame.to_json',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.io.json.build_table_schema.html#pandas.io.json.build_table_schema',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html#pandas.json_normalize',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_html.html#pandas.read_html',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.to_html.html#pandas.io.formats.style.Styler.to_html',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_html.html#pandas.DataFrame.to_html',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_xml.html#pandas.read_xml',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_xml.html#pandas.DataFrame.to_xml',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_latex.html#pandas.DataFrame.to_latex',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.to_latex.html#pandas.io.formats.style.Styler.to_latex',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_hdf.html#pandas.read_hdf',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.append.html#pandas.HDFStore.append',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.select.html#pandas.HDFStore.select',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.keys.html#pandas.HDFStore.keys',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.walk.html#pandas.HDFStore.walk',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.put.html#pandas.HDFStore.put',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.get.html#pandas.HDFStore.get',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.info.html#pandas.HDFStore.info',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.groups.html#pandas.HDFStore.groups',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_feather.html#pandas.read_feather',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_feather.html#pandas.DataFrame.to_feather',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html#pandas.read_parquet',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_orc.html#pandas.read_orc',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_sas.html#pandas.read_sas',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_spss.html#pandas.read_spss',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html#pandas.read_sql_table',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html#pandas.read_sql',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html#pandas.read_sql_query',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html#pandas.read_gbq',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.read_stata.html#pandas.read_stata',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.io.stata.StataReader.data_label.html#pandas.io.stata.StataReader.data_label',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.io.stata.StataReader.variable_labels.html#pandas.io.stata.StataReader.variable_labels',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_stata.html#pandas.DataFrame.to_stata',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.io.stata.StataReader.value_labels.html#pandas.io.stata.StataReader.value_labels',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.io.stata.StataWriter.write_file.html#pandas.io.stata.StataWriter.write_file'],\n",
       " 'function_definitions': [{'function_name': 'pandas.read_pickle',\n",
       "   'full_function': \"pandas.read_pickle(filepath_or_buffer, compression='infer', storage_options=None)\",\n",
       "   'function_text': 'Load pickled pandas object (or any object) from file.',\n",
       "   'parameter_names_desc': [{'param_name': 'filepath_or_bufferstr, path object, or file-like object',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary readlines() function.\\nAlso accepts URL. URL is not limited to S3 and GCS.\\n'},\n",
       "    {'param_name': 'compressionstr or dict, default ‘infer’',\n",
       "     'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'}]},\n",
       "  {'function_name': 'pandas.DataFrame.to_pickle',\n",
       "   'full_function': \"DataFrame.to_pickle(path, *, compression='infer', protocol=5, storage_options=None)\",\n",
       "   'function_text': 'Pickle (serialize) object to file.',\n",
       "   'parameter_names_desc': [{'param_name': 'pathstr, path object, or file-like object',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary write() function. File path where\\nthe pickled object will be stored.\\n'},\n",
       "    {'param_name': 'compressionstr or dict, default ‘infer’',\n",
       "     'param_desc': \"For on-the-fly compression of the output data. If ‘infer’ and ‘path’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nSet to None for no compression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for faster compression and to create\\na reproducible gzip archive:\\ncompression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\"},\n",
       "    {'param_name': 'protocolint',\n",
       "     'param_desc': 'Int which indicates which protocol should be used by the pickler,\\ndefault HIGHEST_PROTOCOL (see [1] paragraph 12.1.2). The possible\\nvalues are 0, 1, 2, 3, 4, 5. A negative value for the protocol\\nparameter is equivalent to setting its value to HIGHEST_PROTOCOL.\\n\\n\\n[1]\\nhttps://docs.python.org/3/library/pickle.html.\\n\\n\\n'},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'}]},\n",
       "  {'function_name': 'pandas.read_table',\n",
       "   'full_function': 'pandas.read_table(filepath_or_buffer, *, sep=_NoDefault.no_default, delimiter=None, header=\\'infer\\', names=_NoDefault.no_default, index_col=None, usecols=None, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=_NoDefault.no_default, skip_blank_lines=True, parse_dates=False, infer_datetime_format=_NoDefault.no_default, keep_date_col=_NoDefault.no_default, date_parser=_NoDefault.no_default, date_format=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression=\\'infer\\', thousands=None, decimal=\\'.\\', lineterminator=None, quotechar=\\'\"\\', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors=\\'strict\\', dialect=None, on_bad_lines=\\'error\\', delim_whitespace=_NoDefault.no_default, low_memory=True, memory_map=False, float_precision=None, storage_options=None, dtype_backend=_NoDefault.no_default)',\n",
       "   'function_text': 'Read general delimited file into DataFrame.',\n",
       "   'parameter_names_desc': [{'param_name': 'filepath_or_bufferstr, path object or file-like object',\n",
       "     'param_desc': 'Any valid string path is acceptable. The string could be a URL. Valid\\nURL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\\nexpected. A local file could be: file://localhost/path/to/table.csv.\\nIf you want to pass in a path object, pandas accepts any os.PathLike.\\nBy file-like object, we refer to objects with a read() method, such as\\na file handle (e.g. via builtin open function) or StringIO.\\n'},\n",
       "    {'param_name': 'sepstr, default ‘\\\\t’ (tab-stop)',\n",
       "     'param_desc': \"Character or regex pattern to treat as the delimiter. If sep=None, the\\nC engine cannot automatically detect\\nthe separator, but the Python parsing engine can, meaning the latter will\\nbe used and automatically detect the separator from only the first valid\\nrow of the file by Python’s builtin sniffer tool, csv.Sniffer.\\nIn addition, separators longer than 1 character and different from\\n'\\\\s+' will be interpreted as regular expressions and will also force\\nthe use of the Python parsing engine. Note that regex delimiters are prone\\nto ignoring quoted data. Regex example: '\\\\r\\\\t'.\\n\"},\n",
       "    {'param_name': 'delimiterstr, optional', 'param_desc': 'Alias for sep.\\n'},\n",
       "    {'param_name': 'headerint, Sequence of int, ‘infer’ or None, default ‘infer’',\n",
       "     'param_desc': 'Row number(s) containing column labels and marking the start of the\\ndata (zero-indexed). Default behavior is to infer the column names: if no names\\nare passed the behavior is identical to header=0 and column\\nnames are inferred from the first line of the file, if column\\nnames are passed explicitly to names then the behavior is identical to\\nheader=None. Explicitly pass header=0 to be able to\\nreplace existing names. The header can be a list of integers that\\nspecify row locations for a MultiIndex on the columns\\ne.g. [0, 1, 3]. Intervening rows that are not specified will be\\nskipped (e.g. 2 in this example is skipped). Note that this\\nparameter ignores commented lines and empty lines if\\nskip_blank_lines=True, so header=0 denotes the first line of\\ndata rather than the first line of the file.\\n'},\n",
       "    {'param_name': 'namesSequence of Hashable, optional',\n",
       "     'param_desc': 'Sequence of column labels to apply. If the file contains a header row,\\nthen you should explicitly pass header=0 to override the column names.\\nDuplicates in this list are not allowed.\\n'},\n",
       "    {'param_name': 'index_colHashable, Sequence of Hashable or False, optional',\n",
       "     'param_desc': 'Column(s) to use as row label(s), denoted either by column labels or column\\nindices. If a sequence of labels or indices is given, MultiIndex\\nwill be formed for the row labels.\\nNote: index_col=False can be used to force pandas to not use the first\\ncolumn as the index, e.g., when you have a malformed file with delimiters at\\nthe end of each line.\\n'},\n",
       "    {'param_name': 'usecolsSequence of Hashable or Callable, optional',\n",
       "     'param_desc': \"Subset of columns to select, denoted either by column labels or column indices.\\nIf list-like, all elements must either\\nbe positional (i.e. integer indices into the document columns) or strings\\nthat correspond to column names provided either by the user in names or\\ninferred from the document header row(s). If names are given, the document\\nheader row(s) are not taken into account. For example, a valid list-like\\nusecols parameter would be [0, 1, 2] or ['foo', 'bar', 'baz'].\\nElement order is ignored, so usecols=[0, 1] is the same as [1, 0].\\nTo instantiate a DataFrame from data with element order\\npreserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]\\nfor columns in ['foo', 'bar'] order or\\npd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]\\nfor ['bar', 'foo'] order.\\nIf callable, the callable function will be evaluated against the column\\nnames, returning names where the callable function evaluates to True. An\\nexample of a valid callable argument would be lambda x: x.upper() in\\n['AAA', 'BBB', 'DDD']. Using this parameter results in much faster\\nparsing time and lower memory usage.\\n\"},\n",
       "    {'param_name': 'dtypedtype or dict of {Hashabledtype}, optional',\n",
       "     'param_desc': \"Data type(s) to apply to either the whole dataset or individual columns.\\nE.g., {'a': np.float64, 'b': np.int32, 'c': 'Int64'}\\nUse str or object together with suitable na_values settings\\nto preserve and not interpret dtype.\\nIf converters are specified, they will be applied INSTEAD\\nof dtype conversion.\\n\\nNew in version 1.5.0: Support for defaultdict was added. Specify a defaultdict as input where\\nthe default determines the dtype of the columns which are not explicitly\\nlisted.\\n\\n\"},\n",
       "    {'param_name': 'engine{‘c’, ‘python’, ‘pyarrow’}, optional',\n",
       "     'param_desc': 'Parser engine to use. The C and pyarrow engines are faster, while the python engine\\nis currently more feature-complete. Multithreading is currently only supported by\\nthe pyarrow engine.\\n\\nNew in version 1.4.0: The ‘pyarrow’ engine was added as an experimental engine, and some features\\nare unsupported, or may not work correctly, with this engine.\\n\\n'},\n",
       "    {'param_name': 'convertersdict of {HashableCallable}, optional',\n",
       "     'param_desc': 'Functions for converting values in specified columns. Keys can either\\nbe column labels or column indices.\\n'},\n",
       "    {'param_name': 'true_valueslist, optional',\n",
       "     'param_desc': 'Values to consider as True in addition to case-insensitive variants of ‘True’.\\n'},\n",
       "    {'param_name': 'false_valueslist, optional',\n",
       "     'param_desc': 'Values to consider as False in addition to case-insensitive variants of ‘False’.\\n'},\n",
       "    {'param_name': 'skipinitialspacebool, default False',\n",
       "     'param_desc': 'Skip spaces after delimiter.\\n'},\n",
       "    {'param_name': 'skiprowsint, list of int or Callable, optional',\n",
       "     'param_desc': 'Line numbers to skip (0-indexed) or number of lines to skip (int)\\nat the start of the file.\\nIf callable, the callable function will be evaluated against the row\\nindices, returning True if the row should be skipped and False otherwise.\\nAn example of a valid callable argument would be lambda x: x in [0, 2].\\n'},\n",
       "    {'param_name': 'skipfooterint, default 0',\n",
       "     'param_desc': \"Number of lines at bottom of file to skip (Unsupported with engine='c').\\n\"},\n",
       "    {'param_name': 'nrowsint, optional',\n",
       "     'param_desc': 'Number of rows of file to read. Useful for reading pieces of large files.\\n'},\n",
       "    {'param_name': 'na_valuesHashable, Iterable of Hashable or dict of {HashableIterable}, optional',\n",
       "     'param_desc': 'Additional strings to recognize as NA/NaN. If dict passed, specific\\nper-column NA values. By default the following values are interpreted as\\nNaN: “ “, “#N/A”, “#N/A N/A”, “#NA”, “-1.#IND”, “-1.#QNAN”, “-NaN”, “-nan”,\\n“1.#IND”, “1.#QNAN”, “<NA>”, “N/A”, “NA”, “NULL”, “NaN”, “None”,\\n“n/a”, “nan”, “null “.\\n'},\n",
       "    {'param_name': 'keep_default_nabool, default True',\n",
       "     'param_desc': 'Whether or not to include the default NaN values when parsing the data.\\nDepending on whether na_values is passed in, the behavior is as follows:\\n\\nIf keep_default_na is True, and na_values are specified, na_values\\nis appended to the default NaN values used for parsing.\\nIf keep_default_na is True, and na_values are not specified, only\\nthe default NaN values are used for parsing.\\nIf keep_default_na is False, and na_values are specified, only\\nthe NaN values specified na_values are used for parsing.\\nIf keep_default_na is False, and na_values are not specified, no\\nstrings will be parsed as NaN.\\n\\nNote that if na_filter is passed in as False, the keep_default_na and\\nna_values parameters will be ignored.\\n'},\n",
       "    {'param_name': 'na_filterbool, default True',\n",
       "     'param_desc': 'Detect missing value markers (empty strings and the value of na_values). In\\ndata without any NA values, passing na_filter=False can improve the\\nperformance of reading a large file.\\n'},\n",
       "    {'param_name': 'verbosebool, default False',\n",
       "     'param_desc': 'Indicate number of NA values placed in non-numeric columns.\\n\\nDeprecated since version 2.2.0.\\n\\n'},\n",
       "    {'param_name': 'skip_blank_linesbool, default True',\n",
       "     'param_desc': 'If True, skip over blank lines rather than interpreting as NaN values.\\n'},\n",
       "    {'param_name': 'parse_datesbool, list of Hashable, list of lists or dict of {Hashablelist}, default False',\n",
       "     'param_desc': \"The behavior is as follows:\\n\\nbool. If True -> try parsing the index. Note: Automatically set to\\nTrue if date_format or date_parser arguments have been passed.\\nlist of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\\neach as a separate date column.\\nlist of list. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse\\nas a single date column. Values are joined with a space before parsing.\\ndict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\\nresult ‘foo’. Values are joined with a space before parsing.\\n\\nIf a column or index cannot be represented as an array of datetime,\\nsay because of an unparsable value or a mixture of timezones, the column\\nor index will be returned unaltered as an object data type. For\\nnon-standard datetime parsing, use to_datetime() after\\nread_csv().\\nNote: A fast-path exists for iso8601-formatted dates.\\n\"},\n",
       "    {'param_name': 'infer_datetime_formatbool, default False',\n",
       "     'param_desc': 'If True and parse_dates is enabled, pandas will attempt to infer the\\nformat of the datetime strings in the columns, and if it can be inferred,\\nswitch to a faster method of parsing them. In some cases this can increase\\nthe parsing speed by 5-10x.\\n\\nDeprecated since version 2.0.0: A strict version of this argument is now the default, passing it has no effect.\\n\\n'},\n",
       "    {'param_name': 'keep_date_colbool, default False',\n",
       "     'param_desc': 'If True and parse_dates specifies combining multiple columns then\\nkeep the original columns.\\n'},\n",
       "    {'param_name': 'date_parserCallable, optional',\n",
       "     'param_desc': 'Function to use for converting a sequence of string columns to an array of\\ndatetime instances. The default uses dateutil.parser.parser to do the\\nconversion. pandas will try to call date_parser in three different ways,\\nadvancing to the next if an exception occurs: 1) Pass one or more arrays\\n(as defined by parse_dates) as arguments; 2) concatenate (row-wise) the\\nstring values from the columns defined by parse_dates into a single array\\nand pass that; and 3) call date_parser once for each row using one or\\nmore strings (corresponding to the columns defined by parse_dates) as\\narguments.\\n\\nDeprecated since version 2.0.0: Use date_format instead, or read in as object and then apply\\nto_datetime() as-needed.\\n\\n'},\n",
       "    {'param_name': 'date_formatstr or dict of column -> format, optional',\n",
       "     'param_desc': 'Format to use for parsing dates when used in conjunction with parse_dates.\\nThe strftime to parse time, e.g. \"%d/%m/%Y\". See\\nstrftime documentation for more information on choices, though\\nnote that \"%f\" will parse all the way up to nanoseconds.\\nYou can also pass:\\n\\n\\n“ISO8601”, to parse any ISO8601time string (not necessarily in exactly the same format);\\n\\n\\n\\n\\n“mixed”, to infer the format for each element individually. This is risky,and you should probably use it along with dayfirst.\\n\\n\\n\\n\\n\\nNew in version 2.0.0.\\n\\n'},\n",
       "    {'param_name': '“ISO8601”, to parse any ISO8601',\n",
       "     'param_desc': 'time string (not necessarily in exactly the same format);\\n'},\n",
       "    {'param_name': '“mixed”, to infer the format for each element individually. This is risky,',\n",
       "     'param_desc': 'and you should probably use it along with dayfirst.\\n'},\n",
       "    {'param_name': 'dayfirstbool, default False',\n",
       "     'param_desc': 'DD/MM format dates, international and European format.\\n'},\n",
       "    {'param_name': 'cache_datesbool, default True',\n",
       "     'param_desc': 'If True, use a cache of unique, converted dates to apply the datetime\\nconversion. May produce significant speed-up when parsing duplicate\\ndate strings, especially ones with timezone offsets.\\n'},\n",
       "    {'param_name': 'iteratorbool, default False',\n",
       "     'param_desc': 'Return TextFileReader object for iteration or getting chunks with\\nget_chunk().\\n'},\n",
       "    {'param_name': 'chunksizeint, optional',\n",
       "     'param_desc': 'Number of lines to read from the file per chunk. Passing a value will cause the\\nfunction to return a TextFileReader object for iteration.\\nSee the IO Tools docs\\nfor more information on iterator and chunksize.\\n'},\n",
       "    {'param_name': 'compressionstr or dict, default ‘infer’',\n",
       "     'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "    {'param_name': 'thousandsstr (length 1), optional',\n",
       "     'param_desc': 'Character acting as the thousands separator in numerical values.\\n'},\n",
       "    {'param_name': 'decimalstr (length 1), default ‘.’',\n",
       "     'param_desc': 'Character to recognize as decimal point (e.g., use ‘,’ for European data).\\n'},\n",
       "    {'param_name': 'lineterminatorstr (length 1), optional',\n",
       "     'param_desc': 'Character used to denote a line break. Only valid with C parser.\\n'},\n",
       "    {'param_name': 'quotecharstr (length 1), optional',\n",
       "     'param_desc': 'Character used to denote the start and end of a quoted item. Quoted\\nitems can include the delimiter and it will be ignored.\\n'},\n",
       "    {'param_name': 'quoting{0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL',\n",
       "     'param_desc': 'Control field quoting behavior per csv.QUOTE_* constants. Default is\\ncsv.QUOTE_MINIMAL (i.e., 0) which implies that only fields containing special\\ncharacters are quoted (e.g., characters defined in quotechar, delimiter,\\nor lineterminator.\\n'},\n",
       "    {'param_name': 'doublequotebool, default True',\n",
       "     'param_desc': 'When quotechar is specified and quoting is not QUOTE_NONE, indicate\\nwhether or not to interpret two consecutive quotechar elements INSIDE a\\nfield as a single quotechar element.\\n'},\n",
       "    {'param_name': 'escapecharstr (length 1), optional',\n",
       "     'param_desc': 'Character used to escape other characters.\\n'},\n",
       "    {'param_name': 'commentstr (length 1), optional',\n",
       "     'param_desc': \"Character indicating that the remainder of line should not be parsed.\\nIf found at the beginning\\nof a line, the line will be ignored altogether. This parameter must be a\\nsingle character. Like empty lines (as long as skip_blank_lines=True),\\nfully commented lines are ignored by the parameter header but not by\\nskiprows. For example, if comment='#', parsing\\n#empty\\\\na,b,c\\\\n1,2,3 with header=0 will result in 'a,b,c' being\\ntreated as the header.\\n\"},\n",
       "    {'param_name': 'encodingstr, optional, default ‘utf-8’',\n",
       "     'param_desc': \"Encoding to use for UTF when reading/writing (ex. 'utf-8'). List of Python\\nstandard encodings .\\n\"},\n",
       "    {'param_name': 'encoding_errorsstr, optional, default ‘strict’',\n",
       "     'param_desc': 'How encoding errors are treated. List of possible values .\\n\\nNew in version 1.3.0.\\n\\n'},\n",
       "    {'param_name': 'dialectstr or csv.Dialect, optional',\n",
       "     'param_desc': 'If provided, this parameter will override values (default or not) for the\\nfollowing parameters: delimiter, doublequote, escapechar,\\nskipinitialspace, quotechar, and quoting. If it is necessary to\\noverride values, a ParserWarning will be issued. See csv.Dialect\\ndocumentation for more details.\\n'},\n",
       "    {'param_name': 'on_bad_lines{‘error’, ‘warn’, ‘skip’} or Callable, default ‘error’',\n",
       "     'param_desc': \"Specifies what to do upon encountering a bad line (a line with too many fields).\\nAllowed values are :\\n\\n'error', raise an Exception when a bad line is encountered.\\n'warn', raise a warning when a bad line is encountered and skip that line.\\n'skip', skip bad lines without raising or warning when they are encountered.\\n\\n\\nNew in version 1.3.0.\\n\\n\\nNew in version 1.4.0: \\n\\nCallable, function with signature\\n(bad_line: list[str]) -> list[str] | None that will process a single\\nbad line. bad_line is a list of strings split by the sep.\\nIf the function returns None, the bad line will be ignored.\\nIf the function returns a new list of strings with more elements than\\nexpected, a ParserWarning will be emitted while dropping extra elements.\\nOnly supported when engine='python'\\n\\n\\n\\nChanged in version 2.2.0: \\n\\nCallable, function with signature\\nas described in pyarrow documentation when engine='pyarrow'\\n\\n\\n\"},\n",
       "    {'param_name': 'delim_whitespacebool, default False',\n",
       "     'param_desc': 'Specifies whether or not whitespace (e.g. \\' \\' or \\'\\\\t\\') will be\\nused as the sep delimiter. Equivalent to setting sep=\\'\\\\s+\\'. If this option\\nis set to True, nothing should be passed in for the delimiter\\nparameter.\\n\\nDeprecated since version 2.2.0: Use sep=\"\\\\s+\" instead.\\n\\n'},\n",
       "    {'param_name': 'low_memorybool, default True',\n",
       "     'param_desc': 'Internally process the file in chunks, resulting in lower memory use\\nwhile parsing, but possibly mixed type inference. To ensure no mixed\\ntypes either set False, or specify the type with the dtype parameter.\\nNote that the entire file is read into a single DataFrame\\nregardless, use the chunksize or iterator parameter to return the data in\\nchunks. (Only valid with C parser).\\n'},\n",
       "    {'param_name': 'memory_mapbool, default False',\n",
       "     'param_desc': 'If a filepath is provided for filepath_or_buffer, map the file object\\ndirectly onto memory and access the data directly from there. Using this\\noption can improve performance because there is no longer any I/O overhead.\\n'},\n",
       "    {'param_name': 'float_precision{‘high’, ‘legacy’, ‘round_trip’}, optional',\n",
       "     'param_desc': \"Specifies which converter the C engine should use for floating-point\\nvalues. The options are None or 'high' for the ordinary converter,\\n'legacy' for the original lower precision pandas converter, and\\n'round_trip' for the round-trip converter.\\n\"},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.DataFrame.to_csv',\n",
       "   'full_function': 'DataFrame.to_csv(path_or_buf=None, *, sep=\\',\\', na_rep=\\'\\', float_format=None, columns=None, header=True, index=True, index_label=None, mode=\\'w\\', encoding=None, compression=\\'infer\\', quoting=None, quotechar=\\'\"\\', lineterminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal=\\'.\\', errors=\\'strict\\', storage_options=None)',\n",
       "   'function_text': 'Write object to a comma-separated values (csv) file.',\n",
       "   'parameter_names_desc': [{'param_name': 'path_or_bufstr, path object, file-like object, or None, default None',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a write() function. If None, the result is\\nreturned as a string. If a non-binary file object is passed, it should\\nbe opened with newline=’’, disabling universal newlines. If a binary\\nfile object is passed, mode might need to contain a ‘b’.\\n'},\n",
       "    {'param_name': 'sepstr, default ‘,’',\n",
       "     'param_desc': 'String of length 1. Field delimiter for the output file.\\n'},\n",
       "    {'param_name': 'na_repstr, default ‘’',\n",
       "     'param_desc': 'Missing data representation.\\n'},\n",
       "    {'param_name': 'float_formatstr, Callable, default None',\n",
       "     'param_desc': 'Format string for floating point numbers. If a Callable is given, it takes\\nprecedence over other numeric formatting parameters, like decimal.\\n'},\n",
       "    {'param_name': 'columnssequence, optional',\n",
       "     'param_desc': 'Columns to write.\\n'},\n",
       "    {'param_name': 'headerbool or list of str, default True',\n",
       "     'param_desc': 'Write out the column names. If a list of strings is given it is\\nassumed to be aliases for the column names.\\n'},\n",
       "    {'param_name': 'indexbool, default True',\n",
       "     'param_desc': 'Write row names (index).\\n'},\n",
       "    {'param_name': 'index_labelstr or sequence, or False, default None',\n",
       "     'param_desc': 'Column label for index column(s) if desired. If None is given, and\\nheader and index are True, then the index names are used. A\\nsequence should be given if the object uses MultiIndex. If\\nFalse do not print fields for index names. Use index_label=False\\nfor easier importing in R.\\n'},\n",
       "    {'param_name': 'mode{‘w’, ‘x’, ‘a’}, default ‘w’',\n",
       "     'param_desc': 'Forwarded to either open(mode=) or fsspec.open(mode=) to control\\nthe file opening. Typical values include:\\n\\n‘w’, truncate the file first.\\n‘x’, exclusive creation, failing if the file already exists.\\n‘a’, append to the end of file if it exists.\\n\\n'},\n",
       "    {'param_name': 'encodingstr, optional',\n",
       "     'param_desc': 'A string representing the encoding to use in the output file,\\ndefaults to ‘utf-8’. encoding is not supported if path_or_buf\\nis a non-binary file object.\\n'},\n",
       "    {'param_name': 'compressionstr or dict, default ‘infer’',\n",
       "     'param_desc': \"For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buf’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nSet to None for no compression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for faster compression and to create\\na reproducible gzip archive:\\ncompression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\nMay be a dict with key ‘method’ as compression mode\\nand other entries as additional compression options if\\ncompression mode is ‘zip’.\\nPassing compression options as keys in dict is\\nsupported for compression modes ‘gzip’, ‘bz2’, ‘zstd’, and ‘zip’.\\n\\n\"},\n",
       "    {'param_name': 'quotingoptional constant from csv module',\n",
       "     'param_desc': 'Defaults to csv.QUOTE_MINIMAL. If you have set a float_format\\nthen floats are converted to strings and thus csv.QUOTE_NONNUMERIC\\nwill treat them as non-numeric.\\n'},\n",
       "    {'param_name': 'quotecharstr, default ‘\"’',\n",
       "     'param_desc': 'String of length 1. Character used to quote fields.\\n'},\n",
       "    {'param_name': 'lineterminatorstr, optional',\n",
       "     'param_desc': 'The newline character or character sequence to use in the output\\nfile. Defaults to os.linesep, which depends on the OS in which\\nthis method is called (’\\\\n’ for linux, ‘\\\\r\\\\n’ for Windows, i.e.).\\n\\nChanged in version 1.5.0: Previously was line_terminator, changed for consistency with\\nread_csv and the standard library ‘csv’ module.\\n\\n'},\n",
       "    {'param_name': 'chunksizeint or None',\n",
       "     'param_desc': 'Rows to write at a time.\\n'},\n",
       "    {'param_name': 'date_formatstr, default None',\n",
       "     'param_desc': 'Format string for datetime objects.\\n'},\n",
       "    {'param_name': 'doublequotebool, default True',\n",
       "     'param_desc': 'Control quoting of quotechar inside a field.\\n'},\n",
       "    {'param_name': 'escapecharstr, default None',\n",
       "     'param_desc': 'String of length 1. Character used to escape sep and quotechar\\nwhen appropriate.\\n'},\n",
       "    {'param_name': 'decimalstr, default ‘.’',\n",
       "     'param_desc': 'Character recognized as decimal separator. E.g. use ‘,’ for\\nEuropean data.\\n'},\n",
       "    {'param_name': 'errorsstr, default ‘strict’',\n",
       "     'param_desc': 'Specifies how encoding and decoding errors are to be handled.\\nSee the errors argument for open() for a full list\\nof options.\\n'},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'}]},\n",
       "  {'function_name': 'pandas.read_csv',\n",
       "   'full_function': 'pandas.read_csv(filepath_or_buffer, *, sep=_NoDefault.no_default, delimiter=None, header=\\'infer\\', names=_NoDefault.no_default, index_col=None, usecols=None, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=_NoDefault.no_default, skip_blank_lines=True, parse_dates=None, infer_datetime_format=_NoDefault.no_default, keep_date_col=_NoDefault.no_default, date_parser=_NoDefault.no_default, date_format=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression=\\'infer\\', thousands=None, decimal=\\'.\\', lineterminator=None, quotechar=\\'\"\\', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors=\\'strict\\', dialect=None, on_bad_lines=\\'error\\', delim_whitespace=_NoDefault.no_default, low_memory=True, memory_map=False, float_precision=None, storage_options=None, dtype_backend=_NoDefault.no_default)',\n",
       "   'function_text': 'Read a comma-separated values (csv) file into DataFrame.',\n",
       "   'parameter_names_desc': [{'param_name': 'filepath_or_bufferstr, path object or file-like object',\n",
       "     'param_desc': 'Any valid string path is acceptable. The string could be a URL. Valid\\nURL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\\nexpected. A local file could be: file://localhost/path/to/table.csv.\\nIf you want to pass in a path object, pandas accepts any os.PathLike.\\nBy file-like object, we refer to objects with a read() method, such as\\na file handle (e.g. via builtin open function) or StringIO.\\n'},\n",
       "    {'param_name': 'sepstr, default ‘,’',\n",
       "     'param_desc': \"Character or regex pattern to treat as the delimiter. If sep=None, the\\nC engine cannot automatically detect\\nthe separator, but the Python parsing engine can, meaning the latter will\\nbe used and automatically detect the separator from only the first valid\\nrow of the file by Python’s builtin sniffer tool, csv.Sniffer.\\nIn addition, separators longer than 1 character and different from\\n'\\\\s+' will be interpreted as regular expressions and will also force\\nthe use of the Python parsing engine. Note that regex delimiters are prone\\nto ignoring quoted data. Regex example: '\\\\r\\\\t'.\\n\"},\n",
       "    {'param_name': 'delimiterstr, optional', 'param_desc': 'Alias for sep.\\n'},\n",
       "    {'param_name': 'headerint, Sequence of int, ‘infer’ or None, default ‘infer’',\n",
       "     'param_desc': 'Row number(s) containing column labels and marking the start of the\\ndata (zero-indexed). Default behavior is to infer the column names: if no names\\nare passed the behavior is identical to header=0 and column\\nnames are inferred from the first line of the file, if column\\nnames are passed explicitly to names then the behavior is identical to\\nheader=None. Explicitly pass header=0 to be able to\\nreplace existing names. The header can be a list of integers that\\nspecify row locations for a MultiIndex on the columns\\ne.g. [0, 1, 3]. Intervening rows that are not specified will be\\nskipped (e.g. 2 in this example is skipped). Note that this\\nparameter ignores commented lines and empty lines if\\nskip_blank_lines=True, so header=0 denotes the first line of\\ndata rather than the first line of the file.\\n'},\n",
       "    {'param_name': 'namesSequence of Hashable, optional',\n",
       "     'param_desc': 'Sequence of column labels to apply. If the file contains a header row,\\nthen you should explicitly pass header=0 to override the column names.\\nDuplicates in this list are not allowed.\\n'},\n",
       "    {'param_name': 'index_colHashable, Sequence of Hashable or False, optional',\n",
       "     'param_desc': 'Column(s) to use as row label(s), denoted either by column labels or column\\nindices. If a sequence of labels or indices is given, MultiIndex\\nwill be formed for the row labels.\\nNote: index_col=False can be used to force pandas to not use the first\\ncolumn as the index, e.g., when you have a malformed file with delimiters at\\nthe end of each line.\\n'},\n",
       "    {'param_name': 'usecolsSequence of Hashable or Callable, optional',\n",
       "     'param_desc': \"Subset of columns to select, denoted either by column labels or column indices.\\nIf list-like, all elements must either\\nbe positional (i.e. integer indices into the document columns) or strings\\nthat correspond to column names provided either by the user in names or\\ninferred from the document header row(s). If names are given, the document\\nheader row(s) are not taken into account. For example, a valid list-like\\nusecols parameter would be [0, 1, 2] or ['foo', 'bar', 'baz'].\\nElement order is ignored, so usecols=[0, 1] is the same as [1, 0].\\nTo instantiate a DataFrame from data with element order\\npreserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]\\nfor columns in ['foo', 'bar'] order or\\npd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]\\nfor ['bar', 'foo'] order.\\nIf callable, the callable function will be evaluated against the column\\nnames, returning names where the callable function evaluates to True. An\\nexample of a valid callable argument would be lambda x: x.upper() in\\n['AAA', 'BBB', 'DDD']. Using this parameter results in much faster\\nparsing time and lower memory usage.\\n\"},\n",
       "    {'param_name': 'dtypedtype or dict of {Hashabledtype}, optional',\n",
       "     'param_desc': \"Data type(s) to apply to either the whole dataset or individual columns.\\nE.g., {'a': np.float64, 'b': np.int32, 'c': 'Int64'}\\nUse str or object together with suitable na_values settings\\nto preserve and not interpret dtype.\\nIf converters are specified, they will be applied INSTEAD\\nof dtype conversion.\\n\\nNew in version 1.5.0: Support for defaultdict was added. Specify a defaultdict as input where\\nthe default determines the dtype of the columns which are not explicitly\\nlisted.\\n\\n\"},\n",
       "    {'param_name': 'engine{‘c’, ‘python’, ‘pyarrow’}, optional',\n",
       "     'param_desc': 'Parser engine to use. The C and pyarrow engines are faster, while the python engine\\nis currently more feature-complete. Multithreading is currently only supported by\\nthe pyarrow engine.\\n\\nNew in version 1.4.0: The ‘pyarrow’ engine was added as an experimental engine, and some features\\nare unsupported, or may not work correctly, with this engine.\\n\\n'},\n",
       "    {'param_name': 'convertersdict of {HashableCallable}, optional',\n",
       "     'param_desc': 'Functions for converting values in specified columns. Keys can either\\nbe column labels or column indices.\\n'},\n",
       "    {'param_name': 'true_valueslist, optional',\n",
       "     'param_desc': 'Values to consider as True in addition to case-insensitive variants of ‘True’.\\n'},\n",
       "    {'param_name': 'false_valueslist, optional',\n",
       "     'param_desc': 'Values to consider as False in addition to case-insensitive variants of ‘False’.\\n'},\n",
       "    {'param_name': 'skipinitialspacebool, default False',\n",
       "     'param_desc': 'Skip spaces after delimiter.\\n'},\n",
       "    {'param_name': 'skiprowsint, list of int or Callable, optional',\n",
       "     'param_desc': 'Line numbers to skip (0-indexed) or number of lines to skip (int)\\nat the start of the file.\\nIf callable, the callable function will be evaluated against the row\\nindices, returning True if the row should be skipped and False otherwise.\\nAn example of a valid callable argument would be lambda x: x in [0, 2].\\n'},\n",
       "    {'param_name': 'skipfooterint, default 0',\n",
       "     'param_desc': \"Number of lines at bottom of file to skip (Unsupported with engine='c').\\n\"},\n",
       "    {'param_name': 'nrowsint, optional',\n",
       "     'param_desc': 'Number of rows of file to read. Useful for reading pieces of large files.\\n'},\n",
       "    {'param_name': 'na_valuesHashable, Iterable of Hashable or dict of {HashableIterable}, optional',\n",
       "     'param_desc': 'Additional strings to recognize as NA/NaN. If dict passed, specific\\nper-column NA values. By default the following values are interpreted as\\nNaN: “ “, “#N/A”, “#N/A N/A”, “#NA”, “-1.#IND”, “-1.#QNAN”, “-NaN”, “-nan”,\\n“1.#IND”, “1.#QNAN”, “<NA>”, “N/A”, “NA”, “NULL”, “NaN”, “None”,\\n“n/a”, “nan”, “null “.\\n'},\n",
       "    {'param_name': 'keep_default_nabool, default True',\n",
       "     'param_desc': 'Whether or not to include the default NaN values when parsing the data.\\nDepending on whether na_values is passed in, the behavior is as follows:\\n\\nIf keep_default_na is True, and na_values are specified, na_values\\nis appended to the default NaN values used for parsing.\\nIf keep_default_na is True, and na_values are not specified, only\\nthe default NaN values are used for parsing.\\nIf keep_default_na is False, and na_values are specified, only\\nthe NaN values specified na_values are used for parsing.\\nIf keep_default_na is False, and na_values are not specified, no\\nstrings will be parsed as NaN.\\n\\nNote that if na_filter is passed in as False, the keep_default_na and\\nna_values parameters will be ignored.\\n'},\n",
       "    {'param_name': 'na_filterbool, default True',\n",
       "     'param_desc': 'Detect missing value markers (empty strings and the value of na_values). In\\ndata without any NA values, passing na_filter=False can improve the\\nperformance of reading a large file.\\n'},\n",
       "    {'param_name': 'verbosebool, default False',\n",
       "     'param_desc': 'Indicate number of NA values placed in non-numeric columns.\\n\\nDeprecated since version 2.2.0.\\n\\n'},\n",
       "    {'param_name': 'skip_blank_linesbool, default True',\n",
       "     'param_desc': 'If True, skip over blank lines rather than interpreting as NaN values.\\n'},\n",
       "    {'param_name': 'parse_datesbool, list of Hashable, list of lists or dict of {Hashablelist}, default False',\n",
       "     'param_desc': \"The behavior is as follows:\\n\\nbool. If True -> try parsing the index. Note: Automatically set to\\nTrue if date_format or date_parser arguments have been passed.\\nlist of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\\neach as a separate date column.\\nlist of list. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse\\nas a single date column. Values are joined with a space before parsing.\\ndict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\\nresult ‘foo’. Values are joined with a space before parsing.\\n\\nIf a column or index cannot be represented as an array of datetime,\\nsay because of an unparsable value or a mixture of timezones, the column\\nor index will be returned unaltered as an object data type. For\\nnon-standard datetime parsing, use to_datetime() after\\nread_csv().\\nNote: A fast-path exists for iso8601-formatted dates.\\n\"},\n",
       "    {'param_name': 'infer_datetime_formatbool, default False',\n",
       "     'param_desc': 'If True and parse_dates is enabled, pandas will attempt to infer the\\nformat of the datetime strings in the columns, and if it can be inferred,\\nswitch to a faster method of parsing them. In some cases this can increase\\nthe parsing speed by 5-10x.\\n\\nDeprecated since version 2.0.0: A strict version of this argument is now the default, passing it has no effect.\\n\\n'},\n",
       "    {'param_name': 'keep_date_colbool, default False',\n",
       "     'param_desc': 'If True and parse_dates specifies combining multiple columns then\\nkeep the original columns.\\n'},\n",
       "    {'param_name': 'date_parserCallable, optional',\n",
       "     'param_desc': 'Function to use for converting a sequence of string columns to an array of\\ndatetime instances. The default uses dateutil.parser.parser to do the\\nconversion. pandas will try to call date_parser in three different ways,\\nadvancing to the next if an exception occurs: 1) Pass one or more arrays\\n(as defined by parse_dates) as arguments; 2) concatenate (row-wise) the\\nstring values from the columns defined by parse_dates into a single array\\nand pass that; and 3) call date_parser once for each row using one or\\nmore strings (corresponding to the columns defined by parse_dates) as\\narguments.\\n\\nDeprecated since version 2.0.0: Use date_format instead, or read in as object and then apply\\nto_datetime() as-needed.\\n\\n'},\n",
       "    {'param_name': 'date_formatstr or dict of column -> format, optional',\n",
       "     'param_desc': 'Format to use for parsing dates when used in conjunction with parse_dates.\\nThe strftime to parse time, e.g. \"%d/%m/%Y\". See\\nstrftime documentation for more information on choices, though\\nnote that \"%f\" will parse all the way up to nanoseconds.\\nYou can also pass:\\n\\n\\n“ISO8601”, to parse any ISO8601time string (not necessarily in exactly the same format);\\n\\n\\n\\n\\n“mixed”, to infer the format for each element individually. This is risky,and you should probably use it along with dayfirst.\\n\\n\\n\\n\\n\\nNew in version 2.0.0.\\n\\n'},\n",
       "    {'param_name': '“ISO8601”, to parse any ISO8601',\n",
       "     'param_desc': 'time string (not necessarily in exactly the same format);\\n'},\n",
       "    {'param_name': '“mixed”, to infer the format for each element individually. This is risky,',\n",
       "     'param_desc': 'and you should probably use it along with dayfirst.\\n'},\n",
       "    {'param_name': 'dayfirstbool, default False',\n",
       "     'param_desc': 'DD/MM format dates, international and European format.\\n'},\n",
       "    {'param_name': 'cache_datesbool, default True',\n",
       "     'param_desc': 'If True, use a cache of unique, converted dates to apply the datetime\\nconversion. May produce significant speed-up when parsing duplicate\\ndate strings, especially ones with timezone offsets.\\n'},\n",
       "    {'param_name': 'iteratorbool, default False',\n",
       "     'param_desc': 'Return TextFileReader object for iteration or getting chunks with\\nget_chunk().\\n'},\n",
       "    {'param_name': 'chunksizeint, optional',\n",
       "     'param_desc': 'Number of lines to read from the file per chunk. Passing a value will cause the\\nfunction to return a TextFileReader object for iteration.\\nSee the IO Tools docs\\nfor more information on iterator and chunksize.\\n'},\n",
       "    {'param_name': 'compressionstr or dict, default ‘infer’',\n",
       "     'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "    {'param_name': 'thousandsstr (length 1), optional',\n",
       "     'param_desc': 'Character acting as the thousands separator in numerical values.\\n'},\n",
       "    {'param_name': 'decimalstr (length 1), default ‘.’',\n",
       "     'param_desc': 'Character to recognize as decimal point (e.g., use ‘,’ for European data).\\n'},\n",
       "    {'param_name': 'lineterminatorstr (length 1), optional',\n",
       "     'param_desc': 'Character used to denote a line break. Only valid with C parser.\\n'},\n",
       "    {'param_name': 'quotecharstr (length 1), optional',\n",
       "     'param_desc': 'Character used to denote the start and end of a quoted item. Quoted\\nitems can include the delimiter and it will be ignored.\\n'},\n",
       "    {'param_name': 'quoting{0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL',\n",
       "     'param_desc': 'Control field quoting behavior per csv.QUOTE_* constants. Default is\\ncsv.QUOTE_MINIMAL (i.e., 0) which implies that only fields containing special\\ncharacters are quoted (e.g., characters defined in quotechar, delimiter,\\nor lineterminator.\\n'},\n",
       "    {'param_name': 'doublequotebool, default True',\n",
       "     'param_desc': 'When quotechar is specified and quoting is not QUOTE_NONE, indicate\\nwhether or not to interpret two consecutive quotechar elements INSIDE a\\nfield as a single quotechar element.\\n'},\n",
       "    {'param_name': 'escapecharstr (length 1), optional',\n",
       "     'param_desc': 'Character used to escape other characters.\\n'},\n",
       "    {'param_name': 'commentstr (length 1), optional',\n",
       "     'param_desc': \"Character indicating that the remainder of line should not be parsed.\\nIf found at the beginning\\nof a line, the line will be ignored altogether. This parameter must be a\\nsingle character. Like empty lines (as long as skip_blank_lines=True),\\nfully commented lines are ignored by the parameter header but not by\\nskiprows. For example, if comment='#', parsing\\n#empty\\\\na,b,c\\\\n1,2,3 with header=0 will result in 'a,b,c' being\\ntreated as the header.\\n\"},\n",
       "    {'param_name': 'encodingstr, optional, default ‘utf-8’',\n",
       "     'param_desc': \"Encoding to use for UTF when reading/writing (ex. 'utf-8'). List of Python\\nstandard encodings .\\n\"},\n",
       "    {'param_name': 'encoding_errorsstr, optional, default ‘strict’',\n",
       "     'param_desc': 'How encoding errors are treated. List of possible values .\\n\\nNew in version 1.3.0.\\n\\n'},\n",
       "    {'param_name': 'dialectstr or csv.Dialect, optional',\n",
       "     'param_desc': 'If provided, this parameter will override values (default or not) for the\\nfollowing parameters: delimiter, doublequote, escapechar,\\nskipinitialspace, quotechar, and quoting. If it is necessary to\\noverride values, a ParserWarning will be issued. See csv.Dialect\\ndocumentation for more details.\\n'},\n",
       "    {'param_name': 'on_bad_lines{‘error’, ‘warn’, ‘skip’} or Callable, default ‘error’',\n",
       "     'param_desc': \"Specifies what to do upon encountering a bad line (a line with too many fields).\\nAllowed values are :\\n\\n'error', raise an Exception when a bad line is encountered.\\n'warn', raise a warning when a bad line is encountered and skip that line.\\n'skip', skip bad lines without raising or warning when they are encountered.\\n\\n\\nNew in version 1.3.0.\\n\\n\\nNew in version 1.4.0: \\n\\nCallable, function with signature\\n(bad_line: list[str]) -> list[str] | None that will process a single\\nbad line. bad_line is a list of strings split by the sep.\\nIf the function returns None, the bad line will be ignored.\\nIf the function returns a new list of strings with more elements than\\nexpected, a ParserWarning will be emitted while dropping extra elements.\\nOnly supported when engine='python'\\n\\n\\n\\nChanged in version 2.2.0: \\n\\nCallable, function with signature\\nas described in pyarrow documentation when engine='pyarrow'\\n\\n\\n\"},\n",
       "    {'param_name': 'delim_whitespacebool, default False',\n",
       "     'param_desc': 'Specifies whether or not whitespace (e.g. \\' \\' or \\'\\\\t\\') will be\\nused as the sep delimiter. Equivalent to setting sep=\\'\\\\s+\\'. If this option\\nis set to True, nothing should be passed in for the delimiter\\nparameter.\\n\\nDeprecated since version 2.2.0: Use sep=\"\\\\s+\" instead.\\n\\n'},\n",
       "    {'param_name': 'low_memorybool, default True',\n",
       "     'param_desc': 'Internally process the file in chunks, resulting in lower memory use\\nwhile parsing, but possibly mixed type inference. To ensure no mixed\\ntypes either set False, or specify the type with the dtype parameter.\\nNote that the entire file is read into a single DataFrame\\nregardless, use the chunksize or iterator parameter to return the data in\\nchunks. (Only valid with C parser).\\n'},\n",
       "    {'param_name': 'memory_mapbool, default False',\n",
       "     'param_desc': 'If a filepath is provided for filepath_or_buffer, map the file object\\ndirectly onto memory and access the data directly from there. Using this\\noption can improve performance because there is no longer any I/O overhead.\\n'},\n",
       "    {'param_name': 'float_precision{‘high’, ‘legacy’, ‘round_trip’}, optional',\n",
       "     'param_desc': \"Specifies which converter the C engine should use for floating-point\\nvalues. The options are None or 'high' for the ordinary converter,\\n'legacy' for the original lower precision pandas converter, and\\n'round_trip' for the round-trip converter.\\n\"},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.read_fwf',\n",
       "   'full_function': \"pandas.read_fwf(filepath_or_buffer, *, colspecs='infer', widths=None, infer_nrows=100, dtype_backend=_NoDefault.no_default, iterator=False, chunksize=None, **kwds)\",\n",
       "   'function_text': 'Read a table of fixed-width formatted lines into DataFrame.',\n",
       "   'parameter_names_desc': [{'param_name': 'filepath_or_bufferstr, path object, or file-like object',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a text read() function.The string could be a URL.\\nValid URL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be:\\nfile://localhost/path/to/table.csv.\\n'},\n",
       "    {'param_name': 'colspecslist of tuple (int, int) or ‘infer’. optional',\n",
       "     'param_desc': 'A list of tuples giving the extents of the fixed-width\\nfields of each line as half-open intervals (i.e., [from, to[ ).\\nString value ‘infer’ can be used to instruct the parser to try\\ndetecting the column specifications from the first 100 rows of\\nthe data which are not being skipped via skiprows (default=’infer’).\\n'},\n",
       "    {'param_name': 'widthslist of int, optional',\n",
       "     'param_desc': 'A list of field widths which can be used instead of ‘colspecs’ if\\nthe intervals are contiguous.\\n'},\n",
       "    {'param_name': 'infer_nrowsint, default 100',\n",
       "     'param_desc': 'The number of rows to consider when letting the parser determine the\\ncolspecs.\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "    {'param_name': '**kwdsoptional',\n",
       "     'param_desc': 'Optional keyword arguments can be passed to TextFileReader.\\n'}]},\n",
       "  {'function_name': 'pandas.read_clipboard',\n",
       "   'full_function': \"pandas.read_clipboard(sep='\\\\\\\\s+', dtype_backend=_NoDefault.no_default, **kwargs)\",\n",
       "   'function_text': 'Read text from clipboard and pass to read_csv().',\n",
       "   'parameter_names_desc': [{'param_name': 'sepstr, default ‘\\\\s+’',\n",
       "     'param_desc': \"A string or regex delimiter. The default of '\\\\\\\\s+' denotes\\none or more whitespace characters.\\n\"},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.DataFrame.to_clipboard',\n",
       "   'full_function': 'DataFrame.to_clipboard(*, excel=True, sep=None, **kwargs)',\n",
       "   'function_text': 'Copy object to the system clipboard.',\n",
       "   'parameter_names_desc': [{'param_name': 'excelbool, default True',\n",
       "     'param_desc': 'Produce output in a csv format for easy pasting into excel.\\n\\nTrue, use the provided separator for csv pasting.\\nFalse, write a string representation of the object to the clipboard.\\n\\n'},\n",
       "    {'param_name': \"sepstr, default '\\\\t'\",\n",
       "     'param_desc': 'Field delimiter.\\n'}]},\n",
       "  {'function_name': 'pandas.read_excel',\n",
       "   'full_function': \"pandas.read_excel(io, sheet_name=0, *, header=0, names=None, index_col=None, usecols=None, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, parse_dates=False, date_parser=_NoDefault.no_default, date_format=None, thousands=None, decimal='.', comment=None, skipfooter=0, storage_options=None, dtype_backend=_NoDefault.no_default, engine_kwargs=None)\",\n",
       "   'function_text': 'Read an Excel file into a pandas DataFrame.',\n",
       "   'parameter_names_desc': [{'param_name': 'iostr, bytes, ExcelFile, xlrd.Book, path object, or file-like object',\n",
       "     'param_desc': 'Any valid string path is acceptable. The string could be a URL. Valid\\nURL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be: file://localhost/path/to/table.xlsx.\\nIf you want to pass in a path object, pandas accepts any os.PathLike.\\nBy file-like object, we refer to objects with a read() method,\\nsuch as a file handle (e.g. via builtin open function)\\nor StringIO.\\n\\nDeprecated since version 2.1.0: Passing byte strings is deprecated. To read from a\\nbyte string, wrap it in a BytesIO object.\\n\\n'},\n",
       "    {'param_name': 'sheet_namestr, int, list, or None, default 0',\n",
       "     'param_desc': 'Strings are used for sheet names. Integers are used in zero-indexed\\nsheet positions (chart sheets do not count as a sheet position).\\nLists of strings/integers are used to request multiple sheets.\\nSpecify None to get all worksheets.\\nAvailable cases:\\n\\nDefaults to 0: 1st sheet as a DataFrame\\n1: 2nd sheet as a DataFrame\\n\"Sheet1\": Load sheet with name “Sheet1”\\n[0, 1, \"Sheet5\"]: Load first, second and sheet named “Sheet5”\\nas a dict of DataFrame\\nNone: All worksheets.\\n\\n'},\n",
       "    {'param_name': 'headerint, list of int, default 0',\n",
       "     'param_desc': 'Row (0-indexed) to use for the column labels of the parsed\\nDataFrame. If a list of integers is passed those row positions will\\nbe combined into a MultiIndex. Use None if there is no header.\\n'},\n",
       "    {'param_name': 'namesarray-like, default None',\n",
       "     'param_desc': 'List of column names to use. If file contains no header row,\\nthen you should explicitly pass header=None.\\n'},\n",
       "    {'param_name': 'index_colint, str, list of int, default None',\n",
       "     'param_desc': 'Column (0-indexed) to use as the row labels of the DataFrame.\\nPass None if there is no such column. If a list is passed,\\nthose columns will be combined into a MultiIndex. If a\\nsubset of data is selected with usecols, index_col\\nis based on the subset.\\nMissing values will be forward filled to allow roundtripping with\\nto_excel for merged_cells=True. To avoid forward filling the\\nmissing values use set_index after reading the data instead of\\nindex_col.\\n'},\n",
       "    {'param_name': 'usecolsstr, list-like, or callable, default None',\n",
       "     'param_desc': '\\nIf None, then parse all columns.\\nIf str, then indicates comma separated list of Excel column letters\\nand column ranges (e.g. “A:E” or “A,C,E:F”). Ranges are inclusive of\\nboth sides.\\nIf list of int, then indicates list of column numbers to be parsed\\n(0-indexed).\\nIf list of string, then indicates list of column names to be parsed.\\nIf callable, then evaluate each column name against it and parse the\\ncolumn if the callable returns True.\\n\\nReturns a subset of the columns according to behavior above.\\n'},\n",
       "    {'param_name': 'dtypeType name or dict of column -> type, default None',\n",
       "     'param_desc': 'Data type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32}\\nUse object to preserve data as stored in Excel and not interpret dtype,\\nwhich will necessarily result in object dtype.\\nIf converters are specified, they will be applied INSTEAD\\nof dtype conversion.\\nIf you use None, it will infer the dtype of each column based on the data.\\n'},\n",
       "    {'param_name': 'engine{‘openpyxl’, ‘calamine’, ‘odf’, ‘pyxlsb’, ‘xlrd’}, default None',\n",
       "     'param_desc': 'If io is not a buffer or path, this must be set to identify io.\\nEngine compatibility :\\n\\nopenpyxl supports newer Excel file formats.\\ncalamine supports Excel (.xls, .xlsx, .xlsm, .xlsb)\\nand OpenDocument (.ods) file formats.\\nodf supports OpenDocument file formats (.odf, .ods, .odt).\\npyxlsb supports Binary Excel files.\\nxlrd supports old-style Excel files (.xls).\\n\\nWhen engine=None, the following logic will be used to determine the engine:\\n\\nIf path_or_buffer is an OpenDocument format (.odf, .ods, .odt),\\nthen odf will be used.\\nOtherwise if path_or_buffer is an xls format, xlrd will be used.\\nOtherwise if path_or_buffer is in xlsb format, pyxlsb will be used.\\nOtherwise openpyxl will be used.\\n\\n'},\n",
       "    {'param_name': 'convertersdict, default None',\n",
       "     'param_desc': 'Dict of functions for converting values in certain columns. Keys can\\neither be integers or column labels, values are functions that take one\\ninput argument, the Excel cell content, and return the transformed\\ncontent.\\n'},\n",
       "    {'param_name': 'true_valueslist, default None',\n",
       "     'param_desc': 'Values to consider as True.\\n'},\n",
       "    {'param_name': 'false_valueslist, default None',\n",
       "     'param_desc': 'Values to consider as False.\\n'},\n",
       "    {'param_name': 'skiprowslist-like, int, or callable, optional',\n",
       "     'param_desc': 'Line numbers to skip (0-indexed) or number of lines to skip (int) at the\\nstart of the file. If callable, the callable function will be evaluated\\nagainst the row indices, returning True if the row should be skipped and\\nFalse otherwise. An example of a valid callable argument would be lambda\\nx: x in [0, 2].\\n'},\n",
       "    {'param_name': 'nrowsint, default None',\n",
       "     'param_desc': 'Number of rows to parse.\\n'},\n",
       "    {'param_name': 'na_valuesscalar, str, list-like, or dict, default None',\n",
       "     'param_desc': 'Additional strings to recognize as NA/NaN. If dict passed, specific\\nper-column NA values. By default the following values are interpreted\\nas NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’,\\n‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘None’,\\n‘n/a’, ‘nan’, ‘null’.\\n'},\n",
       "    {'param_name': 'keep_default_nabool, default True',\n",
       "     'param_desc': 'Whether or not to include the default NaN values when parsing the data.\\nDepending on whether na_values is passed in, the behavior is as follows:\\n\\nIf keep_default_na is True, and na_values are specified,\\nna_values is appended to the default NaN values used for parsing.\\nIf keep_default_na is True, and na_values are not specified, only\\nthe default NaN values are used for parsing.\\nIf keep_default_na is False, and na_values are specified, only\\nthe NaN values specified na_values are used for parsing.\\nIf keep_default_na is False, and na_values are not specified, no\\nstrings will be parsed as NaN.\\n\\nNote that if na_filter is passed in as False, the keep_default_na and\\nna_values parameters will be ignored.\\n'},\n",
       "    {'param_name': 'na_filterbool, default True',\n",
       "     'param_desc': 'Detect missing value markers (empty strings and the value of na_values). In\\ndata without any NAs, passing na_filter=False can improve the\\nperformance of reading a large file.\\n'},\n",
       "    {'param_name': 'verbosebool, default False',\n",
       "     'param_desc': 'Indicate number of NA values placed in non-numeric columns.\\n'},\n",
       "    {'param_name': 'parse_datesbool, list-like, or dict, default False',\n",
       "     'param_desc': 'The behavior is as follows:\\n\\nbool. If True -> try parsing the index.\\nlist of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\\neach as a separate date column.\\nlist of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as\\na single date column.\\ndict, e.g. {‘foo’ : [1, 3]} -> parse columns 1, 3 as date and call\\nresult ‘foo’\\n\\nIf a column or index contains an unparsable date, the entire column or\\nindex will be returned unaltered as an object data type. If you don`t want to\\nparse some cells as date just change their type in Excel to “Text”.\\nFor non-standard datetime parsing, use pd.to_datetime after pd.read_excel.\\nNote: A fast-path exists for iso8601-formatted dates.\\n'},\n",
       "    {'param_name': 'date_parserfunction, optional',\n",
       "     'param_desc': 'Function to use for converting a sequence of string columns to an array of\\ndatetime instances. The default uses dateutil.parser.parser to do the\\nconversion. Pandas will try to call date_parser in three different ways,\\nadvancing to the next if an exception occurs: 1) Pass one or more arrays\\n(as defined by parse_dates) as arguments; 2) concatenate (row-wise) the\\nstring values from the columns defined by parse_dates into a single array\\nand pass that; and 3) call date_parser once for each row using one or\\nmore strings (corresponding to the columns defined by parse_dates) as\\narguments.\\n\\nDeprecated since version 2.0.0: Use date_format instead, or read in as object and then apply\\nto_datetime() as-needed.\\n\\n'},\n",
       "    {'param_name': 'date_formatstr or dict of column -> format, default None',\n",
       "     'param_desc': 'If used in conjunction with parse_dates, will parse dates according to this\\nformat. For anything more complex,\\nplease read in as object and then apply to_datetime() as-needed.\\n\\nNew in version 2.0.0.\\n\\n'},\n",
       "    {'param_name': 'thousandsstr, default None',\n",
       "     'param_desc': 'Thousands separator for parsing string columns to numeric. Note that\\nthis parameter is only necessary for columns stored as TEXT in Excel,\\nany numeric columns will automatically be parsed, regardless of display\\nformat.\\n'},\n",
       "    {'param_name': 'decimalstr, default ‘.’',\n",
       "     'param_desc': 'Character to recognize as decimal point for parsing string columns to numeric.\\nNote that this parameter is only necessary for columns stored as TEXT in Excel,\\nany numeric columns will automatically be parsed, regardless of display\\nformat.(e.g. use ‘,’ for European data).\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "    {'param_name': 'commentstr, default None',\n",
       "     'param_desc': 'Comments out remainder of line. Pass a character or characters to this\\nargument to indicate comments in the input file. Any data between the\\ncomment string and the end of the current line is ignored.\\n'},\n",
       "    {'param_name': 'skipfooterint, default 0',\n",
       "     'param_desc': 'Rows at the end to skip (0-indexed).\\n'},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "    {'param_name': 'engine_kwargsdict, optional',\n",
       "     'param_desc': 'Arbitrary keyword arguments passed to excel engine.\\n'}]},\n",
       "  {'function_name': 'pandas.ExcelFile',\n",
       "   'full_function': 'class pandas.ExcelFile(path_or_buffer, engine=None, storage_options=None, engine_kwargs=None)',\n",
       "   'function_text': 'Class for parsing tabular Excel sheets into DataFrame objects.',\n",
       "   'parameter_names_desc': [{'param_name': 'path_or_bufferstr, bytes, path object (pathlib.Path or py._path.local.LocalPath),',\n",
       "     'param_desc': 'A file-like object, xlrd workbook or openpyxl workbook.\\nIf a string or path object, expected to be a path to a\\n.xls, .xlsx, .xlsb, .xlsm, .odf, .ods, or .odt file.\\n'},\n",
       "    {'param_name': 'enginestr, default None',\n",
       "     'param_desc': 'If io is not a buffer or path, this must be set to identify io.\\nSupported engines: xlrd, openpyxl, odf, pyxlsb, calamine\\nEngine compatibility :\\n\\nxlrd supports old-style Excel files (.xls).\\nopenpyxl supports newer Excel file formats.\\nodf supports OpenDocument file formats (.odf, .ods, .odt).\\npyxlsb supports Binary Excel files.\\ncalamine supports Excel (.xls, .xlsx, .xlsm, .xlsb)\\nand OpenDocument (.ods) file formats.\\n\\n\\nChanged in version 1.2.0: The engine xlrd\\nnow only supports old-style .xls files.\\nWhen engine=None, the following logic will be\\nused to determine the engine:\\n\\nIf path_or_buffer is an OpenDocument format (.odf, .ods, .odt),\\nthen odf will be used.\\nOtherwise if path_or_buffer is an xls format,\\nxlrd will be used.\\nOtherwise if path_or_buffer is in xlsb format,\\npyxlsb will be used.\\n\\n\\nNew in version 1.3.0.\\n\\n\\nOtherwise if openpyxl is installed,\\nthen openpyxl will be used.\\nOtherwise if xlrd >= 2.0 is installed, a ValueError will be raised.\\n\\n\\nWarning\\nPlease do not report issues when using xlrd to read .xlsx files.\\nThis is not supported, switch to using openpyxl instead.\\n\\n\\n'},\n",
       "    {'param_name': 'engine_kwargsdict, optional',\n",
       "     'param_desc': 'Arbitrary keyword arguments passed to excel engine.\\n'}]},\n",
       "  {'function_name': 'pandas.io.formats.style.Styler.to_excel',\n",
       "   'full_function': \"Styler.to_excel(excel_writer, sheet_name='Sheet1', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, encoding=None, inf_rep='inf', verbose=True, freeze_panes=None, storage_options=None)\",\n",
       "   'function_text': 'Write Styler to an Excel sheet.',\n",
       "   'parameter_names_desc': [{'param_name': 'excel_writerpath-like, file-like, or ExcelWriter object',\n",
       "     'param_desc': 'File path or existing ExcelWriter.\\n'},\n",
       "    {'param_name': 'sheet_namestr, default ‘Sheet1’',\n",
       "     'param_desc': 'Name of sheet which will contain DataFrame.\\n'},\n",
       "    {'param_name': 'na_repstr, default ‘’',\n",
       "     'param_desc': 'Missing data representation.\\n'},\n",
       "    {'param_name': 'float_formatstr, optional',\n",
       "     'param_desc': 'Format string for floating point numbers. For example\\nfloat_format=\"%.2f\" will format 0.1234 to 0.12.\\n'},\n",
       "    {'param_name': 'columnssequence or list of str, optional',\n",
       "     'param_desc': 'Columns to write.\\n'},\n",
       "    {'param_name': 'headerbool or list of str, default True',\n",
       "     'param_desc': 'Write out the column names. If a list of string is given it is\\nassumed to be aliases for the column names.\\n'},\n",
       "    {'param_name': 'indexbool, default True',\n",
       "     'param_desc': 'Write row names (index).\\n'},\n",
       "    {'param_name': 'index_labelstr or sequence, optional',\n",
       "     'param_desc': 'Column label for index column(s) if desired. If not specified, and\\nheader and index are True, then the index names are used. A\\nsequence should be given if the DataFrame uses MultiIndex.\\n'},\n",
       "    {'param_name': 'startrowint, default 0',\n",
       "     'param_desc': 'Upper left cell row to dump data frame.\\n'},\n",
       "    {'param_name': 'startcolint, default 0',\n",
       "     'param_desc': 'Upper left cell column to dump data frame.\\n'},\n",
       "    {'param_name': 'enginestr, optional',\n",
       "     'param_desc': 'Write engine to use, ‘openpyxl’ or ‘xlsxwriter’. You can also set this\\nvia the options io.excel.xlsx.writer or\\nio.excel.xlsm.writer.\\n'},\n",
       "    {'param_name': 'merge_cellsbool, default True',\n",
       "     'param_desc': 'Write MultiIndex and Hierarchical Rows as merged cells.\\n'},\n",
       "    {'param_name': 'inf_repstr, default ‘inf’',\n",
       "     'param_desc': 'Representation for infinity (there is no native representation for\\ninfinity in Excel).\\n'},\n",
       "    {'param_name': 'freeze_panestuple of int (length 2), optional',\n",
       "     'param_desc': 'Specifies the one-based bottommost row and rightmost column that\\nis to be frozen.\\n'},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n\\nNew in version 1.5.0.\\n\\n'},\n",
       "    {'param_name': 'engine_kwargsdict, optional',\n",
       "     'param_desc': 'Arbitrary keyword arguments passed to excel engine.\\n'}]},\n",
       "  {'function_name': 'pandas.ExcelWriter',\n",
       "   'full_function': \"class pandas.ExcelWriter(path, engine=None, date_format=None, datetime_format=None, mode='w', storage_options=None, if_sheet_exists=None, engine_kwargs=None)\",\n",
       "   'function_text': 'Class for writing DataFrame objects into excel sheets.',\n",
       "   'parameter_names_desc': [{'param_name': 'pathstr or typing.BinaryIO',\n",
       "     'param_desc': 'Path to xls or xlsx or ods file.\\n'},\n",
       "    {'param_name': 'enginestr (optional)',\n",
       "     'param_desc': 'Engine to use for writing. If None, defaults to\\nio.excel.<extension>.writer. NOTE: can only be passed as a keyword\\nargument.\\n'},\n",
       "    {'param_name': 'date_formatstr, default None',\n",
       "     'param_desc': 'Format string for dates written into Excel files (e.g. ‘YYYY-MM-DD’).\\n'},\n",
       "    {'param_name': 'datetime_formatstr, default None',\n",
       "     'param_desc': 'Format string for datetime objects written into Excel files.\\n(e.g. ‘YYYY-MM-DD HH:MM:SS’).\\n'},\n",
       "    {'param_name': 'mode{‘w’, ‘a’}, default ‘w’',\n",
       "     'param_desc': 'File mode to use (write or append). Append does not work with fsspec URLs.\\n'},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "    {'param_name': 'if_sheet_exists{‘error’, ‘new’, ‘replace’, ‘overlay’}, default ‘error’',\n",
       "     'param_desc': 'How to behave when trying to write to a sheet that already\\nexists (append mode only).\\n\\nerror: raise a ValueError.\\nnew: Create a new sheet, with a name determined by the engine.\\nreplace: Delete the contents of the sheet before writing to it.\\noverlay: Write contents to the existing sheet without first removing,\\nbut possibly over top of, the existing contents.\\n\\n\\nNew in version 1.3.0.\\n\\n\\nChanged in version 1.4.0: Added overlay option\\n\\n'},\n",
       "    {'param_name': 'engine_kwargsdict, optional',\n",
       "     'param_desc': 'Keyword arguments to be passed into the engine. These will be passed to\\nthe following functions of the respective engines:\\n\\nxlsxwriter: xlsxwriter.Workbook(file, **engine_kwargs)\\nopenpyxl (write mode): openpyxl.Workbook(**engine_kwargs)\\nopenpyxl (append mode): openpyxl.load_workbook(file, **engine_kwargs)\\nodswriter: odf.opendocument.OpenDocumentSpreadsheet(**engine_kwargs)\\n\\n\\nNew in version 1.3.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.DataFrame.to_excel',\n",
       "   'full_function': \"DataFrame.to_excel(excel_writer, *, sheet_name='Sheet1', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, inf_rep='inf', freeze_panes=None, storage_options=None, engine_kwargs=None)\",\n",
       "   'function_text': 'Write object to an Excel sheet.',\n",
       "   'parameter_names_desc': [{'param_name': 'excel_writerpath-like, file-like, or ExcelWriter object',\n",
       "     'param_desc': 'File path or existing ExcelWriter.\\n'},\n",
       "    {'param_name': 'sheet_namestr, default ‘Sheet1’',\n",
       "     'param_desc': 'Name of sheet which will contain DataFrame.\\n'},\n",
       "    {'param_name': 'na_repstr, default ‘’',\n",
       "     'param_desc': 'Missing data representation.\\n'},\n",
       "    {'param_name': 'float_formatstr, optional',\n",
       "     'param_desc': 'Format string for floating point numbers. For example\\nfloat_format=\"%.2f\" will format 0.1234 to 0.12.\\n'},\n",
       "    {'param_name': 'columnssequence or list of str, optional',\n",
       "     'param_desc': 'Columns to write.\\n'},\n",
       "    {'param_name': 'headerbool or list of str, default True',\n",
       "     'param_desc': 'Write out the column names. If a list of string is given it is\\nassumed to be aliases for the column names.\\n'},\n",
       "    {'param_name': 'indexbool, default True',\n",
       "     'param_desc': 'Write row names (index).\\n'},\n",
       "    {'param_name': 'index_labelstr or sequence, optional',\n",
       "     'param_desc': 'Column label for index column(s) if desired. If not specified, and\\nheader and index are True, then the index names are used. A\\nsequence should be given if the DataFrame uses MultiIndex.\\n'},\n",
       "    {'param_name': 'startrowint, default 0',\n",
       "     'param_desc': 'Upper left cell row to dump data frame.\\n'},\n",
       "    {'param_name': 'startcolint, default 0',\n",
       "     'param_desc': 'Upper left cell column to dump data frame.\\n'},\n",
       "    {'param_name': 'enginestr, optional',\n",
       "     'param_desc': 'Write engine to use, ‘openpyxl’ or ‘xlsxwriter’. You can also set this\\nvia the options io.excel.xlsx.writer or\\nio.excel.xlsm.writer.\\n'},\n",
       "    {'param_name': 'merge_cellsbool, default True',\n",
       "     'param_desc': 'Write MultiIndex and Hierarchical Rows as merged cells.\\n'},\n",
       "    {'param_name': 'inf_repstr, default ‘inf’',\n",
       "     'param_desc': 'Representation for infinity (there is no native representation for\\ninfinity in Excel).\\n'},\n",
       "    {'param_name': 'freeze_panestuple of int (length 2), optional',\n",
       "     'param_desc': 'Specifies the one-based bottommost row and rightmost column that\\nis to be frozen.\\n'},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n\\nNew in version 1.2.0.\\n\\n'},\n",
       "    {'param_name': 'engine_kwargsdict, optional',\n",
       "     'param_desc': 'Arbitrary keyword arguments passed to excel engine.\\n'}]},\n",
       "  {'function_name': 'pandas.ExcelFile.parse',\n",
       "   'full_function': 'ExcelFile.parse(sheet_name=0, header=0, names=None, index_col=None, usecols=None, converters=None, true_values=None, false_values=None, skiprows=None, nrows=None, na_values=None, parse_dates=False, date_parser=_NoDefault.no_default, date_format=None, thousands=None, comment=None, skipfooter=0, dtype_backend=_NoDefault.no_default, **kwds)',\n",
       "   'function_text': 'Parse specified sheet(s) into a DataFrame.',\n",
       "   'parameter_names_desc': [{'param_name': 'DataFrame or dict of DataFrames',\n",
       "     'param_desc': 'DataFrame from the passed in Excel file.\\n'}]},\n",
       "  {'function_name': 'pandas.read_json',\n",
       "   'full_function': \"pandas.read_json(path_or_buf, *, orient=None, typ='frame', dtype=None, convert_axes=None, convert_dates=True, keep_default_dates=True, precise_float=False, date_unit=None, encoding=None, encoding_errors='strict', lines=False, chunksize=None, compression='infer', nrows=None, storage_options=None, dtype_backend=_NoDefault.no_default, engine='ujson')\",\n",
       "   'function_text': 'Convert a JSON string to pandas object.',\n",
       "   'parameter_names_desc': [{'param_name': 'path_or_bufa valid JSON str, path object or file-like object',\n",
       "     'param_desc': 'Any valid string path is acceptable. The string could be a URL. Valid\\nURL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be:\\nfile://localhost/path/to/table.json.\\nIf you want to pass in a path object, pandas accepts any\\nos.PathLike.\\nBy file-like object, we refer to objects with a read() method,\\nsuch as a file handle (e.g. via builtin open function)\\nor StringIO.\\n\\nDeprecated since version 2.1.0: Passing json literal strings is deprecated.\\n\\n'},\n",
       "    {'param_name': 'orientstr, optional',\n",
       "     'param_desc': \"Indication of expected JSON string format.\\nCompatible JSON strings can be produced by to_json() with a\\ncorresponding orient value.\\nThe set of possible orients is:\\n\\n'split' : dict like\\n{index -> [index], columns -> [columns], data -> [values]}\\n'records' : list like\\n[{column -> value}, ... , {column -> value}]\\n'index' : dict like {index -> {column -> value}}\\n'columns' : dict like {column -> {index -> value}}\\n'values' : just the values array\\n'table' : dict like {'schema': {schema}, 'data': {data}}\\n\\nThe allowed and default values depend on the value\\nof the typ parameter.\\n\\nwhen typ == 'series',\\n\\nallowed orients are {'split','records','index'}\\ndefault is 'index'\\nThe Series index must be unique for orient 'index'.\\n\\n\\nwhen typ == 'frame',\\n\\nallowed orients are {'split','records','index',\\n'columns','values', 'table'}\\ndefault is 'columns'\\nThe DataFrame index must be unique for orients 'index' and\\n'columns'.\\nThe DataFrame columns must be unique for orients 'index',\\n'columns', and 'records'.\\n\\n\\n\\n\"},\n",
       "    {'param_name': 'typ{‘frame’, ‘series’}, default ‘frame’',\n",
       "     'param_desc': 'The type of object to recover.\\n'},\n",
       "    {'param_name': 'dtypebool or dict, default None',\n",
       "     'param_desc': \"If True, infer dtypes; if a dict of column to dtype, then use those;\\nif False, then don’t infer dtypes at all, applies only to the data.\\nFor all orient values except 'table', default is True.\\n\"},\n",
       "    {'param_name': 'convert_axesbool, default None',\n",
       "     'param_desc': \"Try to convert the axes to the proper dtypes.\\nFor all orient values except 'table', default is True.\\n\"},\n",
       "    {'param_name': 'convert_datesbool or list of str, default True',\n",
       "     'param_desc': 'If True then default datelike columns may be converted (depending on\\nkeep_default_dates).\\nIf False, no dates will be converted.\\nIf a list of column names, then those columns will be converted and\\ndefault datelike columns may also be converted (depending on\\nkeep_default_dates).\\n'},\n",
       "    {'param_name': 'keep_default_datesbool, default True',\n",
       "     'param_desc': \"If parsing dates (convert_dates is not False), then try to parse the\\ndefault datelike columns.\\nA column label is datelike if\\n\\nit ends with '_at',\\nit ends with '_time',\\nit begins with 'timestamp',\\nit is 'modified', or\\nit is 'date'.\\n\\n\"},\n",
       "    {'param_name': 'precise_floatbool, default False',\n",
       "     'param_desc': 'Set to enable usage of higher precision (strtod) function when\\ndecoding string to double values. Default (False) is to use fast but\\nless precise builtin functionality.\\n'},\n",
       "    {'param_name': 'date_unitstr, default None',\n",
       "     'param_desc': 'The timestamp unit to detect if converting dates. The default behaviour\\nis to try and detect the correct precision, but if this is not desired\\nthen pass one of ‘s’, ‘ms’, ‘us’ or ‘ns’ to force parsing only seconds,\\nmilliseconds, microseconds or nanoseconds respectively.\\n'},\n",
       "    {'param_name': 'encodingstr, default is ‘utf-8’',\n",
       "     'param_desc': 'The encoding to use to decode py3 bytes.\\n'},\n",
       "    {'param_name': 'encoding_errorsstr, optional, default “strict”',\n",
       "     'param_desc': 'How encoding errors are treated. List of possible values .\\n\\nNew in version 1.3.0.\\n\\n'},\n",
       "    {'param_name': 'linesbool, default False',\n",
       "     'param_desc': 'Read the file as a json object per line.\\n'},\n",
       "    {'param_name': 'chunksizeint, optional',\n",
       "     'param_desc': 'Return JsonReader object for iteration.\\nSee the line-delimited json docs\\nfor more information on chunksize.\\nThis can only be passed if lines=True.\\nIf this is None, the file will be read into memory all at once.\\n'},\n",
       "    {'param_name': 'compressionstr or dict, default ‘infer’',\n",
       "     'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘path_or_buf’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "    {'param_name': 'nrowsint, optional',\n",
       "     'param_desc': 'The number of lines from the line-delimited jsonfile that has to be read.\\nThis can only be passed if lines=True.\\nIf this is None, all the rows will be returned.\\n'},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "    {'param_name': 'engine{“ujson”, “pyarrow”}, default “ujson”',\n",
       "     'param_desc': 'Parser engine to use. The \"pyarrow\" engine is only available when\\nlines=True.\\n\\nNew in version 2.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.DataFrame.to_json',\n",
       "   'full_function': \"DataFrame.to_json(path_or_buf=None, *, orient=None, date_format=None, double_precision=10, force_ascii=True, date_unit='ms', default_handler=None, lines=False, compression='infer', index=None, indent=None, storage_options=None, mode='w')\",\n",
       "   'function_text': 'Convert the object to a JSON string.',\n",
       "   'parameter_names_desc': [{'param_name': 'path_or_bufstr, path object, file-like object, or None, default None',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a write() function. If None, the result is\\nreturned as a string.\\n'},\n",
       "    {'param_name': 'orientstr',\n",
       "     'param_desc': \"Indication of expected JSON string format.\\n\\nSeries:\\n\\n\\ndefault is ‘index’\\nallowed values are: {‘split’, ‘records’, ‘index’, ‘table’}.\\n\\n\\n\\nDataFrame:\\n\\n\\ndefault is ‘columns’\\nallowed values are: {‘split’, ‘records’, ‘index’, ‘columns’,\\n‘values’, ‘table’}.\\n\\n\\n\\nThe format of the JSON string:\\n\\n\\n‘split’ : dict like {‘index’ -> [index], ‘columns’ -> [columns],\\n‘data’ -> [values]}\\n‘records’ : list like [{column -> value}, … , {column -> value}]\\n‘index’ : dict like {index -> {column -> value}}\\n‘columns’ : dict like {column -> {index -> value}}\\n‘values’ : just the values array\\n‘table’ : dict like {‘schema’: {schema}, ‘data’: {data}}\\n\\nDescribing the data, where data component is like orient='records'.\\n\\n\\n\\n\"},\n",
       "    {'param_name': 'date_format{None, ‘epoch’, ‘iso’}',\n",
       "     'param_desc': \"Type of date conversion. ‘epoch’ = epoch milliseconds,\\n‘iso’ = ISO8601. The default depends on the orient. For\\norient='table', the default is ‘iso’. For all other orients,\\nthe default is ‘epoch’.\\n\"},\n",
       "    {'param_name': 'double_precisionint, default 10',\n",
       "     'param_desc': 'The number of decimal places to use when encoding\\nfloating point values. The possible maximal value is 15.\\nPassing double_precision greater than 15 will raise a ValueError.\\n'},\n",
       "    {'param_name': 'force_asciibool, default True',\n",
       "     'param_desc': 'Force encoded string to be ASCII.\\n'},\n",
       "    {'param_name': 'date_unitstr, default ‘ms’ (milliseconds)',\n",
       "     'param_desc': 'The time unit to encode to, governs timestamp and ISO8601\\nprecision. One of ‘s’, ‘ms’, ‘us’, ‘ns’ for second, millisecond,\\nmicrosecond, and nanosecond respectively.\\n'},\n",
       "    {'param_name': 'default_handlercallable, default None',\n",
       "     'param_desc': 'Handler to call if object cannot otherwise be converted to a\\nsuitable format for JSON. Should receive a single argument which is\\nthe object to convert and return a serialisable object.\\n'},\n",
       "    {'param_name': 'linesbool, default False',\n",
       "     'param_desc': 'If ‘orient’ is ‘records’ write out line-delimited json format. Will\\nthrow ValueError if incorrect ‘orient’ since others are not\\nlist-like.\\n'},\n",
       "    {'param_name': 'compressionstr or dict, default ‘infer’',\n",
       "     'param_desc': \"For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buf’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nSet to None for no compression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for faster compression and to create\\na reproducible gzip archive:\\ncompression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "    {'param_name': 'indexbool or None, default None',\n",
       "     'param_desc': 'The index is only used when ‘orient’ is ‘split’, ‘index’, ‘column’,\\nor ‘table’. Of these, ‘index’ and ‘column’ do not support\\nindex=False.\\n'},\n",
       "    {'param_name': 'indentint, optional',\n",
       "     'param_desc': 'Length of whitespace used to indent each record.\\n'},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "    {'param_name': 'modestr, default ‘w’ (writing)',\n",
       "     'param_desc': 'Specify the IO mode for output when supplying a path_or_buf.\\nAccepted args are ‘w’ (writing) and ‘a’ (append) only.\\nmode=’a’ is only supported when lines is True and orient is ‘records’.\\n'}]},\n",
       "  {'function_name': 'pandas.io.json.build_table_schema',\n",
       "   'full_function': 'pandas.io.json.build_table_schema(data, index=True, primary_key=None, version=True)',\n",
       "   'function_text': 'Create a Table schema from data.',\n",
       "   'parameter_names_desc': [{'param_name': 'dataSeries, DataFrame',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'indexbool, default True',\n",
       "     'param_desc': 'Whether to include data.index in the schema.\\n'},\n",
       "    {'param_name': 'primary_keybool or None, default True',\n",
       "     'param_desc': 'Column names to designate as the primary key.\\nThe default None will set ‘primaryKey’ to the index\\nlevel or levels if the index is unique.\\n'},\n",
       "    {'param_name': 'versionbool, default True',\n",
       "     'param_desc': 'Whether to include a field pandas_version with the version\\nof pandas that last revised the table schema. This version\\ncan be different from the installed pandas version.\\n'}]},\n",
       "  {'function_name': 'pandas.json_normalize',\n",
       "   'full_function': \"pandas.json_normalize(data, record_path=None, meta=None, meta_prefix=None, record_prefix=None, errors='raise', sep='.', max_level=None)\",\n",
       "   'function_text': 'Normalize semi-structured JSON data into a flat table.',\n",
       "   'parameter_names_desc': [{'param_name': 'datadict or list of dicts',\n",
       "     'param_desc': 'Unserialized JSON objects.\\n'},\n",
       "    {'param_name': 'record_pathstr or list of str, default None',\n",
       "     'param_desc': 'Path in each object to list of records. If not passed, data will be\\nassumed to be an array of records.\\n'},\n",
       "    {'param_name': 'metalist of paths (str or list of str), default None',\n",
       "     'param_desc': 'Fields to use as metadata for each record in resulting table.\\n'},\n",
       "    {'param_name': 'meta_prefixstr, default None',\n",
       "     'param_desc': 'If True, prefix records with dotted (?) path, e.g. foo.bar.field if\\nmeta is [‘foo’, ‘bar’].\\n'},\n",
       "    {'param_name': 'record_prefixstr, default None',\n",
       "     'param_desc': 'If True, prefix records with dotted (?) path, e.g. foo.bar.field if\\npath to records is [‘foo’, ‘bar’].\\n'},\n",
       "    {'param_name': 'errors{‘raise’, ‘ignore’}, default ‘raise’',\n",
       "     'param_desc': 'Configures error handling.\\n\\n‘ignore’ : will ignore KeyError if keys listed in meta are not\\nalways present.\\n‘raise’ : will raise KeyError if keys listed in meta are not\\nalways present.\\n\\n'},\n",
       "    {'param_name': 'sepstr, default ‘.’',\n",
       "     'param_desc': 'Nested records will generate names separated by sep.\\ne.g., for sep=’.’, {‘foo’: {‘bar’: 0}} -> foo.bar.\\n'},\n",
       "    {'param_name': 'max_levelint, default None',\n",
       "     'param_desc': 'Max number of levels(depth of dict) to normalize.\\nif None, normalizes all levels.\\n'}]},\n",
       "  {'function_name': 'pandas.read_html',\n",
       "   'full_function': \"pandas.read_html(io, *, match='.+', flavor=None, header=None, index_col=None, skiprows=None, attrs=None, parse_dates=False, thousands=',', encoding=None, decimal='.', converters=None, na_values=None, keep_default_na=True, displayed_only=True, extract_links=None, dtype_backend=_NoDefault.no_default, storage_options=None)\",\n",
       "   'function_text': 'Read HTML tables into a list of DataFrame objects.',\n",
       "   'parameter_names_desc': [{'param_name': 'iostr, path object, or file-like object',\n",
       "     'param_desc': \"String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a string read() function.\\nThe string can represent a URL or the HTML itself. Note that\\nlxml only accepts the http, ftp and file url protocols. If you have a\\nURL that starts with 'https' you might try removing the 's'.\\n\\nDeprecated since version 2.1.0: Passing html literal strings is deprecated.\\nWrap literal string/bytes input in io.StringIO/io.BytesIO instead.\\n\\n\"},\n",
       "    {'param_name': 'matchstr or compiled regular expression, optional',\n",
       "     'param_desc': 'The set of tables containing text matching this regex or string will be\\nreturned. Unless the HTML is extremely simple you will probably need to\\npass a non-empty string here. Defaults to ‘.+’ (match any non-empty\\nstring). The default value will return all tables contained on a page.\\nThis value is converted to a regular expression so that there is\\nconsistent behavior between Beautiful Soup and lxml.\\n'},\n",
       "    {'param_name': 'flavor{“lxml”, “html5lib”, “bs4”} or list-like, optional',\n",
       "     'param_desc': 'The parsing engine (or list of parsing engines) to use. ‘bs4’ and\\n‘html5lib’ are synonymous with each other, they are both there for\\nbackwards compatibility. The default of None tries to use lxml\\nto parse and if that fails it falls back on bs4 + html5lib.\\n'},\n",
       "    {'param_name': 'headerint or list-like, optional',\n",
       "     'param_desc': 'The row (or list of rows for a MultiIndex) to use to\\nmake the columns headers.\\n'},\n",
       "    {'param_name': 'index_colint or list-like, optional',\n",
       "     'param_desc': 'The column (or list of columns) to use to create the index.\\n'},\n",
       "    {'param_name': 'skiprowsint, list-like or slice, optional',\n",
       "     'param_desc': 'Number of rows to skip after parsing the column integer. 0-based. If a\\nsequence of integers or a slice is given, will skip the rows indexed by\\nthat sequence. Note that a single element sequence means ‘skip the nth\\nrow’ whereas an integer means ‘skip n rows’.\\n'},\n",
       "    {'param_name': 'attrsdict, optional',\n",
       "     'param_desc': \"This is a dictionary of attributes that you can pass to use to identify\\nthe table in the HTML. These are not checked for validity before being\\npassed to lxml or Beautiful Soup. However, these attributes must be\\nvalid HTML table attributes to work correctly. For example,\\nattrs = {'id': 'table'}\\n\\n\\nis a valid attribute dictionary because the ‘id’ HTML tag attribute is\\na valid HTML attribute for any HTML tag as per this document.\\nattrs = {'asdf': 'table'}\\n\\n\\nis not a valid attribute dictionary because ‘asdf’ is not a valid\\nHTML attribute even if it is a valid XML attribute. Valid HTML 4.01\\ntable attributes can be found here. A\\nworking draft of the HTML 5 spec can be found here. It contains the\\nlatest information on table attributes for the modern web.\\n\"},\n",
       "    {'param_name': 'parse_datesbool, optional',\n",
       "     'param_desc': 'See read_csv() for more details.\\n'},\n",
       "    {'param_name': 'thousandsstr, optional',\n",
       "     'param_desc': \"Separator to use to parse thousands. Defaults to ','.\\n\"},\n",
       "    {'param_name': 'encodingstr, optional',\n",
       "     'param_desc': 'The encoding used to decode the web page. Defaults to None.``None``\\npreserves the previous encoding behavior, which depends on the\\nunderlying parser library (e.g., the parser library will try to use\\nthe encoding provided by the document).\\n'},\n",
       "    {'param_name': 'decimalstr, default ‘.’',\n",
       "     'param_desc': 'Character to recognize as decimal point (e.g. use ‘,’ for European\\ndata).\\n'},\n",
       "    {'param_name': 'convertersdict, default None',\n",
       "     'param_desc': 'Dict of functions for converting values in certain columns. Keys can\\neither be integers or column labels, values are functions that take one\\ninput argument, the cell (not column) content, and return the\\ntransformed content.\\n'},\n",
       "    {'param_name': 'na_valuesiterable, default None',\n",
       "     'param_desc': 'Custom NA values.\\n'},\n",
       "    {'param_name': 'keep_default_nabool, default True',\n",
       "     'param_desc': 'If na_values are specified and keep_default_na is False the default NaN\\nvalues are overridden, otherwise they’re appended to.\\n'},\n",
       "    {'param_name': 'displayed_onlybool, default True',\n",
       "     'param_desc': 'Whether elements with “display: none” should be parsed.\\n'},\n",
       "    {'param_name': 'extract_links{None, “all”, “header”, “body”, “footer”}',\n",
       "     'param_desc': 'Table elements in the specified section(s) with <a> tags will have their\\nhref extracted.\\n\\nNew in version 1.5.0.\\n\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n\\nNew in version 2.1.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.io.formats.style.Styler.to_html',\n",
       "   'full_function': 'Styler.to_html(buf=None, *, table_uuid=None, table_attributes=None, sparse_index=None, sparse_columns=None, bold_headers=False, caption=None, max_rows=None, max_columns=None, encoding=None, doctype_html=False, exclude_styles=False, **kwargs)',\n",
       "   'function_text': 'Write Styler to a file, buffer or string in HTML-CSS format.',\n",
       "   'parameter_names_desc': [{'param_name': 'bufstr, path object, file-like object, optional',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a string write() function. If None, the result is\\nreturned as a string.\\n'},\n",
       "    {'param_name': 'table_uuidstr, optional',\n",
       "     'param_desc': 'Id attribute assigned to the <table> HTML element in the format:\\n<table id=\"T_<table_uuid>\" ..>\\nIf not given uses Styler’s initially assigned value.\\n'},\n",
       "    {'param_name': 'table_attributesstr, optional',\n",
       "     'param_desc': 'Attributes to assign within the <table> HTML element in the format:\\n<table .. <table_attributes> >\\nIf not given defaults to Styler’s preexisting value.\\n'},\n",
       "    {'param_name': 'sparse_indexbool, optional',\n",
       "     'param_desc': 'Whether to sparsify the display of a hierarchical index. Setting to False\\nwill display each explicit level element in a hierarchical key for each row.\\nDefaults to pandas.options.styler.sparse.index value.\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "    {'param_name': 'sparse_columnsbool, optional',\n",
       "     'param_desc': 'Whether to sparsify the display of a hierarchical index. Setting to False\\nwill display each explicit level element in a hierarchical key for each\\ncolumn. Defaults to pandas.options.styler.sparse.columns value.\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "    {'param_name': 'bold_headersbool, optional',\n",
       "     'param_desc': 'Adds “font-weight: bold;” as a CSS property to table style header cells.\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "    {'param_name': 'captionstr, optional',\n",
       "     'param_desc': 'Set, or overwrite, the caption on Styler before rendering.\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "    {'param_name': 'max_rowsint, optional',\n",
       "     'param_desc': 'The maximum number of rows that will be rendered. Defaults to\\npandas.options.styler.render.max_rows/max_columns.\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "    {'param_name': 'max_columnsint, optional',\n",
       "     'param_desc': 'The maximum number of columns that will be rendered. Defaults to\\npandas.options.styler.render.max_columns, which is None.\\nRows and columns may be reduced if the number of total elements is\\nlarge. This value is set to pandas.options.styler.render.max_elements,\\nwhich is 262144 (18 bit browser rendering).\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "    {'param_name': 'encodingstr, optional',\n",
       "     'param_desc': 'Character encoding setting for file output (and meta tags if available).\\nDefaults to pandas.options.styler.render.encoding value of “utf-8”.\\n'},\n",
       "    {'param_name': 'doctype_htmlbool, default False',\n",
       "     'param_desc': 'Whether to output a fully structured HTML file including all\\nHTML elements, or just the core <style> and <table> elements.\\n'},\n",
       "    {'param_name': 'exclude_stylesbool, default False',\n",
       "     'param_desc': 'Whether to include the <style> element and all associated element\\nclass and id identifiers, or solely the <table> element without\\nstyling identifiers.\\n'}]},\n",
       "  {'function_name': 'pandas.DataFrame.to_html',\n",
       "   'full_function': \"DataFrame.to_html(buf=None, *, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, max_cols=None, show_dimensions=False, decimal='.', bold_rows=True, classes=None, escape=True, notebook=False, border=None, table_id=None, render_links=False, encoding=None)\",\n",
       "   'function_text': 'Render a DataFrame as an HTML table.',\n",
       "   'parameter_names_desc': [{'param_name': 'bufstr, Path or StringIO-like, optional, default None',\n",
       "     'param_desc': 'Buffer to write to. If None, the output is returned as a string.\\n'},\n",
       "    {'param_name': 'columnsarray-like, optional, default None',\n",
       "     'param_desc': 'The subset of columns to write. Writes all columns by default.\\n'},\n",
       "    {'param_name': 'col_spacestr or int, list or dict of int or str, optional',\n",
       "     'param_desc': 'The minimum width of each column in CSS length units. An int is assumed to be px units..\\n'},\n",
       "    {'param_name': 'headerbool, optional',\n",
       "     'param_desc': 'Whether to print column labels, default True.\\n'},\n",
       "    {'param_name': 'indexbool, optional, default True',\n",
       "     'param_desc': 'Whether to print index (row) labels.\\n'},\n",
       "    {'param_name': 'na_repstr, optional, default ‘NaN’',\n",
       "     'param_desc': 'String representation of NaN to use.\\n'},\n",
       "    {'param_name': 'formatterslist, tuple or dict of one-param. functions, optional',\n",
       "     'param_desc': 'Formatter functions to apply to columns’ elements by position or\\nname.\\nThe result of each function must be a unicode string.\\nList/tuple must be of length equal to the number of columns.\\n'},\n",
       "    {'param_name': 'float_formatone-parameter function, optional, default None',\n",
       "     'param_desc': 'Formatter function to apply to columns’ elements if they are\\nfloats. This function must return a unicode string and will be\\napplied only to the non-NaN elements, with NaN being\\nhandled by na_rep.\\n'},\n",
       "    {'param_name': 'sparsifybool, optional, default True',\n",
       "     'param_desc': 'Set to False for a DataFrame with a hierarchical index to print\\nevery multiindex key at each row.\\n'},\n",
       "    {'param_name': 'index_namesbool, optional, default True',\n",
       "     'param_desc': 'Prints the names of the indexes.\\n'},\n",
       "    {'param_name': 'justifystr, default None',\n",
       "     'param_desc': 'How to justify the column labels. If None uses the option from\\nthe print configuration (controlled by set_option), ‘right’ out\\nof the box. Valid values are\\n\\nleft\\nright\\ncenter\\njustify\\njustify-all\\nstart\\nend\\ninherit\\nmatch-parent\\ninitial\\nunset.\\n\\n'},\n",
       "    {'param_name': 'max_rowsint, optional',\n",
       "     'param_desc': 'Maximum number of rows to display in the console.\\n'},\n",
       "    {'param_name': 'max_colsint, optional',\n",
       "     'param_desc': 'Maximum number of columns to display in the console.\\n'},\n",
       "    {'param_name': 'show_dimensionsbool, default False',\n",
       "     'param_desc': 'Display DataFrame dimensions (number of rows by number of columns).\\n'},\n",
       "    {'param_name': 'decimalstr, default ‘.’',\n",
       "     'param_desc': 'Character recognized as decimal separator, e.g. ‘,’ in Europe.\\n'},\n",
       "    {'param_name': 'bold_rowsbool, default True',\n",
       "     'param_desc': 'Make the row labels bold in the output.\\n'},\n",
       "    {'param_name': 'classesstr or list or tuple, default None',\n",
       "     'param_desc': 'CSS class(es) to apply to the resulting html table.\\n'},\n",
       "    {'param_name': 'escapebool, default True',\n",
       "     'param_desc': 'Convert the characters <, >, and & to HTML-safe sequences.\\n'},\n",
       "    {'param_name': 'notebook{True, False}, default False',\n",
       "     'param_desc': 'Whether the generated HTML is for IPython Notebook.\\n'},\n",
       "    {'param_name': 'borderint',\n",
       "     'param_desc': 'A border=border attribute is included in the opening\\n<table> tag. Default pd.options.display.html.border.\\n'},\n",
       "    {'param_name': 'table_idstr, optional',\n",
       "     'param_desc': 'A css id is included in the opening <table> tag if specified.\\n'},\n",
       "    {'param_name': 'render_linksbool, default False',\n",
       "     'param_desc': 'Convert URLs to HTML links.\\n'},\n",
       "    {'param_name': 'encodingstr, default “utf-8”',\n",
       "     'param_desc': 'Set character encoding.\\n'}]},\n",
       "  {'function_name': 'pandas.read_xml',\n",
       "   'full_function': \"pandas.read_xml(path_or_buffer, *, xpath='./*', namespaces=None, elems_only=False, attrs_only=False, names=None, dtype=None, converters=None, parse_dates=None, encoding='utf-8', parser='lxml', stylesheet=None, iterparse=None, compression='infer', storage_options=None, dtype_backend=_NoDefault.no_default)\",\n",
       "   'function_text': 'Read XML document into a DataFrame object.',\n",
       "   'parameter_names_desc': [{'param_name': 'path_or_bufferstr, path object, or file-like object',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a read() function. The string can be any valid XML\\nstring or a path. The string can further be a URL. Valid URL schemes\\ninclude http, ftp, s3, and file.\\n\\nDeprecated since version 2.1.0: Passing xml literal strings is deprecated.\\nWrap literal xml input in io.StringIO or io.BytesIO instead.\\n\\n'},\n",
       "    {'param_name': 'xpathstr, optional, default ‘./*’',\n",
       "     'param_desc': 'The XPath to parse required set of nodes for migration to\\nDataFrame.``XPath`` should return a collection of elements\\nand not a single element. Note: The etree parser supports limited XPath\\nexpressions. For more complex XPath, use lxml which requires\\ninstallation.\\n'},\n",
       "    {'param_name': 'namespacesdict, optional',\n",
       "     'param_desc': 'The namespaces defined in XML document as dicts with key being\\nnamespace prefix and value the URI. There is no need to include all\\nnamespaces in XML, only the ones used in xpath expression.\\nNote: if XML document uses default namespace denoted as\\nxmlns=’<URI>’ without a prefix, you must assign any temporary\\nnamespace prefix such as ‘doc’ to the URI in order to parse\\nunderlying nodes and/or attributes. For example,\\nnamespaces = {\"doc\": \"https://example.com\"}\\n\\n\\n'},\n",
       "    {'param_name': 'elems_onlybool, optional, default False',\n",
       "     'param_desc': 'Parse only the child elements at the specified xpath. By default,\\nall child elements and non-empty text nodes are returned.\\n'},\n",
       "    {'param_name': 'attrs_onlybool, optional, default False',\n",
       "     'param_desc': 'Parse only the attributes at the specified xpath.\\nBy default, all attributes are returned.\\n'},\n",
       "    {'param_name': 'nameslist-like, optional',\n",
       "     'param_desc': 'Column names for DataFrame of parsed XML data. Use this parameter to\\nrename original element names and distinguish same named elements and\\nattributes.\\n'},\n",
       "    {'param_name': 'dtypeType name or dict of column -> type, optional',\n",
       "     'param_desc': 'Data type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32,\\n‘c’: ‘Int64’}\\nUse str or object together with suitable na_values settings\\nto preserve and not interpret dtype.\\nIf converters are specified, they will be applied INSTEAD\\nof dtype conversion.\\n\\nNew in version 1.5.0.\\n\\n'},\n",
       "    {'param_name': 'convertersdict, optional',\n",
       "     'param_desc': 'Dict of functions for converting values in certain columns. Keys can either\\nbe integers or column labels.\\n\\nNew in version 1.5.0.\\n\\n'},\n",
       "    {'param_name': 'parse_datesbool or list of int or names or list of lists or dict, default False',\n",
       "     'param_desc': 'Identifiers to parse index or columns to datetime. The behavior is as follows:\\n\\nboolean. If True -> try parsing the index.\\nlist of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\\neach as a separate date column.\\nlist of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as\\na single date column.\\ndict, e.g. {‘foo’ : [1, 3]} -> parse columns 1, 3 as date and call\\nresult ‘foo’\\n\\n\\nNew in version 1.5.0.\\n\\n'},\n",
       "    {'param_name': 'encodingstr, optional, default ‘utf-8’',\n",
       "     'param_desc': 'Encoding of XML document.\\n'},\n",
       "    {'param_name': 'parser{‘lxml’,’etree’}, default ‘lxml’',\n",
       "     'param_desc': 'Parser module to use for retrieval of data. Only ‘lxml’ and\\n‘etree’ are supported. With ‘lxml’ more complex XPath searches\\nand ability to use XSLT stylesheet are supported.\\n'},\n",
       "    {'param_name': 'stylesheetstr, path object or file-like object',\n",
       "     'param_desc': 'A URL, file-like object, or a raw string containing an XSLT script.\\nThis stylesheet should flatten complex, deeply nested XML documents\\nfor easier parsing. To use this feature you must have lxml module\\ninstalled and specify ‘lxml’ as parser. The xpath must\\nreference nodes of transformed XML document generated after XSLT\\ntransformation and not the original XML document. Only XSLT 1.0\\nscripts and not later versions is currently supported.\\n'},\n",
       "    {'param_name': 'iterparsedict, optional',\n",
       "     'param_desc': 'The nodes or attributes to retrieve in iterparsing of XML document\\nas a dict with key being the name of repeating element and value being\\nlist of elements or attribute names that are descendants of the repeated\\nelement. Note: If this option is used, it will replace xpath parsing\\nand unlike xpath, descendants do not need to relate to each other but can\\nexist any where in document under the repeating element. This memory-\\nefficient method should be used for very large XML files (500MB, 1GB, or 5GB+).\\nFor example,\\niterparse = {\"row_element\": [\"child_elem\", \"attr\", \"grandchild_elem\"]}\\n\\n\\n\\nNew in version 1.5.0.\\n\\n'},\n",
       "    {'param_name': 'compressionstr or dict, default ‘infer’',\n",
       "     'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘path_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.DataFrame.to_xml',\n",
       "   'full_function': \"DataFrame.to_xml(path_or_buffer=None, *, index=True, root_name='data', row_name='row', na_rep=None, attr_cols=None, elem_cols=None, namespaces=None, prefix=None, encoding='utf-8', xml_declaration=True, pretty_print=True, parser='lxml', stylesheet=None, compression='infer', storage_options=None)\",\n",
       "   'function_text': 'Render a DataFrame to an XML document.',\n",
       "   'parameter_names_desc': [{'param_name': 'path_or_bufferstr, path object, file-like object, or None, default None',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a write() function. If None, the result is returned\\nas a string.\\n'},\n",
       "    {'param_name': 'indexbool, default True',\n",
       "     'param_desc': 'Whether to include index in XML document.\\n'},\n",
       "    {'param_name': 'root_namestr, default ‘data’',\n",
       "     'param_desc': 'The name of root element in XML document.\\n'},\n",
       "    {'param_name': 'row_namestr, default ‘row’',\n",
       "     'param_desc': 'The name of row element in XML document.\\n'},\n",
       "    {'param_name': 'na_repstr, optional',\n",
       "     'param_desc': 'Missing data representation.\\n'},\n",
       "    {'param_name': 'attr_colslist-like, optional',\n",
       "     'param_desc': 'List of columns to write as attributes in row element.\\nHierarchical columns will be flattened with underscore\\ndelimiting the different levels.\\n'},\n",
       "    {'param_name': 'elem_colslist-like, optional',\n",
       "     'param_desc': 'List of columns to write as children in row element. By default,\\nall columns output as children of row element. Hierarchical\\ncolumns will be flattened with underscore delimiting the\\ndifferent levels.\\n'},\n",
       "    {'param_name': 'namespacesdict, optional',\n",
       "     'param_desc': 'All namespaces to be defined in root element. Keys of dict\\nshould be prefix names and values of dict corresponding URIs.\\nDefault namespaces should be given empty string key. For\\nexample,\\nnamespaces = {\"\": \"https://example.com\"}\\n\\n\\n'},\n",
       "    {'param_name': 'prefixstr, optional',\n",
       "     'param_desc': 'Namespace prefix to be used for every element and/or attribute\\nin document. This should be one of the keys in namespaces\\ndict.\\n'},\n",
       "    {'param_name': 'encodingstr, default ‘utf-8’',\n",
       "     'param_desc': 'Encoding of the resulting document.\\n'},\n",
       "    {'param_name': 'xml_declarationbool, default True',\n",
       "     'param_desc': 'Whether to include the XML declaration at start of document.\\n'},\n",
       "    {'param_name': 'pretty_printbool, default True',\n",
       "     'param_desc': 'Whether output should be pretty printed with indentation and\\nline breaks.\\n'},\n",
       "    {'param_name': 'parser{‘lxml’,’etree’}, default ‘lxml’',\n",
       "     'param_desc': 'Parser module to use for building of tree. Only ‘lxml’ and\\n‘etree’ are supported. With ‘lxml’, the ability to use XSLT\\nstylesheet is supported.\\n'},\n",
       "    {'param_name': 'stylesheetstr, path object or file-like object, optional',\n",
       "     'param_desc': 'A URL, file-like object, or a raw string containing an XSLT\\nscript used to transform the raw XML output. Script should use\\nlayout of elements and attributes from original output. This\\nargument requires lxml to be installed. Only XSLT 1.0\\nscripts and not later versions is currently supported.\\n'},\n",
       "    {'param_name': 'compressionstr or dict, default ‘infer’',\n",
       "     'param_desc': \"For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nSet to None for no compression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for faster compression and to create\\na reproducible gzip archive:\\ncompression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'}]},\n",
       "  {'function_name': 'pandas.DataFrame.to_latex',\n",
       "   'full_function': \"DataFrame.to_latex(buf=None, *, columns=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, bold_rows=False, column_format=None, longtable=None, escape=None, encoding=None, decimal='.', multicolumn=None, multicolumn_format=None, multirow=None, caption=None, label=None, position=None)\",\n",
       "   'function_text': 'Render object to a LaTeX tabular, longtable, or nested table.',\n",
       "   'parameter_names_desc': [{'param_name': 'bufstr, Path or StringIO-like, optional, default None',\n",
       "     'param_desc': 'Buffer to write to. If None, the output is returned as a string.\\n'},\n",
       "    {'param_name': 'columnslist of label, optional',\n",
       "     'param_desc': 'The subset of columns to write. Writes all columns by default.\\n'},\n",
       "    {'param_name': 'headerbool or list of str, default True',\n",
       "     'param_desc': 'Write out the column names. If a list of strings is given,\\nit is assumed to be aliases for the column names.\\n'},\n",
       "    {'param_name': 'indexbool, default True',\n",
       "     'param_desc': 'Write row names (index).\\n'},\n",
       "    {'param_name': 'na_repstr, default ‘NaN’',\n",
       "     'param_desc': 'Missing data representation.\\n'},\n",
       "    {'param_name': 'formatterslist of functions or dict of {{str: function}}, optional',\n",
       "     'param_desc': 'Formatter functions to apply to columns’ elements by position or\\nname. The result of each function must be a unicode string.\\nList must be of length equal to the number of columns.\\n'},\n",
       "    {'param_name': 'float_formatone-parameter function or str, optional, default None',\n",
       "     'param_desc': 'Formatter for floating point numbers. For example\\nfloat_format=\"%.2f\" and float_format=\"{{:0.2f}}\".format will\\nboth result in 0.1234 being formatted as 0.12.\\n'},\n",
       "    {'param_name': 'sparsifybool, optional',\n",
       "     'param_desc': 'Set to False for a DataFrame with a hierarchical index to print\\nevery multiindex key at each row. By default, the value will be\\nread from the config module.\\n'},\n",
       "    {'param_name': 'index_namesbool, default True',\n",
       "     'param_desc': 'Prints the names of the indexes.\\n'},\n",
       "    {'param_name': 'bold_rowsbool, default False',\n",
       "     'param_desc': 'Make the row labels bold in the output.\\n'},\n",
       "    {'param_name': 'column_formatstr, optional',\n",
       "     'param_desc': 'The columns format as specified in LaTeX table format e.g. ‘rcl’ for 3\\ncolumns. By default, ‘l’ will be used for all columns except\\ncolumns of numbers, which default to ‘r’.\\n'},\n",
       "    {'param_name': 'longtablebool, optional',\n",
       "     'param_desc': 'Use a longtable environment instead of tabular. Requires\\nadding a usepackage{{longtable}} to your LaTeX preamble.\\nBy default, the value will be read from the pandas config\\nmodule, and set to True if the option styler.latex.environment is\\n“longtable”.\\n\\nChanged in version 2.0.0: The pandas option affecting this argument has changed.\\n\\n'},\n",
       "    {'param_name': 'escapebool, optional',\n",
       "     'param_desc': 'By default, the value will be read from the pandas config\\nmodule and set to True if the option styler.format.escape is\\n“latex”. When set to False prevents from escaping latex special\\ncharacters in column names.\\n\\nChanged in version 2.0.0: The pandas option affecting this argument has changed, as has the\\ndefault value to False.\\n\\n'},\n",
       "    {'param_name': 'encodingstr, optional',\n",
       "     'param_desc': 'A string representing the encoding to use in the output file,\\ndefaults to ‘utf-8’.\\n'},\n",
       "    {'param_name': 'decimalstr, default ‘.’',\n",
       "     'param_desc': 'Character recognized as decimal separator, e.g. ‘,’ in Europe.\\n'},\n",
       "    {'param_name': 'multicolumnbool, default True',\n",
       "     'param_desc': 'Use multicolumn to enhance MultiIndex columns.\\nThe default will be read from the config module, and is set\\nas the option styler.sparse.columns.\\n\\nChanged in version 2.0.0: The pandas option affecting this argument has changed.\\n\\n'},\n",
       "    {'param_name': 'multicolumn_formatstr, default ‘r’',\n",
       "     'param_desc': 'The alignment for multicolumns, similar to column_format\\nThe default will be read from the config module, and is set as the option\\nstyler.latex.multicol_align.\\n\\nChanged in version 2.0.0: The pandas option affecting this argument has changed, as has the\\ndefault value to “r”.\\n\\n'},\n",
       "    {'param_name': 'multirowbool, default True',\n",
       "     'param_desc': 'Use multirow to enhance MultiIndex rows. Requires adding a\\nusepackage{{multirow}} to your LaTeX preamble. Will print\\ncentered labels (instead of top-aligned) across the contained\\nrows, separating groups via clines. The default will be read\\nfrom the pandas config module, and is set as the option\\nstyler.sparse.index.\\n\\nChanged in version 2.0.0: The pandas option affecting this argument has changed, as has the\\ndefault value to True.\\n\\n'},\n",
       "    {'param_name': 'captionstr or tuple, optional',\n",
       "     'param_desc': 'Tuple (full_caption, short_caption),\\nwhich results in \\\\caption[short_caption]{{full_caption}};\\nif a single string is passed, no short caption will be set.\\n'},\n",
       "    {'param_name': 'labelstr, optional',\n",
       "     'param_desc': 'The LaTeX label to be placed inside \\\\label{{}} in the output.\\nThis is used with \\\\ref{{}} in the main .tex file.\\n'},\n",
       "    {'param_name': 'positionstr, optional',\n",
       "     'param_desc': 'The LaTeX positional argument for tables, to be placed after\\n\\\\begin{{}} in the output.\\n'}]},\n",
       "  {'function_name': 'pandas.io.formats.style.Styler.to_latex',\n",
       "   'full_function': 'Styler.to_latex(buf=None, *, column_format=None, position=None, position_float=None, hrules=None, clines=None, label=None, caption=None, sparse_index=None, sparse_columns=None, multirow_align=None, multicol_align=None, siunitx=False, environment=None, encoding=None, convert_css=False)',\n",
       "   'function_text': 'Write Styler to a file, buffer or string in LaTeX format.',\n",
       "   'parameter_names_desc': [{'param_name': 'bufstr, path object, file-like object, or None, default None',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a string write() function. If None, the result is\\nreturned as a string.\\n'},\n",
       "    {'param_name': 'column_formatstr, optional',\n",
       "     'param_desc': 'The LaTeX column specification placed in location:\\n\\\\begin{tabular}{<column_format>}\\nDefaults to ‘l’ for index and\\nnon-numeric data columns, and, for numeric data columns,\\nto ‘r’ by default, or ‘S’ if siunitx is True.\\n'},\n",
       "    {'param_name': 'positionstr, optional',\n",
       "     'param_desc': 'The LaTeX positional argument (e.g. ‘h!’) for tables, placed in location:\\n\\\\\\\\begin{table}[<position>].\\n'},\n",
       "    {'param_name': 'position_float{“centering”, “raggedleft”, “raggedright”}, optional',\n",
       "     'param_desc': 'The LaTeX float command placed in location:\\n\\\\begin{table}[<position>]\\n\\\\<position_float>\\nCannot be used if environment is “longtable”.\\n'},\n",
       "    {'param_name': 'hrulesbool',\n",
       "     'param_desc': 'Set to True to add \\\\toprule, \\\\midrule and \\\\bottomrule from the\\n{booktabs} LaTeX package.\\nDefaults to pandas.options.styler.latex.hrules, which is False.\\n\\nChanged in version 1.4.0.\\n\\n'},\n",
       "    {'param_name': 'clinesstr, optional',\n",
       "     'param_desc': 'Use to control adding \\\\cline commands for the index labels separation.\\nPossible values are:\\n\\n\\nNone: no cline commands are added (default).\\n“all;data”: a cline is added for every index value extending the\\nwidth of the table, including data entries.\\n“all;index”: as above with lines extending only the width of the\\nindex entries.\\n“skip-last;data”: a cline is added for each index value except the\\nlast level (which is never sparsified), extending the widtn of the\\ntable.\\n“skip-last;index”: as above with lines extending only the width of the\\nindex entries.\\n\\n\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "    {'param_name': 'labelstr, optional',\n",
       "     'param_desc': 'The LaTeX label included as: \\\\label{<label>}.\\nThis is used with \\\\ref{<label>} in the main .tex file.\\n'},\n",
       "    {'param_name': 'captionstr, tuple, optional',\n",
       "     'param_desc': 'If string, the LaTeX table caption included as: \\\\caption{<caption>}.\\nIf tuple, i.e (“full caption”, “short caption”), the caption included\\nas: \\\\caption[<caption[1]>]{<caption[0]>}.\\n'},\n",
       "    {'param_name': 'sparse_indexbool, optional',\n",
       "     'param_desc': 'Whether to sparsify the display of a hierarchical index. Setting to False\\nwill display each explicit level element in a hierarchical key for each row.\\nDefaults to pandas.options.styler.sparse.index, which is True.\\n'},\n",
       "    {'param_name': 'sparse_columnsbool, optional',\n",
       "     'param_desc': 'Whether to sparsify the display of a hierarchical index. Setting to False\\nwill display each explicit level element in a hierarchical key for each\\ncolumn. Defaults to pandas.options.styler.sparse.columns, which\\nis True.\\n'},\n",
       "    {'param_name': 'multirow_align{“c”, “t”, “b”, “naive”}, optional',\n",
       "     'param_desc': 'If sparsifying hierarchical MultiIndexes whether to align text centrally,\\nat the top or bottom using the multirow package. If not given defaults to\\npandas.options.styler.latex.multirow_align, which is “c”.\\nIf “naive” is given renders without multirow.\\n\\nChanged in version 1.4.0.\\n\\n'},\n",
       "    {'param_name': 'multicol_align{“r”, “c”, “l”, “naive-l”, “naive-r”}, optional',\n",
       "     'param_desc': 'If sparsifying hierarchical MultiIndex columns whether to align text at\\nthe left, centrally, or at the right. If not given defaults to\\npandas.options.styler.latex.multicol_align, which is “r”.\\nIf a naive option is given renders without multicol.\\nPipe decorators can also be added to non-naive values to draw vertical\\nrules, e.g. “|r” will draw a rule on the left side of right aligned merged\\ncells.\\n\\nChanged in version 1.4.0.\\n\\n'},\n",
       "    {'param_name': 'siunitxbool, default False',\n",
       "     'param_desc': 'Set to True to structure LaTeX compatible with the {siunitx} package.\\n'},\n",
       "    {'param_name': 'environmentstr, optional',\n",
       "     'param_desc': 'If given, the environment that will replace ‘table’ in \\\\\\\\begin{table}.\\nIf ‘longtable’ is specified then a more suitable template is\\nrendered. If not given defaults to\\npandas.options.styler.latex.environment, which is None.\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "    {'param_name': 'encodingstr, optional',\n",
       "     'param_desc': 'Character encoding setting. Defaults\\nto pandas.options.styler.render.encoding, which is “utf-8”.\\n'},\n",
       "    {'param_name': 'convert_cssbool, default False',\n",
       "     'param_desc': 'Convert simple cell-styles from CSS to LaTeX format. Any CSS not found in\\nconversion table is dropped. A style can be forced by adding option\\n–latex. See notes.\\n'}]},\n",
       "  {'function_name': 'pandas.read_hdf',\n",
       "   'full_function': \"pandas.read_hdf(path_or_buf, key=None, mode='r', errors='strict', where=None, start=None, stop=None, columns=None, iterator=False, chunksize=None, **kwargs)\",\n",
       "   'function_text': 'Read from the store, close it if we opened it.',\n",
       "   'parameter_names_desc': [{'param_name': 'path_or_bufstr, path object, pandas.HDFStore',\n",
       "     'param_desc': 'Any valid string path is acceptable. Only supports the local file system,\\nremote URLs and file-like objects are not supported.\\nIf you want to pass in a path object, pandas accepts any\\nos.PathLike.\\nAlternatively, pandas accepts an open pandas.HDFStore object.\\n'},\n",
       "    {'param_name': 'keyobject, optional',\n",
       "     'param_desc': 'The group identifier in the store. Can be omitted if the HDF file\\ncontains a single pandas object.\\n'},\n",
       "    {'param_name': 'mode{‘r’, ‘r+’, ‘a’}, default ‘r’',\n",
       "     'param_desc': 'Mode to use when opening the file. Ignored if path_or_buf is a\\npandas.HDFStore. Default is ‘r’.\\n'},\n",
       "    {'param_name': 'errorsstr, default ‘strict’',\n",
       "     'param_desc': 'Specifies how encoding and decoding errors are to be handled.\\nSee the errors argument for open() for a full list\\nof options.\\n'},\n",
       "    {'param_name': 'wherelist, optional',\n",
       "     'param_desc': 'A list of Term (or convertible) objects.\\n'},\n",
       "    {'param_name': 'startint, optional',\n",
       "     'param_desc': 'Row number to start selection.\\n'},\n",
       "    {'param_name': 'stopint, optional',\n",
       "     'param_desc': 'Row number to stop selection.\\n'},\n",
       "    {'param_name': 'columnslist, optional',\n",
       "     'param_desc': 'A list of columns names to return.\\n'},\n",
       "    {'param_name': 'iteratorbool, optional',\n",
       "     'param_desc': 'Return an iterator object.\\n'},\n",
       "    {'param_name': 'chunksizeint, optional',\n",
       "     'param_desc': 'Number of rows to include in an iteration when using an iterator.\\n'}]},\n",
       "  {'function_name': 'pandas.HDFStore.append',\n",
       "   'full_function': \"HDFStore.append(key, value, format=None, axes=None, index=True, append=True, complib=None, complevel=None, columns=None, min_itemsize=None, nan_rep=None, chunksize=None, expectedrows=None, dropna=None, data_columns=None, encoding=None, errors='strict')\",\n",
       "   'function_text': 'Append to Table in file.',\n",
       "   'parameter_names_desc': [{'param_name': 'keystr', 'param_desc': ''},\n",
       "    {'param_name': 'value{Series, DataFrame}', 'param_desc': ''},\n",
       "    {'param_name': 'format‘table’ is the default',\n",
       "     'param_desc': \"Format to use when storing object in HDFStore. Value can be one of:\\n\\n'table'Table format. Write as a PyTables Table structure which may perform\\nworse but allow more flexible operations like searching / selecting\\nsubsets of the data.\\n\\n\\n\"},\n",
       "    {'param_name': \"'table'\",\n",
       "     'param_desc': 'Table format. Write as a PyTables Table structure which may perform\\nworse but allow more flexible operations like searching / selecting\\nsubsets of the data.\\n'},\n",
       "    {'param_name': 'indexbool, default True',\n",
       "     'param_desc': 'Write DataFrame index as a column.\\n'},\n",
       "    {'param_name': 'appendbool, default True',\n",
       "     'param_desc': 'Append the input data to the existing.\\n'},\n",
       "    {'param_name': 'data_columnslist of columns, or True, default None',\n",
       "     'param_desc': 'List of columns to create as indexed data columns for on-disk\\nqueries, or True to use all columns. By default only the axes\\nof the object are indexed. See here.\\n'},\n",
       "    {'param_name': 'min_itemsizedict of columns that specify minimum str sizes',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'nan_repstr to use as str nan representation',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'chunksizesize to chunk the writing', 'param_desc': ''},\n",
       "    {'param_name': 'expectedrowsexpected TOTAL row size of this table',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'encodingdefault None, provide an encoding for str',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'dropnabool, default False, optional',\n",
       "     'param_desc': 'Do not write an ALL nan row to the store settable\\nby the option ‘io.hdf.dropna_table’.\\n'}]},\n",
       "  {'function_name': 'pandas.HDFStore.select',\n",
       "   'full_function': 'HDFStore.select(key, where=None, start=None, stop=None, columns=None, iterator=False, chunksize=None, auto_close=False)',\n",
       "   'function_text': 'Retrieve pandas object stored in file, optionally based on where criteria.',\n",
       "   'parameter_names_desc': [{'param_name': 'keystr',\n",
       "     'param_desc': 'Object being retrieved from file.\\n'},\n",
       "    {'param_name': 'wherelist or None',\n",
       "     'param_desc': 'List of Term (or convertible) objects, optional.\\n'},\n",
       "    {'param_name': 'startint or None',\n",
       "     'param_desc': 'Row number to start selection.\\n'},\n",
       "    {'param_name': 'stopint, default None',\n",
       "     'param_desc': 'Row number to stop selection.\\n'},\n",
       "    {'param_name': 'columnslist or None',\n",
       "     'param_desc': 'A list of columns that if not None, will limit the return columns.\\n'},\n",
       "    {'param_name': 'iteratorbool or False',\n",
       "     'param_desc': 'Returns an iterator.\\n'},\n",
       "    {'param_name': 'chunksizeint or None',\n",
       "     'param_desc': 'Number or rows to include in iteration, return an iterator.\\n'},\n",
       "    {'param_name': 'auto_closebool or False',\n",
       "     'param_desc': 'Should automatically close the store when finished.\\n'}]},\n",
       "  {'function_name': 'pandas.HDFStore.keys',\n",
       "   'full_function': \"HDFStore.keys(include='pandas')\",\n",
       "   'function_text': 'Return a list of keys corresponding to objects stored in HDFStore.',\n",
       "   'parameter_names_desc': [{'param_name': 'raises ValueError if kind has an illegal value',\n",
       "     'param_desc': ''}]},\n",
       "  {'function_name': 'pandas.HDFStore.walk',\n",
       "   'full_function': \"HDFStore.walk(where='/')\",\n",
       "   'function_text': 'Walk the pytables group hierarchy for pandas objects.',\n",
       "   'parameter_names_desc': [{'param_name': 'wherestr, default “/”',\n",
       "     'param_desc': 'Group where to start walking.\\n'}]},\n",
       "  {'function_name': 'pandas.HDFStore.put',\n",
       "   'full_function': \"HDFStore.put(key, value, format=None, index=True, append=False, complib=None, complevel=None, min_itemsize=None, nan_rep=None, data_columns=None, encoding=None, errors='strict', track_times=True, dropna=False)\",\n",
       "   'function_text': 'Store object in HDFStore.',\n",
       "   'parameter_names_desc': [{'param_name': 'keystr', 'param_desc': ''},\n",
       "    {'param_name': 'value{Series, DataFrame}', 'param_desc': ''},\n",
       "    {'param_name': 'format‘fixed(f)|table(t)’, default is ‘fixed’',\n",
       "     'param_desc': \"Format to use when storing object in HDFStore. Value can be one of:\\n\\n'fixed'Fixed format. Fast writing/reading. Not-appendable, nor searchable.\\n\\n'table'Table format. Write as a PyTables Table structure which may perform\\nworse but allow more flexible operations like searching / selecting\\nsubsets of the data.\\n\\n\\n\"},\n",
       "    {'param_name': \"'fixed'\",\n",
       "     'param_desc': 'Fixed format. Fast writing/reading. Not-appendable, nor searchable.\\n'},\n",
       "    {'param_name': \"'table'\",\n",
       "     'param_desc': 'Table format. Write as a PyTables Table structure which may perform\\nworse but allow more flexible operations like searching / selecting\\nsubsets of the data.\\n'},\n",
       "    {'param_name': 'indexbool, default True',\n",
       "     'param_desc': 'Write DataFrame index as a column.\\n'},\n",
       "    {'param_name': 'appendbool, default False',\n",
       "     'param_desc': 'This will force Table format, append the input data to the existing.\\n'},\n",
       "    {'param_name': 'data_columnslist of columns or True, default None',\n",
       "     'param_desc': 'List of columns to create as data columns, or True to use all columns.\\nSee here.\\n'},\n",
       "    {'param_name': 'encodingstr, default None',\n",
       "     'param_desc': 'Provide an encoding for strings.\\n'},\n",
       "    {'param_name': 'track_timesbool, default True',\n",
       "     'param_desc': 'Parameter is propagated to ‘create_table’ method of ‘PyTables’.\\nIf set to False it enables to have the same h5 files (same hashes)\\nindependent on creation time.\\n'},\n",
       "    {'param_name': 'dropnabool, default False, optional',\n",
       "     'param_desc': 'Remove missing values.\\n'}]},\n",
       "  {'function_name': 'pandas.HDFStore.get',\n",
       "   'full_function': 'HDFStore.get(key)',\n",
       "   'function_text': 'Retrieve pandas object stored in file.',\n",
       "   'parameter_names_desc': [{'param_name': 'keystr', 'param_desc': ''}]},\n",
       "  {'function_name': 'pandas.HDFStore.info',\n",
       "   'full_function': 'HDFStore.info()',\n",
       "   'function_text': 'Print detailed information on the store.',\n",
       "   'parameter_names_desc': [{'param_name': 'str', 'param_desc': ''}]},\n",
       "  {'function_name': 'pandas.HDFStore.groups',\n",
       "   'full_function': 'HDFStore.groups()',\n",
       "   'function_text': 'Return a list of all the top-level nodes.',\n",
       "   'parameter_names_desc': [{'param_name': 'list',\n",
       "     'param_desc': 'List of objects.\\n'}]},\n",
       "  {'function_name': 'pandas.read_feather',\n",
       "   'full_function': 'pandas.read_feather(path, columns=None, use_threads=True, storage_options=None, dtype_backend=_NoDefault.no_default)',\n",
       "   'function_text': 'Load a feather-format object from the file path.',\n",
       "   'parameter_names_desc': [{'param_name': 'pathstr, path object, or file-like object',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary read() function. The string could be a URL.\\nValid URL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be: file://localhost/path/to/table.feather.\\n'},\n",
       "    {'param_name': 'columnssequence, default None',\n",
       "     'param_desc': 'If not provided, all columns are read.\\n'},\n",
       "    {'param_name': 'use_threadsbool, default True',\n",
       "     'param_desc': 'Whether to parallelize reading using multiple threads.\\n'},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.DataFrame.to_feather',\n",
       "   'full_function': 'DataFrame.to_feather(path, **kwargs)',\n",
       "   'function_text': 'Write a DataFrame to the binary Feather format.',\n",
       "   'parameter_names_desc': [{'param_name': 'pathstr, path object, file-like object',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary write() function. If a string or a path,\\nit will be used as Root Directory path when writing a partitioned dataset.\\n'}]},\n",
       "  {'function_name': 'pandas.read_parquet',\n",
       "   'full_function': \"pandas.read_parquet(path, engine='auto', columns=None, storage_options=None, use_nullable_dtypes=_NoDefault.no_default, dtype_backend=_NoDefault.no_default, filesystem=None, filters=None, **kwargs)\",\n",
       "   'function_text': 'Load a parquet object from the file path, returning a DataFrame.',\n",
       "   'parameter_names_desc': [{'param_name': 'pathstr, path object or file-like object',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary read() function.\\nThe string could be a URL. Valid URL schemes include http, ftp, s3,\\ngs, and file. For file URLs, a host is expected. A local file could be:\\nfile://localhost/path/to/table.parquet.\\nA file URL can also be a path to a directory that contains multiple\\npartitioned parquet files. Both pyarrow and fastparquet support\\npaths to directories as well as file URLs. A directory path could be:\\nfile://localhost/path/to/tables or s3://bucket/partition_dir.\\n'},\n",
       "    {'param_name': 'engine{‘auto’, ‘pyarrow’, ‘fastparquet’}, default ‘auto’',\n",
       "     'param_desc': \"Parquet library to use. If ‘auto’, then the option\\nio.parquet.engine is used. The default io.parquet.engine\\nbehavior is to try ‘pyarrow’, falling back to ‘fastparquet’ if\\n‘pyarrow’ is unavailable.\\nWhen using the 'pyarrow' engine and no storage options are provided\\nand a filesystem is implemented by both pyarrow.fs and fsspec\\n(e.g. “s3://”), then the pyarrow.fs filesystem is attempted first.\\nUse the filesystem keyword with an instantiated fsspec filesystem\\nif you wish to use its implementation.\\n\"},\n",
       "    {'param_name': 'columnslist, default=None',\n",
       "     'param_desc': 'If not None, only these columns will be read from the file.\\n'},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n\\nNew in version 1.3.0.\\n\\n'},\n",
       "    {'param_name': 'use_nullable_dtypesbool, default False',\n",
       "     'param_desc': 'If True, use dtypes that use pd.NA as missing value indicator\\nfor the resulting DataFrame. (only applicable for the pyarrow\\nengine)\\nAs new dtypes are added that support pd.NA in the future, the\\noutput with this option will change to use those dtypes.\\nNote: this is an experimental option, and behaviour (e.g. additional\\nsupport dtypes) may change without notice.\\n\\nDeprecated since version 2.0.\\n\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "    {'param_name': 'filesystemfsspec or pyarrow filesystem, default None',\n",
       "     'param_desc': 'Filesystem object to use when reading the parquet file. Only implemented\\nfor engine=\"pyarrow\".\\n\\nNew in version 2.1.0.\\n\\n'},\n",
       "    {'param_name': 'filtersList[Tuple] or List[List[Tuple]], default None',\n",
       "     'param_desc': 'To filter out data.\\nFilter syntax: [[(column, op, val), …],…]\\nwhere op is [==, =, >, >=, <, <=, !=, in, not in]\\nThe innermost tuples are transposed into a set of filters applied\\nthrough an AND operation.\\nThe outer list combines these sets of filters through an OR\\noperation.\\nA single list of tuples can also be used, meaning that no OR\\noperation between set of filters is to be conducted.\\nUsing this argument will NOT result in row-wise filtering of the final\\npartitions unless engine=\"pyarrow\" is also specified. For\\nother engines, filtering is only performed at the partition level, that is,\\nto prevent the loading of some row-groups and/or files.\\n\\nNew in version 2.1.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.DataFrame.to_parquet',\n",
       "   'full_function': \"DataFrame.to_parquet(path=None, *, engine='auto', compression='snappy', index=None, partition_cols=None, storage_options=None, **kwargs)\",\n",
       "   'function_text': 'Write a DataFrame to the binary parquet format.',\n",
       "   'parameter_names_desc': [{'param_name': 'pathstr, path object, file-like object, or None, default None',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary write() function. If None, the result is\\nreturned as bytes. If a string or path, it will be used as Root Directory\\npath when writing a partitioned dataset.\\n'},\n",
       "    {'param_name': 'engine{‘auto’, ‘pyarrow’, ‘fastparquet’}, default ‘auto’',\n",
       "     'param_desc': 'Parquet library to use. If ‘auto’, then the option\\nio.parquet.engine is used. The default io.parquet.engine\\nbehavior is to try ‘pyarrow’, falling back to ‘fastparquet’ if\\n‘pyarrow’ is unavailable.\\n'},\n",
       "    {'param_name': 'compressionstr or None, default ‘snappy’',\n",
       "     'param_desc': 'Name of the compression to use. Use None for no compression.\\nSupported options: ‘snappy’, ‘gzip’, ‘brotli’, ‘lz4’, ‘zstd’.\\n'},\n",
       "    {'param_name': 'indexbool, default None',\n",
       "     'param_desc': 'If True, include the dataframe’s index(es) in the file output.\\nIf False, they will not be written to the file.\\nIf None, similar to True the dataframe’s index(es)\\nwill be saved. However, instead of being saved as values,\\nthe RangeIndex will be stored as a range in the metadata so it\\ndoesn’t require much space and is faster. Other indexes will\\nbe included as columns in the file output.\\n'},\n",
       "    {'param_name': 'partition_colslist, optional, default None',\n",
       "     'param_desc': 'Column names by which to partition the dataset.\\nColumns are partitioned in the order they are given.\\nMust be None if path is not a string.\\n'},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'}]},\n",
       "  {'function_name': 'pandas.read_orc',\n",
       "   'full_function': 'pandas.read_orc(path, columns=None, dtype_backend=_NoDefault.no_default, filesystem=None, **kwargs)',\n",
       "   'function_text': 'Load an ORC object from the file path, returning a DataFrame.',\n",
       "   'parameter_names_desc': [{'param_name': 'pathstr, path object, or file-like object',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary read() function. The string could be a URL.\\nValid URL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be:\\nfile://localhost/path/to/table.orc.\\n'},\n",
       "    {'param_name': 'columnslist, default None',\n",
       "     'param_desc': 'If not None, only these columns will be read from the file.\\nOutput always follows the ordering of the file and not the columns list.\\nThis mirrors the original behaviour of\\npyarrow.orc.ORCFile.read().\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "    {'param_name': 'filesystemfsspec or pyarrow filesystem, default None',\n",
       "     'param_desc': 'Filesystem object to use when reading the parquet file.\\n\\nNew in version 2.1.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.DataFrame.to_orc',\n",
       "   'full_function': \"DataFrame.to_orc(path=None, *, engine='pyarrow', index=None, engine_kwargs=None)\",\n",
       "   'function_text': 'Write a DataFrame to the ORC format.',\n",
       "   'parameter_names_desc': [{'param_name': 'NotImplementedError',\n",
       "     'param_desc': 'Dtype of one or more columns is category, unsigned integers, interval,\\nperiod or sparse.\\n'},\n",
       "    {'param_name': 'ValueError', 'param_desc': 'engine is not pyarrow.\\n'}]},\n",
       "  {'function_name': 'pandas.read_sas',\n",
       "   'full_function': \"pandas.read_sas(filepath_or_buffer, *, format=None, index=None, encoding=None, chunksize=None, iterator=False, compression='infer')\",\n",
       "   'function_text': 'Read SAS files stored as either XPORT or SAS7BDAT format files.',\n",
       "   'parameter_names_desc': [{'param_name': 'filepath_or_bufferstr, path object, or file-like object',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary read() function. The string could be a URL.\\nValid URL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be:\\nfile://localhost/path/to/table.sas7bdat.\\n'},\n",
       "    {'param_name': 'formatstr {‘xport’, ‘sas7bdat’} or None',\n",
       "     'param_desc': 'If None, file format is inferred from file extension. If ‘xport’ or\\n‘sas7bdat’, uses the corresponding format.\\n'},\n",
       "    {'param_name': 'indexidentifier of index column, defaults to None',\n",
       "     'param_desc': 'Identifier of column that should be used as index of the DataFrame.\\n'},\n",
       "    {'param_name': 'encodingstr, default is None',\n",
       "     'param_desc': 'Encoding for text data. If None, text data are stored as raw bytes.\\n'},\n",
       "    {'param_name': 'chunksizeint',\n",
       "     'param_desc': 'Read file chunksize lines at a time, returns iterator.\\n'},\n",
       "    {'param_name': 'iteratorbool, defaults to False',\n",
       "     'param_desc': 'If True, returns an iterator for reading the file incrementally.\\n'},\n",
       "    {'param_name': 'compressionstr or dict, default ‘infer’',\n",
       "     'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\"}]},\n",
       "  {'function_name': 'pandas.read_spss',\n",
       "   'full_function': 'pandas.read_spss(path, usecols=None, convert_categoricals=True, dtype_backend=_NoDefault.no_default)',\n",
       "   'function_text': 'Load an SPSS file from the file path, returning a DataFrame.',\n",
       "   'parameter_names_desc': [{'param_name': 'pathstr or Path',\n",
       "     'param_desc': 'File path.\\n'},\n",
       "    {'param_name': 'usecolslist-like, optional',\n",
       "     'param_desc': 'Return a subset of the columns. If None, return all columns.\\n'},\n",
       "    {'param_name': 'convert_categoricalsbool, default is True',\n",
       "     'param_desc': 'Convert categorical columns into pd.Categorical.\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.read_sql_table',\n",
       "   'full_function': 'pandas.read_sql_table(table_name, con, schema=None, index_col=None, coerce_float=True, parse_dates=None, columns=None, chunksize=None, dtype_backend=_NoDefault.no_default)',\n",
       "   'function_text': 'Read SQL database table into a DataFrame.',\n",
       "   'parameter_names_desc': [{'param_name': 'table_namestr',\n",
       "     'param_desc': 'Name of SQL table in database.\\n'},\n",
       "    {'param_name': 'conSQLAlchemy connectable or str',\n",
       "     'param_desc': 'A database URI could be provided as str.\\nSQLite DBAPI connection mode not supported.\\n'},\n",
       "    {'param_name': 'schemastr, default None',\n",
       "     'param_desc': 'Name of SQL schema in database to query (if database flavor\\nsupports this). Uses default schema if None (default).\\n'},\n",
       "    {'param_name': 'index_colstr or list of str, optional, default: None',\n",
       "     'param_desc': 'Column(s) to set as index(MultiIndex).\\n'},\n",
       "    {'param_name': 'coerce_floatbool, default True',\n",
       "     'param_desc': 'Attempts to convert values of non-string, non-numeric objects (like\\ndecimal.Decimal) to floating point. Can result in loss of Precision.\\n'},\n",
       "    {'param_name': 'parse_dateslist or dict, default None',\n",
       "     'param_desc': '\\nList of column names to parse as dates.\\nDict of {column_name: format string} where format string is\\nstrftime compatible in case of parsing string times or is one of\\n(D, s, ns, ms, us) in case of parsing integer timestamps.\\nDict of {column_name: arg dict}, where the arg dict corresponds\\nto the keyword arguments of pandas.to_datetime()\\nEspecially useful with databases without native Datetime support,\\nsuch as SQLite.\\n\\n'},\n",
       "    {'param_name': 'columnslist, default None',\n",
       "     'param_desc': 'List of column names to select from SQL table.\\n'},\n",
       "    {'param_name': 'chunksizeint, default None',\n",
       "     'param_desc': 'If specified, returns an iterator where chunksize is the number of\\nrows to include in each chunk.\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.read_sql',\n",
       "   'full_function': 'pandas.read_sql(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, columns=None, chunksize=None, dtype_backend=_NoDefault.no_default, dtype=None)',\n",
       "   'function_text': 'Read SQL query or database table into a DataFrame.',\n",
       "   'parameter_names_desc': [{'param_name': 'sqlstr or SQLAlchemy Selectable (select or text object)',\n",
       "     'param_desc': 'SQL query to be executed or a table name.\\n'},\n",
       "    {'param_name': 'conADBC Connection, SQLAlchemy connectable, str, or sqlite3 connection',\n",
       "     'param_desc': 'ADBC provides high performance I/O with native type support, where available.\\nUsing SQLAlchemy makes it possible to use any DB supported by that\\nlibrary. If a DBAPI2 object, only sqlite3 is supported. The user is responsible\\nfor engine disposal and connection closure for the ADBC connection and\\nSQLAlchemy connectable; str connections are closed automatically. See\\nhere.\\n'},\n",
       "    {'param_name': 'index_colstr or list of str, optional, default: None',\n",
       "     'param_desc': 'Column(s) to set as index(MultiIndex).\\n'},\n",
       "    {'param_name': 'coerce_floatbool, default True',\n",
       "     'param_desc': 'Attempts to convert values of non-string, non-numeric objects (like\\ndecimal.Decimal) to floating point, useful for SQL result sets.\\n'},\n",
       "    {'param_name': 'paramslist, tuple or dict, optional, default: None',\n",
       "     'param_desc': 'List of parameters to pass to execute method. The syntax used\\nto pass parameters is database driver dependent. Check your\\ndatabase driver documentation for which of the five syntax styles,\\ndescribed in PEP 249’s paramstyle, is supported.\\nEg. for psycopg2, uses %(name)s so use params={‘name’ : ‘value’}.\\n'},\n",
       "    {'param_name': 'parse_dateslist or dict, default: None',\n",
       "     'param_desc': '\\nList of column names to parse as dates.\\nDict of {column_name: format string} where format string is\\nstrftime compatible in case of parsing string times, or is one of\\n(D, s, ns, ms, us) in case of parsing integer timestamps.\\nDict of {column_name: arg dict}, where the arg dict corresponds\\nto the keyword arguments of pandas.to_datetime()\\nEspecially useful with databases without native Datetime support,\\nsuch as SQLite.\\n\\n'},\n",
       "    {'param_name': 'columnslist, default: None',\n",
       "     'param_desc': 'List of column names to select from SQL table (only used when reading\\na table).\\n'},\n",
       "    {'param_name': 'chunksizeint, default None',\n",
       "     'param_desc': 'If specified, return an iterator where chunksize is the\\nnumber of rows to include in each chunk.\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "    {'param_name': 'dtypeType name or dict of columns',\n",
       "     'param_desc': 'Data type for data or columns. E.g. np.float64 or\\n{‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’}.\\nThe argument is ignored if a table is passed instead of a query.\\n\\nNew in version 2.0.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.read_sql_query',\n",
       "   'full_function': 'pandas.read_sql_query(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, chunksize=None, dtype=None, dtype_backend=_NoDefault.no_default)',\n",
       "   'function_text': 'Read SQL query into a DataFrame.',\n",
       "   'parameter_names_desc': [{'param_name': 'sqlstr SQL query or SQLAlchemy Selectable (select or text object)',\n",
       "     'param_desc': 'SQL query to be executed.\\n'},\n",
       "    {'param_name': 'conSQLAlchemy connectable, str, or sqlite3 connection',\n",
       "     'param_desc': 'Using SQLAlchemy makes it possible to use any DB supported by that\\nlibrary. If a DBAPI2 object, only sqlite3 is supported.\\n'},\n",
       "    {'param_name': 'index_colstr or list of str, optional, default: None',\n",
       "     'param_desc': 'Column(s) to set as index(MultiIndex).\\n'},\n",
       "    {'param_name': 'coerce_floatbool, default True',\n",
       "     'param_desc': 'Attempts to convert values of non-string, non-numeric objects (like\\ndecimal.Decimal) to floating point. Useful for SQL result sets.\\n'},\n",
       "    {'param_name': 'paramslist, tuple or mapping, optional, default: None',\n",
       "     'param_desc': 'List of parameters to pass to execute method. The syntax used\\nto pass parameters is database driver dependent. Check your\\ndatabase driver documentation for which of the five syntax styles,\\ndescribed in PEP 249’s paramstyle, is supported.\\nEg. for psycopg2, uses %(name)s so use params={‘name’ : ‘value’}.\\n'},\n",
       "    {'param_name': 'parse_dateslist or dict, default: None',\n",
       "     'param_desc': '\\nList of column names to parse as dates.\\nDict of {column_name: format string} where format string is\\nstrftime compatible in case of parsing string times, or is one of\\n(D, s, ns, ms, us) in case of parsing integer timestamps.\\nDict of {column_name: arg dict}, where the arg dict corresponds\\nto the keyword arguments of pandas.to_datetime()\\nEspecially useful with databases without native Datetime support,\\nsuch as SQLite.\\n\\n'},\n",
       "    {'param_name': 'chunksizeint, default None',\n",
       "     'param_desc': 'If specified, return an iterator where chunksize is the number of\\nrows to include in each chunk.\\n'},\n",
       "    {'param_name': 'dtypeType name or dict of columns',\n",
       "     'param_desc': 'Data type for data or columns. E.g. np.float64 or\\n{‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’}.\\n\\nNew in version 1.3.0.\\n\\n'},\n",
       "    {'param_name': 'dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "     'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.DataFrame.to_sql',\n",
       "   'full_function': \"DataFrame.to_sql(name, con, *, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None, method=None)\",\n",
       "   'function_text': 'Write records stored in a DataFrame to a SQL database.',\n",
       "   'parameter_names_desc': [{'param_name': 'ValueError',\n",
       "     'param_desc': 'When the table already exists and if_exists is ‘fail’ (the\\ndefault).\\n'}]},\n",
       "  {'function_name': 'pandas.read_gbq',\n",
       "   'full_function': 'pandas.read_gbq(query, project_id=None, index_col=None, col_order=None, reauth=False, auth_local_webserver=True, dialect=None, location=None, configuration=None, credentials=None, use_bqstorage_api=None, max_results=None, progress_bar_type=None)',\n",
       "   'function_text': 'Load data from Google BigQuery.',\n",
       "   'parameter_names_desc': [{'param_name': 'querystr',\n",
       "     'param_desc': 'SQL-Like Query to return data values.\\n'},\n",
       "    {'param_name': 'project_idstr, optional',\n",
       "     'param_desc': 'Google BigQuery Account project ID. Optional when available from\\nthe environment.\\n'},\n",
       "    {'param_name': 'index_colstr, optional',\n",
       "     'param_desc': 'Name of result column to use for index in results DataFrame.\\n'},\n",
       "    {'param_name': 'col_orderlist(str), optional',\n",
       "     'param_desc': 'List of BigQuery column names in the desired order for results\\nDataFrame.\\n'},\n",
       "    {'param_name': 'reauthbool, default False',\n",
       "     'param_desc': 'Force Google BigQuery to re-authenticate the user. This is useful\\nif multiple accounts are used.\\n'},\n",
       "    {'param_name': 'auth_local_webserverbool, default True',\n",
       "     'param_desc': 'Use the local webserver flow instead of the console flow\\nwhen getting user credentials.\\nNew in version 0.2.0 of pandas-gbq.\\n\\nChanged in version 1.5.0: Default value is changed to True. Google has deprecated the\\nauth_local_webserver = False “out of band” (copy-paste)\\nflow.\\n\\n'},\n",
       "    {'param_name': 'dialectstr, default ‘legacy’',\n",
       "     'param_desc': \"Note: The default value is changing to ‘standard’ in a future version.\\nSQL syntax dialect to use. Value can be one of:\\n\\n'legacy'Use BigQuery’s legacy SQL dialect. For more information see\\nBigQuery Legacy SQL Reference.\\n\\n'standard'Use BigQuery’s standard SQL, which is\\ncompliant with the SQL 2011 standard. For more information\\nsee BigQuery Standard SQL Reference.\\n\\n\\n\"},\n",
       "    {'param_name': \"'legacy'\",\n",
       "     'param_desc': 'Use BigQuery’s legacy SQL dialect. For more information see\\nBigQuery Legacy SQL Reference.\\n'},\n",
       "    {'param_name': \"'standard'\",\n",
       "     'param_desc': 'Use BigQuery’s standard SQL, which is\\ncompliant with the SQL 2011 standard. For more information\\nsee BigQuery Standard SQL Reference.\\n'},\n",
       "    {'param_name': 'locationstr, optional',\n",
       "     'param_desc': 'Location where the query job should run. See the BigQuery locations\\ndocumentation for a\\nlist of available locations. The location must match that of any\\ndatasets used in the query.\\nNew in version 0.5.0 of pandas-gbq.\\n'},\n",
       "    {'param_name': 'configurationdict, optional',\n",
       "     'param_desc': 'Query config parameters for job processing.\\nFor example:\\n\\nconfiguration = {‘query’: {‘useQueryCache’: False}}\\n\\nFor more information see BigQuery REST API Reference.\\n'},\n",
       "    {'param_name': 'credentialsgoogle.auth.credentials.Credentials, optional',\n",
       "     'param_desc': 'Credentials for accessing Google APIs. Use this parameter to override\\ndefault credentials, such as to use Compute Engine\\ngoogle.auth.compute_engine.Credentials or Service Account\\ngoogle.oauth2.service_account.Credentials directly.\\nNew in version 0.8.0 of pandas-gbq.\\n'},\n",
       "    {'param_name': 'use_bqstorage_apibool, default False',\n",
       "     'param_desc': 'Use the BigQuery Storage API to\\ndownload query results quickly, but at an increased cost. To use this\\nAPI, first enable it in the Cloud Console.\\nYou must also have the bigquery.readsessions.create\\npermission on the project you are billing queries to.\\nThis feature requires version 0.10.0 or later of the pandas-gbq\\npackage. It also requires the google-cloud-bigquery-storage and\\nfastavro packages.\\n'},\n",
       "    {'param_name': 'max_resultsint, optional',\n",
       "     'param_desc': 'If set, limit the maximum number of rows to fetch from the query\\nresults.\\n'},\n",
       "    {'param_name': 'progress_bar_typeOptional, str',\n",
       "     'param_desc': \"If set, use the tqdm library to\\ndisplay a progress bar while the data downloads. Install the\\ntqdm package to use this feature.\\nPossible values of progress_bar_type include:\\n\\nNoneNo progress bar.\\n\\n'tqdm'Use the tqdm.tqdm() function to print a progress bar\\nto sys.stderr.\\n\\n'tqdm_notebook'Use the tqdm.tqdm_notebook() function to display a\\nprogress bar as a Jupyter notebook widget.\\n\\n'tqdm_gui'Use the tqdm.tqdm_gui() function to display a\\nprogress bar as a graphical dialog box.\\n\\n\\n\"},\n",
       "    {'param_name': 'None', 'param_desc': 'No progress bar.\\n'},\n",
       "    {'param_name': \"'tqdm'\",\n",
       "     'param_desc': 'Use the tqdm.tqdm() function to print a progress bar\\nto sys.stderr.\\n'},\n",
       "    {'param_name': \"'tqdm_notebook'\",\n",
       "     'param_desc': 'Use the tqdm.tqdm_notebook() function to display a\\nprogress bar as a Jupyter notebook widget.\\n'},\n",
       "    {'param_name': \"'tqdm_gui'\",\n",
       "     'param_desc': 'Use the tqdm.tqdm_gui() function to display a\\nprogress bar as a graphical dialog box.\\n'}]},\n",
       "  {'function_name': 'pandas.read_stata',\n",
       "   'full_function': \"pandas.read_stata(filepath_or_buffer, *, convert_dates=True, convert_categoricals=True, index_col=None, convert_missing=False, preserve_dtypes=True, columns=None, order_categoricals=True, chunksize=None, iterator=False, compression='infer', storage_options=None)\",\n",
       "   'function_text': 'Read Stata file into DataFrame.',\n",
       "   'parameter_names_desc': [{'param_name': 'filepath_or_bufferstr, path object or file-like object',\n",
       "     'param_desc': 'Any valid string path is acceptable. The string could be a URL. Valid\\nURL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be: file://localhost/path/to/table.dta.\\nIf you want to pass in a path object, pandas accepts any os.PathLike.\\nBy file-like object, we refer to objects with a read() method,\\nsuch as a file handle (e.g. via builtin open function)\\nor StringIO.\\n'},\n",
       "    {'param_name': 'convert_datesbool, default True',\n",
       "     'param_desc': 'Convert date variables to DataFrame time values.\\n'},\n",
       "    {'param_name': 'convert_categoricalsbool, default True',\n",
       "     'param_desc': 'Read value labels and convert columns to Categorical/Factor variables.\\n'},\n",
       "    {'param_name': 'index_colstr, optional',\n",
       "     'param_desc': 'Column to set as index.\\n'},\n",
       "    {'param_name': 'convert_missingbool, default False',\n",
       "     'param_desc': 'Flag indicating whether to convert missing values to their Stata\\nrepresentations. If False, missing values are replaced with nan.\\nIf True, columns containing missing values are returned with\\nobject data types and missing values are represented by\\nStataMissingValue objects.\\n'},\n",
       "    {'param_name': 'preserve_dtypesbool, default True',\n",
       "     'param_desc': 'Preserve Stata datatypes. If False, numeric data are upcast to pandas\\ndefault types for foreign data (float64 or int64).\\n'},\n",
       "    {'param_name': 'columnslist or None',\n",
       "     'param_desc': 'Columns to retain. Columns will be returned in the given order. None\\nreturns all columns.\\n'},\n",
       "    {'param_name': 'order_categoricalsbool, default True',\n",
       "     'param_desc': 'Flag indicating whether converted categorical data are ordered.\\n'},\n",
       "    {'param_name': 'chunksizeint, default None',\n",
       "     'param_desc': 'Return StataReader object for iterations, returns chunks with\\ngiven number of lines.\\n'},\n",
       "    {'param_name': 'iteratorbool, default False',\n",
       "     'param_desc': 'Return StataReader object.\\n'},\n",
       "    {'param_name': 'compressionstr or dict, default ‘infer’',\n",
       "     'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\"},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'}]},\n",
       "  {'function_name': 'pandas.io.stata.StataReader.variable_labels',\n",
       "   'full_function': 'StataReader.variable_labels()',\n",
       "   'function_text': 'Return a dict associating each variable name with corresponding label.',\n",
       "   'parameter_names_desc': [{'param_name': 'dict', 'param_desc': ''}]},\n",
       "  {'function_name': 'pandas.DataFrame.to_stata',\n",
       "   'full_function': \"DataFrame.to_stata(path, *, convert_dates=None, write_index=True, byteorder=None, time_stamp=None, data_label=None, variable_labels=None, version=114, convert_strl=None, compression='infer', storage_options=None, value_labels=None)\",\n",
       "   'function_text': 'Export DataFrame object to Stata dta format.',\n",
       "   'parameter_names_desc': [{'param_name': 'pathstr, path object, or buffer',\n",
       "     'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary write() function.\\n'},\n",
       "    {'param_name': 'convert_datesdict',\n",
       "     'param_desc': 'Dictionary mapping columns containing datetime types to stata\\ninternal format to use when writing the dates. Options are ‘tc’,\\n‘td’, ‘tm’, ‘tw’, ‘th’, ‘tq’, ‘ty’. Column can be either an integer\\nor a name. Datetime columns that do not have a conversion type\\nspecified will be converted to ‘tc’. Raises NotImplementedError if\\na datetime column has timezone information.\\n'},\n",
       "    {'param_name': 'write_indexbool',\n",
       "     'param_desc': 'Write the index to Stata dataset.\\n'},\n",
       "    {'param_name': 'byteorderstr',\n",
       "     'param_desc': 'Can be “>”, “<”, “little”, or “big”. default is sys.byteorder.\\n'},\n",
       "    {'param_name': 'time_stampdatetime',\n",
       "     'param_desc': 'A datetime to use as file creation date. Default is the current\\ntime.\\n'},\n",
       "    {'param_name': 'data_labelstr, optional',\n",
       "     'param_desc': 'A label for the data set. Must be 80 characters or smaller.\\n'},\n",
       "    {'param_name': 'variable_labelsdict',\n",
       "     'param_desc': 'Dictionary containing columns as keys and variable labels as\\nvalues. Each label must be 80 characters or smaller.\\n'},\n",
       "    {'param_name': 'version{114, 117, 118, 119, None}, default 114',\n",
       "     'param_desc': 'Version to use in the output dta file. Set to None to let pandas\\ndecide between 118 or 119 formats depending on the number of\\ncolumns in the frame. Version 114 can be read by Stata 10 and\\nlater. Version 117 can be read by Stata 13 or later. Version 118\\nis supported in Stata 14 and later. Version 119 is supported in\\nStata 15 and later. Version 114 limits string variables to 244\\ncharacters or fewer while versions 117 and later allow strings\\nwith lengths up to 2,000,000 characters. Versions 118 and 119\\nsupport Unicode characters, and version 119 supports more than\\n32,767 variables.\\nVersion 119 should usually only be used when the number of\\nvariables exceeds the capacity of dta format 118. Exporting\\nsmaller datasets in format 119 may have unintended consequences,\\nand, as of November 2020, Stata SE cannot read version 119 files.\\n'},\n",
       "    {'param_name': 'convert_strllist, optional',\n",
       "     'param_desc': 'List of column names to convert to string columns to Stata StrL\\nformat. Only available if version is 117. Storing strings in the\\nStrL format can produce smaller dta files if strings have more than\\n8 characters and values are repeated.\\n'},\n",
       "    {'param_name': 'compressionstr or dict, default ‘infer’',\n",
       "     'param_desc': \"For on-the-fly compression of the output data. If ‘infer’ and ‘path’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nSet to None for no compression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for faster compression and to create\\na reproducible gzip archive:\\ncompression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "    {'param_name': 'storage_optionsdict, optional',\n",
       "     'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "    {'param_name': 'value_labelsdict of dicts',\n",
       "     'param_desc': 'Dictionary containing columns as keys and dictionaries of column value\\nto labels as values. Labels for a single variable must be 32,000\\ncharacters or smaller.\\n\\nNew in version 1.4.0.\\n\\n'}]},\n",
       "  {'function_name': 'pandas.io.stata.StataReader.value_labels',\n",
       "   'full_function': 'StataReader.value_labels()',\n",
       "   'function_text': 'Return a nested dict associating each variable name to its value and label.',\n",
       "   'parameter_names_desc': [{'param_name': 'dict', 'param_desc': ''}]}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['io.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Objects',\n",
       " 'url': 'https://pandas.pydata.org/docs/reference/arrays.html#objects',\n",
       " 'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html#pandas.Timedelta',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.IntervalDtype.html#pandas.IntervalDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Float64Dtype.html#pandas.Float64Dtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.SparseDtype.html#pandas.SparseDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.BooleanDtype.html#pandas.BooleanDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.array.html#pandas.array',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.int8.html#pyarrow.int8',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.int32.html#pyarrow.int32',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.uint8.html#pyarrow.uint8',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.uint32.html#pyarrow.uint32',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.float32.html#pyarrow.float32',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.time32.html#pyarrow.time32',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.timestamp.html#pyarrow.timestamp',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.date64.html#pyarrow.date64',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.binary.html#pyarrow.binary',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.decimal128.html#pyarrow.decimal128',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.map_.html#pyarrow.map_',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.arrays.ArrowExtensionArray.html#pandas.arrays.ArrowExtensionArray',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html#pandas.Timestamp',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.asm8.html#pandas.Timestamp.asm8',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.dayofweek.html#pandas.Timestamp.dayofweek',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.dayofyear.html#pandas.Timestamp.dayofyear',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.days_in_month.html#pandas.Timestamp.days_in_month',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.fold.html#pandas.Timestamp.fold',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_leap_year.html#pandas.Timestamp.is_leap_year',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_month_start.html#pandas.Timestamp.is_month_start',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_quarter_start.html#pandas.Timestamp.is_quarter_start',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_year_start.html#pandas.Timestamp.is_year_start',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.microsecond.html#pandas.Timestamp.microsecond',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.minute.html#pandas.Timestamp.minute',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.nanosecond.html#pandas.Timestamp.nanosecond',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.resolution.html#pandas.Timestamp.resolution',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.tz.html#pandas.Timestamp.tz',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.unit.html#pandas.Timestamp.unit',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.week.html#pandas.Timestamp.week',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.year.html#pandas.Timestamp.year',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.as_unit.html#pandas.Timestamp.as_unit',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.ceil.html#pandas.Timestamp.ceil',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.ctime.html#pandas.Timestamp.ctime',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.day_name.html#pandas.Timestamp.day_name',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.floor.html#pandas.Timestamp.floor',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.fromtimestamp.html#pandas.Timestamp.fromtimestamp',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.isoformat.html#pandas.Timestamp.isoformat',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.month_name.html#pandas.Timestamp.month_name',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.now.html#pandas.Timestamp.now',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.round.html#pandas.Timestamp.round',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.strptime.html#pandas.Timestamp.strptime',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.timestamp.html#pandas.Timestamp.timestamp',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.timetz.html#pandas.Timestamp.timetz',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.to_numpy.html#pandas.Timestamp.to_numpy',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.to_period.html#pandas.Timestamp.to_period',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.today.html#pandas.Timestamp.today',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.tz_convert.html#pandas.Timestamp.tz_convert',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.tzname.html#pandas.Timestamp.tzname',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.utcnow.html#pandas.Timestamp.utcnow',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.utctimetuple.html#pandas.Timestamp.utctimetuple',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.arrays.DatetimeArray.html#pandas.arrays.DatetimeArray',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DatetimeTZDtype.html#pandas.DatetimeTZDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html#pandas.Timedelta',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.asm8.html#pandas.Timedelta.asm8',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.days.html#pandas.Timedelta.days',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.microseconds.html#pandas.Timedelta.microseconds',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.nanoseconds.html#pandas.Timedelta.nanoseconds',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.seconds.html#pandas.Timedelta.seconds',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.value.html#pandas.Timedelta.value',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.as_unit.html#pandas.Timedelta.as_unit',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.floor.html#pandas.Timedelta.floor',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.round.html#pandas.Timedelta.round',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.to_timedelta64.html#pandas.Timedelta.to_timedelta64',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.total_seconds.html#pandas.Timedelta.total_seconds',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.arrays.TimedeltaArray.html#pandas.arrays.TimedeltaArray',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.html#pandas.Period',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.day.html#pandas.Period.day',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.day_of_week.html#pandas.Period.day_of_week',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.day_of_year.html#pandas.Period.day_of_year',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.daysinmonth.html#pandas.Period.daysinmonth',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.freq.html#pandas.Period.freq',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.hour.html#pandas.Period.hour',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.minute.html#pandas.Period.minute',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.ordinal.html#pandas.Period.ordinal',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.qyear.html#pandas.Period.qyear',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.start_time.html#pandas.Period.start_time',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.weekday.html#pandas.Period.weekday',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.year.html#pandas.Period.year',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.asfreq.html#pandas.Period.asfreq',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.strftime.html#pandas.Period.strftime',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.arrays.PeriodArray.html#pandas.arrays.PeriodArray',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.PeriodDtype.html#pandas.PeriodDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Interval.html#pandas.Interval',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Interval.closed.html#pandas.Interval.closed',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Interval.closed_right.html#pandas.Interval.closed_right',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Interval.left.html#pandas.Interval.left',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Interval.mid.html#pandas.Interval.mid',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Interval.open_right.html#pandas.Interval.open_right',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Interval.right.html#pandas.Interval.right',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.arrays.IntervalArray.html#pandas.arrays.IntervalArray',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.IntervalDtype.html#pandas.IntervalDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.arrays.IntegerArray.html#pandas.arrays.IntegerArray',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Int8Dtype.html#pandas.Int8Dtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Int32Dtype.html#pandas.Int32Dtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.UInt8Dtype.html#pandas.UInt8Dtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.UInt32Dtype.html#pandas.UInt32Dtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.arrays.FloatingArray.html#pandas.arrays.FloatingArray',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Float32Dtype.html#pandas.Float32Dtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.CategoricalDtype.html#pandas.CategoricalDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.CategoricalDtype.categories.html#pandas.CategoricalDtype.categories',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.html#pandas.Categorical',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.from_codes.html#pandas.Categorical.from_codes',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.dtype.html#pandas.Categorical.dtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.ordered.html#pandas.Categorical.ordered',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.__array__.html#pandas.Categorical.__array__',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.arrays.SparseArray.html#pandas.arrays.SparseArray',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.SparseDtype.html#pandas.SparseDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.arrays.StringArray.html#pandas.arrays.StringArray',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.StringDtype.html#pandas.StringDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.arrays.BooleanArray.html#pandas.arrays.BooleanArray',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.BooleanDtype.html#pandas.BooleanDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.DatetimeTZDtype.html#pandas.DatetimeTZDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.PeriodDtype.html#pandas.PeriodDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Int64Dtype.html#pandas.Int64Dtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.CategoricalDtype.html#pandas.CategoricalDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.StringDtype.html#pandas.StringDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.bool_.html#pyarrow.bool_',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.int16.html#pyarrow.int16',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.int64.html#pyarrow.int64',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.uint16.html#pyarrow.uint16',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.uint64.html#pyarrow.uint64',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.float64.html#pyarrow.float64',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.time64.html#pyarrow.time64',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.date32.html#pyarrow.date32',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.duration.html#pyarrow.duration',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.string.html#pyarrow.string',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.list_.html#pyarrow.list_',\n",
       "  'https://pandas.pydata.org/docs/reference/https://arrow.apache.org/docs/python/generated/pyarrow.dictionary.html#pyarrow.dictionary',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.day.html#pandas.Timestamp.day',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.day_of_week.html#pandas.Timestamp.day_of_week',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.day_of_year.html#pandas.Timestamp.day_of_year',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.daysinmonth.html#pandas.Timestamp.daysinmonth',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.hour.html#pandas.Timestamp.hour',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_month_end.html#pandas.Timestamp.is_month_end',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_quarter_end.html#pandas.Timestamp.is_quarter_end',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_year_end.html#pandas.Timestamp.is_year_end',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.max.html#pandas.Timestamp.max',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.min.html#pandas.Timestamp.min',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.month.html#pandas.Timestamp.month',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.quarter.html#pandas.Timestamp.quarter',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.second.html#pandas.Timestamp.second',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.tzinfo.html#pandas.Timestamp.tzinfo',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.value.html#pandas.Timestamp.value',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.weekofyear.html#pandas.Timestamp.weekofyear',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.astimezone.html#pandas.Timestamp.astimezone',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.combine.html#pandas.Timestamp.combine',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.date.html#pandas.Timestamp.date',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.dst.html#pandas.Timestamp.dst',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.fromordinal.html#pandas.Timestamp.fromordinal',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.isocalendar.html#pandas.Timestamp.isocalendar',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.isoweekday.html#pandas.Timestamp.isoweekday',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.normalize.html#pandas.Timestamp.normalize',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.replace.html#pandas.Timestamp.replace',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.strftime.html#pandas.Timestamp.strftime',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.time.html#pandas.Timestamp.time',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.timetuple.html#pandas.Timestamp.timetuple',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.to_datetime64.html#pandas.Timestamp.to_datetime64',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.to_julian_date.html#pandas.Timestamp.to_julian_date',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.to_pydatetime.html#pandas.Timestamp.to_pydatetime',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.toordinal.html#pandas.Timestamp.toordinal',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.tz_localize.html#pandas.Timestamp.tz_localize',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.utcfromtimestamp.html#pandas.Timestamp.utcfromtimestamp',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.utcoffset.html#pandas.Timestamp.utcoffset',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.weekday.html#pandas.Timestamp.weekday',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.components.html#pandas.Timedelta.components',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.max.html#pandas.Timedelta.max',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.min.html#pandas.Timedelta.min',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.resolution.html#pandas.Timedelta.resolution',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.unit.html#pandas.Timedelta.unit',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.view.html#pandas.Timedelta.view',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.ceil.html#pandas.Timedelta.ceil',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.isoformat.html#pandas.Timedelta.isoformat',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.to_pytimedelta.html#pandas.Timedelta.to_pytimedelta',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.to_numpy.html#pandas.Timedelta.to_numpy',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.dayofweek.html#pandas.Period.dayofweek',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.dayofyear.html#pandas.Period.dayofyear',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.days_in_month.html#pandas.Period.days_in_month',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.end_time.html#pandas.Period.end_time',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.freqstr.html#pandas.Period.freqstr',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.is_leap_year.html#pandas.Period.is_leap_year',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.month.html#pandas.Period.month',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.quarter.html#pandas.Period.quarter',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.second.html#pandas.Period.second',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.week.html#pandas.Period.week',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.weekofyear.html#pandas.Period.weekofyear',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.now.html#pandas.Period.now',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Period.to_timestamp.html#pandas.Period.to_timestamp',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Interval.closed_left.html#pandas.Interval.closed_left',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Interval.is_empty.html#pandas.Interval.is_empty',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Interval.length.html#pandas.Interval.length',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Interval.open_left.html#pandas.Interval.open_left',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Interval.overlaps.html#pandas.Interval.overlaps',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Int16Dtype.html#pandas.Int16Dtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Int64Dtype.html#pandas.Int64Dtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.UInt16Dtype.html#pandas.UInt16Dtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.UInt64Dtype.html#pandas.UInt64Dtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Float64Dtype.html#pandas.Float64Dtype',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.CategoricalDtype.ordered.html#pandas.CategoricalDtype.ordered',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.categories.html#pandas.Categorical.categories',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.codes.html#pandas.Categorical.codes',\n",
       "  'https://pandas.pydata.org/docs/reference/api/pandas.arrays.ArrowStringArray.html#pandas.arrays.ArrowStringArray'],\n",
       " 'function_definitions': [{'function_name': 'pandas.Timedelta',\n",
       "   'full_function': 'class pandas.Timedelta(value=<object object>, unit=None, **kwargs)#',\n",
       "   'function_text': 'Represents a duration, the difference between two dates or times.',\n",
       "   'parameter_names_desc': [{'param_name': 'value',\n",
       "     'param_type': 'Timedelta, timedelta, np.timedelta64, str, or int',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'unit',\n",
       "     'param_type': 'str, default ‘ns’',\n",
       "     'param_desc': 'Denote the unit of the input, if input is an integer.\\nPossible values:\\n\\n‘W’, or ‘D’\\n‘days’, or ‘day’\\n‘hours’, ‘hour’, ‘hr’, or ‘h’\\n‘minutes’, ‘minute’, ‘min’, or ‘m’\\n‘seconds’, ‘second’, ‘sec’, or ‘s’\\n‘milliseconds’, ‘millisecond’, ‘millis’, ‘milli’, or ‘ms’\\n‘microseconds’, ‘microsecond’, ‘micros’, ‘micro’, or ‘us’\\n‘nanoseconds’, ‘nanosecond’, ‘nanos’, ‘nano’, or ‘ns’.\\n\\n\\nDeprecated since version 2.2.0: Values H, T, S, L, U, and N are deprecated in favour\\nof the values h, min, s, ms, us, and ns.\\n\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html#pandas.Timedelta'},\n",
       "  {'function_name': 'pandas.IntervalDtype',\n",
       "   'full_function': 'class pandas.IntervalDtype(subtype=None, closed=None)',\n",
       "   'function_text': 'An ExtensionDtype for Interval data.',\n",
       "   'parameter_names_desc': [{'param_name': 'subtype',\n",
       "     'param_type': 'str, np.dtype',\n",
       "     'param_desc': 'The dtype of the Interval bounds.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.IntervalDtype.html#pandas.IntervalDtype'},\n",
       "  {'function_name': 'pandas.Float64Dtype',\n",
       "   'full_function': 'class pandas.Float64Dtype',\n",
       "   'function_text': 'An ExtensionDtype for float64 data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Float64Dtype.html#pandas.Float64Dtype'},\n",
       "  {'function_name': 'pandas.SparseDtype',\n",
       "   'full_function': \"class pandas.SparseDtype(dtype=<class 'numpy.float64'>, fill_value=None)\",\n",
       "   'function_text': 'Dtype for data stored in SparseArray.',\n",
       "   'parameter_names_desc': [{'param_name': 'dtype',\n",
       "     'param_type': 'str, ExtensionDtype, numpy.dtype, type, default numpy.float64',\n",
       "     'param_desc': 'The dtype of the underlying array storing the non-fill value values.\\n'},\n",
       "    {'param_name': 'fill_value',\n",
       "     'param_type': 'scalar, optional',\n",
       "     'param_desc': 'The scalar value not stored in the SparseArray. By default, this\\ndepends on dtype.\\n\\n\\ndtype\\nna_value\\n\\n\\n\\nfloat\\nnp.nan\\n\\nint\\n0\\n\\nbool\\nFalse\\n\\ndatetime64\\npd.NaT\\n\\ntimedelta64\\npd.NaT\\n\\n\\n\\nThe default value may be overridden by specifying a fill_value.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.SparseDtype.html#pandas.SparseDtype'},\n",
       "  {'function_name': 'pandas.BooleanDtype',\n",
       "   'full_function': 'class pandas.BooleanDtype',\n",
       "   'function_text': 'Extension dtype for boolean data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.BooleanDtype.html#pandas.BooleanDtype'},\n",
       "  {'function_name': 'pandas.array',\n",
       "   'full_function': 'pandas.array(data, dtype=None, copy=True)',\n",
       "   'function_text': 'Create an array.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.array.html#pandas.array'},\n",
       "  {'function_name': 'pandas.array',\n",
       "   'full_function': 'pandas.array(data, dtype=None, copy=True)',\n",
       "   'function_text': 'Create an array.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.array.html#pandas.array'},\n",
       "  {'function_name': 'pandas.array',\n",
       "   'full_function': 'pandas.array(data, dtype=None, copy=True)',\n",
       "   'function_text': 'Create an array.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.array.html#pandas.array'},\n",
       "  {'function_name': 'pandas.array',\n",
       "   'full_function': 'pandas.array(data, dtype=None, copy=True)',\n",
       "   'function_text': 'Create an array.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.array.html#pandas.array'},\n",
       "  {'function_name': 'pandas.array',\n",
       "   'full_function': 'pandas.array(data, dtype=None, copy=True)',\n",
       "   'function_text': 'Create an array.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.array.html#pandas.array'},\n",
       "  {'function_name': 'pandas.array',\n",
       "   'full_function': 'pandas.array(data, dtype=None, copy=True)',\n",
       "   'function_text': 'Create an array.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.array.html#pandas.array'},\n",
       "  {'function_name': 'pandas.array',\n",
       "   'full_function': 'pandas.array(data, dtype=None, copy=True)',\n",
       "   'function_text': 'Create an array.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.array.html#pandas.array'},\n",
       "  {'function_name': 'pandas.array',\n",
       "   'full_function': 'pandas.array(data, dtype=None, copy=True)',\n",
       "   'function_text': 'Create an array.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.array.html#pandas.array'},\n",
       "  {'function_name': 'pandas.array',\n",
       "   'full_function': 'pandas.array(data, dtype=None, copy=True)',\n",
       "   'function_text': 'Create an array.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.array.html#pandas.array'},\n",
       "  {'function_name': 'pandas.array',\n",
       "   'full_function': 'pandas.array(data, dtype=None, copy=True)',\n",
       "   'function_text': 'Create an array.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.array.html#pandas.array'},\n",
       "  {'function_name': 'pandas.array',\n",
       "   'full_function': 'pandas.array(data, dtype=None, copy=True)',\n",
       "   'function_text': 'Create an array.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.array.html#pandas.array'},\n",
       "  {'function_name': 'pandas.array',\n",
       "   'full_function': 'pandas.array(data, dtype=None, copy=True)',\n",
       "   'function_text': 'Create an array.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.array.html#pandas.array'},\n",
       "  {'function_name': 'pandas.arrays.ArrowExtensionArray',\n",
       "   'full_function': 'class pandas.arrays.ArrowExtensionArray(values)',\n",
       "   'function_text': 'Pandas ExtensionArray backed by a PyArrow ChunkedArray.',\n",
       "   'parameter_names_desc': [{'param_name': 'values',\n",
       "     'param_type': 'pyarrow.Array or pyarrow.ChunkedArray',\n",
       "     'param_desc': ''}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.arrays.ArrowExtensionArray.html#pandas.arrays.ArrowExtensionArray'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.Timestamp',\n",
       "   'full_function': 'class pandas.Timestamp(ts_input=<object object>, year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, tzinfo=None, *, nanosecond=None, tz=None, unit=None, fold=None)#',\n",
       "   'function_text': 'Pandas replacement for python datetime.datetime object.',\n",
       "   'parameter_names_desc': [{'param_name': 'ts_input',\n",
       "     'param_type': 'datetime-like, str, int, float',\n",
       "     'param_desc': 'Value to be converted to Timestamp.\\n'},\n",
       "    {'param_name': 'year, month, day', 'param_type': 'int', 'param_desc': ''},\n",
       "    {'param_name': 'hour, minute, second, microsecond',\n",
       "     'param_type': 'int, optional, default 0',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'tzinfo',\n",
       "     'param_type': 'datetime.tzinfo, optional, default None',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'nanosecond',\n",
       "     'param_type': 'int, optional, default 0',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'tz',\n",
       "     'param_type': 'str, pytz.timezone, dateutil.tz.tzfile or None',\n",
       "     'param_desc': 'Time zone for time which Timestamp will have.\\n'},\n",
       "    {'param_name': 'unit',\n",
       "     'param_type': 'str',\n",
       "     'param_desc': \"Unit used for conversion if ts_input is of type int or float. The\\nvalid values are ‘D’, ‘h’, ‘m’, ‘s’, ‘ms’, ‘us’, and ‘ns’. For\\nexample, ‘s’ means seconds and ‘ms’ means milliseconds.\\nFor float inputs, the result will be stored in nanoseconds, and\\nthe unit attribute will be set as 'ns'.\\n\"},\n",
       "    {'param_name': 'fold',\n",
       "     'param_type': '{0, 1}, default None, keyword-only',\n",
       "     'param_desc': 'Due to daylight saving time, one wall clock time can occur twice\\nwhen shifting from summer to winter time; fold describes whether the\\ndatetime-like corresponds to the first (0) or the second time (1)\\nthe wall clock hits the ambiguous time.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html#pandas.Timestamp'},\n",
       "  {'function_name': 'pandas.Timestamp.asm8',\n",
       "   'full_function': 'Timestamp.asm8#',\n",
       "   'function_text': 'Return numpy datetime64 format in nanoseconds.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.asm8.html#pandas.Timestamp.asm8'},\n",
       "  {'function_name': 'pandas.Timestamp.dayofweek',\n",
       "   'full_function': 'Timestamp.dayofweek#',\n",
       "   'function_text': 'Return day of the week.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.dayofweek.html#pandas.Timestamp.dayofweek'},\n",
       "  {'function_name': 'pandas.Timestamp.dayofyear',\n",
       "   'full_function': 'Timestamp.dayofyear#',\n",
       "   'function_text': 'Return the day of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.dayofyear.html#pandas.Timestamp.dayofyear'},\n",
       "  {'function_name': 'pandas.Timestamp.days_in_month',\n",
       "   'full_function': 'Timestamp.days_in_month#',\n",
       "   'function_text': 'Return the number of days in the month.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.days_in_month.html#pandas.Timestamp.days_in_month'},\n",
       "  {'function_name': 'pandas.Timestamp.days_in_month',\n",
       "   'full_function': 'Timestamp.days_in_month#',\n",
       "   'function_text': 'Return the number of days in the month.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.days_in_month.html#pandas.Timestamp.days_in_month'},\n",
       "  {'function_name': 'pandas.Timestamp.is_leap_year',\n",
       "   'full_function': 'Timestamp.is_leap_year#',\n",
       "   'function_text': 'Return True if year is a leap year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_leap_year.html#pandas.Timestamp.is_leap_year'},\n",
       "  {'function_name': 'pandas.Timestamp.is_month_start',\n",
       "   'full_function': 'Timestamp.is_month_start#',\n",
       "   'function_text': 'Check if the date is the first day of the month.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_month_start.html#pandas.Timestamp.is_month_start'},\n",
       "  {'function_name': 'pandas.Timestamp.is_quarter_start',\n",
       "   'full_function': 'Timestamp.is_quarter_start#',\n",
       "   'function_text': 'Check if the date is the first day of the quarter.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_quarter_start.html#pandas.Timestamp.is_quarter_start'},\n",
       "  {'function_name': 'pandas.Timestamp.is_year_start',\n",
       "   'full_function': 'Timestamp.is_year_start#',\n",
       "   'function_text': 'Return True if date is first day of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_year_start.html#pandas.Timestamp.is_year_start'},\n",
       "  {'function_name': 'pandas.Timestamp.is_year_start',\n",
       "   'full_function': 'Timestamp.is_year_start#',\n",
       "   'function_text': 'Return True if date is first day of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_year_start.html#pandas.Timestamp.is_year_start'},\n",
       "  {'function_name': 'pandas.Timestamp.is_year_start',\n",
       "   'full_function': 'Timestamp.is_year_start#',\n",
       "   'function_text': 'Return True if date is first day of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_year_start.html#pandas.Timestamp.is_year_start'},\n",
       "  {'function_name': 'pandas.Timestamp.is_year_start',\n",
       "   'full_function': 'Timestamp.is_year_start#',\n",
       "   'function_text': 'Return True if date is first day of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_year_start.html#pandas.Timestamp.is_year_start'},\n",
       "  {'function_name': 'pandas.Timestamp.is_year_start',\n",
       "   'full_function': 'Timestamp.is_year_start#',\n",
       "   'function_text': 'Return True if date is first day of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_year_start.html#pandas.Timestamp.is_year_start'},\n",
       "  {'function_name': 'pandas.Timestamp.tz',\n",
       "   'full_function': 'property Timestamp.tz#',\n",
       "   'function_text': 'Alias for tzinfo.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.tz.html#pandas.Timestamp.tz'},\n",
       "  {'function_name': 'pandas.Timestamp.unit',\n",
       "   'full_function': 'Timestamp.unit#',\n",
       "   'function_text': 'The abbreviation associated with self._creso.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.unit.html#pandas.Timestamp.unit'},\n",
       "  {'function_name': 'pandas.Timestamp.week',\n",
       "   'full_function': 'Timestamp.week#',\n",
       "   'function_text': 'Return the week number of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.week.html#pandas.Timestamp.week'},\n",
       "  {'function_name': 'pandas.Timestamp.week',\n",
       "   'full_function': 'Timestamp.week#',\n",
       "   'function_text': 'Return the week number of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.week.html#pandas.Timestamp.week'},\n",
       "  {'function_name': 'pandas.Timestamp.as_unit',\n",
       "   'full_function': 'Timestamp.as_unit(unit, round_ok=True)#',\n",
       "   'function_text': 'Convert the underlying int64 representaton to the given unit.',\n",
       "   'parameter_names_desc': [{'param_name': 'unit',\n",
       "     'param_type': '{“ns”, “us”, “ms”, “s”}',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'round_ok',\n",
       "     'param_type': 'bool, default True',\n",
       "     'param_desc': 'If False and the conversion requires rounding, raise.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.as_unit.html#pandas.Timestamp.as_unit'},\n",
       "  {'function_name': 'pandas.Timestamp.ceil',\n",
       "   'full_function': \"Timestamp.ceil(freq, ambiguous='raise', nonexistent='raise')#\",\n",
       "   'function_text': 'Return a new Timestamp ceiled to this resolution.',\n",
       "   'parameter_names_desc': [{'param_name': 'freq',\n",
       "     'param_type': 'str',\n",
       "     'param_desc': 'Frequency string indicating the ceiling resolution.\\n'},\n",
       "    {'param_name': 'ambiguous',\n",
       "     'param_type': 'bool or {‘raise’, ‘NaT’}, default ‘raise’',\n",
       "     'param_desc': 'The behavior is as follows:\\n\\nbool contains flags to determine if time is dst or not (note\\nthat this flag is only applicable for ambiguous fall dst dates).\\n‘NaT’ will return NaT for an ambiguous time.\\n‘raise’ will raise an AmbiguousTimeError for an ambiguous time.\\n\\n'},\n",
       "    {'param_name': 'nonexistent',\n",
       "     'param_type': '{‘raise’, ‘shift_forward’, ‘shift_backward, ‘NaT’, timedelta}, default ‘raise’',\n",
       "     'param_desc': 'A nonexistent time does not exist in a particular timezone\\nwhere clocks moved forward due to DST.\\n\\n‘shift_forward’ will shift the nonexistent time forward to the\\nclosest existing time.\\n‘shift_backward’ will shift the nonexistent time backward to the\\nclosest existing time.\\n‘NaT’ will return NaT where there are nonexistent times.\\ntimedelta objects will shift nonexistent times by the timedelta.\\n‘raise’ will raise an NonExistentTimeError if there are\\nnonexistent times.\\n\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.ceil.html#pandas.Timestamp.ceil'},\n",
       "  {'function_name': 'pandas.Timestamp.ctime',\n",
       "   'full_function': 'Timestamp.ctime()#',\n",
       "   'function_text': 'Return ctime() style string.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.ctime.html#pandas.Timestamp.ctime'},\n",
       "  {'function_name': 'pandas.Timestamp.day_name',\n",
       "   'full_function': 'Timestamp.day_name(locale=None)#',\n",
       "   'function_text': 'Return the day name of the Timestamp with specified locale.',\n",
       "   'parameter_names_desc': [{'param_name': 'locale',\n",
       "     'param_type': 'str, default None (English locale)',\n",
       "     'param_desc': 'Locale determining the language in which to return the day name.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.day_name.html#pandas.Timestamp.day_name'},\n",
       "  {'function_name': 'pandas.Timestamp.floor',\n",
       "   'full_function': \"Timestamp.floor(freq, ambiguous='raise', nonexistent='raise')#\",\n",
       "   'function_text': 'Return a new Timestamp floored to this resolution.',\n",
       "   'parameter_names_desc': [{'param_name': 'freq',\n",
       "     'param_type': 'str',\n",
       "     'param_desc': 'Frequency string indicating the flooring resolution.\\n'},\n",
       "    {'param_name': 'ambiguous',\n",
       "     'param_type': 'bool or {‘raise’, ‘NaT’}, default ‘raise’',\n",
       "     'param_desc': 'The behavior is as follows:\\n\\nbool contains flags to determine if time is dst or not (note\\nthat this flag is only applicable for ambiguous fall dst dates).\\n‘NaT’ will return NaT for an ambiguous time.\\n‘raise’ will raise an AmbiguousTimeError for an ambiguous time.\\n\\n'},\n",
       "    {'param_name': 'nonexistent',\n",
       "     'param_type': '{‘raise’, ‘shift_forward’, ‘shift_backward, ‘NaT’, timedelta}, default ‘raise’',\n",
       "     'param_desc': 'A nonexistent time does not exist in a particular timezone\\nwhere clocks moved forward due to DST.\\n\\n‘shift_forward’ will shift the nonexistent time forward to the\\nclosest existing time.\\n‘shift_backward’ will shift the nonexistent time backward to the\\nclosest existing time.\\n‘NaT’ will return NaT where there are nonexistent times.\\ntimedelta objects will shift nonexistent times by the timedelta.\\n‘raise’ will raise an NonExistentTimeError if there are\\nnonexistent times.\\n\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.floor.html#pandas.Timestamp.floor'},\n",
       "  {'function_name': 'pandas.Timestamp.fromtimestamp',\n",
       "   'full_function': 'classmethod Timestamp.fromtimestamp(ts)#',\n",
       "   'function_text': 'Transform timestamp[, tz] to tz’s local time from POSIX timestamp.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.fromtimestamp.html#pandas.Timestamp.fromtimestamp'},\n",
       "  {'function_name': 'pandas.Timestamp.isoformat',\n",
       "   'full_function': \"Timestamp.isoformat(sep='T', timespec='auto')#\",\n",
       "   'function_text': 'Return the time formatted according to ISO 8601.',\n",
       "   'parameter_names_desc': [{'param_name': 'sep',\n",
       "     'param_type': 'str, default ‘T’',\n",
       "     'param_desc': 'String used as the separator between the date and time.\\n'},\n",
       "    {'param_name': 'timespec',\n",
       "     'param_type': 'str, default ‘auto’',\n",
       "     'param_desc': 'Specifies the number of additional terms of the time to include.\\nThe valid values are ‘auto’, ‘hours’, ‘minutes’, ‘seconds’,\\n‘milliseconds’, ‘microseconds’, and ‘nanoseconds’.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.isoformat.html#pandas.Timestamp.isoformat'},\n",
       "  {'function_name': 'pandas.Timestamp.month_name',\n",
       "   'full_function': 'Timestamp.month_name(locale=None)#',\n",
       "   'function_text': 'Return the month name of the Timestamp with specified locale.',\n",
       "   'parameter_names_desc': [{'param_name': 'locale',\n",
       "     'param_type': 'str, default None (English locale)',\n",
       "     'param_desc': 'Locale determining the language in which to return the month name.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.month_name.html#pandas.Timestamp.month_name'},\n",
       "  {'function_name': 'pandas.Timestamp.now',\n",
       "   'full_function': 'classmethod Timestamp.now(tz=None)#',\n",
       "   'function_text': 'Return new Timestamp object representing current time local to tz.',\n",
       "   'parameter_names_desc': [{'param_name': 'tz',\n",
       "     'param_type': 'str or timezone object, default None',\n",
       "     'param_desc': 'Timezone to localize to.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.now.html#pandas.Timestamp.now'},\n",
       "  {'function_name': 'pandas.Timestamp.round',\n",
       "   'full_function': \"Timestamp.round(freq, ambiguous='raise', nonexistent='raise')#\",\n",
       "   'function_text': 'Round the Timestamp to the specified resolution.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.round.html#pandas.Timestamp.round'},\n",
       "  {'function_name': 'pandas.Timestamp.strptime',\n",
       "   'full_function': 'classmethod Timestamp.strptime(string, format)#',\n",
       "   'function_text': 'Function is not implemented. Use pd.to_datetime().',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.strptime.html#pandas.Timestamp.strptime'},\n",
       "  {'function_name': 'pandas.Timestamp.timestamp',\n",
       "   'full_function': 'Timestamp.timestamp()#',\n",
       "   'function_text': 'Return POSIX timestamp as float.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.timestamp.html#pandas.Timestamp.timestamp'},\n",
       "  {'function_name': 'pandas.Timestamp.timetz',\n",
       "   'full_function': 'Timestamp.timetz()#',\n",
       "   'function_text': 'Return time object with same time and tzinfo.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.timetz.html#pandas.Timestamp.timetz'},\n",
       "  {'function_name': 'pandas.Timestamp.to_numpy',\n",
       "   'full_function': 'Timestamp.to_numpy(dtype=None, copy=False)#',\n",
       "   'function_text': 'Convert the Timestamp to a NumPy datetime64.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.to_numpy.html#pandas.Timestamp.to_numpy'},\n",
       "  {'function_name': 'pandas.Timestamp.to_period',\n",
       "   'full_function': 'Timestamp.to_period(freq=None)#',\n",
       "   'function_text': 'Return an period of which this timestamp is an observation.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.to_period.html#pandas.Timestamp.to_period'},\n",
       "  {'function_name': 'pandas.Timestamp.today',\n",
       "   'full_function': 'classmethod Timestamp.today(tz=None)#',\n",
       "   'function_text': 'Return the current time in the local timezone.',\n",
       "   'parameter_names_desc': [{'param_name': 'tz',\n",
       "     'param_type': 'str or timezone object, default None',\n",
       "     'param_desc': 'Timezone to localize to.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.today.html#pandas.Timestamp.today'},\n",
       "  {'function_name': 'pandas.Timestamp.tz_convert',\n",
       "   'full_function': 'Timestamp.tz_convert(tz)#',\n",
       "   'function_text': 'Convert timezone-aware Timestamp to another time zone.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.tz_convert.html#pandas.Timestamp.tz_convert'},\n",
       "  {'function_name': 'pandas.Timestamp.tzname',\n",
       "   'full_function': 'Timestamp.tzname()#',\n",
       "   'function_text': 'Return time zone name.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.tzname.html#pandas.Timestamp.tzname'},\n",
       "  {'function_name': 'pandas.Timestamp.utcnow',\n",
       "   'full_function': 'classmethod Timestamp.utcnow()#',\n",
       "   'function_text': 'Return a new Timestamp representing UTC day and time.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.utcnow.html#pandas.Timestamp.utcnow'},\n",
       "  {'function_name': 'pandas.Timestamp.utctimetuple',\n",
       "   'full_function': 'Timestamp.utctimetuple()#',\n",
       "   'function_text': 'Return UTC time tuple, compatible with time.localtime().',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.utctimetuple.html#pandas.Timestamp.utctimetuple'},\n",
       "  {'function_name': 'pandas.arrays.DatetimeArray',\n",
       "   'full_function': 'class pandas.arrays.DatetimeArray(values, dtype=None, freq=_NoDefault.no_default, copy=False)',\n",
       "   'function_text': 'Pandas ExtensionArray for tz-naive or tz-aware datetime data.',\n",
       "   'parameter_names_desc': [{'param_name': 'values',\n",
       "     'param_type': 'Series, Index, DatetimeArray, ndarray',\n",
       "     'param_desc': 'The datetime data.\\nFor DatetimeArray values (or a Series or Index boxing one),\\ndtype and freq will be extracted from values.\\n'},\n",
       "    {'param_name': 'dtype',\n",
       "     'param_type': 'numpy.dtype or DatetimeTZDtype',\n",
       "     'param_desc': 'Note that the only NumPy dtype allowed is ‘datetime64[ns]’.\\n'},\n",
       "    {'param_name': 'freq',\n",
       "     'param_type': 'str or Offset, optional',\n",
       "     'param_desc': 'The frequency.\\n'},\n",
       "    {'param_name': 'copy',\n",
       "     'param_type': 'bool, default False',\n",
       "     'param_desc': 'Whether to copy the underlying array of values.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.arrays.DatetimeArray.html#pandas.arrays.DatetimeArray'},\n",
       "  {'function_name': 'pandas.DatetimeTZDtype',\n",
       "   'full_function': \"class pandas.DatetimeTZDtype(unit='ns', tz=None)\",\n",
       "   'function_text': 'An ExtensionDtype for timezone-aware datetime data.',\n",
       "   'parameter_names_desc': [{'param_name': 'unit',\n",
       "     'param_type': 'str, default “ns”',\n",
       "     'param_desc': 'The precision of the datetime data. Currently limited\\nto \"ns\".\\n'},\n",
       "    {'param_name': 'tz',\n",
       "     'param_type': 'str, int, or datetime.tzinfo',\n",
       "     'param_desc': 'The timezone.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DatetimeTZDtype.html#pandas.DatetimeTZDtype'},\n",
       "  {'function_name': 'pandas.Timedelta',\n",
       "   'full_function': 'class pandas.Timedelta(value=<object object>, unit=None, **kwargs)#',\n",
       "   'function_text': 'Represents a duration, the difference between two dates or times.',\n",
       "   'parameter_names_desc': [{'param_name': 'value',\n",
       "     'param_type': 'Timedelta, timedelta, np.timedelta64, str, or int',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'unit',\n",
       "     'param_type': 'str, default ‘ns’',\n",
       "     'param_desc': 'Denote the unit of the input, if input is an integer.\\nPossible values:\\n\\n‘W’, or ‘D’\\n‘days’, or ‘day’\\n‘hours’, ‘hour’, ‘hr’, or ‘h’\\n‘minutes’, ‘minute’, ‘min’, or ‘m’\\n‘seconds’, ‘second’, ‘sec’, or ‘s’\\n‘milliseconds’, ‘millisecond’, ‘millis’, ‘milli’, or ‘ms’\\n‘microseconds’, ‘microsecond’, ‘micros’, ‘micro’, or ‘us’\\n‘nanoseconds’, ‘nanosecond’, ‘nanos’, ‘nano’, or ‘ns’.\\n\\n\\nDeprecated since version 2.2.0: Values H, T, S, L, U, and N are deprecated in favour\\nof the values h, min, s, ms, us, and ns.\\n\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html#pandas.Timedelta'},\n",
       "  {'function_name': 'pandas.Timedelta.asm8',\n",
       "   'full_function': 'Timedelta.asm8#',\n",
       "   'function_text': 'Return a numpy timedelta64 array scalar view.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.asm8.html#pandas.Timedelta.asm8'},\n",
       "  {'function_name': 'pandas.Timedelta.days',\n",
       "   'full_function': 'Timedelta.days#',\n",
       "   'function_text': 'Returns the days of the timedelta.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.days.html#pandas.Timedelta.days'},\n",
       "  {'function_name': 'pandas.Timedelta.days',\n",
       "   'full_function': 'Timedelta.days#',\n",
       "   'function_text': 'Returns the days of the timedelta.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.days.html#pandas.Timedelta.days'},\n",
       "  {'function_name': 'pandas.Timedelta.nanoseconds',\n",
       "   'full_function': 'Timedelta.nanoseconds#',\n",
       "   'function_text': 'Return the number of nanoseconds (n), where 0 <= n < 1 microsecond.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.nanoseconds.html#pandas.Timedelta.nanoseconds'},\n",
       "  {'function_name': 'pandas.Timedelta.seconds',\n",
       "   'full_function': 'Timedelta.seconds#',\n",
       "   'function_text': 'Return the total hours, minutes, and seconds of the timedelta as seconds.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.seconds.html#pandas.Timedelta.seconds'},\n",
       "  {'function_name': 'pandas.Timedelta.seconds',\n",
       "   'full_function': 'Timedelta.seconds#',\n",
       "   'function_text': 'Return the total hours, minutes, and seconds of the timedelta as seconds.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.seconds.html#pandas.Timedelta.seconds'},\n",
       "  {'function_name': 'pandas.Timedelta.as_unit',\n",
       "   'full_function': 'Timedelta.as_unit(unit, round_ok=True)#',\n",
       "   'function_text': 'Convert the underlying int64 representation to the given unit.',\n",
       "   'parameter_names_desc': [{'param_name': 'unit',\n",
       "     'param_type': '{“ns”, “us”, “ms”, “s”}',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'round_ok',\n",
       "     'param_type': 'bool, default True',\n",
       "     'param_desc': 'If False and the conversion requires rounding, raise.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.as_unit.html#pandas.Timedelta.as_unit'},\n",
       "  {'function_name': 'pandas.Timedelta.floor',\n",
       "   'full_function': 'Timedelta.floor(freq)#',\n",
       "   'function_text': 'Return a new Timedelta floored to this resolution.',\n",
       "   'parameter_names_desc': [{'param_name': 'freq',\n",
       "     'param_type': 'str',\n",
       "     'param_desc': 'Frequency string indicating the flooring resolution.\\nIt uses the same units as class constructor Timedelta.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.floor.html#pandas.Timedelta.floor'},\n",
       "  {'function_name': 'pandas.Timedelta.round',\n",
       "   'full_function': 'Timedelta.round(freq)#',\n",
       "   'function_text': 'Round the Timedelta to the specified resolution.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.round.html#pandas.Timedelta.round'},\n",
       "  {'function_name': 'pandas.Timedelta.to_timedelta64',\n",
       "   'full_function': 'Timedelta.to_timedelta64()#',\n",
       "   'function_text': 'Return a numpy.timedelta64 object with ‘ns’ precision.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.to_timedelta64.html#pandas.Timedelta.to_timedelta64'},\n",
       "  {'function_name': 'pandas.Timedelta.total_seconds',\n",
       "   'full_function': 'Timedelta.total_seconds()#',\n",
       "   'function_text': 'Total seconds in the duration.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.total_seconds.html#pandas.Timedelta.total_seconds'},\n",
       "  {'function_name': 'pandas.arrays.TimedeltaArray',\n",
       "   'full_function': 'class pandas.arrays.TimedeltaArray(values, dtype=None, freq=_NoDefault.no_default, copy=False)',\n",
       "   'function_text': 'Pandas ExtensionArray for timedelta data.',\n",
       "   'parameter_names_desc': [{'param_name': 'values',\n",
       "     'param_type': 'array-like',\n",
       "     'param_desc': 'The timedelta data.\\n'},\n",
       "    {'param_name': 'dtype',\n",
       "     'param_type': 'numpy.dtype',\n",
       "     'param_desc': 'Currently, only numpy.dtype(\"timedelta64[ns]\") is accepted.\\n'},\n",
       "    {'param_name': 'freq', 'param_type': 'Offset, optional', 'param_desc': ''},\n",
       "    {'param_name': 'copy',\n",
       "     'param_type': 'bool, default False',\n",
       "     'param_desc': 'Whether to copy the underlying array of data.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.arrays.TimedeltaArray.html#pandas.arrays.TimedeltaArray'},\n",
       "  {'function_name': 'pandas.Period',\n",
       "   'full_function': 'class pandas.Period(value=None, freq=None, ordinal=None, year=None, month=None, quarter=None, day=None, hour=None, minute=None, second=None)#',\n",
       "   'function_text': 'Represents a period of time.',\n",
       "   'parameter_names_desc': [{'param_name': 'value',\n",
       "     'param_type': 'Period, str, datetime, date or pandas.Timestamp, default None',\n",
       "     'param_desc': 'The time period represented (e.g., ‘4Q2005’). This represents neither\\nthe start or the end of the period, but rather the entire period itself.\\n'},\n",
       "    {'param_name': 'freq',\n",
       "     'param_type': 'str, default None',\n",
       "     'param_desc': 'One of pandas period strings or corresponding objects. Accepted\\nstrings are listed in the\\nperiod alias section in the user docs.\\nIf value is datetime, freq is required.\\n'},\n",
       "    {'param_name': 'ordinal',\n",
       "     'param_type': 'int, default None',\n",
       "     'param_desc': 'The period offset from the proleptic Gregorian epoch.\\n'},\n",
       "    {'param_name': 'year',\n",
       "     'param_type': 'int, default None',\n",
       "     'param_desc': 'Year value of the period.\\n'},\n",
       "    {'param_name': 'month',\n",
       "     'param_type': 'int, default 1',\n",
       "     'param_desc': 'Month value of the period.\\n'},\n",
       "    {'param_name': 'quarter',\n",
       "     'param_type': 'int, default None',\n",
       "     'param_desc': 'Quarter value of the period.\\n'},\n",
       "    {'param_name': 'day',\n",
       "     'param_type': 'int, default 1',\n",
       "     'param_desc': 'Day value of the period.\\n'},\n",
       "    {'param_name': 'hour',\n",
       "     'param_type': 'int, default 0',\n",
       "     'param_desc': 'Hour value of the period.\\n'},\n",
       "    {'param_name': 'minute',\n",
       "     'param_type': 'int, default 0',\n",
       "     'param_desc': 'Minute value of the period.\\n'},\n",
       "    {'param_name': 'second',\n",
       "     'param_type': 'int, default 0',\n",
       "     'param_desc': 'Second value of the period.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.html#pandas.Period'},\n",
       "  {'function_name': 'pandas.Period.day',\n",
       "   'full_function': 'Period.day#',\n",
       "   'function_text': 'Get day of the month that a Period falls on.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.day.html#pandas.Period.day'},\n",
       "  {'function_name': 'pandas.Period.day_of_week',\n",
       "   'full_function': 'Period.day_of_week#',\n",
       "   'function_text': 'Day of the week the period lies in, with Monday=0 and Sunday=6.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.day_of_week.html#pandas.Period.day_of_week'},\n",
       "  {'function_name': 'pandas.Period.day_of_year',\n",
       "   'full_function': 'Period.day_of_year#',\n",
       "   'function_text': 'Return the day of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.day_of_year.html#pandas.Period.day_of_year'},\n",
       "  {'function_name': 'pandas.Period.daysinmonth',\n",
       "   'full_function': 'Period.daysinmonth#',\n",
       "   'function_text': 'Get the total number of days of the month that this period falls on.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.daysinmonth.html#pandas.Period.daysinmonth'},\n",
       "  {'function_name': 'pandas.Period.daysinmonth',\n",
       "   'full_function': 'Period.daysinmonth#',\n",
       "   'function_text': 'Get the total number of days of the month that this period falls on.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.daysinmonth.html#pandas.Period.daysinmonth'},\n",
       "  {'function_name': 'pandas.Period.hour',\n",
       "   'full_function': 'Period.hour#',\n",
       "   'function_text': 'Get the hour of the day component of the Period.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.hour.html#pandas.Period.hour'},\n",
       "  {'function_name': 'pandas.Period.minute',\n",
       "   'full_function': 'Period.minute#',\n",
       "   'function_text': 'Get minute of the hour component of the Period.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.minute.html#pandas.Period.minute'},\n",
       "  {'function_name': 'pandas.Period.minute',\n",
       "   'full_function': 'Period.minute#',\n",
       "   'function_text': 'Get minute of the hour component of the Period.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.minute.html#pandas.Period.minute'},\n",
       "  {'function_name': 'pandas.Period.qyear',\n",
       "   'full_function': 'Period.qyear#',\n",
       "   'function_text': 'Fiscal year the Period lies in according to its starting-quarter.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.qyear.html#pandas.Period.qyear'},\n",
       "  {'function_name': 'pandas.Period.start_time',\n",
       "   'full_function': 'Period.start_time#',\n",
       "   'function_text': 'Get the Timestamp for the start of the period.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.start_time.html#pandas.Period.start_time'},\n",
       "  {'function_name': 'pandas.Period.weekday',\n",
       "   'full_function': 'Period.weekday#',\n",
       "   'function_text': 'Day of the week the period lies in, with Monday=0 and Sunday=6.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.weekday.html#pandas.Period.weekday'},\n",
       "  {'function_name': 'pandas.Period.year',\n",
       "   'full_function': 'Period.year#',\n",
       "   'function_text': 'Return the year this Period falls on.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.year.html#pandas.Period.year'},\n",
       "  {'function_name': 'pandas.Period.asfreq',\n",
       "   'full_function': \"Period.asfreq(freq, how='E')#\",\n",
       "   'function_text': 'Convert Period to desired frequency, at the start or end of the interval.',\n",
       "   'parameter_names_desc': [{'param_name': 'freq',\n",
       "     'param_type': 'str, BaseOffset',\n",
       "     'param_desc': 'The desired frequency. If passing a str, it needs to be a\\nvalid period alias.\\n'},\n",
       "    {'param_name': 'how',\n",
       "     'param_type': '{‘E’, ‘S’, ‘end’, ‘start’}, default ‘end’',\n",
       "     'param_desc': 'Start or end of the timespan.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.asfreq.html#pandas.Period.asfreq'},\n",
       "  {'function_name': 'pandas.Period.strftime',\n",
       "   'full_function': 'Period.strftime(fmt)#',\n",
       "   'function_text': 'Returns a formatted string representation of the Period.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.strftime.html#pandas.Period.strftime'},\n",
       "  {'function_name': 'pandas.arrays.PeriodArray',\n",
       "   'full_function': 'class pandas.arrays.PeriodArray(values, dtype=None, freq=None, copy=False)',\n",
       "   'function_text': 'Pandas ExtensionArray for storing Period data.',\n",
       "   'parameter_names_desc': [{'param_name': 'values',\n",
       "     'param_type': 'Union[PeriodArray, Series[period], ndarray[int], PeriodIndex]',\n",
       "     'param_desc': 'The data to store. These should be arrays that can be directly\\nconverted to ordinals without inference or copy (PeriodArray,\\nndarray[int64]), or a box around such an array (Series[period],\\nPeriodIndex).\\n'},\n",
       "    {'param_name': 'dtype',\n",
       "     'param_type': 'PeriodDtype, optional',\n",
       "     'param_desc': 'A PeriodDtype instance from which to extract a freq. If both\\nfreq and dtype are specified, then the frequencies must match.\\n'},\n",
       "    {'param_name': 'freq',\n",
       "     'param_type': 'str or DateOffset',\n",
       "     'param_desc': 'The freq to use for the array. Mostly applicable when values\\nis an ndarray of integers, when freq is required. When values\\nis a PeriodArray (or box around), it’s checked that values.freq\\nmatches freq.\\n'},\n",
       "    {'param_name': 'copy',\n",
       "     'param_type': 'bool, default False',\n",
       "     'param_desc': 'Whether to copy the ordinals before storing.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.arrays.PeriodArray.html#pandas.arrays.PeriodArray'},\n",
       "  {'function_name': 'pandas.PeriodDtype',\n",
       "   'full_function': 'class pandas.PeriodDtype(freq)',\n",
       "   'function_text': 'An ExtensionDtype for Period data.',\n",
       "   'parameter_names_desc': [{'param_name': 'freq',\n",
       "     'param_type': 'str or DateOffset',\n",
       "     'param_desc': 'The frequency of this PeriodDtype.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.PeriodDtype.html#pandas.PeriodDtype'},\n",
       "  {'function_name': 'pandas.Interval',\n",
       "   'full_function': 'class pandas.Interval#',\n",
       "   'function_text': 'Immutable object implementing an Interval, a bounded slice-like interval.',\n",
       "   'parameter_names_desc': [{'param_name': 'left',\n",
       "     'param_type': 'orderable scalar',\n",
       "     'param_desc': 'Left bound for the interval.\\n'},\n",
       "    {'param_name': 'right',\n",
       "     'param_type': 'orderable scalar',\n",
       "     'param_desc': 'Right bound for the interval.\\n'},\n",
       "    {'param_name': 'closed',\n",
       "     'param_type': '{‘right’, ‘left’, ‘both’, ‘neither’}, default ‘right’',\n",
       "     'param_desc': 'Whether the interval is closed on the left-side, right-side, both or\\nneither. See the Notes for more detailed explanation.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Interval.html#pandas.Interval'},\n",
       "  {'function_name': 'pandas.Interval.closed',\n",
       "   'full_function': 'Interval.closed#',\n",
       "   'function_text': 'String describing the inclusive side the intervals.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Interval.closed.html#pandas.Interval.closed'},\n",
       "  {'function_name': 'pandas.Interval.closed_right',\n",
       "   'full_function': 'Interval.closed_right#',\n",
       "   'function_text': 'Check if the interval is closed on the right side.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Interval.closed_right.html#pandas.Interval.closed_right'},\n",
       "  {'function_name': 'pandas.Interval.left',\n",
       "   'full_function': 'Interval.left#',\n",
       "   'function_text': 'Left bound for the interval.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Interval.left.html#pandas.Interval.left'},\n",
       "  {'function_name': 'pandas.Interval.mid',\n",
       "   'full_function': 'Interval.mid#',\n",
       "   'function_text': 'Return the midpoint of the Interval.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Interval.mid.html#pandas.Interval.mid'},\n",
       "  {'function_name': 'pandas.Interval.open_right',\n",
       "   'full_function': 'Interval.open_right#',\n",
       "   'function_text': 'Check if the interval is open on the right side.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Interval.open_right.html#pandas.Interval.open_right'},\n",
       "  {'function_name': 'pandas.Interval.right',\n",
       "   'full_function': 'Interval.right#',\n",
       "   'function_text': 'Right bound for the interval.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Interval.right.html#pandas.Interval.right'},\n",
       "  {'function_name': 'pandas.arrays.IntervalArray',\n",
       "   'full_function': 'class pandas.arrays.IntervalArray(data, closed=None, dtype=None, copy=False, verify_integrity=True)',\n",
       "   'function_text': 'Pandas array for interval data that are closed on the same side.',\n",
       "   'parameter_names_desc': [{'param_name': 'data',\n",
       "     'param_type': 'array-like (1-dimensional)',\n",
       "     'param_desc': 'Array-like (ndarray, DateTimeArray, TimeDeltaArray) containing\\nInterval objects from which to build the IntervalArray.\\n'},\n",
       "    {'param_name': 'closed',\n",
       "     'param_type': '{‘left’, ‘right’, ‘both’, ‘neither’}, default ‘right’',\n",
       "     'param_desc': 'Whether the intervals are closed on the left-side, right-side, both or\\nneither.\\n'},\n",
       "    {'param_name': 'dtype',\n",
       "     'param_type': 'dtype or None, default None',\n",
       "     'param_desc': 'If None, dtype will be inferred.\\n'},\n",
       "    {'param_name': 'copy',\n",
       "     'param_type': 'bool, default False',\n",
       "     'param_desc': 'Copy the input data.\\n'},\n",
       "    {'param_name': 'verify_integrity',\n",
       "     'param_type': 'bool, default True',\n",
       "     'param_desc': 'Verify that the IntervalArray is valid.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.arrays.IntervalArray.html#pandas.arrays.IntervalArray'},\n",
       "  {'function_name': 'pandas.IntervalDtype',\n",
       "   'full_function': 'class pandas.IntervalDtype(subtype=None, closed=None)',\n",
       "   'function_text': 'An ExtensionDtype for Interval data.',\n",
       "   'parameter_names_desc': [{'param_name': 'subtype',\n",
       "     'param_type': 'str, np.dtype',\n",
       "     'param_desc': 'The dtype of the Interval bounds.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.IntervalDtype.html#pandas.IntervalDtype'},\n",
       "  {'function_name': 'pandas.arrays.IntegerArray',\n",
       "   'full_function': 'class pandas.arrays.IntegerArray(values, mask, copy=False)',\n",
       "   'function_text': 'Array of integer (optional missing) values.',\n",
       "   'parameter_names_desc': [{'param_name': 'values',\n",
       "     'param_type': 'numpy.ndarray',\n",
       "     'param_desc': 'A 1-d integer-dtype array.\\n'},\n",
       "    {'param_name': 'mask',\n",
       "     'param_type': 'numpy.ndarray',\n",
       "     'param_desc': 'A 1-d boolean-dtype array indicating missing values.\\n'},\n",
       "    {'param_name': 'copy',\n",
       "     'param_type': 'bool, default False',\n",
       "     'param_desc': 'Whether to copy the values and mask.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.arrays.IntegerArray.html#pandas.arrays.IntegerArray'},\n",
       "  {'function_name': 'pandas.Int8Dtype',\n",
       "   'full_function': 'class pandas.Int8Dtype',\n",
       "   'function_text': 'An ExtensionDtype for int8 integer data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Int8Dtype.html#pandas.Int8Dtype'},\n",
       "  {'function_name': 'pandas.Int32Dtype',\n",
       "   'full_function': 'class pandas.Int32Dtype',\n",
       "   'function_text': 'An ExtensionDtype for int32 integer data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Int32Dtype.html#pandas.Int32Dtype'},\n",
       "  {'function_name': 'pandas.UInt8Dtype',\n",
       "   'full_function': 'class pandas.UInt8Dtype',\n",
       "   'function_text': 'An ExtensionDtype for uint8 integer data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.UInt8Dtype.html#pandas.UInt8Dtype'},\n",
       "  {'function_name': 'pandas.UInt32Dtype',\n",
       "   'full_function': 'class pandas.UInt32Dtype',\n",
       "   'function_text': 'An ExtensionDtype for uint32 integer data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.UInt32Dtype.html#pandas.UInt32Dtype'},\n",
       "  {'function_name': 'pandas.arrays.FloatingArray',\n",
       "   'full_function': 'class pandas.arrays.FloatingArray(values, mask, copy=False)',\n",
       "   'function_text': 'Array of floating (optional missing) values.',\n",
       "   'parameter_names_desc': [{'param_name': 'values',\n",
       "     'param_type': 'numpy.ndarray',\n",
       "     'param_desc': 'A 1-d float-dtype array.\\n'},\n",
       "    {'param_name': 'mask',\n",
       "     'param_type': 'numpy.ndarray',\n",
       "     'param_desc': 'A 1-d boolean-dtype array indicating missing values.\\n'},\n",
       "    {'param_name': 'copy',\n",
       "     'param_type': 'bool, default False',\n",
       "     'param_desc': 'Whether to copy the values and mask.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.arrays.FloatingArray.html#pandas.arrays.FloatingArray'},\n",
       "  {'function_name': 'pandas.Float32Dtype',\n",
       "   'full_function': 'class pandas.Float32Dtype',\n",
       "   'function_text': 'An ExtensionDtype for float32 data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Float32Dtype.html#pandas.Float32Dtype'},\n",
       "  {'function_name': 'pandas.CategoricalDtype',\n",
       "   'full_function': 'class pandas.CategoricalDtype(categories=None, ordered=False)',\n",
       "   'function_text': 'Type for categorical data with the categories and orderedness.',\n",
       "   'parameter_names_desc': [{'param_name': 'categories',\n",
       "     'param_type': 'sequence, optional',\n",
       "     'param_desc': 'Must be unique, and must not contain any nulls.\\nThe categories are stored in an Index,\\nand if an index is provided the dtype of that index will be used.\\n'},\n",
       "    {'param_name': 'ordered',\n",
       "     'param_type': 'bool or None, default False',\n",
       "     'param_desc': 'Whether or not this categorical is treated as a ordered categorical.\\nNone can be used to maintain the ordered value of existing categoricals when\\nused in operations that combine categoricals, e.g. astype, and will resolve to\\nFalse if there is no existing ordered to maintain.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.CategoricalDtype.html#pandas.CategoricalDtype'},\n",
       "  {'function_name': 'pandas.CategoricalDtype.categories',\n",
       "   'full_function': 'property CategoricalDtype.categories',\n",
       "   'function_text': 'An Index containing the unique categories allowed.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.CategoricalDtype.categories.html#pandas.CategoricalDtype.categories'},\n",
       "  {'function_name': 'pandas.Categorical',\n",
       "   'full_function': 'class pandas.Categorical(values, categories=None, ordered=None, dtype=None, fastpath=_NoDefault.no_default, copy=True)',\n",
       "   'function_text': 'Represent a categorical variable in classic R / S-plus fashion.',\n",
       "   'parameter_names_desc': [{'param_name': 'values',\n",
       "     'param_type': 'list-like',\n",
       "     'param_desc': 'The values of the categorical. If categories are given, values not in\\ncategories will be replaced with NaN.\\n'},\n",
       "    {'param_name': 'categories',\n",
       "     'param_type': 'Index-like (unique), optional',\n",
       "     'param_desc': 'The unique categories for this categorical. If not given, the\\ncategories are assumed to be the unique values of values (sorted, if\\npossible, otherwise in the order in which they appear).\\n'},\n",
       "    {'param_name': 'ordered',\n",
       "     'param_type': 'bool, default False',\n",
       "     'param_desc': 'Whether or not this categorical is treated as a ordered categorical.\\nIf True, the resulting categorical will be ordered.\\nAn ordered categorical respects, when sorted, the order of its\\ncategories attribute (which in turn is the categories argument, if\\nprovided).\\n'},\n",
       "    {'param_name': 'dtype',\n",
       "     'param_type': 'CategoricalDtype',\n",
       "     'param_desc': 'An instance of CategoricalDtype to use for this categorical.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.html#pandas.Categorical'},\n",
       "  {'function_name': 'pandas.Categorical.from_codes',\n",
       "   'full_function': 'classmethod Categorical.from_codes(codes, categories=None, ordered=None, dtype=None, validate=True)',\n",
       "   'function_text': 'Make a Categorical type from codes and categories or dtype.',\n",
       "   'parameter_names_desc': [{'param_name': 'codes',\n",
       "     'param_type': 'array-like of int',\n",
       "     'param_desc': 'An integer array, where each integer points to a category in\\ncategories or dtype.categories, or else is -1 for NaN.\\n'},\n",
       "    {'param_name': 'categories',\n",
       "     'param_type': 'index-like, optional',\n",
       "     'param_desc': 'The categories for the categorical. Items need to be unique.\\nIf the categories are not given here, then they must be provided\\nin dtype.\\n'},\n",
       "    {'param_name': 'ordered',\n",
       "     'param_type': 'bool, optional',\n",
       "     'param_desc': 'Whether or not this categorical is treated as an ordered\\ncategorical. If not given here or in dtype, the resulting\\ncategorical will be unordered.\\n'},\n",
       "    {'param_name': 'dtype',\n",
       "     'param_type': 'CategoricalDtype or “category”, optional',\n",
       "     'param_desc': 'If CategoricalDtype, cannot be used together with\\ncategories or ordered.\\n'},\n",
       "    {'param_name': 'validate',\n",
       "     'param_type': 'bool, default True',\n",
       "     'param_desc': 'If True, validate that the codes are valid for the dtype.\\nIf False, don’t validate that the codes are valid. Be careful about skipping\\nvalidation, as invalid codes can lead to severe problems, such as segfaults.\\n\\nNew in version 2.1.0.\\n\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.from_codes.html#pandas.Categorical.from_codes'},\n",
       "  {'function_name': 'pandas.Categorical.dtype',\n",
       "   'full_function': 'property Categorical.dtype',\n",
       "   'function_text': 'The CategoricalDtype for this instance.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.dtype.html#pandas.Categorical.dtype'},\n",
       "  {'function_name': 'pandas.Categorical.ordered',\n",
       "   'full_function': 'property Categorical.ordered',\n",
       "   'function_text': 'Whether the categories have an ordered relationship.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.ordered.html#pandas.Categorical.ordered'},\n",
       "  {'function_name': 'pandas.Categorical.__array__',\n",
       "   'full_function': 'Categorical.__array__(dtype=None, copy=None)',\n",
       "   'function_text': 'The numpy array interface.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.__array__.html#pandas.Categorical.__array__'},\n",
       "  {'function_name': 'pandas.arrays.SparseArray',\n",
       "   'full_function': \"class pandas.arrays.SparseArray(data, sparse_index=None, fill_value=None, kind='integer', dtype=None, copy=False)\",\n",
       "   'function_text': 'An ExtensionArray for storing sparse data.',\n",
       "   'parameter_names_desc': [{'param_name': 'data',\n",
       "     'param_type': 'array-like or scalar',\n",
       "     'param_desc': 'A dense array of values to store in the SparseArray. This may contain\\nfill_value.\\n'},\n",
       "    {'param_name': 'sparse_index',\n",
       "     'param_type': 'SparseIndex, optional',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'fill_value',\n",
       "     'param_type': 'scalar, optional',\n",
       "     'param_desc': 'Elements in data that are fill_value are not stored in the\\nSparseArray. For memory savings, this should be the most common value\\nin data. By default, fill_value depends on the dtype of data:\\n\\n\\ndata.dtype\\nna_value\\n\\n\\n\\nfloat\\nnp.nan\\n\\nint\\n0\\n\\nbool\\nFalse\\n\\ndatetime64\\npd.NaT\\n\\ntimedelta64\\npd.NaT\\n\\n\\n\\nThe fill value is potentially specified in three ways. In order of\\nprecedence, these are\\n\\nThe fill_value argument\\ndtype.fill_value if fill_value is None and dtype is\\na SparseDtype\\ndata.dtype.fill_value if fill_value is None and dtype\\nis not a SparseDtype and data is a SparseArray.\\n\\n'},\n",
       "    {'param_name': 'kind',\n",
       "     'param_type': 'str',\n",
       "     'param_desc': 'Can be ‘integer’ or ‘block’, default is ‘integer’.\\nThe type of storage for sparse locations.\\n\\n‘block’: Stores a block and block_length for each\\ncontiguous span of sparse values. This is best when\\nsparse data tends to be clumped together, with large\\nregions of fill-value values between sparse values.\\n‘integer’: uses an integer to store the location of\\neach sparse value.\\n\\n'},\n",
       "    {'param_name': 'dtype',\n",
       "     'param_type': 'np.dtype or SparseDtype, optional',\n",
       "     'param_desc': 'The dtype to use for the SparseArray. For numpy dtypes, this\\ndetermines the dtype of self.sp_values. For SparseDtype,\\nthis determines self.sp_values and self.fill_value.\\n'},\n",
       "    {'param_name': 'copy',\n",
       "     'param_type': 'bool, default False',\n",
       "     'param_desc': 'Whether to explicitly copy the incoming data array.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.arrays.SparseArray.html#pandas.arrays.SparseArray'},\n",
       "  {'function_name': 'pandas.SparseDtype',\n",
       "   'full_function': \"class pandas.SparseDtype(dtype=<class 'numpy.float64'>, fill_value=None)\",\n",
       "   'function_text': 'Dtype for data stored in SparseArray.',\n",
       "   'parameter_names_desc': [{'param_name': 'dtype',\n",
       "     'param_type': 'str, ExtensionDtype, numpy.dtype, type, default numpy.float64',\n",
       "     'param_desc': 'The dtype of the underlying array storing the non-fill value values.\\n'},\n",
       "    {'param_name': 'fill_value',\n",
       "     'param_type': 'scalar, optional',\n",
       "     'param_desc': 'The scalar value not stored in the SparseArray. By default, this\\ndepends on dtype.\\n\\n\\ndtype\\nna_value\\n\\n\\n\\nfloat\\nnp.nan\\n\\nint\\n0\\n\\nbool\\nFalse\\n\\ndatetime64\\npd.NaT\\n\\ntimedelta64\\npd.NaT\\n\\n\\n\\nThe default value may be overridden by specifying a fill_value.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.SparseDtype.html#pandas.SparseDtype'},\n",
       "  {'function_name': 'pandas.arrays.StringArray',\n",
       "   'full_function': 'class pandas.arrays.StringArray(values, copy=False)',\n",
       "   'function_text': 'Extension array for string data.',\n",
       "   'parameter_names_desc': [{'param_name': 'values',\n",
       "     'param_type': 'array-like',\n",
       "     'param_desc': 'The array of data.\\n\\nWarning\\nCurrently, this expects an object-dtype ndarray\\nwhere the elements are Python strings\\nor nan-likes (None, np.nan, NA).\\nThis may change without warning in the future. Use\\npandas.array() with dtype=\"string\" for a stable way of\\ncreating a StringArray from any sequence.\\n\\n\\nChanged in version 1.5.0: StringArray now accepts array-likes containing\\nnan-likes(None, np.nan) for the values parameter\\nin addition to strings and pandas.NA\\n\\n'},\n",
       "    {'param_name': 'copy',\n",
       "     'param_type': 'bool, default False',\n",
       "     'param_desc': 'Whether to copy the array of data.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.arrays.StringArray.html#pandas.arrays.StringArray'},\n",
       "  {'function_name': 'pandas.StringDtype',\n",
       "   'full_function': 'class pandas.StringDtype(storage=None)',\n",
       "   'function_text': 'Extension dtype for string data.',\n",
       "   'parameter_names_desc': [{'param_name': 'storage',\n",
       "     'param_type': '{“python”, “pyarrow”, “pyarrow_numpy”}, optional',\n",
       "     'param_desc': 'If not given, the value of pd.options.mode.string_storage.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.StringDtype.html#pandas.StringDtype'},\n",
       "  {'function_name': 'pandas.arrays.BooleanArray',\n",
       "   'full_function': 'class pandas.arrays.BooleanArray(values, mask, copy=False)',\n",
       "   'function_text': 'Array of boolean (True/False) data with missing values.',\n",
       "   'parameter_names_desc': [{'param_name': 'values',\n",
       "     'param_type': 'numpy.ndarray',\n",
       "     'param_desc': 'A 1-d boolean-dtype array with the data.\\n'},\n",
       "    {'param_name': 'mask',\n",
       "     'param_type': 'numpy.ndarray',\n",
       "     'param_desc': 'A 1-d boolean-dtype array indicating missing values (True\\nindicates missing).\\n'},\n",
       "    {'param_name': 'copy',\n",
       "     'param_type': 'bool, default False',\n",
       "     'param_desc': 'Whether to copy the values and mask arrays.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.arrays.BooleanArray.html#pandas.arrays.BooleanArray'},\n",
       "  {'function_name': 'pandas.BooleanDtype',\n",
       "   'full_function': 'class pandas.BooleanDtype',\n",
       "   'function_text': 'Extension dtype for boolean data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.BooleanDtype.html#pandas.BooleanDtype'},\n",
       "  {'function_name': 'pandas.DatetimeTZDtype',\n",
       "   'full_function': \"class pandas.DatetimeTZDtype(unit='ns', tz=None)\",\n",
       "   'function_text': 'An ExtensionDtype for timezone-aware datetime data.',\n",
       "   'parameter_names_desc': [{'param_name': 'unit',\n",
       "     'param_type': 'str, default “ns”',\n",
       "     'param_desc': 'The precision of the datetime data. Currently limited\\nto \"ns\".\\n'},\n",
       "    {'param_name': 'tz',\n",
       "     'param_type': 'str, int, or datetime.tzinfo',\n",
       "     'param_desc': 'The timezone.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DatetimeTZDtype.html#pandas.DatetimeTZDtype'},\n",
       "  {'function_name': 'pandas.PeriodDtype',\n",
       "   'full_function': 'class pandas.PeriodDtype(freq)',\n",
       "   'function_text': 'An ExtensionDtype for Period data.',\n",
       "   'parameter_names_desc': [{'param_name': 'freq',\n",
       "     'param_type': 'str or DateOffset',\n",
       "     'param_desc': 'The frequency of this PeriodDtype.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.PeriodDtype.html#pandas.PeriodDtype'},\n",
       "  {'function_name': 'pandas.Int64Dtype',\n",
       "   'full_function': 'class pandas.Int64Dtype',\n",
       "   'function_text': 'An ExtensionDtype for int64 integer data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Int64Dtype.html#pandas.Int64Dtype'},\n",
       "  {'function_name': 'pandas.CategoricalDtype',\n",
       "   'full_function': 'class pandas.CategoricalDtype(categories=None, ordered=False)',\n",
       "   'function_text': 'Type for categorical data with the categories and orderedness.',\n",
       "   'parameter_names_desc': [{'param_name': 'categories',\n",
       "     'param_type': 'sequence, optional',\n",
       "     'param_desc': 'Must be unique, and must not contain any nulls.\\nThe categories are stored in an Index,\\nand if an index is provided the dtype of that index will be used.\\n'},\n",
       "    {'param_name': 'ordered',\n",
       "     'param_type': 'bool or None, default False',\n",
       "     'param_desc': 'Whether or not this categorical is treated as a ordered categorical.\\nNone can be used to maintain the ordered value of existing categoricals when\\nused in operations that combine categoricals, e.g. astype, and will resolve to\\nFalse if there is no existing ordered to maintain.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.CategoricalDtype.html#pandas.CategoricalDtype'},\n",
       "  {'function_name': 'pandas.StringDtype',\n",
       "   'full_function': 'class pandas.StringDtype(storage=None)',\n",
       "   'function_text': 'Extension dtype for string data.',\n",
       "   'parameter_names_desc': [{'param_name': 'storage',\n",
       "     'param_type': '{“python”, “pyarrow”, “pyarrow_numpy”}, optional',\n",
       "     'param_desc': 'If not given, the value of pd.options.mode.string_storage.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.StringDtype.html#pandas.StringDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.ArrowDtype',\n",
       "   'full_function': 'class pandas.ArrowDtype(pyarrow_dtype)',\n",
       "   'function_text': 'An ExtensionDtype for PyArrow data types.',\n",
       "   'parameter_names_desc': [{'param_name': 'pyarrow_dtype',\n",
       "     'param_type': 'pa.DataType',\n",
       "     'param_desc': 'An instance of a pyarrow.DataType.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype'},\n",
       "  {'function_name': 'pandas.Timestamp.day_of_week',\n",
       "   'full_function': 'Timestamp.day_of_week#',\n",
       "   'function_text': 'Return day of the week.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.day_of_week.html#pandas.Timestamp.day_of_week'},\n",
       "  {'function_name': 'pandas.Timestamp.day_of_year',\n",
       "   'full_function': 'Timestamp.day_of_year#',\n",
       "   'function_text': 'Return the day of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.day_of_year.html#pandas.Timestamp.day_of_year'},\n",
       "  {'function_name': 'pandas.Timestamp.daysinmonth',\n",
       "   'full_function': 'Timestamp.daysinmonth#',\n",
       "   'function_text': 'Return the number of days in the month.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.daysinmonth.html#pandas.Timestamp.daysinmonth'},\n",
       "  {'function_name': 'pandas.Timestamp.daysinmonth',\n",
       "   'full_function': 'Timestamp.daysinmonth#',\n",
       "   'function_text': 'Return the number of days in the month.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.daysinmonth.html#pandas.Timestamp.daysinmonth'},\n",
       "  {'function_name': 'pandas.Timestamp.is_month_end',\n",
       "   'full_function': 'Timestamp.is_month_end#',\n",
       "   'function_text': 'Check if the date is the last day of the month.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_month_end.html#pandas.Timestamp.is_month_end'},\n",
       "  {'function_name': 'pandas.Timestamp.is_quarter_end',\n",
       "   'full_function': 'Timestamp.is_quarter_end#',\n",
       "   'function_text': 'Check if date is last day of the quarter.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_quarter_end.html#pandas.Timestamp.is_quarter_end'},\n",
       "  {'function_name': 'pandas.Timestamp.is_year_end',\n",
       "   'full_function': 'Timestamp.is_year_end#',\n",
       "   'function_text': 'Return True if date is last day of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_year_end.html#pandas.Timestamp.is_year_end'},\n",
       "  {'function_name': 'pandas.Timestamp.is_year_end',\n",
       "   'full_function': 'Timestamp.is_year_end#',\n",
       "   'function_text': 'Return True if date is last day of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_year_end.html#pandas.Timestamp.is_year_end'},\n",
       "  {'function_name': 'pandas.Timestamp.is_year_end',\n",
       "   'full_function': 'Timestamp.is_year_end#',\n",
       "   'function_text': 'Return True if date is last day of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_year_end.html#pandas.Timestamp.is_year_end'},\n",
       "  {'function_name': 'pandas.Timestamp.is_year_end',\n",
       "   'full_function': 'Timestamp.is_year_end#',\n",
       "   'function_text': 'Return True if date is last day of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.is_year_end.html#pandas.Timestamp.is_year_end'},\n",
       "  {'function_name': 'pandas.Timestamp.quarter',\n",
       "   'full_function': 'Timestamp.quarter#',\n",
       "   'function_text': 'Return the quarter of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.quarter.html#pandas.Timestamp.quarter'},\n",
       "  {'function_name': 'pandas.Timestamp.quarter',\n",
       "   'full_function': 'Timestamp.quarter#',\n",
       "   'function_text': 'Return the quarter of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.quarter.html#pandas.Timestamp.quarter'},\n",
       "  {'function_name': 'pandas.Timestamp.quarter',\n",
       "   'full_function': 'Timestamp.quarter#',\n",
       "   'function_text': 'Return the quarter of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.quarter.html#pandas.Timestamp.quarter'},\n",
       "  {'function_name': 'pandas.Timestamp.quarter',\n",
       "   'full_function': 'Timestamp.quarter#',\n",
       "   'function_text': 'Return the quarter of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.quarter.html#pandas.Timestamp.quarter'},\n",
       "  {'function_name': 'pandas.Timestamp.weekofyear',\n",
       "   'full_function': 'Timestamp.weekofyear#',\n",
       "   'function_text': 'Return the week number of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.weekofyear.html#pandas.Timestamp.weekofyear'},\n",
       "  {'function_name': 'pandas.Timestamp.astimezone',\n",
       "   'full_function': 'Timestamp.astimezone(tz)#',\n",
       "   'function_text': 'Convert timezone-aware Timestamp to another time zone.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.astimezone.html#pandas.Timestamp.astimezone'},\n",
       "  {'function_name': 'pandas.Timestamp.combine',\n",
       "   'full_function': 'classmethod Timestamp.combine(date, time)#',\n",
       "   'function_text': 'Combine date, time into datetime with same date and time fields.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.combine.html#pandas.Timestamp.combine'},\n",
       "  {'function_name': 'pandas.Timestamp.date',\n",
       "   'full_function': 'Timestamp.date()#',\n",
       "   'function_text': 'Return date object with same year, month and day.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.date.html#pandas.Timestamp.date'},\n",
       "  {'function_name': 'pandas.Timestamp.dst',\n",
       "   'full_function': 'Timestamp.dst()#',\n",
       "   'function_text': 'Return the daylight saving time (DST) adjustment.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.dst.html#pandas.Timestamp.dst'},\n",
       "  {'function_name': 'pandas.Timestamp.fromordinal',\n",
       "   'full_function': 'classmethod Timestamp.fromordinal(ordinal, tz=None)#',\n",
       "   'function_text': 'Construct a timestamp from a a proleptic Gregorian ordinal.',\n",
       "   'parameter_names_desc': [{'param_name': 'ordinal',\n",
       "     'param_type': 'int',\n",
       "     'param_desc': 'Date corresponding to a proleptic Gregorian ordinal.\\n'},\n",
       "    {'param_name': 'tz',\n",
       "     'param_type': 'str, pytz.timezone, dateutil.tz.tzfile or None',\n",
       "     'param_desc': 'Time zone for the Timestamp.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.fromordinal.html#pandas.Timestamp.fromordinal'},\n",
       "  {'function_name': 'pandas.Timestamp.isocalendar',\n",
       "   'full_function': 'Timestamp.isocalendar()#',\n",
       "   'function_text': 'Return a named tuple containing ISO year, week number, and weekday.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.isocalendar.html#pandas.Timestamp.isocalendar'},\n",
       "  {'function_name': 'pandas.Timestamp.isoweekday',\n",
       "   'full_function': 'Timestamp.isoweekday()#',\n",
       "   'function_text': 'Return the day of the week represented by the date.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.isoweekday.html#pandas.Timestamp.isoweekday'},\n",
       "  {'function_name': 'pandas.Timestamp.normalize',\n",
       "   'full_function': 'Timestamp.normalize()#',\n",
       "   'function_text': 'Normalize Timestamp to midnight, preserving tz information.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.normalize.html#pandas.Timestamp.normalize'},\n",
       "  {'function_name': 'pandas.Timestamp.replace',\n",
       "   'full_function': \"Timestamp.replace(year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, nanosecond=None, tzinfo=<class 'object'>, fold=None)#\",\n",
       "   'function_text': 'Implements datetime.replace, handles nanoseconds.',\n",
       "   'parameter_names_desc': [{'param_name': 'year',\n",
       "     'param_type': 'int, optional',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'month', 'param_type': 'int, optional', 'param_desc': ''},\n",
       "    {'param_name': 'day', 'param_type': 'int, optional', 'param_desc': ''},\n",
       "    {'param_name': 'hour', 'param_type': 'int, optional', 'param_desc': ''},\n",
       "    {'param_name': 'minute', 'param_type': 'int, optional', 'param_desc': ''},\n",
       "    {'param_name': 'second', 'param_type': 'int, optional', 'param_desc': ''},\n",
       "    {'param_name': 'microsecond',\n",
       "     'param_type': 'int, optional',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'nanosecond',\n",
       "     'param_type': 'int, optional',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'tzinfo',\n",
       "     'param_type': 'tz-convertible, optional',\n",
       "     'param_desc': ''},\n",
       "    {'param_name': 'fold', 'param_type': 'int, optional', 'param_desc': ''}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.replace.html#pandas.Timestamp.replace'},\n",
       "  {'function_name': 'pandas.Timestamp.strftime',\n",
       "   'full_function': 'Timestamp.strftime(format)#',\n",
       "   'function_text': 'Return a formatted string of the Timestamp.',\n",
       "   'parameter_names_desc': [{'param_name': 'format',\n",
       "     'param_type': 'str',\n",
       "     'param_desc': 'Format string to convert Timestamp to string.\\nSee strftime documentation for more information on the format string:\\nhttps://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.strftime.html#pandas.Timestamp.strftime'},\n",
       "  {'function_name': 'pandas.Timestamp.time',\n",
       "   'full_function': 'Timestamp.time()#',\n",
       "   'function_text': 'Return time object with same time but with tzinfo=None.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.time.html#pandas.Timestamp.time'},\n",
       "  {'function_name': 'pandas.Timestamp.timetuple',\n",
       "   'full_function': 'Timestamp.timetuple()#',\n",
       "   'function_text': 'Return time tuple, compatible with time.localtime().',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.timetuple.html#pandas.Timestamp.timetuple'},\n",
       "  {'function_name': 'pandas.Timestamp.to_datetime64',\n",
       "   'full_function': 'Timestamp.to_datetime64()#',\n",
       "   'function_text': 'Return a numpy.datetime64 object with same precision.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.to_datetime64.html#pandas.Timestamp.to_datetime64'},\n",
       "  {'function_name': 'pandas.Timestamp.to_julian_date',\n",
       "   'full_function': 'Timestamp.to_julian_date()#',\n",
       "   'function_text': 'Convert TimeStamp to a Julian Date.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.to_julian_date.html#pandas.Timestamp.to_julian_date'},\n",
       "  {'function_name': 'pandas.Timestamp.to_pydatetime',\n",
       "   'full_function': 'Timestamp.to_pydatetime(warn=True)#',\n",
       "   'function_text': 'Convert a Timestamp object to a native Python datetime object.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.to_pydatetime.html#pandas.Timestamp.to_pydatetime'},\n",
       "  {'function_name': 'pandas.Timestamp.toordinal',\n",
       "   'full_function': 'Timestamp.toordinal()#',\n",
       "   'function_text': 'Return proleptic Gregorian ordinal. January 1 of year 1 is day 1.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.toordinal.html#pandas.Timestamp.toordinal'},\n",
       "  {'function_name': 'pandas.Timestamp.tz_localize',\n",
       "   'full_function': \"Timestamp.tz_localize(tz, ambiguous='raise', nonexistent='raise')#\",\n",
       "   'function_text': 'Localize the Timestamp to a timezone.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.tz_localize.html#pandas.Timestamp.tz_localize'},\n",
       "  {'function_name': 'pandas.Timestamp.utcfromtimestamp',\n",
       "   'full_function': 'classmethod Timestamp.utcfromtimestamp(ts)#',\n",
       "   'function_text': 'Construct a timezone-aware UTC datetime from a POSIX timestamp.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.utcfromtimestamp.html#pandas.Timestamp.utcfromtimestamp'},\n",
       "  {'function_name': 'pandas.Timestamp.utcoffset',\n",
       "   'full_function': 'Timestamp.utcoffset()#',\n",
       "   'function_text': 'Return utc offset.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.utcoffset.html#pandas.Timestamp.utcoffset'},\n",
       "  {'function_name': 'pandas.Timestamp.weekday',\n",
       "   'full_function': 'Timestamp.weekday()#',\n",
       "   'function_text': 'Return the day of the week represented by the date.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.weekday.html#pandas.Timestamp.weekday'},\n",
       "  {'function_name': 'pandas.Timedelta.components',\n",
       "   'full_function': 'Timedelta.components#',\n",
       "   'function_text': 'Return a components namedtuple-like.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.components.html#pandas.Timedelta.components'},\n",
       "  {'function_name': 'pandas.Timedelta.components',\n",
       "   'full_function': 'Timedelta.components#',\n",
       "   'function_text': 'Return a components namedtuple-like.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.components.html#pandas.Timedelta.components'},\n",
       "  {'function_name': 'pandas.Timedelta.components',\n",
       "   'full_function': 'Timedelta.components#',\n",
       "   'function_text': 'Return a components namedtuple-like.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.components.html#pandas.Timedelta.components'},\n",
       "  {'function_name': 'pandas.Timedelta.components',\n",
       "   'full_function': 'Timedelta.components#',\n",
       "   'function_text': 'Return a components namedtuple-like.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.components.html#pandas.Timedelta.components'},\n",
       "  {'function_name': 'pandas.Timedelta.components',\n",
       "   'full_function': 'Timedelta.components#',\n",
       "   'function_text': 'Return a components namedtuple-like.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.components.html#pandas.Timedelta.components'},\n",
       "  {'function_name': 'pandas.Timedelta.view',\n",
       "   'full_function': 'Timedelta.view(dtype)#',\n",
       "   'function_text': 'Array view compatibility.',\n",
       "   'parameter_names_desc': [{'param_name': 'dtype',\n",
       "     'param_type': 'str or dtype',\n",
       "     'param_desc': 'The dtype to view the underlying data as.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.view.html#pandas.Timedelta.view'},\n",
       "  {'function_name': 'pandas.Timedelta.ceil',\n",
       "   'full_function': 'Timedelta.ceil(freq)#',\n",
       "   'function_text': 'Return a new Timedelta ceiled to this resolution.',\n",
       "   'parameter_names_desc': [{'param_name': 'freq',\n",
       "     'param_type': 'str',\n",
       "     'param_desc': 'Frequency string indicating the ceiling resolution.\\nIt uses the same units as class constructor Timedelta.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.ceil.html#pandas.Timedelta.ceil'},\n",
       "  {'function_name': 'pandas.Timedelta.isoformat',\n",
       "   'full_function': 'Timedelta.isoformat()#',\n",
       "   'function_text': 'Format the Timedelta as ISO 8601 Duration.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.isoformat.html#pandas.Timedelta.isoformat'},\n",
       "  {'function_name': 'pandas.Timedelta.to_pytimedelta',\n",
       "   'full_function': 'Timedelta.to_pytimedelta()#',\n",
       "   'function_text': 'Convert a pandas Timedelta object into a python datetime.timedelta object.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.to_pytimedelta.html#pandas.Timedelta.to_pytimedelta'},\n",
       "  {'function_name': 'pandas.Timedelta.to_numpy',\n",
       "   'full_function': 'Timedelta.to_numpy(dtype=None, copy=False)#',\n",
       "   'function_text': 'Convert the Timedelta to a NumPy timedelta64.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.to_numpy.html#pandas.Timedelta.to_numpy'},\n",
       "  {'function_name': 'pandas.Period.dayofweek',\n",
       "   'full_function': 'Period.dayofweek#',\n",
       "   'function_text': 'Day of the week the period lies in, with Monday=0 and Sunday=6.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.dayofweek.html#pandas.Period.dayofweek'},\n",
       "  {'function_name': 'pandas.Period.dayofyear',\n",
       "   'full_function': 'Period.dayofyear#',\n",
       "   'function_text': 'Return the day of the year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.dayofyear.html#pandas.Period.dayofyear'},\n",
       "  {'function_name': 'pandas.Period.days_in_month',\n",
       "   'full_function': 'Period.days_in_month#',\n",
       "   'function_text': 'Get the total number of days in the month that this period falls on.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.days_in_month.html#pandas.Period.days_in_month'},\n",
       "  {'function_name': 'pandas.Period.end_time',\n",
       "   'full_function': 'Period.end_time#',\n",
       "   'function_text': 'Get the Timestamp for the end of the period.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.end_time.html#pandas.Period.end_time'},\n",
       "  {'function_name': 'pandas.Period.freqstr',\n",
       "   'full_function': 'Period.freqstr#',\n",
       "   'function_text': 'Return a string representation of the frequency.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.freqstr.html#pandas.Period.freqstr'},\n",
       "  {'function_name': 'pandas.Period.is_leap_year',\n",
       "   'full_function': 'Period.is_leap_year#',\n",
       "   'function_text': 'Return True if the period’s year is in a leap year.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.is_leap_year.html#pandas.Period.is_leap_year'},\n",
       "  {'function_name': 'pandas.Period.month',\n",
       "   'full_function': 'Period.month#',\n",
       "   'function_text': 'Return the month this Period falls on.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.month.html#pandas.Period.month'},\n",
       "  {'function_name': 'pandas.Period.quarter',\n",
       "   'full_function': 'Period.quarter#',\n",
       "   'function_text': 'Return the quarter this Period falls on.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.quarter.html#pandas.Period.quarter'},\n",
       "  {'function_name': 'pandas.Period.second',\n",
       "   'full_function': 'Period.second#',\n",
       "   'function_text': 'Get the second component of the Period.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.second.html#pandas.Period.second'},\n",
       "  {'function_name': 'pandas.Period.week',\n",
       "   'full_function': 'Period.week#',\n",
       "   'function_text': 'Get the week of the year on the given Period.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.week.html#pandas.Period.week'},\n",
       "  {'function_name': 'pandas.Period.weekofyear',\n",
       "   'full_function': 'Period.weekofyear#',\n",
       "   'function_text': 'Get the week of the year on the given Period.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.weekofyear.html#pandas.Period.weekofyear'},\n",
       "  {'function_name': 'pandas.Period.now',\n",
       "   'full_function': 'classmethod Period.now(freq)#',\n",
       "   'function_text': 'Return the period of now’s date.',\n",
       "   'parameter_names_desc': [{'param_name': 'freq',\n",
       "     'param_type': 'str, BaseOffset',\n",
       "     'param_desc': 'Frequency to use for the returned period.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.now.html#pandas.Period.now'},\n",
       "  {'function_name': 'pandas.Period.to_timestamp',\n",
       "   'full_function': \"Period.to_timestamp(freq=None, how='start')#\",\n",
       "   'function_text': 'Return the Timestamp representation of the Period.',\n",
       "   'parameter_names_desc': [{'param_name': 'freq',\n",
       "     'param_type': 'str or DateOffset',\n",
       "     'param_desc': 'Target frequency. Default is ‘D’ if self.freq is week or\\nlonger and ‘S’ otherwise.\\n'},\n",
       "    {'param_name': 'how',\n",
       "     'param_type': 'str, default ‘S’ (start)',\n",
       "     'param_desc': 'One of ‘S’, ‘E’. Can be aliased as case insensitive\\n‘Start’, ‘Finish’, ‘Begin’, ‘End’.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Period.to_timestamp.html#pandas.Period.to_timestamp'},\n",
       "  {'function_name': 'pandas.Interval.closed_left',\n",
       "   'full_function': 'Interval.closed_left#',\n",
       "   'function_text': 'Check if the interval is closed on the left side.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Interval.closed_left.html#pandas.Interval.closed_left'},\n",
       "  {'function_name': 'pandas.Interval.is_empty',\n",
       "   'full_function': 'Interval.is_empty#',\n",
       "   'function_text': 'Indicates if an interval is empty, meaning it contains no points.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Interval.is_empty.html#pandas.Interval.is_empty'},\n",
       "  {'function_name': 'pandas.Interval.length',\n",
       "   'full_function': 'Interval.length#',\n",
       "   'function_text': 'Return the length of the Interval.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Interval.length.html#pandas.Interval.length'},\n",
       "  {'function_name': 'pandas.Interval.open_left',\n",
       "   'full_function': 'Interval.open_left#',\n",
       "   'function_text': 'Check if the interval is open on the left side.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Interval.open_left.html#pandas.Interval.open_left'},\n",
       "  {'function_name': 'pandas.Interval.overlaps',\n",
       "   'full_function': 'Interval.overlaps(other)#',\n",
       "   'function_text': 'Check whether two Interval objects overlap.',\n",
       "   'parameter_names_desc': [{'param_name': 'other',\n",
       "     'param_type': 'Interval',\n",
       "     'param_desc': 'Interval to check against for an overlap.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Interval.overlaps.html#pandas.Interval.overlaps'},\n",
       "  {'function_name': 'pandas.Int16Dtype',\n",
       "   'full_function': 'class pandas.Int16Dtype',\n",
       "   'function_text': 'An ExtensionDtype for int16 integer data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Int16Dtype.html#pandas.Int16Dtype'},\n",
       "  {'function_name': 'pandas.Int64Dtype',\n",
       "   'full_function': 'class pandas.Int64Dtype',\n",
       "   'function_text': 'An ExtensionDtype for int64 integer data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Int64Dtype.html#pandas.Int64Dtype'},\n",
       "  {'function_name': 'pandas.UInt16Dtype',\n",
       "   'full_function': 'class pandas.UInt16Dtype',\n",
       "   'function_text': 'An ExtensionDtype for uint16 integer data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.UInt16Dtype.html#pandas.UInt16Dtype'},\n",
       "  {'function_name': 'pandas.UInt64Dtype',\n",
       "   'full_function': 'class pandas.UInt64Dtype',\n",
       "   'function_text': 'An ExtensionDtype for uint64 integer data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.UInt64Dtype.html#pandas.UInt64Dtype'},\n",
       "  {'function_name': 'pandas.Float64Dtype',\n",
       "   'full_function': 'class pandas.Float64Dtype',\n",
       "   'function_text': 'An ExtensionDtype for float64 data.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Float64Dtype.html#pandas.Float64Dtype'},\n",
       "  {'function_name': 'pandas.CategoricalDtype.ordered',\n",
       "   'full_function': 'property CategoricalDtype.ordered',\n",
       "   'function_text': 'Whether the categories have an ordered relationship.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.CategoricalDtype.ordered.html#pandas.CategoricalDtype.ordered'},\n",
       "  {'function_name': 'pandas.Categorical.categories',\n",
       "   'full_function': 'property Categorical.categories',\n",
       "   'function_text': 'The categories of this categorical.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.categories.html#pandas.Categorical.categories'},\n",
       "  {'function_name': 'pandas.Categorical.codes',\n",
       "   'full_function': 'property Categorical.codes',\n",
       "   'function_text': 'The category codes of this categorical index.',\n",
       "   'parameter_names_desc': [],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.Categorical.codes.html#pandas.Categorical.codes'},\n",
       "  {'function_name': 'pandas.arrays.ArrowStringArray',\n",
       "   'full_function': 'class pandas.arrays.ArrowStringArray(values)',\n",
       "   'function_text': 'Extension array for string data in a pyarrow.ChunkedArray.',\n",
       "   'parameter_names_desc': [{'param_name': 'values',\n",
       "     'param_type': 'pyarrow.Array or pyarrow.ChunkedArray',\n",
       "     'param_desc': 'The array of data.\\n'}],\n",
       "   'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.arrays.ArrowStringArray.html#pandas.arrays.ArrowStringArray'}]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_def_dict['arrays.html']['functions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
