{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://pandas.pydata.org/docs/reference/index.html\"\n",
    "response = requests.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_url = \"https://pandas.pydata.org/docs/reference/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_elems = soup.find_all(class_=\"toctree-l1\")\n",
    "req_l1_elems = l1_elems[-len(l1_elems)+16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level = {}\n",
    "curr_parent = \"\"\n",
    "for parent_functions in req_l1_elems:\n",
    "    for func in parent_functions.find_all('a'):\n",
    "        href = func['href']\n",
    "        if \"#\" not in href and \"api/pandas\" not in href:\n",
    "            first_level.update({href:{\"functions\":[],\"name\":func.text,\"url\":ref_url+href}})\n",
    "            curr_parent = href\n",
    "        else:\n",
    "            first_level[curr_parent]['functions'].append({\n",
    "                \"name\":func.text,\n",
    "                \"url\":ref_url+href\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'pandas.plotting.andrews_curves',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.andrews_curves.html'},\n",
       " {'name': 'pandas.plotting.autocorrelation_plot',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html'},\n",
       " {'name': 'pandas.plotting.bootstrap_plot',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.bootstrap_plot.html'},\n",
       " {'name': 'pandas.plotting.boxplot',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.boxplot.html'},\n",
       " {'name': 'pandas.plotting.deregister_matplotlib_converters',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.deregister_matplotlib_converters.html'},\n",
       " {'name': 'pandas.plotting.lag_plot',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.lag_plot.html'},\n",
       " {'name': 'pandas.plotting.parallel_coordinates',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.parallel_coordinates.html'},\n",
       " {'name': 'pandas.plotting.plot_params',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.plot_params.html'},\n",
       " {'name': 'pandas.plotting.radviz',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.radviz.html'},\n",
       " {'name': 'pandas.plotting.register_matplotlib_converters',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.register_matplotlib_converters.html'},\n",
       " {'name': 'pandas.plotting.scatter_matrix',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html'},\n",
       " {'name': 'pandas.plotting.table',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.table.html'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_level['plotting.html']['functions']\n",
    "# https://pandas.pydata.org/docs/reference/io.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACTING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent_url = first_level['io.html']['url']\n",
    "\n",
    "# parent_soup = BeautifulSoup(requests.get(parent_url).text,'lxml')\n",
    "# url_id = \"pickling\"\n",
    "# s1 = parent_soup.find(attrs={\"id\":url_id})\n",
    "# s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_level['io.html']['functions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'functions': [{'name': 'pandas.plotting.andrews_curves',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.andrews_curves.html'},\n",
       "  {'name': 'pandas.plotting.autocorrelation_plot',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html'},\n",
       "  {'name': 'pandas.plotting.bootstrap_plot',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.bootstrap_plot.html'},\n",
       "  {'name': 'pandas.plotting.boxplot',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.boxplot.html'},\n",
       "  {'name': 'pandas.plotting.deregister_matplotlib_converters',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.deregister_matplotlib_converters.html'},\n",
       "  {'name': 'pandas.plotting.lag_plot',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.lag_plot.html'},\n",
       "  {'name': 'pandas.plotting.parallel_coordinates',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.parallel_coordinates.html'},\n",
       "  {'name': 'pandas.plotting.plot_params',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.plot_params.html'},\n",
       "  {'name': 'pandas.plotting.radviz',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.radviz.html'},\n",
       "  {'name': 'pandas.plotting.register_matplotlib_converters',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.register_matplotlib_converters.html'},\n",
       "  {'name': 'pandas.plotting.scatter_matrix',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html'},\n",
       "  {'name': 'pandas.plotting.table',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.table.html'}],\n",
       " 'name': 'Plotting',\n",
       " 'url': 'https://pandas.pydata.org/docs/reference/plotting.html'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_level['plotting.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object is not subscriptable\n",
      "series.html\n",
      "<tr class=\"row-odd\"><th class=\"head\"><p>Data Type</p></th>\n",
      "<th class=\"head\"><p>Accessor</p></th>\n",
      "</tr>\n",
      "accessors\n",
      "----------------------------------------------------------------------------------------------------\n",
      "'NoneType' object is not subscriptable\n",
      "arrays.html\n",
      "<tr class=\"row-odd\"><th class=\"head\"><p>Kind of Data</p></th>\n",
      "<th class=\"head\"><p>pandas Data Type</p></th>\n",
      "<th class=\"head\"><p>Scalar</p></th>\n",
      "<th class=\"head\"><p>Array</p></th>\n",
      "</tr>\n",
      "objects\n",
      "----------------------------------------------------------------------------------------------------\n",
      "'NoneType' object is not subscriptable\n",
      "arrays.html\n",
      "<tr class=\"row-odd\"><th class=\"head\"><p>PyArrow type</p></th>\n",
      "<th class=\"head\"><p>pandas extension type</p></th>\n",
      "<th class=\"head\"><p>NumPy type</p></th>\n",
      "</tr>\n",
      "objects\n",
      "----------------------------------------------------------------------------------------------------\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.tseries.frequencies.to_offset.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.tseries.frequencies.to_offset.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.andrews_curves.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.andrews_curves.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.bootstrap_plot.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.bootstrap_plot.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.boxplot.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.boxplot.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.deregister_matplotlib_converters.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.deregister_matplotlib_converters.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.lag_plot.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.lag_plot.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.parallel_coordinates.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.parallel_coordinates.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.plot_params.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.plot_params.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.radviz.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.radviz.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.register_matplotlib_converters.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.register_matplotlib_converters.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.table.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.plotting.table.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.register_extension_dtype.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.register_extension_dtype.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.register_dataframe_accessor.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.register_dataframe_accessor.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.register_series_accessor.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.register_series_accessor.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.register_index_accessor.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.register_index_accessor.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.ExtensionDtype.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.ExtensionDtype.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.ExtensionArray.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.ExtensionArray.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.arrays.NumpyExtensionArray.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.arrays.NumpyExtensionArray.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.indexers.check_array_indexer.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.api.indexers.check_array_indexer.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.NA.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.NA.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.NaT.html\n",
      "'NoneType' object has no attribute 'find_all' https://pandas.pydata.org/docs/reference/api/pandas.NaT.html\n"
     ]
    }
   ],
   "source": [
    "def get_links(id_elem,base_func_url,class_name,first_level_name,url_id):\n",
    "    curr_urls = []\n",
    "    try:\n",
    "        func_urls = id_elem.find_all(attrs={\"class\":class_name})\n",
    "        for odd_url in func_urls:\n",
    "            try:\n",
    "                func_url = odd_url.find('a')['href']\n",
    "                curr_urls.append(base_func_url+func_url)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(first_level_name)\n",
    "                print(odd_url)\n",
    "                print(url_id)\n",
    "                print('-'*100)\n",
    "        return curr_urls\n",
    "    except Exception as e:\n",
    "        print(e,url_id)\n",
    "        func_urls = id_elem.find(attrs={\"class\":class_name}).find('a')['href']\n",
    "        curr_urls.append(base_func_url+func_url)\n",
    "        return curr_urls\n",
    "    finally:\n",
    "        return curr_urls\n",
    "    \n",
    "base_func_url = \"https://pandas.pydata.org/docs/reference/\"\n",
    "for first_level_name in first_level:\n",
    "    parent_url = first_level[first_level_name]['url']\n",
    "    parent_soup = BeautifulSoup(requests.get(parent_url).text,'lxml')\n",
    "    for idx,func in enumerate(first_level[first_level_name]['functions']):\n",
    "        url_id = func['url'].split(\"#\")[-1]\n",
    "        id_elem = parent_soup.find(attrs={\"id\":url_id})\n",
    "        odd_urls = get_links(id_elem,base_func_url,\"row-odd\",first_level_name,url_id)     \n",
    "        even_urls = get_links(id_elem,base_func_url,\"row-even\",first_level_name,url_id)     \n",
    "        # all_urls.extend(odd_urls)\n",
    "        # all_urls.extend(even_urls)\n",
    "        if odd_urls == [] and even_urls == []:\n",
    "            function_urls = [first_level[first_level_name]['functions'][idx]['url']]\n",
    "        else:\n",
    "            function_urls = odd_urls + even_urls\n",
    "        first_level[first_level_name]['functions'][idx]['function_urls'] = function_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'pandas.plotting.andrews_curves',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.andrews_curves.html',\n",
       "  'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.andrews_curves.html']},\n",
       " {'name': 'pandas.plotting.autocorrelation_plot',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html',\n",
       "  'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html']},\n",
       " {'name': 'pandas.plotting.bootstrap_plot',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.bootstrap_plot.html',\n",
       "  'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.bootstrap_plot.html']},\n",
       " {'name': 'pandas.plotting.boxplot',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.boxplot.html',\n",
       "  'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.boxplot.html']},\n",
       " {'name': 'pandas.plotting.deregister_matplotlib_converters',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.deregister_matplotlib_converters.html',\n",
       "  'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.deregister_matplotlib_converters.html']},\n",
       " {'name': 'pandas.plotting.lag_plot',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.lag_plot.html',\n",
       "  'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.lag_plot.html']},\n",
       " {'name': 'pandas.plotting.parallel_coordinates',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.parallel_coordinates.html',\n",
       "  'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.parallel_coordinates.html']},\n",
       " {'name': 'pandas.plotting.plot_params',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.plot_params.html',\n",
       "  'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.plot_params.html']},\n",
       " {'name': 'pandas.plotting.radviz',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.radviz.html',\n",
       "  'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.radviz.html']},\n",
       " {'name': 'pandas.plotting.register_matplotlib_converters',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.register_matplotlib_converters.html',\n",
       "  'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.register_matplotlib_converters.html']},\n",
       " {'name': 'pandas.plotting.scatter_matrix',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html',\n",
       "  'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html']},\n",
       " {'name': 'pandas.plotting.table',\n",
       "  'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.table.html',\n",
       "  'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.table.html']}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_level['plotting.html']['functions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "logname = \"pandas.log\"\n",
    "# logging.basicConfig(filename=logname,\n",
    "#                     filemode='a',\n",
    "#                     format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "#                     datefmt='%H:%M:%S',\n",
    "#                     level=logging.DEBUG)\n",
    "\n",
    "def get_param_data(first_level):\n",
    "    not_worked = []\n",
    "    for parent in first_level:\n",
    "        parent_dict = first_level[parent]['functions']\n",
    "        for idx,sub_level in enumerate(parent_dict):\n",
    "            parent_dict[idx].update({\"function_definitions\":[]})\n",
    "            for func_url in tqdm(sub_level['function_urls']):\n",
    "                func_response = requests.get(func_url)\n",
    "                func_soup = BeautifulSoup(func_response.content, \"lxml\",from_encoding=\"utf-8\")\n",
    "                \n",
    "                func_name = func_soup.find(\"h1\").text.replace(\"#\",\"\") #remove #\n",
    "                elem = func_soup.find(attrs={\"class\":\"sig sig-object py\"})\n",
    "                try:\n",
    "                    full_function = elem.text.replace(\"[source]#\",\"\").replace(\"\\n\",\"\")\n",
    "                    func_text = func_soup.find(\"dd\").find('p').text\n",
    "                    curr_dict = {\"function_name\":func_name,\"full_function\":full_function,\"function_text\":func_text,\"parameter_names_desc\":[],\"function_url\":func_url}\n",
    "                    em = func_soup.find_all(attrs={\"class\":\"field-odd\"})\n",
    "                    if em[0].text =='Parameters:':\n",
    "                        param_names = em[-1].find_all(\"dt\")\n",
    "                        desc_list = em[-1].find_all(\"dd\")\n",
    "\n",
    "                        # if len(param_names)!=len(desc_list):\n",
    "                        #     print(func_name)\n",
    "                        #     print(f\"param_names={len(param_names)} and desc_list={len(desc_list)}\")\n",
    "                        # first_level[parent]['function_definitions'].append()\n",
    "                        for pn,dn in zip(param_names,desc_list):\n",
    "                            try:\n",
    "                                param_name = pn.strong.text\n",
    "                                param_type = pn.find(attrs={\"class\":\"classifier\"}).text\n",
    "                                if param_name == \"**kwargs\": \n",
    "                                    continue\n",
    "                                param_desc = dn.text\n",
    "                                curr_dict['parameter_names_desc'].append({\"param_name\":param_name,\"param_type\":param_type,\"param_desc\":param_desc})\n",
    "                            except Exception as e:\n",
    "                                print(e,pn.text)\n",
    "                    # else:\n",
    "                    #     parent_dict[idx]['function_definitions'].append(curr_dict)\n",
    "                    #     continue\n",
    "                except Exception as e:\n",
    "                    # print(e)\n",
    "                    not_worked.append((func_name,func_text,e,func_url))\n",
    "                    # logging.debug(\"With error {e} for {func_name} and {func_text}\")\n",
    "                finally:\n",
    "                    parent_dict[idx]['function_definitions'].append(curr_dict)\n",
    "        clear_output(wait=True)\n",
    "    return first_level,not_worked\n",
    "\n",
    "function_def_dict,not_worked = get_param_data(first_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'functions': [{'name': 'pandas.plotting.andrews_curves',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.andrews_curves.html',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.andrews_curves.html'],\n",
       "   'function_definitions': [{'function_name': 'pandas.plotting.andrews_curves',\n",
       "     'full_function': 'pandas.plotting.andrews_curves(frame, class_column, ax=None, samples=200, color=None, colormap=None, **kwargs)',\n",
       "     'function_text': 'Generate a matplotlib plot for visualizing clusters of multivariate data.',\n",
       "     'parameter_names_desc': [{'param_name': 'frame',\n",
       "       'param_type': 'DataFrame',\n",
       "       'param_desc': 'Data to be plotted, preferably normalized to (0.0, 1.0).\\n'},\n",
       "      {'param_name': 'class_column',\n",
       "       'param_type': 'label',\n",
       "       'param_desc': 'Name of the column containing class names.\\n'},\n",
       "      {'param_name': 'ax',\n",
       "       'param_type': 'axes object, default None',\n",
       "       'param_desc': 'Axes to use.\\n'},\n",
       "      {'param_name': 'samples',\n",
       "       'param_type': 'int',\n",
       "       'param_desc': 'Number of points to plot in each curve.\\n'},\n",
       "      {'param_name': 'color',\n",
       "       'param_type': 'str, list[str] or tuple[str], optional',\n",
       "       'param_desc': 'Colors to use for the different classes. Colors can be strings\\nor 3-element floating point RGB values.\\n'},\n",
       "      {'param_name': 'colormap',\n",
       "       'param_type': 'str or matplotlib colormap object, default None',\n",
       "       'param_desc': 'Colormap to select colors from. If a string, load colormap with that\\nname from matplotlib.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.andrews_curves.html'}]},\n",
       "  {'name': 'pandas.plotting.autocorrelation_plot',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html'],\n",
       "   'function_definitions': [{'function_name': 'pandas.plotting.autocorrelation_plot',\n",
       "     'full_function': 'pandas.plotting.autocorrelation_plot(series, ax=None, **kwargs)',\n",
       "     'function_text': 'Autocorrelation plot for time series.',\n",
       "     'parameter_names_desc': [{'param_name': 'series',\n",
       "       'param_type': 'Series',\n",
       "       'param_desc': 'The time series to visualize.\\n'},\n",
       "      {'param_name': 'ax',\n",
       "       'param_type': 'Matplotlib axis object, optional',\n",
       "       'param_desc': 'The matplotlib axis object to use.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html'}]},\n",
       "  {'name': 'pandas.plotting.bootstrap_plot',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.bootstrap_plot.html',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.bootstrap_plot.html'],\n",
       "   'function_definitions': [{'function_name': 'pandas.plotting.bootstrap_plot',\n",
       "     'full_function': 'pandas.plotting.bootstrap_plot(series, fig=None, size=50, samples=500, **kwds)',\n",
       "     'function_text': 'Bootstrap plot on mean, median and mid-range statistics.',\n",
       "     'parameter_names_desc': [{'param_name': 'series',\n",
       "       'param_type': 'pandas.Series',\n",
       "       'param_desc': 'Series from where to get the samplings for the bootstrapping.\\n'},\n",
       "      {'param_name': 'fig',\n",
       "       'param_type': 'matplotlib.figure.Figure, default None',\n",
       "       'param_desc': 'If given, it will use the fig reference for plotting instead of\\ncreating a new one with default parameters.\\n'},\n",
       "      {'param_name': 'size',\n",
       "       'param_type': 'int, default 50',\n",
       "       'param_desc': 'Number of data points to consider during each sampling. It must be\\nless than or equal to the length of the series.\\n'},\n",
       "      {'param_name': 'samples',\n",
       "       'param_type': 'int, default 500',\n",
       "       'param_desc': 'Number of times the bootstrap procedure is performed.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.bootstrap_plot.html'}]},\n",
       "  {'name': 'pandas.plotting.boxplot',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.boxplot.html',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.boxplot.html'],\n",
       "   'function_definitions': [{'function_name': 'pandas.plotting.boxplot',\n",
       "     'full_function': 'pandas.plotting.boxplot(data, column=None, by=None, ax=None, fontsize=None, rot=0, grid=True, figsize=None, layout=None, return_type=None, **kwargs)',\n",
       "     'function_text': 'Make a box plot from DataFrame columns.',\n",
       "     'parameter_names_desc': [{'param_name': 'data',\n",
       "       'param_type': 'DataFrame',\n",
       "       'param_desc': 'The data to visualize.\\n'},\n",
       "      {'param_name': 'column',\n",
       "       'param_type': 'str or list of str, optional',\n",
       "       'param_desc': 'Column name or list of names, or vector.\\nCan be any valid input to pandas.DataFrame.groupby().\\n'},\n",
       "      {'param_name': 'by',\n",
       "       'param_type': 'str or array-like, optional',\n",
       "       'param_desc': 'Column in the DataFrame to pandas.DataFrame.groupby().\\nOne box-plot will be done per value of columns in by.\\n'},\n",
       "      {'param_name': 'ax',\n",
       "       'param_type': 'object of class matplotlib.axes.Axes, optional',\n",
       "       'param_desc': 'The matplotlib axes to be used by boxplot.\\n'},\n",
       "      {'param_name': 'fontsize',\n",
       "       'param_type': 'float or str',\n",
       "       'param_desc': 'Tick label font size in points or as a string (e.g., large).\\n'},\n",
       "      {'param_name': 'rot',\n",
       "       'param_type': 'float, default 0',\n",
       "       'param_desc': 'The rotation angle of labels (in degrees)\\nwith respect to the screen coordinate system.\\n'},\n",
       "      {'param_name': 'grid',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Setting this to True will show the grid.\\n'},\n",
       "      {'param_name': 'figsize',\n",
       "       'param_type': 'A tuple (width, height) in inches',\n",
       "       'param_desc': 'The size of the figure to create in matplotlib.\\n'},\n",
       "      {'param_name': 'layout',\n",
       "       'param_type': 'tuple (rows, columns), optional',\n",
       "       'param_desc': 'For example, (3, 5) will display the subplots\\nusing 3 rows and 5 columns, starting from the top-left.\\n'},\n",
       "      {'param_name': 'return_type',\n",
       "       'param_type': '{‘axes’, ‘dict’, ‘both’} or None, default ‘axes’',\n",
       "       'param_desc': 'The kind of object to return. The default is axes.\\n\\n‘axes’ returns the matplotlib axes the boxplot is drawn on.\\n‘dict’ returns a dictionary whose values are the matplotlib\\nLines of the boxplot.\\n‘both’ returns a namedtuple with the axes and dict.\\nwhen grouping with by, a Series mapping columns to\\nreturn_type is returned.\\nIf return_type is None, a NumPy array\\nof axes with the same shape as layout is returned.\\n\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.boxplot.html'}]},\n",
       "  {'name': 'pandas.plotting.deregister_matplotlib_converters',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.deregister_matplotlib_converters.html',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.deregister_matplotlib_converters.html'],\n",
       "   'function_definitions': [{'function_name': 'pandas.plotting.deregister_matplotlib_converters',\n",
       "     'full_function': 'pandas.plotting.deregister_matplotlib_converters()',\n",
       "     'function_text': 'Remove pandas formatters and converters.',\n",
       "     'parameter_names_desc': [],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.deregister_matplotlib_converters.html'}]},\n",
       "  {'name': 'pandas.plotting.lag_plot',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.lag_plot.html',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.lag_plot.html'],\n",
       "   'function_definitions': [{'function_name': 'pandas.plotting.lag_plot',\n",
       "     'full_function': 'pandas.plotting.lag_plot(series, lag=1, ax=None, **kwds)',\n",
       "     'function_text': 'Lag plot for time series.',\n",
       "     'parameter_names_desc': [{'param_name': 'series',\n",
       "       'param_type': 'Series',\n",
       "       'param_desc': 'The time series to visualize.\\n'},\n",
       "      {'param_name': 'lag',\n",
       "       'param_type': 'int, default 1',\n",
       "       'param_desc': 'Lag length of the scatter plot.\\n'},\n",
       "      {'param_name': 'ax',\n",
       "       'param_type': 'Matplotlib axis object, optional',\n",
       "       'param_desc': 'The matplotlib axis object to use.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.lag_plot.html'}]},\n",
       "  {'name': 'pandas.plotting.parallel_coordinates',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.parallel_coordinates.html',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.parallel_coordinates.html'],\n",
       "   'function_definitions': [{'function_name': 'pandas.plotting.parallel_coordinates',\n",
       "     'full_function': 'pandas.plotting.parallel_coordinates(frame, class_column, cols=None, ax=None, color=None, use_columns=False, xticks=None, colormap=None, axvlines=True, axvlines_kwds=None, sort_labels=False, **kwargs)',\n",
       "     'function_text': 'Parallel coordinates plotting.',\n",
       "     'parameter_names_desc': [{'param_name': 'frame',\n",
       "       'param_type': 'DataFrame',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'class_column',\n",
       "       'param_type': 'str',\n",
       "       'param_desc': 'Column name containing class names.\\n'},\n",
       "      {'param_name': 'cols',\n",
       "       'param_type': 'list, optional',\n",
       "       'param_desc': 'A list of column names to use.\\n'},\n",
       "      {'param_name': 'ax',\n",
       "       'param_type': 'matplotlib.axis, optional',\n",
       "       'param_desc': 'Matplotlib axis object.\\n'},\n",
       "      {'param_name': 'color',\n",
       "       'param_type': 'list or tuple, optional',\n",
       "       'param_desc': 'Colors to use for the different classes.\\n'},\n",
       "      {'param_name': 'use_columns',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'If true, columns will be used as xticks.\\n'},\n",
       "      {'param_name': 'xticks',\n",
       "       'param_type': 'list or tuple, optional',\n",
       "       'param_desc': 'A list of values to use for xticks.\\n'},\n",
       "      {'param_name': 'colormap',\n",
       "       'param_type': 'str or matplotlib colormap, default None',\n",
       "       'param_desc': 'Colormap to use for line colors.\\n'},\n",
       "      {'param_name': 'axvlines',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'If true, vertical lines will be added at each xtick.\\n'},\n",
       "      {'param_name': 'axvlines_kwds',\n",
       "       'param_type': 'keywords, optional',\n",
       "       'param_desc': 'Options to be passed to axvline method for vertical lines.\\n'},\n",
       "      {'param_name': 'sort_labels',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Sort class_column labels, useful when assigning colors.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.parallel_coordinates.html'}]},\n",
       "  {'name': 'pandas.plotting.plot_params',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.plot_params.html',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.plot_params.html'],\n",
       "   'function_definitions': [{'function_name': 'pandas.plotting.plot_params',\n",
       "     'full_function': \"pandas.plotting.plot_params = {'xaxis.compat': False}#\",\n",
       "     'function_text': 'Stores pandas plotting options.',\n",
       "     'parameter_names_desc': [],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.plot_params.html'}]},\n",
       "  {'name': 'pandas.plotting.radviz',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.radviz.html',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.radviz.html'],\n",
       "   'function_definitions': [{'function_name': 'pandas.plotting.radviz',\n",
       "     'full_function': 'pandas.plotting.radviz(frame, class_column, ax=None, color=None, colormap=None, **kwds)',\n",
       "     'function_text': 'Plot a multidimensional dataset in 2D.',\n",
       "     'parameter_names_desc': [{'param_name': 'frame',\n",
       "       'param_type': 'DataFrame',\n",
       "       'param_desc': 'Object holding the data.\\n'},\n",
       "      {'param_name': 'class_column',\n",
       "       'param_type': 'str',\n",
       "       'param_desc': 'Column name containing the name of the data point category.\\n'},\n",
       "      {'param_name': 'ax',\n",
       "       'param_type': 'matplotlib.axes.Axes, optional',\n",
       "       'param_desc': 'A plot instance to which to add the information.\\n'},\n",
       "      {'param_name': 'color',\n",
       "       'param_type': 'list[str] or tuple[str], optional',\n",
       "       'param_desc': 'Assign a color to each category. Example: [‘blue’, ‘green’].\\n'},\n",
       "      {'param_name': 'colormap',\n",
       "       'param_type': 'str or matplotlib.colors.Colormap, default None',\n",
       "       'param_desc': 'Colormap to select colors from. If string, load colormap with that\\nname from matplotlib.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.radviz.html'}]},\n",
       "  {'name': 'pandas.plotting.register_matplotlib_converters',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.register_matplotlib_converters.html',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.register_matplotlib_converters.html'],\n",
       "   'function_definitions': [{'function_name': 'pandas.plotting.register_matplotlib_converters',\n",
       "     'full_function': 'pandas.plotting.register_matplotlib_converters()',\n",
       "     'function_text': 'Register pandas formatters and converters with matplotlib.',\n",
       "     'parameter_names_desc': [],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.register_matplotlib_converters.html'}]},\n",
       "  {'name': 'pandas.plotting.scatter_matrix',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html'],\n",
       "   'function_definitions': [{'function_name': 'pandas.plotting.scatter_matrix',\n",
       "     'full_function': \"pandas.plotting.scatter_matrix(frame, alpha=0.5, figsize=None, ax=None, grid=False, diagonal='hist', marker='.', density_kwds=None, hist_kwds=None, range_padding=0.05, **kwargs)\",\n",
       "     'function_text': 'Draw a matrix of scatter plots.',\n",
       "     'parameter_names_desc': [{'param_name': 'frame',\n",
       "       'param_type': 'DataFrame',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'alpha',\n",
       "       'param_type': 'float, optional',\n",
       "       'param_desc': 'Amount of transparency applied.\\n'},\n",
       "      {'param_name': 'figsize',\n",
       "       'param_type': '(float,float), optional',\n",
       "       'param_desc': 'A tuple (width, height) in inches.\\n'},\n",
       "      {'param_name': 'ax',\n",
       "       'param_type': 'Matplotlib axis object, optional',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'grid',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'Setting this to True will show the grid.\\n'},\n",
       "      {'param_name': 'diagonal',\n",
       "       'param_type': '{‘hist’, ‘kde’}',\n",
       "       'param_desc': 'Pick between ‘kde’ and ‘hist’ for either Kernel Density Estimation or\\nHistogram plot in the diagonal.\\n'},\n",
       "      {'param_name': 'marker',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Matplotlib marker type, default ‘.’.\\n'},\n",
       "      {'param_name': 'density_kwds',\n",
       "       'param_type': 'keywords',\n",
       "       'param_desc': 'Keyword arguments to be passed to kernel density estimate plot.\\n'},\n",
       "      {'param_name': 'hist_kwds',\n",
       "       'param_type': 'keywords',\n",
       "       'param_desc': 'Keyword arguments to be passed to hist function.\\n'},\n",
       "      {'param_name': 'range_padding',\n",
       "       'param_type': 'float, default 0.05',\n",
       "       'param_desc': 'Relative extension of axis range in x and y with respect to\\n(x_max - x_min) or (y_max - y_min).\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html'}]},\n",
       "  {'name': 'pandas.plotting.table',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.table.html',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.plotting.table.html'],\n",
       "   'function_definitions': [{'function_name': 'pandas.plotting.table',\n",
       "     'full_function': 'pandas.plotting.table(ax, data, **kwargs)',\n",
       "     'function_text': 'Helper function to convert DataFrame and Series to matplotlib.table.',\n",
       "     'parameter_names_desc': [{'param_name': 'ax',\n",
       "       'param_type': 'Matplotlib axes object',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'data',\n",
       "       'param_type': 'DataFrame or Series',\n",
       "       'param_desc': 'Data for table contents.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.plotting.table.html'}]}],\n",
       " 'name': 'Plotting',\n",
       " 'url': 'https://pandas.pydata.org/docs/reference/plotting.html'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_level['plotting.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "894"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_worked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('not_worked.txt', 'w') as file:\n",
    "    for item in not_worked:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to pandas_function_v3.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "filename = \"pandas_function_v3.json\"\n",
    "\n",
    "# Serialize the dictionary to a JSON string\n",
    "json_data = json.dumps(function_def_dict, ensure_ascii=False)\n",
    "\n",
    "# Write the JSON string to a file with UTF-8 encoding\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "\n",
    "print(\"Data has been successfully written to\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = \"pandas_function_v3.json\"\n",
    "\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        # Load the JSON data from the file\n",
    "        data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'functions': [{'name': 'Pickling',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#pickling',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_pickle.html#pandas.read_pickle',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html#pandas.DataFrame.to_pickle'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_pickle',\n",
       "     'full_function': \"pandas.read_pickle(filepath_or_buffer, compression='infer', storage_options=None)\",\n",
       "     'function_text': 'Load pickled pandas object (or any object) from file.',\n",
       "     'parameter_names_desc': [{'param_name': 'filepath_or_buffer',\n",
       "       'param_type': 'str, path object, or file-like object',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary readlines() function.\\nAlso accepts URL. URL is not limited to S3 and GCS.\\n'},\n",
       "      {'param_name': 'compression',\n",
       "       'param_type': 'str or dict, default ‘infer’',\n",
       "       'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_pickle.html#pandas.read_pickle'},\n",
       "    {'function_name': 'pandas.DataFrame.to_pickle',\n",
       "     'full_function': \"DataFrame.to_pickle(path, *, compression='infer', protocol=5, storage_options=None)\",\n",
       "     'function_text': 'Pickle (serialize) object to file.',\n",
       "     'parameter_names_desc': [{'param_name': 'path',\n",
       "       'param_type': 'str, path object, or file-like object',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary write() function. File path where\\nthe pickled object will be stored.\\n'},\n",
       "      {'param_name': 'compression',\n",
       "       'param_type': 'str or dict, default ‘infer’',\n",
       "       'param_desc': \"For on-the-fly compression of the output data. If ‘infer’ and ‘path’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nSet to None for no compression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for faster compression and to create\\na reproducible gzip archive:\\ncompression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\"},\n",
       "      {'param_name': 'protocol',\n",
       "       'param_type': 'int',\n",
       "       'param_desc': 'Int which indicates which protocol should be used by the pickler,\\ndefault HIGHEST_PROTOCOL (see [1] paragraph 12.1.2). The possible\\nvalues are 0, 1, 2, 3, 4, 5. A negative value for the protocol\\nparameter is equivalent to setting its value to HIGHEST_PROTOCOL.\\n\\n\\n[1]\\nhttps://docs.python.org/3/library/pickle.html.\\n\\n\\n'},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html#pandas.DataFrame.to_pickle'}]},\n",
       "  {'name': 'Flat file',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#flat-file',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_table.html#pandas.read_table',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#pandas.read_csv',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.read_fwf.html#pandas.read_fwf'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_table',\n",
       "     'full_function': 'pandas.read_table(filepath_or_buffer, *, sep=_NoDefault.no_default, delimiter=None, header=\\'infer\\', names=_NoDefault.no_default, index_col=None, usecols=None, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=_NoDefault.no_default, skip_blank_lines=True, parse_dates=False, infer_datetime_format=_NoDefault.no_default, keep_date_col=_NoDefault.no_default, date_parser=_NoDefault.no_default, date_format=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression=\\'infer\\', thousands=None, decimal=\\'.\\', lineterminator=None, quotechar=\\'\"\\', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors=\\'strict\\', dialect=None, on_bad_lines=\\'error\\', delim_whitespace=_NoDefault.no_default, low_memory=True, memory_map=False, float_precision=None, storage_options=None, dtype_backend=_NoDefault.no_default)',\n",
       "     'function_text': 'Read general delimited file into DataFrame.',\n",
       "     'parameter_names_desc': [{'param_name': 'filepath_or_buffer',\n",
       "       'param_type': 'str, path object or file-like object',\n",
       "       'param_desc': 'Any valid string path is acceptable. The string could be a URL. Valid\\nURL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\\nexpected. A local file could be: file://localhost/path/to/table.csv.\\nIf you want to pass in a path object, pandas accepts any os.PathLike.\\nBy file-like object, we refer to objects with a read() method, such as\\na file handle (e.g. via builtin open function) or StringIO.\\n'},\n",
       "      {'param_name': 'sep',\n",
       "       'param_type': 'str, default ‘\\\\t’ (tab-stop)',\n",
       "       'param_desc': \"Character or regex pattern to treat as the delimiter. If sep=None, the\\nC engine cannot automatically detect\\nthe separator, but the Python parsing engine can, meaning the latter will\\nbe used and automatically detect the separator from only the first valid\\nrow of the file by Python’s builtin sniffer tool, csv.Sniffer.\\nIn addition, separators longer than 1 character and different from\\n'\\\\s+' will be interpreted as regular expressions and will also force\\nthe use of the Python parsing engine. Note that regex delimiters are prone\\nto ignoring quoted data. Regex example: '\\\\r\\\\t'.\\n\"},\n",
       "      {'param_name': 'delimiter',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Alias for sep.\\n'},\n",
       "      {'param_name': 'header',\n",
       "       'param_type': 'int, Sequence of int, ‘infer’ or None, default ‘infer’',\n",
       "       'param_desc': 'Row number(s) containing column labels and marking the start of the\\ndata (zero-indexed). Default behavior is to infer the column names: if no names\\nare passed the behavior is identical to header=0 and column\\nnames are inferred from the first line of the file, if column\\nnames are passed explicitly to names then the behavior is identical to\\nheader=None. Explicitly pass header=0 to be able to\\nreplace existing names. The header can be a list of integers that\\nspecify row locations for a MultiIndex on the columns\\ne.g. [0, 1, 3]. Intervening rows that are not specified will be\\nskipped (e.g. 2 in this example is skipped). Note that this\\nparameter ignores commented lines and empty lines if\\nskip_blank_lines=True, so header=0 denotes the first line of\\ndata rather than the first line of the file.\\n'},\n",
       "      {'param_name': 'names',\n",
       "       'param_type': 'Sequence of Hashable, optional',\n",
       "       'param_desc': 'Sequence of column labels to apply. If the file contains a header row,\\nthen you should explicitly pass header=0 to override the column names.\\nDuplicates in this list are not allowed.\\n'},\n",
       "      {'param_name': 'index_col',\n",
       "       'param_type': 'Hashable, Sequence of Hashable or False, optional',\n",
       "       'param_desc': 'Column(s) to use as row label(s), denoted either by column labels or column\\nindices. If a sequence of labels or indices is given, MultiIndex\\nwill be formed for the row labels.\\nNote: index_col=False can be used to force pandas to not use the first\\ncolumn as the index, e.g., when you have a malformed file with delimiters at\\nthe end of each line.\\n'},\n",
       "      {'param_name': 'usecols',\n",
       "       'param_type': 'Sequence of Hashable or Callable, optional',\n",
       "       'param_desc': \"Subset of columns to select, denoted either by column labels or column indices.\\nIf list-like, all elements must either\\nbe positional (i.e. integer indices into the document columns) or strings\\nthat correspond to column names provided either by the user in names or\\ninferred from the document header row(s). If names are given, the document\\nheader row(s) are not taken into account. For example, a valid list-like\\nusecols parameter would be [0, 1, 2] or ['foo', 'bar', 'baz'].\\nElement order is ignored, so usecols=[0, 1] is the same as [1, 0].\\nTo instantiate a DataFrame from data with element order\\npreserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]\\nfor columns in ['foo', 'bar'] order or\\npd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]\\nfor ['bar', 'foo'] order.\\nIf callable, the callable function will be evaluated against the column\\nnames, returning names where the callable function evaluates to True. An\\nexample of a valid callable argument would be lambda x: x.upper() in\\n['AAA', 'BBB', 'DDD']. Using this parameter results in much faster\\nparsing time and lower memory usage.\\n\"},\n",
       "      {'param_name': 'dtype',\n",
       "       'param_type': 'dtype or dict of {Hashable',\n",
       "       'param_desc': \"Data type(s) to apply to either the whole dataset or individual columns.\\nE.g., {'a': np.float64, 'b': np.int32, 'c': 'Int64'}\\nUse str or object together with suitable na_values settings\\nto preserve and not interpret dtype.\\nIf converters are specified, they will be applied INSTEAD\\nof dtype conversion.\\n\\nNew in version 1.5.0: Support for defaultdict was added. Specify a defaultdict as input where\\nthe default determines the dtype of the columns which are not explicitly\\nlisted.\\n\\n\"},\n",
       "      {'param_name': 'engine',\n",
       "       'param_type': '{‘c’, ‘python’, ‘pyarrow’}, optional',\n",
       "       'param_desc': 'Parser engine to use. The C and pyarrow engines are faster, while the python engine\\nis currently more feature-complete. Multithreading is currently only supported by\\nthe pyarrow engine.\\n\\nNew in version 1.4.0: The ‘pyarrow’ engine was added as an experimental engine, and some features\\nare unsupported, or may not work correctly, with this engine.\\n\\n'},\n",
       "      {'param_name': 'converters',\n",
       "       'param_type': 'dict of {Hashable',\n",
       "       'param_desc': 'Functions for converting values in specified columns. Keys can either\\nbe column labels or column indices.\\n'},\n",
       "      {'param_name': 'true_values',\n",
       "       'param_type': 'list, optional',\n",
       "       'param_desc': 'Values to consider as True in addition to case-insensitive variants of ‘True’.\\n'},\n",
       "      {'param_name': 'false_values',\n",
       "       'param_type': 'list, optional',\n",
       "       'param_desc': 'Values to consider as False in addition to case-insensitive variants of ‘False’.\\n'},\n",
       "      {'param_name': 'skipinitialspace',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Skip spaces after delimiter.\\n'},\n",
       "      {'param_name': 'skiprows',\n",
       "       'param_type': 'int, list of int or Callable, optional',\n",
       "       'param_desc': 'Line numbers to skip (0-indexed) or number of lines to skip (int)\\nat the start of the file.\\nIf callable, the callable function will be evaluated against the row\\nindices, returning True if the row should be skipped and False otherwise.\\nAn example of a valid callable argument would be lambda x: x in [0, 2].\\n'},\n",
       "      {'param_name': 'skipfooter',\n",
       "       'param_type': 'int, default 0',\n",
       "       'param_desc': \"Number of lines at bottom of file to skip (Unsupported with engine='c').\\n\"},\n",
       "      {'param_name': 'nrows',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'Number of rows of file to read. Useful for reading pieces of large files.\\n'},\n",
       "      {'param_name': 'na_values',\n",
       "       'param_type': 'Hashable, Iterable of Hashable or dict of {Hashable',\n",
       "       'param_desc': 'Additional strings to recognize as NA/NaN. If dict passed, specific\\nper-column NA values. By default the following values are interpreted as\\nNaN: “ “, “#N/A”, “#N/A N/A”, “#NA”, “-1.#IND”, “-1.#QNAN”, “-NaN”, “-nan”,\\n“1.#IND”, “1.#QNAN”, “<NA>”, “N/A”, “NA”, “NULL”, “NaN”, “None”,\\n“n/a”, “nan”, “null “.\\n'},\n",
       "      {'param_name': 'keep_default_na',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Whether or not to include the default NaN values when parsing the data.\\nDepending on whether na_values is passed in, the behavior is as follows:\\n\\nIf keep_default_na is True, and na_values are specified, na_values\\nis appended to the default NaN values used for parsing.\\nIf keep_default_na is True, and na_values are not specified, only\\nthe default NaN values are used for parsing.\\nIf keep_default_na is False, and na_values are specified, only\\nthe NaN values specified na_values are used for parsing.\\nIf keep_default_na is False, and na_values are not specified, no\\nstrings will be parsed as NaN.\\n\\nNote that if na_filter is passed in as False, the keep_default_na and\\nna_values parameters will be ignored.\\n'},\n",
       "      {'param_name': 'na_filter',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Detect missing value markers (empty strings and the value of na_values). In\\ndata without any NA values, passing na_filter=False can improve the\\nperformance of reading a large file.\\n'},\n",
       "      {'param_name': 'verbose',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Indicate number of NA values placed in non-numeric columns.\\n\\nDeprecated since version 2.2.0.\\n\\n'},\n",
       "      {'param_name': 'skip_blank_lines',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'If True, skip over blank lines rather than interpreting as NaN values.\\n'},\n",
       "      {'param_name': 'parse_dates',\n",
       "       'param_type': 'bool, list of Hashable, list of lists or dict of {Hashable',\n",
       "       'param_desc': \"The behavior is as follows:\\n\\nbool. If True -> try parsing the index. Note: Automatically set to\\nTrue if date_format or date_parser arguments have been passed.\\nlist of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\\neach as a separate date column.\\nlist of list. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse\\nas a single date column. Values are joined with a space before parsing.\\ndict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\\nresult ‘foo’. Values are joined with a space before parsing.\\n\\nIf a column or index cannot be represented as an array of datetime,\\nsay because of an unparsable value or a mixture of timezones, the column\\nor index will be returned unaltered as an object data type. For\\nnon-standard datetime parsing, use to_datetime() after\\nread_csv().\\nNote: A fast-path exists for iso8601-formatted dates.\\n\"},\n",
       "      {'param_name': 'infer_datetime_format',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'If True and parse_dates is enabled, pandas will attempt to infer the\\nformat of the datetime strings in the columns, and if it can be inferred,\\nswitch to a faster method of parsing them. In some cases this can increase\\nthe parsing speed by 5-10x.\\n\\nDeprecated since version 2.0.0: A strict version of this argument is now the default, passing it has no effect.\\n\\n'},\n",
       "      {'param_name': 'keep_date_col',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'If True and parse_dates specifies combining multiple columns then\\nkeep the original columns.\\n'},\n",
       "      {'param_name': 'date_parser',\n",
       "       'param_type': 'Callable, optional',\n",
       "       'param_desc': 'Function to use for converting a sequence of string columns to an array of\\ndatetime instances. The default uses dateutil.parser.parser to do the\\nconversion. pandas will try to call date_parser in three different ways,\\nadvancing to the next if an exception occurs: 1) Pass one or more arrays\\n(as defined by parse_dates) as arguments; 2) concatenate (row-wise) the\\nstring values from the columns defined by parse_dates into a single array\\nand pass that; and 3) call date_parser once for each row using one or\\nmore strings (corresponding to the columns defined by parse_dates) as\\narguments.\\n\\nDeprecated since version 2.0.0: Use date_format instead, or read in as object and then apply\\nto_datetime() as-needed.\\n\\n'},\n",
       "      {'param_name': 'date_format',\n",
       "       'param_type': 'str or dict of column -> format, optional',\n",
       "       'param_desc': 'Format to use for parsing dates when used in conjunction with parse_dates.\\nThe strftime to parse time, e.g. \"%d/%m/%Y\". See\\nstrftime documentation for more information on choices, though\\nnote that \"%f\" will parse all the way up to nanoseconds.\\nYou can also pass:\\n\\n\\n“ISO8601”, to parse any ISO8601time string (not necessarily in exactly the same format);\\n\\n\\n\\n\\n“mixed”, to infer the format for each element individually. This is risky,and you should probably use it along with dayfirst.\\n\\n\\n\\n\\n\\nNew in version 2.0.0.\\n\\n'},\n",
       "      {'param_name': 'dayfirst',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'DD/MM format dates, international and European format.\\n'},\n",
       "      {'param_name': 'cache_dates',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'If True, use a cache of unique, converted dates to apply the datetime\\nconversion. May produce significant speed-up when parsing duplicate\\ndate strings, especially ones with timezone offsets.\\n'},\n",
       "      {'param_name': 'iterator',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Return TextFileReader object for iteration or getting chunks with\\nget_chunk().\\n'},\n",
       "      {'param_name': 'chunksize',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'Number of lines to read from the file per chunk. Passing a value will cause the\\nfunction to return a TextFileReader object for iteration.\\nSee the IO Tools docs\\nfor more information on iterator and chunksize.\\n'},\n",
       "      {'param_name': 'compression',\n",
       "       'param_type': 'str or dict, default ‘infer’',\n",
       "       'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "      {'param_name': 'thousands',\n",
       "       'param_type': 'str (length 1), optional',\n",
       "       'param_desc': 'Character acting as the thousands separator in numerical values.\\n'},\n",
       "      {'param_name': 'decimal',\n",
       "       'param_type': 'str (length 1), default ‘.’',\n",
       "       'param_desc': 'Character to recognize as decimal point (e.g., use ‘,’ for European data).\\n'},\n",
       "      {'param_name': 'lineterminator',\n",
       "       'param_type': 'str (length 1), optional',\n",
       "       'param_desc': 'Character used to denote a line break. Only valid with C parser.\\n'},\n",
       "      {'param_name': 'quotechar',\n",
       "       'param_type': 'str (length 1), optional',\n",
       "       'param_desc': 'Character used to denote the start and end of a quoted item. Quoted\\nitems can include the delimiter and it will be ignored.\\n'},\n",
       "      {'param_name': 'quoting',\n",
       "       'param_type': '{0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL',\n",
       "       'param_desc': 'Control field quoting behavior per csv.QUOTE_* constants. Default is\\ncsv.QUOTE_MINIMAL (i.e., 0) which implies that only fields containing special\\ncharacters are quoted (e.g., characters defined in quotechar, delimiter,\\nor lineterminator.\\n'},\n",
       "      {'param_name': 'doublequote',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'When quotechar is specified and quoting is not QUOTE_NONE, indicate\\nwhether or not to interpret two consecutive quotechar elements INSIDE a\\nfield as a single quotechar element.\\n'},\n",
       "      {'param_name': 'escapechar',\n",
       "       'param_type': 'str (length 1), optional',\n",
       "       'param_desc': 'Character used to escape other characters.\\n'},\n",
       "      {'param_name': 'comment',\n",
       "       'param_type': 'str (length 1), optional',\n",
       "       'param_desc': \"Character indicating that the remainder of line should not be parsed.\\nIf found at the beginning\\nof a line, the line will be ignored altogether. This parameter must be a\\nsingle character. Like empty lines (as long as skip_blank_lines=True),\\nfully commented lines are ignored by the parameter header but not by\\nskiprows. For example, if comment='#', parsing\\n#empty\\\\na,b,c\\\\n1,2,3 with header=0 will result in 'a,b,c' being\\ntreated as the header.\\n\"},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'str, optional, default ‘utf-8’',\n",
       "       'param_desc': \"Encoding to use for UTF when reading/writing (ex. 'utf-8'). List of Python\\nstandard encodings .\\n\"},\n",
       "      {'param_name': 'encoding_errors',\n",
       "       'param_type': 'str, optional, default ‘strict’',\n",
       "       'param_desc': 'How encoding errors are treated. List of possible values .\\n\\nNew in version 1.3.0.\\n\\n'},\n",
       "      {'param_name': 'dialect',\n",
       "       'param_type': 'str or csv.Dialect, optional',\n",
       "       'param_desc': 'If provided, this parameter will override values (default or not) for the\\nfollowing parameters: delimiter, doublequote, escapechar,\\nskipinitialspace, quotechar, and quoting. If it is necessary to\\noverride values, a ParserWarning will be issued. See csv.Dialect\\ndocumentation for more details.\\n'},\n",
       "      {'param_name': 'on_bad_lines',\n",
       "       'param_type': '{‘error’, ‘warn’, ‘skip’} or Callable, default ‘error’',\n",
       "       'param_desc': \"Specifies what to do upon encountering a bad line (a line with too many fields).\\nAllowed values are :\\n\\n'error', raise an Exception when a bad line is encountered.\\n'warn', raise a warning when a bad line is encountered and skip that line.\\n'skip', skip bad lines without raising or warning when they are encountered.\\n\\n\\nNew in version 1.3.0.\\n\\n\\nNew in version 1.4.0: \\n\\nCallable, function with signature\\n(bad_line: list[str]) -> list[str] | None that will process a single\\nbad line. bad_line is a list of strings split by the sep.\\nIf the function returns None, the bad line will be ignored.\\nIf the function returns a new list of strings with more elements than\\nexpected, a ParserWarning will be emitted while dropping extra elements.\\nOnly supported when engine='python'\\n\\n\\n\\nChanged in version 2.2.0: \\n\\nCallable, function with signature\\nas described in pyarrow documentation when engine='pyarrow'\\n\\n\\n\"},\n",
       "      {'param_name': 'delim_whitespace',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Specifies whether or not whitespace (e.g. \\' \\' or \\'\\\\t\\') will be\\nused as the sep delimiter. Equivalent to setting sep=\\'\\\\s+\\'. If this option\\nis set to True, nothing should be passed in for the delimiter\\nparameter.\\n\\nDeprecated since version 2.2.0: Use sep=\"\\\\s+\" instead.\\n\\n'},\n",
       "      {'param_name': 'low_memory',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Internally process the file in chunks, resulting in lower memory use\\nwhile parsing, but possibly mixed type inference. To ensure no mixed\\ntypes either set False, or specify the type with the dtype parameter.\\nNote that the entire file is read into a single DataFrame\\nregardless, use the chunksize or iterator parameter to return the data in\\nchunks. (Only valid with C parser).\\n'},\n",
       "      {'param_name': 'memory_map',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'If a filepath is provided for filepath_or_buffer, map the file object\\ndirectly onto memory and access the data directly from there. Using this\\noption can improve performance because there is no longer any I/O overhead.\\n'},\n",
       "      {'param_name': 'float_precision',\n",
       "       'param_type': '{‘high’, ‘legacy’, ‘round_trip’}, optional',\n",
       "       'param_desc': \"Specifies which converter the C engine should use for floating-point\\nvalues. The options are None or 'high' for the ordinary converter,\\n'legacy' for the original lower precision pandas converter, and\\n'round_trip' for the round-trip converter.\\n\"},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_table.html#pandas.read_table'},\n",
       "    {'function_name': 'pandas.DataFrame.to_csv',\n",
       "     'full_function': 'DataFrame.to_csv(path_or_buf=None, *, sep=\\',\\', na_rep=\\'\\', float_format=None, columns=None, header=True, index=True, index_label=None, mode=\\'w\\', encoding=None, compression=\\'infer\\', quoting=None, quotechar=\\'\"\\', lineterminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal=\\'.\\', errors=\\'strict\\', storage_options=None)',\n",
       "     'function_text': 'Write object to a comma-separated values (csv) file.',\n",
       "     'parameter_names_desc': [{'param_name': 'path_or_buf',\n",
       "       'param_type': 'str, path object, file-like object, or None, default None',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a write() function. If None, the result is\\nreturned as a string. If a non-binary file object is passed, it should\\nbe opened with newline=’’, disabling universal newlines. If a binary\\nfile object is passed, mode might need to contain a ‘b’.\\n'},\n",
       "      {'param_name': 'sep',\n",
       "       'param_type': 'str, default ‘,’',\n",
       "       'param_desc': 'String of length 1. Field delimiter for the output file.\\n'},\n",
       "      {'param_name': 'na_rep',\n",
       "       'param_type': 'str, default ‘’',\n",
       "       'param_desc': 'Missing data representation.\\n'},\n",
       "      {'param_name': 'float_format',\n",
       "       'param_type': 'str, Callable, default None',\n",
       "       'param_desc': 'Format string for floating point numbers. If a Callable is given, it takes\\nprecedence over other numeric formatting parameters, like decimal.\\n'},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'sequence, optional',\n",
       "       'param_desc': 'Columns to write.\\n'},\n",
       "      {'param_name': 'header',\n",
       "       'param_type': 'bool or list of str, default True',\n",
       "       'param_desc': 'Write out the column names. If a list of strings is given it is\\nassumed to be aliases for the column names.\\n'},\n",
       "      {'param_name': 'index',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Write row names (index).\\n'},\n",
       "      {'param_name': 'index_label',\n",
       "       'param_type': 'str or sequence, or False, default None',\n",
       "       'param_desc': 'Column label for index column(s) if desired. If None is given, and\\nheader and index are True, then the index names are used. A\\nsequence should be given if the object uses MultiIndex. If\\nFalse do not print fields for index names. Use index_label=False\\nfor easier importing in R.\\n'},\n",
       "      {'param_name': 'mode',\n",
       "       'param_type': '{‘w’, ‘x’, ‘a’}, default ‘w’',\n",
       "       'param_desc': 'Forwarded to either open(mode=) or fsspec.open(mode=) to control\\nthe file opening. Typical values include:\\n\\n‘w’, truncate the file first.\\n‘x’, exclusive creation, failing if the file already exists.\\n‘a’, append to the end of file if it exists.\\n\\n'},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'A string representing the encoding to use in the output file,\\ndefaults to ‘utf-8’. encoding is not supported if path_or_buf\\nis a non-binary file object.\\n'},\n",
       "      {'param_name': 'compression',\n",
       "       'param_type': 'str or dict, default ‘infer’',\n",
       "       'param_desc': \"For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buf’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nSet to None for no compression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for faster compression and to create\\na reproducible gzip archive:\\ncompression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\nMay be a dict with key ‘method’ as compression mode\\nand other entries as additional compression options if\\ncompression mode is ‘zip’.\\nPassing compression options as keys in dict is\\nsupported for compression modes ‘gzip’, ‘bz2’, ‘zstd’, and ‘zip’.\\n\\n\"},\n",
       "      {'param_name': 'quoting',\n",
       "       'param_type': 'optional constant from csv module',\n",
       "       'param_desc': 'Defaults to csv.QUOTE_MINIMAL. If you have set a float_format\\nthen floats are converted to strings and thus csv.QUOTE_NONNUMERIC\\nwill treat them as non-numeric.\\n'},\n",
       "      {'param_name': 'quotechar',\n",
       "       'param_type': 'str, default ‘\"’',\n",
       "       'param_desc': 'String of length 1. Character used to quote fields.\\n'},\n",
       "      {'param_name': 'lineterminator',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'The newline character or character sequence to use in the output\\nfile. Defaults to os.linesep, which depends on the OS in which\\nthis method is called (’\\\\n’ for linux, ‘\\\\r\\\\n’ for Windows, i.e.).\\n\\nChanged in version 1.5.0: Previously was line_terminator, changed for consistency with\\nread_csv and the standard library ‘csv’ module.\\n\\n'},\n",
       "      {'param_name': 'chunksize',\n",
       "       'param_type': 'int or None',\n",
       "       'param_desc': 'Rows to write at a time.\\n'},\n",
       "      {'param_name': 'date_format',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'Format string for datetime objects.\\n'},\n",
       "      {'param_name': 'doublequote',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Control quoting of quotechar inside a field.\\n'},\n",
       "      {'param_name': 'escapechar',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'String of length 1. Character used to escape sep and quotechar\\nwhen appropriate.\\n'},\n",
       "      {'param_name': 'decimal',\n",
       "       'param_type': 'str, default ‘.’',\n",
       "       'param_desc': 'Character recognized as decimal separator. E.g. use ‘,’ for\\nEuropean data.\\n'},\n",
       "      {'param_name': 'errors',\n",
       "       'param_type': 'str, default ‘strict’',\n",
       "       'param_desc': 'Specifies how encoding and decoding errors are to be handled.\\nSee the errors argument for open() for a full list\\nof options.\\n'},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv'},\n",
       "    {'function_name': 'pandas.read_csv',\n",
       "     'full_function': 'pandas.read_csv(filepath_or_buffer, *, sep=_NoDefault.no_default, delimiter=None, header=\\'infer\\', names=_NoDefault.no_default, index_col=None, usecols=None, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=_NoDefault.no_default, skip_blank_lines=True, parse_dates=None, infer_datetime_format=_NoDefault.no_default, keep_date_col=_NoDefault.no_default, date_parser=_NoDefault.no_default, date_format=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression=\\'infer\\', thousands=None, decimal=\\'.\\', lineterminator=None, quotechar=\\'\"\\', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors=\\'strict\\', dialect=None, on_bad_lines=\\'error\\', delim_whitespace=_NoDefault.no_default, low_memory=True, memory_map=False, float_precision=None, storage_options=None, dtype_backend=_NoDefault.no_default)',\n",
       "     'function_text': 'Read a comma-separated values (csv) file into DataFrame.',\n",
       "     'parameter_names_desc': [{'param_name': 'filepath_or_buffer',\n",
       "       'param_type': 'str, path object or file-like object',\n",
       "       'param_desc': 'Any valid string path is acceptable. The string could be a URL. Valid\\nURL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\\nexpected. A local file could be: file://localhost/path/to/table.csv.\\nIf you want to pass in a path object, pandas accepts any os.PathLike.\\nBy file-like object, we refer to objects with a read() method, such as\\na file handle (e.g. via builtin open function) or StringIO.\\n'},\n",
       "      {'param_name': 'sep',\n",
       "       'param_type': 'str, default ‘,’',\n",
       "       'param_desc': \"Character or regex pattern to treat as the delimiter. If sep=None, the\\nC engine cannot automatically detect\\nthe separator, but the Python parsing engine can, meaning the latter will\\nbe used and automatically detect the separator from only the first valid\\nrow of the file by Python’s builtin sniffer tool, csv.Sniffer.\\nIn addition, separators longer than 1 character and different from\\n'\\\\s+' will be interpreted as regular expressions and will also force\\nthe use of the Python parsing engine. Note that regex delimiters are prone\\nto ignoring quoted data. Regex example: '\\\\r\\\\t'.\\n\"},\n",
       "      {'param_name': 'delimiter',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Alias for sep.\\n'},\n",
       "      {'param_name': 'header',\n",
       "       'param_type': 'int, Sequence of int, ‘infer’ or None, default ‘infer’',\n",
       "       'param_desc': 'Row number(s) containing column labels and marking the start of the\\ndata (zero-indexed). Default behavior is to infer the column names: if no names\\nare passed the behavior is identical to header=0 and column\\nnames are inferred from the first line of the file, if column\\nnames are passed explicitly to names then the behavior is identical to\\nheader=None. Explicitly pass header=0 to be able to\\nreplace existing names. The header can be a list of integers that\\nspecify row locations for a MultiIndex on the columns\\ne.g. [0, 1, 3]. Intervening rows that are not specified will be\\nskipped (e.g. 2 in this example is skipped). Note that this\\nparameter ignores commented lines and empty lines if\\nskip_blank_lines=True, so header=0 denotes the first line of\\ndata rather than the first line of the file.\\n'},\n",
       "      {'param_name': 'names',\n",
       "       'param_type': 'Sequence of Hashable, optional',\n",
       "       'param_desc': 'Sequence of column labels to apply. If the file contains a header row,\\nthen you should explicitly pass header=0 to override the column names.\\nDuplicates in this list are not allowed.\\n'},\n",
       "      {'param_name': 'index_col',\n",
       "       'param_type': 'Hashable, Sequence of Hashable or False, optional',\n",
       "       'param_desc': 'Column(s) to use as row label(s), denoted either by column labels or column\\nindices. If a sequence of labels or indices is given, MultiIndex\\nwill be formed for the row labels.\\nNote: index_col=False can be used to force pandas to not use the first\\ncolumn as the index, e.g., when you have a malformed file with delimiters at\\nthe end of each line.\\n'},\n",
       "      {'param_name': 'usecols',\n",
       "       'param_type': 'Sequence of Hashable or Callable, optional',\n",
       "       'param_desc': \"Subset of columns to select, denoted either by column labels or column indices.\\nIf list-like, all elements must either\\nbe positional (i.e. integer indices into the document columns) or strings\\nthat correspond to column names provided either by the user in names or\\ninferred from the document header row(s). If names are given, the document\\nheader row(s) are not taken into account. For example, a valid list-like\\nusecols parameter would be [0, 1, 2] or ['foo', 'bar', 'baz'].\\nElement order is ignored, so usecols=[0, 1] is the same as [1, 0].\\nTo instantiate a DataFrame from data with element order\\npreserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]\\nfor columns in ['foo', 'bar'] order or\\npd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]\\nfor ['bar', 'foo'] order.\\nIf callable, the callable function will be evaluated against the column\\nnames, returning names where the callable function evaluates to True. An\\nexample of a valid callable argument would be lambda x: x.upper() in\\n['AAA', 'BBB', 'DDD']. Using this parameter results in much faster\\nparsing time and lower memory usage.\\n\"},\n",
       "      {'param_name': 'dtype',\n",
       "       'param_type': 'dtype or dict of {Hashable',\n",
       "       'param_desc': \"Data type(s) to apply to either the whole dataset or individual columns.\\nE.g., {'a': np.float64, 'b': np.int32, 'c': 'Int64'}\\nUse str or object together with suitable na_values settings\\nto preserve and not interpret dtype.\\nIf converters are specified, they will be applied INSTEAD\\nof dtype conversion.\\n\\nNew in version 1.5.0: Support for defaultdict was added. Specify a defaultdict as input where\\nthe default determines the dtype of the columns which are not explicitly\\nlisted.\\n\\n\"},\n",
       "      {'param_name': 'engine',\n",
       "       'param_type': '{‘c’, ‘python’, ‘pyarrow’}, optional',\n",
       "       'param_desc': 'Parser engine to use. The C and pyarrow engines are faster, while the python engine\\nis currently more feature-complete. Multithreading is currently only supported by\\nthe pyarrow engine.\\n\\nNew in version 1.4.0: The ‘pyarrow’ engine was added as an experimental engine, and some features\\nare unsupported, or may not work correctly, with this engine.\\n\\n'},\n",
       "      {'param_name': 'converters',\n",
       "       'param_type': 'dict of {Hashable',\n",
       "       'param_desc': 'Functions for converting values in specified columns. Keys can either\\nbe column labels or column indices.\\n'},\n",
       "      {'param_name': 'true_values',\n",
       "       'param_type': 'list, optional',\n",
       "       'param_desc': 'Values to consider as True in addition to case-insensitive variants of ‘True’.\\n'},\n",
       "      {'param_name': 'false_values',\n",
       "       'param_type': 'list, optional',\n",
       "       'param_desc': 'Values to consider as False in addition to case-insensitive variants of ‘False’.\\n'},\n",
       "      {'param_name': 'skipinitialspace',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Skip spaces after delimiter.\\n'},\n",
       "      {'param_name': 'skiprows',\n",
       "       'param_type': 'int, list of int or Callable, optional',\n",
       "       'param_desc': 'Line numbers to skip (0-indexed) or number of lines to skip (int)\\nat the start of the file.\\nIf callable, the callable function will be evaluated against the row\\nindices, returning True if the row should be skipped and False otherwise.\\nAn example of a valid callable argument would be lambda x: x in [0, 2].\\n'},\n",
       "      {'param_name': 'skipfooter',\n",
       "       'param_type': 'int, default 0',\n",
       "       'param_desc': \"Number of lines at bottom of file to skip (Unsupported with engine='c').\\n\"},\n",
       "      {'param_name': 'nrows',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'Number of rows of file to read. Useful for reading pieces of large files.\\n'},\n",
       "      {'param_name': 'na_values',\n",
       "       'param_type': 'Hashable, Iterable of Hashable or dict of {Hashable',\n",
       "       'param_desc': 'Additional strings to recognize as NA/NaN. If dict passed, specific\\nper-column NA values. By default the following values are interpreted as\\nNaN: “ “, “#N/A”, “#N/A N/A”, “#NA”, “-1.#IND”, “-1.#QNAN”, “-NaN”, “-nan”,\\n“1.#IND”, “1.#QNAN”, “<NA>”, “N/A”, “NA”, “NULL”, “NaN”, “None”,\\n“n/a”, “nan”, “null “.\\n'},\n",
       "      {'param_name': 'keep_default_na',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Whether or not to include the default NaN values when parsing the data.\\nDepending on whether na_values is passed in, the behavior is as follows:\\n\\nIf keep_default_na is True, and na_values are specified, na_values\\nis appended to the default NaN values used for parsing.\\nIf keep_default_na is True, and na_values are not specified, only\\nthe default NaN values are used for parsing.\\nIf keep_default_na is False, and na_values are specified, only\\nthe NaN values specified na_values are used for parsing.\\nIf keep_default_na is False, and na_values are not specified, no\\nstrings will be parsed as NaN.\\n\\nNote that if na_filter is passed in as False, the keep_default_na and\\nna_values parameters will be ignored.\\n'},\n",
       "      {'param_name': 'na_filter',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Detect missing value markers (empty strings and the value of na_values). In\\ndata without any NA values, passing na_filter=False can improve the\\nperformance of reading a large file.\\n'},\n",
       "      {'param_name': 'verbose',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Indicate number of NA values placed in non-numeric columns.\\n\\nDeprecated since version 2.2.0.\\n\\n'},\n",
       "      {'param_name': 'skip_blank_lines',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'If True, skip over blank lines rather than interpreting as NaN values.\\n'},\n",
       "      {'param_name': 'parse_dates',\n",
       "       'param_type': 'bool, list of Hashable, list of lists or dict of {Hashable',\n",
       "       'param_desc': \"The behavior is as follows:\\n\\nbool. If True -> try parsing the index. Note: Automatically set to\\nTrue if date_format or date_parser arguments have been passed.\\nlist of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\\neach as a separate date column.\\nlist of list. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse\\nas a single date column. Values are joined with a space before parsing.\\ndict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\\nresult ‘foo’. Values are joined with a space before parsing.\\n\\nIf a column or index cannot be represented as an array of datetime,\\nsay because of an unparsable value or a mixture of timezones, the column\\nor index will be returned unaltered as an object data type. For\\nnon-standard datetime parsing, use to_datetime() after\\nread_csv().\\nNote: A fast-path exists for iso8601-formatted dates.\\n\"},\n",
       "      {'param_name': 'infer_datetime_format',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'If True and parse_dates is enabled, pandas will attempt to infer the\\nformat of the datetime strings in the columns, and if it can be inferred,\\nswitch to a faster method of parsing them. In some cases this can increase\\nthe parsing speed by 5-10x.\\n\\nDeprecated since version 2.0.0: A strict version of this argument is now the default, passing it has no effect.\\n\\n'},\n",
       "      {'param_name': 'keep_date_col',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'If True and parse_dates specifies combining multiple columns then\\nkeep the original columns.\\n'},\n",
       "      {'param_name': 'date_parser',\n",
       "       'param_type': 'Callable, optional',\n",
       "       'param_desc': 'Function to use for converting a sequence of string columns to an array of\\ndatetime instances. The default uses dateutil.parser.parser to do the\\nconversion. pandas will try to call date_parser in three different ways,\\nadvancing to the next if an exception occurs: 1) Pass one or more arrays\\n(as defined by parse_dates) as arguments; 2) concatenate (row-wise) the\\nstring values from the columns defined by parse_dates into a single array\\nand pass that; and 3) call date_parser once for each row using one or\\nmore strings (corresponding to the columns defined by parse_dates) as\\narguments.\\n\\nDeprecated since version 2.0.0: Use date_format instead, or read in as object and then apply\\nto_datetime() as-needed.\\n\\n'},\n",
       "      {'param_name': 'date_format',\n",
       "       'param_type': 'str or dict of column -> format, optional',\n",
       "       'param_desc': 'Format to use for parsing dates when used in conjunction with parse_dates.\\nThe strftime to parse time, e.g. \"%d/%m/%Y\". See\\nstrftime documentation for more information on choices, though\\nnote that \"%f\" will parse all the way up to nanoseconds.\\nYou can also pass:\\n\\n\\n“ISO8601”, to parse any ISO8601time string (not necessarily in exactly the same format);\\n\\n\\n\\n\\n“mixed”, to infer the format for each element individually. This is risky,and you should probably use it along with dayfirst.\\n\\n\\n\\n\\n\\nNew in version 2.0.0.\\n\\n'},\n",
       "      {'param_name': 'dayfirst',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'DD/MM format dates, international and European format.\\n'},\n",
       "      {'param_name': 'cache_dates',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'If True, use a cache of unique, converted dates to apply the datetime\\nconversion. May produce significant speed-up when parsing duplicate\\ndate strings, especially ones with timezone offsets.\\n'},\n",
       "      {'param_name': 'iterator',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Return TextFileReader object for iteration or getting chunks with\\nget_chunk().\\n'},\n",
       "      {'param_name': 'chunksize',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'Number of lines to read from the file per chunk. Passing a value will cause the\\nfunction to return a TextFileReader object for iteration.\\nSee the IO Tools docs\\nfor more information on iterator and chunksize.\\n'},\n",
       "      {'param_name': 'compression',\n",
       "       'param_type': 'str or dict, default ‘infer’',\n",
       "       'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "      {'param_name': 'thousands',\n",
       "       'param_type': 'str (length 1), optional',\n",
       "       'param_desc': 'Character acting as the thousands separator in numerical values.\\n'},\n",
       "      {'param_name': 'decimal',\n",
       "       'param_type': 'str (length 1), default ‘.’',\n",
       "       'param_desc': 'Character to recognize as decimal point (e.g., use ‘,’ for European data).\\n'},\n",
       "      {'param_name': 'lineterminator',\n",
       "       'param_type': 'str (length 1), optional',\n",
       "       'param_desc': 'Character used to denote a line break. Only valid with C parser.\\n'},\n",
       "      {'param_name': 'quotechar',\n",
       "       'param_type': 'str (length 1), optional',\n",
       "       'param_desc': 'Character used to denote the start and end of a quoted item. Quoted\\nitems can include the delimiter and it will be ignored.\\n'},\n",
       "      {'param_name': 'quoting',\n",
       "       'param_type': '{0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL',\n",
       "       'param_desc': 'Control field quoting behavior per csv.QUOTE_* constants. Default is\\ncsv.QUOTE_MINIMAL (i.e., 0) which implies that only fields containing special\\ncharacters are quoted (e.g., characters defined in quotechar, delimiter,\\nor lineterminator.\\n'},\n",
       "      {'param_name': 'doublequote',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'When quotechar is specified and quoting is not QUOTE_NONE, indicate\\nwhether or not to interpret two consecutive quotechar elements INSIDE a\\nfield as a single quotechar element.\\n'},\n",
       "      {'param_name': 'escapechar',\n",
       "       'param_type': 'str (length 1), optional',\n",
       "       'param_desc': 'Character used to escape other characters.\\n'},\n",
       "      {'param_name': 'comment',\n",
       "       'param_type': 'str (length 1), optional',\n",
       "       'param_desc': \"Character indicating that the remainder of line should not be parsed.\\nIf found at the beginning\\nof a line, the line will be ignored altogether. This parameter must be a\\nsingle character. Like empty lines (as long as skip_blank_lines=True),\\nfully commented lines are ignored by the parameter header but not by\\nskiprows. For example, if comment='#', parsing\\n#empty\\\\na,b,c\\\\n1,2,3 with header=0 will result in 'a,b,c' being\\ntreated as the header.\\n\"},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'str, optional, default ‘utf-8’',\n",
       "       'param_desc': \"Encoding to use for UTF when reading/writing (ex. 'utf-8'). List of Python\\nstandard encodings .\\n\"},\n",
       "      {'param_name': 'encoding_errors',\n",
       "       'param_type': 'str, optional, default ‘strict’',\n",
       "       'param_desc': 'How encoding errors are treated. List of possible values .\\n\\nNew in version 1.3.0.\\n\\n'},\n",
       "      {'param_name': 'dialect',\n",
       "       'param_type': 'str or csv.Dialect, optional',\n",
       "       'param_desc': 'If provided, this parameter will override values (default or not) for the\\nfollowing parameters: delimiter, doublequote, escapechar,\\nskipinitialspace, quotechar, and quoting. If it is necessary to\\noverride values, a ParserWarning will be issued. See csv.Dialect\\ndocumentation for more details.\\n'},\n",
       "      {'param_name': 'on_bad_lines',\n",
       "       'param_type': '{‘error’, ‘warn’, ‘skip’} or Callable, default ‘error’',\n",
       "       'param_desc': \"Specifies what to do upon encountering a bad line (a line with too many fields).\\nAllowed values are :\\n\\n'error', raise an Exception when a bad line is encountered.\\n'warn', raise a warning when a bad line is encountered and skip that line.\\n'skip', skip bad lines without raising or warning when they are encountered.\\n\\n\\nNew in version 1.3.0.\\n\\n\\nNew in version 1.4.0: \\n\\nCallable, function with signature\\n(bad_line: list[str]) -> list[str] | None that will process a single\\nbad line. bad_line is a list of strings split by the sep.\\nIf the function returns None, the bad line will be ignored.\\nIf the function returns a new list of strings with more elements than\\nexpected, a ParserWarning will be emitted while dropping extra elements.\\nOnly supported when engine='python'\\n\\n\\n\\nChanged in version 2.2.0: \\n\\nCallable, function with signature\\nas described in pyarrow documentation when engine='pyarrow'\\n\\n\\n\"},\n",
       "      {'param_name': 'delim_whitespace',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Specifies whether or not whitespace (e.g. \\' \\' or \\'\\\\t\\') will be\\nused as the sep delimiter. Equivalent to setting sep=\\'\\\\s+\\'. If this option\\nis set to True, nothing should be passed in for the delimiter\\nparameter.\\n\\nDeprecated since version 2.2.0: Use sep=\"\\\\s+\" instead.\\n\\n'},\n",
       "      {'param_name': 'low_memory',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Internally process the file in chunks, resulting in lower memory use\\nwhile parsing, but possibly mixed type inference. To ensure no mixed\\ntypes either set False, or specify the type with the dtype parameter.\\nNote that the entire file is read into a single DataFrame\\nregardless, use the chunksize or iterator parameter to return the data in\\nchunks. (Only valid with C parser).\\n'},\n",
       "      {'param_name': 'memory_map',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'If a filepath is provided for filepath_or_buffer, map the file object\\ndirectly onto memory and access the data directly from there. Using this\\noption can improve performance because there is no longer any I/O overhead.\\n'},\n",
       "      {'param_name': 'float_precision',\n",
       "       'param_type': '{‘high’, ‘legacy’, ‘round_trip’}, optional',\n",
       "       'param_desc': \"Specifies which converter the C engine should use for floating-point\\nvalues. The options are None or 'high' for the ordinary converter,\\n'legacy' for the original lower precision pandas converter, and\\n'round_trip' for the round-trip converter.\\n\"},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#pandas.read_csv'},\n",
       "    {'function_name': 'pandas.read_fwf',\n",
       "     'full_function': \"pandas.read_fwf(filepath_or_buffer, *, colspecs='infer', widths=None, infer_nrows=100, dtype_backend=_NoDefault.no_default, iterator=False, chunksize=None, **kwds)\",\n",
       "     'function_text': 'Read a table of fixed-width formatted lines into DataFrame.',\n",
       "     'parameter_names_desc': [{'param_name': 'filepath_or_buffer',\n",
       "       'param_type': 'str, path object, or file-like object',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a text read() function.The string could be a URL.\\nValid URL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be:\\nfile://localhost/path/to/table.csv.\\n'},\n",
       "      {'param_name': 'colspecs',\n",
       "       'param_type': 'list of tuple (int, int) or ‘infer’. optional',\n",
       "       'param_desc': 'A list of tuples giving the extents of the fixed-width\\nfields of each line as half-open intervals (i.e., [from, to[ ).\\nString value ‘infer’ can be used to instruct the parser to try\\ndetecting the column specifications from the first 100 rows of\\nthe data which are not being skipped via skiprows (default=’infer’).\\n'},\n",
       "      {'param_name': 'widths',\n",
       "       'param_type': 'list of int, optional',\n",
       "       'param_desc': 'A list of field widths which can be used instead of ‘colspecs’ if\\nthe intervals are contiguous.\\n'},\n",
       "      {'param_name': 'infer_nrows',\n",
       "       'param_type': 'int, default 100',\n",
       "       'param_desc': 'The number of rows to consider when letting the parser determine the\\ncolspecs.\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "      {'param_name': '**kwds',\n",
       "       'param_type': 'optional',\n",
       "       'param_desc': 'Optional keyword arguments can be passed to TextFileReader.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_fwf.html#pandas.read_fwf'}]},\n",
       "  {'name': 'Clipboard',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#clipboard',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_clipboard.html#pandas.read_clipboard',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_clipboard.html#pandas.DataFrame.to_clipboard'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_clipboard',\n",
       "     'full_function': \"pandas.read_clipboard(sep='\\\\\\\\s+', dtype_backend=_NoDefault.no_default, **kwargs)\",\n",
       "     'function_text': 'Read text from clipboard and pass to read_csv().',\n",
       "     'parameter_names_desc': [{'param_name': 'sep',\n",
       "       'param_type': 'str, default ‘\\\\s+’',\n",
       "       'param_desc': \"A string or regex delimiter. The default of '\\\\\\\\s+' denotes\\none or more whitespace characters.\\n\"},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_clipboard.html#pandas.read_clipboard'},\n",
       "    {'function_name': 'pandas.DataFrame.to_clipboard',\n",
       "     'full_function': 'DataFrame.to_clipboard(*, excel=True, sep=None, **kwargs)',\n",
       "     'function_text': 'Copy object to the system clipboard.',\n",
       "     'parameter_names_desc': [{'param_name': 'excel',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Produce output in a csv format for easy pasting into excel.\\n\\nTrue, use the provided separator for csv pasting.\\nFalse, write a string representation of the object to the clipboard.\\n\\n'},\n",
       "      {'param_name': 'sep',\n",
       "       'param_type': \"str, default '\\\\t'\",\n",
       "       'param_desc': 'Field delimiter.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_clipboard.html#pandas.DataFrame.to_clipboard'}]},\n",
       "  {'name': 'Excel',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#excel',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html#pandas.read_excel',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.html#pandas.ExcelFile',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.sheet_names.html#pandas.ExcelFile.sheet_names',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.to_excel.html#pandas.io.formats.style.Styler.to_excel',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.ExcelWriter.html#pandas.ExcelWriter',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.book.html#pandas.ExcelFile.book',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.parse.html#pandas.ExcelFile.parse'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_excel',\n",
       "     'full_function': \"pandas.read_excel(io, sheet_name=0, *, header=0, names=None, index_col=None, usecols=None, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, parse_dates=False, date_parser=_NoDefault.no_default, date_format=None, thousands=None, decimal='.', comment=None, skipfooter=0, storage_options=None, dtype_backend=_NoDefault.no_default, engine_kwargs=None)\",\n",
       "     'function_text': 'Read an Excel file into a pandas DataFrame.',\n",
       "     'parameter_names_desc': [{'param_name': 'io',\n",
       "       'param_type': 'str, bytes, ExcelFile, xlrd.Book, path object, or file-like object',\n",
       "       'param_desc': 'Any valid string path is acceptable. The string could be a URL. Valid\\nURL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be: file://localhost/path/to/table.xlsx.\\nIf you want to pass in a path object, pandas accepts any os.PathLike.\\nBy file-like object, we refer to objects with a read() method,\\nsuch as a file handle (e.g. via builtin open function)\\nor StringIO.\\n\\nDeprecated since version 2.1.0: Passing byte strings is deprecated. To read from a\\nbyte string, wrap it in a BytesIO object.\\n\\n'},\n",
       "      {'param_name': 'sheet_name',\n",
       "       'param_type': 'str, int, list, or None, default 0',\n",
       "       'param_desc': 'Strings are used for sheet names. Integers are used in zero-indexed\\nsheet positions (chart sheets do not count as a sheet position).\\nLists of strings/integers are used to request multiple sheets.\\nSpecify None to get all worksheets.\\nAvailable cases:\\n\\nDefaults to 0: 1st sheet as a DataFrame\\n1: 2nd sheet as a DataFrame\\n\"Sheet1\": Load sheet with name “Sheet1”\\n[0, 1, \"Sheet5\"]: Load first, second and sheet named “Sheet5”\\nas a dict of DataFrame\\nNone: All worksheets.\\n\\n'},\n",
       "      {'param_name': 'header',\n",
       "       'param_type': 'int, list of int, default 0',\n",
       "       'param_desc': 'Row (0-indexed) to use for the column labels of the parsed\\nDataFrame. If a list of integers is passed those row positions will\\nbe combined into a MultiIndex. Use None if there is no header.\\n'},\n",
       "      {'param_name': 'names',\n",
       "       'param_type': 'array-like, default None',\n",
       "       'param_desc': 'List of column names to use. If file contains no header row,\\nthen you should explicitly pass header=None.\\n'},\n",
       "      {'param_name': 'index_col',\n",
       "       'param_type': 'int, str, list of int, default None',\n",
       "       'param_desc': 'Column (0-indexed) to use as the row labels of the DataFrame.\\nPass None if there is no such column. If a list is passed,\\nthose columns will be combined into a MultiIndex. If a\\nsubset of data is selected with usecols, index_col\\nis based on the subset.\\nMissing values will be forward filled to allow roundtripping with\\nto_excel for merged_cells=True. To avoid forward filling the\\nmissing values use set_index after reading the data instead of\\nindex_col.\\n'},\n",
       "      {'param_name': 'usecols',\n",
       "       'param_type': 'str, list-like, or callable, default None',\n",
       "       'param_desc': '\\nIf None, then parse all columns.\\nIf str, then indicates comma separated list of Excel column letters\\nand column ranges (e.g. “A:E” or “A,C,E:F”). Ranges are inclusive of\\nboth sides.\\nIf list of int, then indicates list of column numbers to be parsed\\n(0-indexed).\\nIf list of string, then indicates list of column names to be parsed.\\nIf callable, then evaluate each column name against it and parse the\\ncolumn if the callable returns True.\\n\\nReturns a subset of the columns according to behavior above.\\n'},\n",
       "      {'param_name': 'dtype',\n",
       "       'param_type': 'Type name or dict of column -> type, default None',\n",
       "       'param_desc': 'Data type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32}\\nUse object to preserve data as stored in Excel and not interpret dtype,\\nwhich will necessarily result in object dtype.\\nIf converters are specified, they will be applied INSTEAD\\nof dtype conversion.\\nIf you use None, it will infer the dtype of each column based on the data.\\n'},\n",
       "      {'param_name': 'engine',\n",
       "       'param_type': '{‘openpyxl’, ‘calamine’, ‘odf’, ‘pyxlsb’, ‘xlrd’}, default None',\n",
       "       'param_desc': 'If io is not a buffer or path, this must be set to identify io.\\nEngine compatibility :\\n\\nopenpyxl supports newer Excel file formats.\\ncalamine supports Excel (.xls, .xlsx, .xlsm, .xlsb)\\nand OpenDocument (.ods) file formats.\\nodf supports OpenDocument file formats (.odf, .ods, .odt).\\npyxlsb supports Binary Excel files.\\nxlrd supports old-style Excel files (.xls).\\n\\nWhen engine=None, the following logic will be used to determine the engine:\\n\\nIf path_or_buffer is an OpenDocument format (.odf, .ods, .odt),\\nthen odf will be used.\\nOtherwise if path_or_buffer is an xls format, xlrd will be used.\\nOtherwise if path_or_buffer is in xlsb format, pyxlsb will be used.\\nOtherwise openpyxl will be used.\\n\\n'},\n",
       "      {'param_name': 'converters',\n",
       "       'param_type': 'dict, default None',\n",
       "       'param_desc': 'Dict of functions for converting values in certain columns. Keys can\\neither be integers or column labels, values are functions that take one\\ninput argument, the Excel cell content, and return the transformed\\ncontent.\\n'},\n",
       "      {'param_name': 'true_values',\n",
       "       'param_type': 'list, default None',\n",
       "       'param_desc': 'Values to consider as True.\\n'},\n",
       "      {'param_name': 'false_values',\n",
       "       'param_type': 'list, default None',\n",
       "       'param_desc': 'Values to consider as False.\\n'},\n",
       "      {'param_name': 'skiprows',\n",
       "       'param_type': 'list-like, int, or callable, optional',\n",
       "       'param_desc': 'Line numbers to skip (0-indexed) or number of lines to skip (int) at the\\nstart of the file. If callable, the callable function will be evaluated\\nagainst the row indices, returning True if the row should be skipped and\\nFalse otherwise. An example of a valid callable argument would be lambda\\nx: x in [0, 2].\\n'},\n",
       "      {'param_name': 'nrows',\n",
       "       'param_type': 'int, default None',\n",
       "       'param_desc': 'Number of rows to parse.\\n'},\n",
       "      {'param_name': 'na_values',\n",
       "       'param_type': 'scalar, str, list-like, or dict, default None',\n",
       "       'param_desc': 'Additional strings to recognize as NA/NaN. If dict passed, specific\\nper-column NA values. By default the following values are interpreted\\nas NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’,\\n‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘None’,\\n‘n/a’, ‘nan’, ‘null’.\\n'},\n",
       "      {'param_name': 'keep_default_na',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Whether or not to include the default NaN values when parsing the data.\\nDepending on whether na_values is passed in, the behavior is as follows:\\n\\nIf keep_default_na is True, and na_values are specified,\\nna_values is appended to the default NaN values used for parsing.\\nIf keep_default_na is True, and na_values are not specified, only\\nthe default NaN values are used for parsing.\\nIf keep_default_na is False, and na_values are specified, only\\nthe NaN values specified na_values are used for parsing.\\nIf keep_default_na is False, and na_values are not specified, no\\nstrings will be parsed as NaN.\\n\\nNote that if na_filter is passed in as False, the keep_default_na and\\nna_values parameters will be ignored.\\n'},\n",
       "      {'param_name': 'na_filter',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Detect missing value markers (empty strings and the value of na_values). In\\ndata without any NAs, passing na_filter=False can improve the\\nperformance of reading a large file.\\n'},\n",
       "      {'param_name': 'verbose',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Indicate number of NA values placed in non-numeric columns.\\n'},\n",
       "      {'param_name': 'parse_dates',\n",
       "       'param_type': 'bool, list-like, or dict, default False',\n",
       "       'param_desc': 'The behavior is as follows:\\n\\nbool. If True -> try parsing the index.\\nlist of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\\neach as a separate date column.\\nlist of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as\\na single date column.\\ndict, e.g. {‘foo’ : [1, 3]} -> parse columns 1, 3 as date and call\\nresult ‘foo’\\n\\nIf a column or index contains an unparsable date, the entire column or\\nindex will be returned unaltered as an object data type. If you don`t want to\\nparse some cells as date just change their type in Excel to “Text”.\\nFor non-standard datetime parsing, use pd.to_datetime after pd.read_excel.\\nNote: A fast-path exists for iso8601-formatted dates.\\n'},\n",
       "      {'param_name': 'date_parser',\n",
       "       'param_type': 'function, optional',\n",
       "       'param_desc': 'Function to use for converting a sequence of string columns to an array of\\ndatetime instances. The default uses dateutil.parser.parser to do the\\nconversion. Pandas will try to call date_parser in three different ways,\\nadvancing to the next if an exception occurs: 1) Pass one or more arrays\\n(as defined by parse_dates) as arguments; 2) concatenate (row-wise) the\\nstring values from the columns defined by parse_dates into a single array\\nand pass that; and 3) call date_parser once for each row using one or\\nmore strings (corresponding to the columns defined by parse_dates) as\\narguments.\\n\\nDeprecated since version 2.0.0: Use date_format instead, or read in as object and then apply\\nto_datetime() as-needed.\\n\\n'},\n",
       "      {'param_name': 'date_format',\n",
       "       'param_type': 'str or dict of column -> format, default None',\n",
       "       'param_desc': 'If used in conjunction with parse_dates, will parse dates according to this\\nformat. For anything more complex,\\nplease read in as object and then apply to_datetime() as-needed.\\n\\nNew in version 2.0.0.\\n\\n'},\n",
       "      {'param_name': 'thousands',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'Thousands separator for parsing string columns to numeric. Note that\\nthis parameter is only necessary for columns stored as TEXT in Excel,\\nany numeric columns will automatically be parsed, regardless of display\\nformat.\\n'},\n",
       "      {'param_name': 'decimal',\n",
       "       'param_type': 'str, default ‘.’',\n",
       "       'param_desc': 'Character to recognize as decimal point for parsing string columns to numeric.\\nNote that this parameter is only necessary for columns stored as TEXT in Excel,\\nany numeric columns will automatically be parsed, regardless of display\\nformat.(e.g. use ‘,’ for European data).\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "      {'param_name': 'comment',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'Comments out remainder of line. Pass a character or characters to this\\nargument to indicate comments in the input file. Any data between the\\ncomment string and the end of the current line is ignored.\\n'},\n",
       "      {'param_name': 'skipfooter',\n",
       "       'param_type': 'int, default 0',\n",
       "       'param_desc': 'Rows at the end to skip (0-indexed).\\n'},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "      {'param_name': 'engine_kwargs',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Arbitrary keyword arguments passed to excel engine.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html#pandas.read_excel'},\n",
       "    {'function_name': 'pandas.ExcelFile',\n",
       "     'full_function': 'class pandas.ExcelFile(path_or_buffer, engine=None, storage_options=None, engine_kwargs=None)',\n",
       "     'function_text': 'Class for parsing tabular Excel sheets into DataFrame objects.',\n",
       "     'parameter_names_desc': [{'param_name': 'path_or_buffer',\n",
       "       'param_type': 'str, bytes, path object (pathlib.Path or py._path.local.LocalPath),',\n",
       "       'param_desc': 'A file-like object, xlrd workbook or openpyxl workbook.\\nIf a string or path object, expected to be a path to a\\n.xls, .xlsx, .xlsb, .xlsm, .odf, .ods, or .odt file.\\n'},\n",
       "      {'param_name': 'engine',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'If io is not a buffer or path, this must be set to identify io.\\nSupported engines: xlrd, openpyxl, odf, pyxlsb, calamine\\nEngine compatibility :\\n\\nxlrd supports old-style Excel files (.xls).\\nopenpyxl supports newer Excel file formats.\\nodf supports OpenDocument file formats (.odf, .ods, .odt).\\npyxlsb supports Binary Excel files.\\ncalamine supports Excel (.xls, .xlsx, .xlsm, .xlsb)\\nand OpenDocument (.ods) file formats.\\n\\n\\nChanged in version 1.2.0: The engine xlrd\\nnow only supports old-style .xls files.\\nWhen engine=None, the following logic will be\\nused to determine the engine:\\n\\nIf path_or_buffer is an OpenDocument format (.odf, .ods, .odt),\\nthen odf will be used.\\nOtherwise if path_or_buffer is an xls format,\\nxlrd will be used.\\nOtherwise if path_or_buffer is in xlsb format,\\npyxlsb will be used.\\n\\n\\nNew in version 1.3.0.\\n\\n\\nOtherwise if openpyxl is installed,\\nthen openpyxl will be used.\\nOtherwise if xlrd >= 2.0 is installed, a ValueError will be raised.\\n\\n\\nWarning\\nPlease do not report issues when using xlrd to read .xlsx files.\\nThis is not supported, switch to using openpyxl instead.\\n\\n\\n'},\n",
       "      {'param_name': 'engine_kwargs',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Arbitrary keyword arguments passed to excel engine.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.html#pandas.ExcelFile'},\n",
       "    {'function_name': 'pandas.ExcelFile',\n",
       "     'full_function': 'class pandas.ExcelFile(path_or_buffer, engine=None, storage_options=None, engine_kwargs=None)',\n",
       "     'function_text': 'Class for parsing tabular Excel sheets into DataFrame objects.',\n",
       "     'parameter_names_desc': [{'param_name': 'path_or_buffer',\n",
       "       'param_type': 'str, bytes, path object (pathlib.Path or py._path.local.LocalPath),',\n",
       "       'param_desc': 'A file-like object, xlrd workbook or openpyxl workbook.\\nIf a string or path object, expected to be a path to a\\n.xls, .xlsx, .xlsb, .xlsm, .odf, .ods, or .odt file.\\n'},\n",
       "      {'param_name': 'engine',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'If io is not a buffer or path, this must be set to identify io.\\nSupported engines: xlrd, openpyxl, odf, pyxlsb, calamine\\nEngine compatibility :\\n\\nxlrd supports old-style Excel files (.xls).\\nopenpyxl supports newer Excel file formats.\\nodf supports OpenDocument file formats (.odf, .ods, .odt).\\npyxlsb supports Binary Excel files.\\ncalamine supports Excel (.xls, .xlsx, .xlsm, .xlsb)\\nand OpenDocument (.ods) file formats.\\n\\n\\nChanged in version 1.2.0: The engine xlrd\\nnow only supports old-style .xls files.\\nWhen engine=None, the following logic will be\\nused to determine the engine:\\n\\nIf path_or_buffer is an OpenDocument format (.odf, .ods, .odt),\\nthen odf will be used.\\nOtherwise if path_or_buffer is an xls format,\\nxlrd will be used.\\nOtherwise if path_or_buffer is in xlsb format,\\npyxlsb will be used.\\n\\n\\nNew in version 1.3.0.\\n\\n\\nOtherwise if openpyxl is installed,\\nthen openpyxl will be used.\\nOtherwise if xlrd >= 2.0 is installed, a ValueError will be raised.\\n\\n\\nWarning\\nPlease do not report issues when using xlrd to read .xlsx files.\\nThis is not supported, switch to using openpyxl instead.\\n\\n\\n'},\n",
       "      {'param_name': 'engine_kwargs',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Arbitrary keyword arguments passed to excel engine.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.html#pandas.ExcelFile'},\n",
       "    {'function_name': 'pandas.io.formats.style.Styler.to_excel',\n",
       "     'full_function': \"Styler.to_excel(excel_writer, sheet_name='Sheet1', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, encoding=None, inf_rep='inf', verbose=True, freeze_panes=None, storage_options=None)\",\n",
       "     'function_text': 'Write Styler to an Excel sheet.',\n",
       "     'parameter_names_desc': [{'param_name': 'excel_writer',\n",
       "       'param_type': 'path-like, file-like, or ExcelWriter object',\n",
       "       'param_desc': 'File path or existing ExcelWriter.\\n'},\n",
       "      {'param_name': 'sheet_name',\n",
       "       'param_type': 'str, default ‘Sheet1’',\n",
       "       'param_desc': 'Name of sheet which will contain DataFrame.\\n'},\n",
       "      {'param_name': 'na_rep',\n",
       "       'param_type': 'str, default ‘’',\n",
       "       'param_desc': 'Missing data representation.\\n'},\n",
       "      {'param_name': 'float_format',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Format string for floating point numbers. For example\\nfloat_format=\"%.2f\" will format 0.1234 to 0.12.\\n'},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'sequence or list of str, optional',\n",
       "       'param_desc': 'Columns to write.\\n'},\n",
       "      {'param_name': 'header',\n",
       "       'param_type': 'bool or list of str, default True',\n",
       "       'param_desc': 'Write out the column names. If a list of string is given it is\\nassumed to be aliases for the column names.\\n'},\n",
       "      {'param_name': 'index',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Write row names (index).\\n'},\n",
       "      {'param_name': 'index_label',\n",
       "       'param_type': 'str or sequence, optional',\n",
       "       'param_desc': 'Column label for index column(s) if desired. If not specified, and\\nheader and index are True, then the index names are used. A\\nsequence should be given if the DataFrame uses MultiIndex.\\n'},\n",
       "      {'param_name': 'startrow',\n",
       "       'param_type': 'int, default 0',\n",
       "       'param_desc': 'Upper left cell row to dump data frame.\\n'},\n",
       "      {'param_name': 'startcol',\n",
       "       'param_type': 'int, default 0',\n",
       "       'param_desc': 'Upper left cell column to dump data frame.\\n'},\n",
       "      {'param_name': 'engine',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Write engine to use, ‘openpyxl’ or ‘xlsxwriter’. You can also set this\\nvia the options io.excel.xlsx.writer or\\nio.excel.xlsm.writer.\\n'},\n",
       "      {'param_name': 'merge_cells',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Write MultiIndex and Hierarchical Rows as merged cells.\\n'},\n",
       "      {'param_name': 'inf_rep',\n",
       "       'param_type': 'str, default ‘inf’',\n",
       "       'param_desc': 'Representation for infinity (there is no native representation for\\ninfinity in Excel).\\n'},\n",
       "      {'param_name': 'freeze_panes',\n",
       "       'param_type': 'tuple of int (length 2), optional',\n",
       "       'param_desc': 'Specifies the one-based bottommost row and rightmost column that\\nis to be frozen.\\n'},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n\\nNew in version 1.5.0.\\n\\n'},\n",
       "      {'param_name': 'engine_kwargs',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Arbitrary keyword arguments passed to excel engine.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.to_excel.html#pandas.io.formats.style.Styler.to_excel'},\n",
       "    {'function_name': 'pandas.ExcelWriter',\n",
       "     'full_function': \"class pandas.ExcelWriter(path, engine=None, date_format=None, datetime_format=None, mode='w', storage_options=None, if_sheet_exists=None, engine_kwargs=None)\",\n",
       "     'function_text': 'Class for writing DataFrame objects into excel sheets.',\n",
       "     'parameter_names_desc': [{'param_name': 'path',\n",
       "       'param_type': 'str or typing.BinaryIO',\n",
       "       'param_desc': 'Path to xls or xlsx or ods file.\\n'},\n",
       "      {'param_name': 'engine',\n",
       "       'param_type': 'str (optional)',\n",
       "       'param_desc': 'Engine to use for writing. If None, defaults to\\nio.excel.<extension>.writer. NOTE: can only be passed as a keyword\\nargument.\\n'},\n",
       "      {'param_name': 'date_format',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'Format string for dates written into Excel files (e.g. ‘YYYY-MM-DD’).\\n'},\n",
       "      {'param_name': 'datetime_format',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'Format string for datetime objects written into Excel files.\\n(e.g. ‘YYYY-MM-DD HH:MM:SS’).\\n'},\n",
       "      {'param_name': 'mode',\n",
       "       'param_type': '{‘w’, ‘a’}, default ‘w’',\n",
       "       'param_desc': 'File mode to use (write or append). Append does not work with fsspec URLs.\\n'},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "      {'param_name': 'if_sheet_exists',\n",
       "       'param_type': '{‘error’, ‘new’, ‘replace’, ‘overlay’}, default ‘error’',\n",
       "       'param_desc': 'How to behave when trying to write to a sheet that already\\nexists (append mode only).\\n\\nerror: raise a ValueError.\\nnew: Create a new sheet, with a name determined by the engine.\\nreplace: Delete the contents of the sheet before writing to it.\\noverlay: Write contents to the existing sheet without first removing,\\nbut possibly over top of, the existing contents.\\n\\n\\nNew in version 1.3.0.\\n\\n\\nChanged in version 1.4.0: Added overlay option\\n\\n'},\n",
       "      {'param_name': 'engine_kwargs',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Keyword arguments to be passed into the engine. These will be passed to\\nthe following functions of the respective engines:\\n\\nxlsxwriter: xlsxwriter.Workbook(file, **engine_kwargs)\\nopenpyxl (write mode): openpyxl.Workbook(**engine_kwargs)\\nopenpyxl (append mode): openpyxl.load_workbook(file, **engine_kwargs)\\nodswriter: odf.opendocument.OpenDocumentSpreadsheet(**engine_kwargs)\\n\\n\\nNew in version 1.3.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ExcelWriter.html#pandas.ExcelWriter'},\n",
       "    {'function_name': 'pandas.DataFrame.to_excel',\n",
       "     'full_function': \"DataFrame.to_excel(excel_writer, *, sheet_name='Sheet1', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, inf_rep='inf', freeze_panes=None, storage_options=None, engine_kwargs=None)\",\n",
       "     'function_text': 'Write object to an Excel sheet.',\n",
       "     'parameter_names_desc': [{'param_name': 'excel_writer',\n",
       "       'param_type': 'path-like, file-like, or ExcelWriter object',\n",
       "       'param_desc': 'File path or existing ExcelWriter.\\n'},\n",
       "      {'param_name': 'sheet_name',\n",
       "       'param_type': 'str, default ‘Sheet1’',\n",
       "       'param_desc': 'Name of sheet which will contain DataFrame.\\n'},\n",
       "      {'param_name': 'na_rep',\n",
       "       'param_type': 'str, default ‘’',\n",
       "       'param_desc': 'Missing data representation.\\n'},\n",
       "      {'param_name': 'float_format',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Format string for floating point numbers. For example\\nfloat_format=\"%.2f\" will format 0.1234 to 0.12.\\n'},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'sequence or list of str, optional',\n",
       "       'param_desc': 'Columns to write.\\n'},\n",
       "      {'param_name': 'header',\n",
       "       'param_type': 'bool or list of str, default True',\n",
       "       'param_desc': 'Write out the column names. If a list of string is given it is\\nassumed to be aliases for the column names.\\n'},\n",
       "      {'param_name': 'index',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Write row names (index).\\n'},\n",
       "      {'param_name': 'index_label',\n",
       "       'param_type': 'str or sequence, optional',\n",
       "       'param_desc': 'Column label for index column(s) if desired. If not specified, and\\nheader and index are True, then the index names are used. A\\nsequence should be given if the DataFrame uses MultiIndex.\\n'},\n",
       "      {'param_name': 'startrow',\n",
       "       'param_type': 'int, default 0',\n",
       "       'param_desc': 'Upper left cell row to dump data frame.\\n'},\n",
       "      {'param_name': 'startcol',\n",
       "       'param_type': 'int, default 0',\n",
       "       'param_desc': 'Upper left cell column to dump data frame.\\n'},\n",
       "      {'param_name': 'engine',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Write engine to use, ‘openpyxl’ or ‘xlsxwriter’. You can also set this\\nvia the options io.excel.xlsx.writer or\\nio.excel.xlsm.writer.\\n'},\n",
       "      {'param_name': 'merge_cells',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Write MultiIndex and Hierarchical Rows as merged cells.\\n'},\n",
       "      {'param_name': 'inf_rep',\n",
       "       'param_type': 'str, default ‘inf’',\n",
       "       'param_desc': 'Representation for infinity (there is no native representation for\\ninfinity in Excel).\\n'},\n",
       "      {'param_name': 'freeze_panes',\n",
       "       'param_type': 'tuple of int (length 2), optional',\n",
       "       'param_desc': 'Specifies the one-based bottommost row and rightmost column that\\nis to be frozen.\\n'},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n\\nNew in version 1.2.0.\\n\\n'},\n",
       "      {'param_name': 'engine_kwargs',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Arbitrary keyword arguments passed to excel engine.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel'},\n",
       "    {'function_name': 'pandas.DataFrame.to_excel',\n",
       "     'full_function': \"DataFrame.to_excel(excel_writer, *, sheet_name='Sheet1', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, inf_rep='inf', freeze_panes=None, storage_options=None, engine_kwargs=None)\",\n",
       "     'function_text': 'Write object to an Excel sheet.',\n",
       "     'parameter_names_desc': [{'param_name': 'excel_writer',\n",
       "       'param_type': 'path-like, file-like, or ExcelWriter object',\n",
       "       'param_desc': 'File path or existing ExcelWriter.\\n'},\n",
       "      {'param_name': 'sheet_name',\n",
       "       'param_type': 'str, default ‘Sheet1’',\n",
       "       'param_desc': 'Name of sheet which will contain DataFrame.\\n'},\n",
       "      {'param_name': 'na_rep',\n",
       "       'param_type': 'str, default ‘’',\n",
       "       'param_desc': 'Missing data representation.\\n'},\n",
       "      {'param_name': 'float_format',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Format string for floating point numbers. For example\\nfloat_format=\"%.2f\" will format 0.1234 to 0.12.\\n'},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'sequence or list of str, optional',\n",
       "       'param_desc': 'Columns to write.\\n'},\n",
       "      {'param_name': 'header',\n",
       "       'param_type': 'bool or list of str, default True',\n",
       "       'param_desc': 'Write out the column names. If a list of string is given it is\\nassumed to be aliases for the column names.\\n'},\n",
       "      {'param_name': 'index',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Write row names (index).\\n'},\n",
       "      {'param_name': 'index_label',\n",
       "       'param_type': 'str or sequence, optional',\n",
       "       'param_desc': 'Column label for index column(s) if desired. If not specified, and\\nheader and index are True, then the index names are used. A\\nsequence should be given if the DataFrame uses MultiIndex.\\n'},\n",
       "      {'param_name': 'startrow',\n",
       "       'param_type': 'int, default 0',\n",
       "       'param_desc': 'Upper left cell row to dump data frame.\\n'},\n",
       "      {'param_name': 'startcol',\n",
       "       'param_type': 'int, default 0',\n",
       "       'param_desc': 'Upper left cell column to dump data frame.\\n'},\n",
       "      {'param_name': 'engine',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Write engine to use, ‘openpyxl’ or ‘xlsxwriter’. You can also set this\\nvia the options io.excel.xlsx.writer or\\nio.excel.xlsm.writer.\\n'},\n",
       "      {'param_name': 'merge_cells',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Write MultiIndex and Hierarchical Rows as merged cells.\\n'},\n",
       "      {'param_name': 'inf_rep',\n",
       "       'param_type': 'str, default ‘inf’',\n",
       "       'param_desc': 'Representation for infinity (there is no native representation for\\ninfinity in Excel).\\n'},\n",
       "      {'param_name': 'freeze_panes',\n",
       "       'param_type': 'tuple of int (length 2), optional',\n",
       "       'param_desc': 'Specifies the one-based bottommost row and rightmost column that\\nis to be frozen.\\n'},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n\\nNew in version 1.2.0.\\n\\n'},\n",
       "      {'param_name': 'engine_kwargs',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Arbitrary keyword arguments passed to excel engine.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel'},\n",
       "    {'function_name': 'pandas.ExcelFile.parse',\n",
       "     'full_function': 'ExcelFile.parse(sheet_name=0, header=0, names=None, index_col=None, usecols=None, converters=None, true_values=None, false_values=None, skiprows=None, nrows=None, na_values=None, parse_dates=False, date_parser=_NoDefault.no_default, date_format=None, thousands=None, comment=None, skipfooter=0, dtype_backend=_NoDefault.no_default, **kwds)',\n",
       "     'function_text': 'Parse specified sheet(s) into a DataFrame.',\n",
       "     'parameter_names_desc': [],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.parse.html#pandas.ExcelFile.parse'}]},\n",
       "  {'name': 'JSON',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#json',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_json.html#pandas.read_json',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html#pandas.DataFrame.to_json',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.io.json.build_table_schema.html#pandas.io.json.build_table_schema',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html#pandas.json_normalize'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_json',\n",
       "     'full_function': \"pandas.read_json(path_or_buf, *, orient=None, typ='frame', dtype=None, convert_axes=None, convert_dates=True, keep_default_dates=True, precise_float=False, date_unit=None, encoding=None, encoding_errors='strict', lines=False, chunksize=None, compression='infer', nrows=None, storage_options=None, dtype_backend=_NoDefault.no_default, engine='ujson')\",\n",
       "     'function_text': 'Convert a JSON string to pandas object.',\n",
       "     'parameter_names_desc': [{'param_name': 'path_or_buf',\n",
       "       'param_type': 'a valid JSON str, path object or file-like object',\n",
       "       'param_desc': 'Any valid string path is acceptable. The string could be a URL. Valid\\nURL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be:\\nfile://localhost/path/to/table.json.\\nIf you want to pass in a path object, pandas accepts any\\nos.PathLike.\\nBy file-like object, we refer to objects with a read() method,\\nsuch as a file handle (e.g. via builtin open function)\\nor StringIO.\\n\\nDeprecated since version 2.1.0: Passing json literal strings is deprecated.\\n\\n'},\n",
       "      {'param_name': 'orient',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': \"Indication of expected JSON string format.\\nCompatible JSON strings can be produced by to_json() with a\\ncorresponding orient value.\\nThe set of possible orients is:\\n\\n'split' : dict like\\n{index -> [index], columns -> [columns], data -> [values]}\\n'records' : list like\\n[{column -> value}, ... , {column -> value}]\\n'index' : dict like {index -> {column -> value}}\\n'columns' : dict like {column -> {index -> value}}\\n'values' : just the values array\\n'table' : dict like {'schema': {schema}, 'data': {data}}\\n\\nThe allowed and default values depend on the value\\nof the typ parameter.\\n\\nwhen typ == 'series',\\n\\nallowed orients are {'split','records','index'}\\ndefault is 'index'\\nThe Series index must be unique for orient 'index'.\\n\\n\\nwhen typ == 'frame',\\n\\nallowed orients are {'split','records','index',\\n'columns','values', 'table'}\\ndefault is 'columns'\\nThe DataFrame index must be unique for orients 'index' and\\n'columns'.\\nThe DataFrame columns must be unique for orients 'index',\\n'columns', and 'records'.\\n\\n\\n\\n\"},\n",
       "      {'param_name': 'typ',\n",
       "       'param_type': '{‘frame’, ‘series’}, default ‘frame’',\n",
       "       'param_desc': 'The type of object to recover.\\n'},\n",
       "      {'param_name': 'dtype',\n",
       "       'param_type': 'bool or dict, default None',\n",
       "       'param_desc': \"If True, infer dtypes; if a dict of column to dtype, then use those;\\nif False, then don’t infer dtypes at all, applies only to the data.\\nFor all orient values except 'table', default is True.\\n\"},\n",
       "      {'param_name': 'convert_axes',\n",
       "       'param_type': 'bool, default None',\n",
       "       'param_desc': \"Try to convert the axes to the proper dtypes.\\nFor all orient values except 'table', default is True.\\n\"},\n",
       "      {'param_name': 'convert_dates',\n",
       "       'param_type': 'bool or list of str, default True',\n",
       "       'param_desc': 'If True then default datelike columns may be converted (depending on\\nkeep_default_dates).\\nIf False, no dates will be converted.\\nIf a list of column names, then those columns will be converted and\\ndefault datelike columns may also be converted (depending on\\nkeep_default_dates).\\n'},\n",
       "      {'param_name': 'keep_default_dates',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': \"If parsing dates (convert_dates is not False), then try to parse the\\ndefault datelike columns.\\nA column label is datelike if\\n\\nit ends with '_at',\\nit ends with '_time',\\nit begins with 'timestamp',\\nit is 'modified', or\\nit is 'date'.\\n\\n\"},\n",
       "      {'param_name': 'precise_float',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Set to enable usage of higher precision (strtod) function when\\ndecoding string to double values. Default (False) is to use fast but\\nless precise builtin functionality.\\n'},\n",
       "      {'param_name': 'date_unit',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'The timestamp unit to detect if converting dates. The default behaviour\\nis to try and detect the correct precision, but if this is not desired\\nthen pass one of ‘s’, ‘ms’, ‘us’ or ‘ns’ to force parsing only seconds,\\nmilliseconds, microseconds or nanoseconds respectively.\\n'},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'str, default is ‘utf-8’',\n",
       "       'param_desc': 'The encoding to use to decode py3 bytes.\\n'},\n",
       "      {'param_name': 'encoding_errors',\n",
       "       'param_type': 'str, optional, default “strict”',\n",
       "       'param_desc': 'How encoding errors are treated. List of possible values .\\n\\nNew in version 1.3.0.\\n\\n'},\n",
       "      {'param_name': 'lines',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Read the file as a json object per line.\\n'},\n",
       "      {'param_name': 'chunksize',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'Return JsonReader object for iteration.\\nSee the line-delimited json docs\\nfor more information on chunksize.\\nThis can only be passed if lines=True.\\nIf this is None, the file will be read into memory all at once.\\n'},\n",
       "      {'param_name': 'compression',\n",
       "       'param_type': 'str or dict, default ‘infer’',\n",
       "       'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘path_or_buf’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "      {'param_name': 'nrows',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'The number of lines from the line-delimited jsonfile that has to be read.\\nThis can only be passed if lines=True.\\nIf this is None, all the rows will be returned.\\n'},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "      {'param_name': 'engine',\n",
       "       'param_type': '{“ujson”, “pyarrow”}, default “ujson”',\n",
       "       'param_desc': 'Parser engine to use. The \"pyarrow\" engine is only available when\\nlines=True.\\n\\nNew in version 2.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_json.html#pandas.read_json'},\n",
       "    {'function_name': 'pandas.DataFrame.to_json',\n",
       "     'full_function': \"DataFrame.to_json(path_or_buf=None, *, orient=None, date_format=None, double_precision=10, force_ascii=True, date_unit='ms', default_handler=None, lines=False, compression='infer', index=None, indent=None, storage_options=None, mode='w')\",\n",
       "     'function_text': 'Convert the object to a JSON string.',\n",
       "     'parameter_names_desc': [{'param_name': 'path_or_buf',\n",
       "       'param_type': 'str, path object, file-like object, or None, default None',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a write() function. If None, the result is\\nreturned as a string.\\n'},\n",
       "      {'param_name': 'orient',\n",
       "       'param_type': 'str',\n",
       "       'param_desc': \"Indication of expected JSON string format.\\n\\nSeries:\\n\\n\\ndefault is ‘index’\\nallowed values are: {‘split’, ‘records’, ‘index’, ‘table’}.\\n\\n\\n\\nDataFrame:\\n\\n\\ndefault is ‘columns’\\nallowed values are: {‘split’, ‘records’, ‘index’, ‘columns’,\\n‘values’, ‘table’}.\\n\\n\\n\\nThe format of the JSON string:\\n\\n\\n‘split’ : dict like {‘index’ -> [index], ‘columns’ -> [columns],\\n‘data’ -> [values]}\\n‘records’ : list like [{column -> value}, … , {column -> value}]\\n‘index’ : dict like {index -> {column -> value}}\\n‘columns’ : dict like {column -> {index -> value}}\\n‘values’ : just the values array\\n‘table’ : dict like {‘schema’: {schema}, ‘data’: {data}}\\n\\nDescribing the data, where data component is like orient='records'.\\n\\n\\n\\n\"},\n",
       "      {'param_name': 'date_format',\n",
       "       'param_type': '{None, ‘epoch’, ‘iso’}',\n",
       "       'param_desc': \"Type of date conversion. ‘epoch’ = epoch milliseconds,\\n‘iso’ = ISO8601. The default depends on the orient. For\\norient='table', the default is ‘iso’. For all other orients,\\nthe default is ‘epoch’.\\n\"},\n",
       "      {'param_name': 'double_precision',\n",
       "       'param_type': 'int, default 10',\n",
       "       'param_desc': 'The number of decimal places to use when encoding\\nfloating point values. The possible maximal value is 15.\\nPassing double_precision greater than 15 will raise a ValueError.\\n'},\n",
       "      {'param_name': 'force_ascii',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Force encoded string to be ASCII.\\n'},\n",
       "      {'param_name': 'date_unit',\n",
       "       'param_type': 'str, default ‘ms’ (milliseconds)',\n",
       "       'param_desc': 'The time unit to encode to, governs timestamp and ISO8601\\nprecision. One of ‘s’, ‘ms’, ‘us’, ‘ns’ for second, millisecond,\\nmicrosecond, and nanosecond respectively.\\n'},\n",
       "      {'param_name': 'default_handler',\n",
       "       'param_type': 'callable, default None',\n",
       "       'param_desc': 'Handler to call if object cannot otherwise be converted to a\\nsuitable format for JSON. Should receive a single argument which is\\nthe object to convert and return a serialisable object.\\n'},\n",
       "      {'param_name': 'lines',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'If ‘orient’ is ‘records’ write out line-delimited json format. Will\\nthrow ValueError if incorrect ‘orient’ since others are not\\nlist-like.\\n'},\n",
       "      {'param_name': 'compression',\n",
       "       'param_type': 'str or dict, default ‘infer’',\n",
       "       'param_desc': \"For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buf’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nSet to None for no compression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for faster compression and to create\\na reproducible gzip archive:\\ncompression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "      {'param_name': 'index',\n",
       "       'param_type': 'bool or None, default None',\n",
       "       'param_desc': 'The index is only used when ‘orient’ is ‘split’, ‘index’, ‘column’,\\nor ‘table’. Of these, ‘index’ and ‘column’ do not support\\nindex=False.\\n'},\n",
       "      {'param_name': 'indent',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'Length of whitespace used to indent each record.\\n'},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "      {'param_name': 'mode',\n",
       "       'param_type': 'str, default ‘w’ (writing)',\n",
       "       'param_desc': 'Specify the IO mode for output when supplying a path_or_buf.\\nAccepted args are ‘w’ (writing) and ‘a’ (append) only.\\nmode=’a’ is only supported when lines is True and orient is ‘records’.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html#pandas.DataFrame.to_json'},\n",
       "    {'function_name': 'pandas.io.json.build_table_schema',\n",
       "     'full_function': 'pandas.io.json.build_table_schema(data, index=True, primary_key=None, version=True)',\n",
       "     'function_text': 'Create a Table schema from data.',\n",
       "     'parameter_names_desc': [{'param_name': 'data',\n",
       "       'param_type': 'Series, DataFrame',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'index',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Whether to include data.index in the schema.\\n'},\n",
       "      {'param_name': 'primary_key',\n",
       "       'param_type': 'bool or None, default True',\n",
       "       'param_desc': 'Column names to designate as the primary key.\\nThe default None will set ‘primaryKey’ to the index\\nlevel or levels if the index is unique.\\n'},\n",
       "      {'param_name': 'version',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Whether to include a field pandas_version with the version\\nof pandas that last revised the table schema. This version\\ncan be different from the installed pandas version.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.io.json.build_table_schema.html#pandas.io.json.build_table_schema'},\n",
       "    {'function_name': 'pandas.json_normalize',\n",
       "     'full_function': \"pandas.json_normalize(data, record_path=None, meta=None, meta_prefix=None, record_prefix=None, errors='raise', sep='.', max_level=None)\",\n",
       "     'function_text': 'Normalize semi-structured JSON data into a flat table.',\n",
       "     'parameter_names_desc': [{'param_name': 'data',\n",
       "       'param_type': 'dict or list of dicts',\n",
       "       'param_desc': 'Unserialized JSON objects.\\n'},\n",
       "      {'param_name': 'record_path',\n",
       "       'param_type': 'str or list of str, default None',\n",
       "       'param_desc': 'Path in each object to list of records. If not passed, data will be\\nassumed to be an array of records.\\n'},\n",
       "      {'param_name': 'meta',\n",
       "       'param_type': 'list of paths (str or list of str), default None',\n",
       "       'param_desc': 'Fields to use as metadata for each record in resulting table.\\n'},\n",
       "      {'param_name': 'meta_prefix',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'If True, prefix records with dotted (?) path, e.g. foo.bar.field if\\nmeta is [‘foo’, ‘bar’].\\n'},\n",
       "      {'param_name': 'record_prefix',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'If True, prefix records with dotted (?) path, e.g. foo.bar.field if\\npath to records is [‘foo’, ‘bar’].\\n'},\n",
       "      {'param_name': 'errors',\n",
       "       'param_type': '{‘raise’, ‘ignore’}, default ‘raise’',\n",
       "       'param_desc': 'Configures error handling.\\n\\n‘ignore’ : will ignore KeyError if keys listed in meta are not\\nalways present.\\n‘raise’ : will raise KeyError if keys listed in meta are not\\nalways present.\\n\\n'},\n",
       "      {'param_name': 'sep',\n",
       "       'param_type': 'str, default ‘.’',\n",
       "       'param_desc': 'Nested records will generate names separated by sep.\\ne.g., for sep=’.’, {‘foo’: {‘bar’: 0}} -> foo.bar.\\n'},\n",
       "      {'param_name': 'max_level',\n",
       "       'param_type': 'int, default None',\n",
       "       'param_desc': 'Max number of levels(depth of dict) to normalize.\\nif None, normalizes all levels.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html#pandas.json_normalize'}]},\n",
       "  {'name': 'HTML',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#html',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_html.html#pandas.read_html',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.to_html.html#pandas.io.formats.style.Styler.to_html',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_html.html#pandas.DataFrame.to_html'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_html',\n",
       "     'full_function': \"pandas.read_html(io, *, match='.+', flavor=None, header=None, index_col=None, skiprows=None, attrs=None, parse_dates=False, thousands=',', encoding=None, decimal='.', converters=None, na_values=None, keep_default_na=True, displayed_only=True, extract_links=None, dtype_backend=_NoDefault.no_default, storage_options=None)\",\n",
       "     'function_text': 'Read HTML tables into a list of DataFrame objects.',\n",
       "     'parameter_names_desc': [{'param_name': 'io',\n",
       "       'param_type': 'str, path object, or file-like object',\n",
       "       'param_desc': \"String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a string read() function.\\nThe string can represent a URL or the HTML itself. Note that\\nlxml only accepts the http, ftp and file url protocols. If you have a\\nURL that starts with 'https' you might try removing the 's'.\\n\\nDeprecated since version 2.1.0: Passing html literal strings is deprecated.\\nWrap literal string/bytes input in io.StringIO/io.BytesIO instead.\\n\\n\"},\n",
       "      {'param_name': 'match',\n",
       "       'param_type': 'str or compiled regular expression, optional',\n",
       "       'param_desc': 'The set of tables containing text matching this regex or string will be\\nreturned. Unless the HTML is extremely simple you will probably need to\\npass a non-empty string here. Defaults to ‘.+’ (match any non-empty\\nstring). The default value will return all tables contained on a page.\\nThis value is converted to a regular expression so that there is\\nconsistent behavior between Beautiful Soup and lxml.\\n'},\n",
       "      {'param_name': 'flavor',\n",
       "       'param_type': '{“lxml”, “html5lib”, “bs4”} or list-like, optional',\n",
       "       'param_desc': 'The parsing engine (or list of parsing engines) to use. ‘bs4’ and\\n‘html5lib’ are synonymous with each other, they are both there for\\nbackwards compatibility. The default of None tries to use lxml\\nto parse and if that fails it falls back on bs4 + html5lib.\\n'},\n",
       "      {'param_name': 'header',\n",
       "       'param_type': 'int or list-like, optional',\n",
       "       'param_desc': 'The row (or list of rows for a MultiIndex) to use to\\nmake the columns headers.\\n'},\n",
       "      {'param_name': 'index_col',\n",
       "       'param_type': 'int or list-like, optional',\n",
       "       'param_desc': 'The column (or list of columns) to use to create the index.\\n'},\n",
       "      {'param_name': 'skiprows',\n",
       "       'param_type': 'int, list-like or slice, optional',\n",
       "       'param_desc': 'Number of rows to skip after parsing the column integer. 0-based. If a\\nsequence of integers or a slice is given, will skip the rows indexed by\\nthat sequence. Note that a single element sequence means ‘skip the nth\\nrow’ whereas an integer means ‘skip n rows’.\\n'},\n",
       "      {'param_name': 'attrs',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': \"This is a dictionary of attributes that you can pass to use to identify\\nthe table in the HTML. These are not checked for validity before being\\npassed to lxml or Beautiful Soup. However, these attributes must be\\nvalid HTML table attributes to work correctly. For example,\\nattrs = {'id': 'table'}\\n\\n\\nis a valid attribute dictionary because the ‘id’ HTML tag attribute is\\na valid HTML attribute for any HTML tag as per this document.\\nattrs = {'asdf': 'table'}\\n\\n\\nis not a valid attribute dictionary because ‘asdf’ is not a valid\\nHTML attribute even if it is a valid XML attribute. Valid HTML 4.01\\ntable attributes can be found here. A\\nworking draft of the HTML 5 spec can be found here. It contains the\\nlatest information on table attributes for the modern web.\\n\"},\n",
       "      {'param_name': 'parse_dates',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'See read_csv() for more details.\\n'},\n",
       "      {'param_name': 'thousands',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': \"Separator to use to parse thousands. Defaults to ','.\\n\"},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'The encoding used to decode the web page. Defaults to None.``None``\\npreserves the previous encoding behavior, which depends on the\\nunderlying parser library (e.g., the parser library will try to use\\nthe encoding provided by the document).\\n'},\n",
       "      {'param_name': 'decimal',\n",
       "       'param_type': 'str, default ‘.’',\n",
       "       'param_desc': 'Character to recognize as decimal point (e.g. use ‘,’ for European\\ndata).\\n'},\n",
       "      {'param_name': 'converters',\n",
       "       'param_type': 'dict, default None',\n",
       "       'param_desc': 'Dict of functions for converting values in certain columns. Keys can\\neither be integers or column labels, values are functions that take one\\ninput argument, the cell (not column) content, and return the\\ntransformed content.\\n'},\n",
       "      {'param_name': 'na_values',\n",
       "       'param_type': 'iterable, default None',\n",
       "       'param_desc': 'Custom NA values.\\n'},\n",
       "      {'param_name': 'keep_default_na',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'If na_values are specified and keep_default_na is False the default NaN\\nvalues are overridden, otherwise they’re appended to.\\n'},\n",
       "      {'param_name': 'displayed_only',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Whether elements with “display: none” should be parsed.\\n'},\n",
       "      {'param_name': 'extract_links',\n",
       "       'param_type': '{None, “all”, “header”, “body”, “footer”}',\n",
       "       'param_desc': 'Table elements in the specified section(s) with <a> tags will have their\\nhref extracted.\\n\\nNew in version 1.5.0.\\n\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n\\nNew in version 2.1.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_html.html#pandas.read_html'},\n",
       "    {'function_name': 'pandas.io.formats.style.Styler.to_html',\n",
       "     'full_function': 'Styler.to_html(buf=None, *, table_uuid=None, table_attributes=None, sparse_index=None, sparse_columns=None, bold_headers=False, caption=None, max_rows=None, max_columns=None, encoding=None, doctype_html=False, exclude_styles=False, **kwargs)',\n",
       "     'function_text': 'Write Styler to a file, buffer or string in HTML-CSS format.',\n",
       "     'parameter_names_desc': [{'param_name': 'buf',\n",
       "       'param_type': 'str, path object, file-like object, optional',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a string write() function. If None, the result is\\nreturned as a string.\\n'},\n",
       "      {'param_name': 'table_uuid',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Id attribute assigned to the <table> HTML element in the format:\\n<table id=\"T_<table_uuid>\" ..>\\nIf not given uses Styler’s initially assigned value.\\n'},\n",
       "      {'param_name': 'table_attributes',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Attributes to assign within the <table> HTML element in the format:\\n<table .. <table_attributes> >\\nIf not given defaults to Styler’s preexisting value.\\n'},\n",
       "      {'param_name': 'sparse_index',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'Whether to sparsify the display of a hierarchical index. Setting to False\\nwill display each explicit level element in a hierarchical key for each row.\\nDefaults to pandas.options.styler.sparse.index value.\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "      {'param_name': 'sparse_columns',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'Whether to sparsify the display of a hierarchical index. Setting to False\\nwill display each explicit level element in a hierarchical key for each\\ncolumn. Defaults to pandas.options.styler.sparse.columns value.\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "      {'param_name': 'bold_headers',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'Adds “font-weight: bold;” as a CSS property to table style header cells.\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "      {'param_name': 'caption',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Set, or overwrite, the caption on Styler before rendering.\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "      {'param_name': 'max_rows',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'The maximum number of rows that will be rendered. Defaults to\\npandas.options.styler.render.max_rows/max_columns.\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "      {'param_name': 'max_columns',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'The maximum number of columns that will be rendered. Defaults to\\npandas.options.styler.render.max_columns, which is None.\\nRows and columns may be reduced if the number of total elements is\\nlarge. This value is set to pandas.options.styler.render.max_elements,\\nwhich is 262144 (18 bit browser rendering).\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Character encoding setting for file output (and meta tags if available).\\nDefaults to pandas.options.styler.render.encoding value of “utf-8”.\\n'},\n",
       "      {'param_name': 'doctype_html',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Whether to output a fully structured HTML file including all\\nHTML elements, or just the core <style> and <table> elements.\\n'},\n",
       "      {'param_name': 'exclude_styles',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Whether to include the <style> element and all associated element\\nclass and id identifiers, or solely the <table> element without\\nstyling identifiers.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.to_html.html#pandas.io.formats.style.Styler.to_html'},\n",
       "    {'function_name': 'pandas.DataFrame.to_html',\n",
       "     'full_function': \"DataFrame.to_html(buf=None, *, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, max_cols=None, show_dimensions=False, decimal='.', bold_rows=True, classes=None, escape=True, notebook=False, border=None, table_id=None, render_links=False, encoding=None)\",\n",
       "     'function_text': 'Render a DataFrame as an HTML table.',\n",
       "     'parameter_names_desc': [{'param_name': 'buf',\n",
       "       'param_type': 'str, Path or StringIO-like, optional, default None',\n",
       "       'param_desc': 'Buffer to write to. If None, the output is returned as a string.\\n'},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'array-like, optional, default None',\n",
       "       'param_desc': 'The subset of columns to write. Writes all columns by default.\\n'},\n",
       "      {'param_name': 'col_space',\n",
       "       'param_type': 'str or int, list or dict of int or str, optional',\n",
       "       'param_desc': 'The minimum width of each column in CSS length units. An int is assumed to be px units..\\n'},\n",
       "      {'param_name': 'header',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'Whether to print column labels, default True.\\n'},\n",
       "      {'param_name': 'index',\n",
       "       'param_type': 'bool, optional, default True',\n",
       "       'param_desc': 'Whether to print index (row) labels.\\n'},\n",
       "      {'param_name': 'na_rep',\n",
       "       'param_type': 'str, optional, default ‘NaN’',\n",
       "       'param_desc': 'String representation of NaN to use.\\n'},\n",
       "      {'param_name': 'formatters',\n",
       "       'param_type': 'list, tuple or dict of one-param. functions, optional',\n",
       "       'param_desc': 'Formatter functions to apply to columns’ elements by position or\\nname.\\nThe result of each function must be a unicode string.\\nList/tuple must be of length equal to the number of columns.\\n'},\n",
       "      {'param_name': 'float_format',\n",
       "       'param_type': 'one-parameter function, optional, default None',\n",
       "       'param_desc': 'Formatter function to apply to columns’ elements if they are\\nfloats. This function must return a unicode string and will be\\napplied only to the non-NaN elements, with NaN being\\nhandled by na_rep.\\n'},\n",
       "      {'param_name': 'sparsify',\n",
       "       'param_type': 'bool, optional, default True',\n",
       "       'param_desc': 'Set to False for a DataFrame with a hierarchical index to print\\nevery multiindex key at each row.\\n'},\n",
       "      {'param_name': 'index_names',\n",
       "       'param_type': 'bool, optional, default True',\n",
       "       'param_desc': 'Prints the names of the indexes.\\n'},\n",
       "      {'param_name': 'justify',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'How to justify the column labels. If None uses the option from\\nthe print configuration (controlled by set_option), ‘right’ out\\nof the box. Valid values are\\n\\nleft\\nright\\ncenter\\njustify\\njustify-all\\nstart\\nend\\ninherit\\nmatch-parent\\ninitial\\nunset.\\n\\n'},\n",
       "      {'param_name': 'max_rows',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'Maximum number of rows to display in the console.\\n'},\n",
       "      {'param_name': 'max_cols',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'Maximum number of columns to display in the console.\\n'},\n",
       "      {'param_name': 'show_dimensions',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Display DataFrame dimensions (number of rows by number of columns).\\n'},\n",
       "      {'param_name': 'decimal',\n",
       "       'param_type': 'str, default ‘.’',\n",
       "       'param_desc': 'Character recognized as decimal separator, e.g. ‘,’ in Europe.\\n'},\n",
       "      {'param_name': 'bold_rows',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Make the row labels bold in the output.\\n'},\n",
       "      {'param_name': 'classes',\n",
       "       'param_type': 'str or list or tuple, default None',\n",
       "       'param_desc': 'CSS class(es) to apply to the resulting html table.\\n'},\n",
       "      {'param_name': 'escape',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Convert the characters <, >, and & to HTML-safe sequences.\\n'},\n",
       "      {'param_name': 'notebook',\n",
       "       'param_type': '{True, False}, default False',\n",
       "       'param_desc': 'Whether the generated HTML is for IPython Notebook.\\n'},\n",
       "      {'param_name': 'border',\n",
       "       'param_type': 'int',\n",
       "       'param_desc': 'A border=border attribute is included in the opening\\n<table> tag. Default pd.options.display.html.border.\\n'},\n",
       "      {'param_name': 'table_id',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'A css id is included in the opening <table> tag if specified.\\n'},\n",
       "      {'param_name': 'render_links',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Convert URLs to HTML links.\\n'},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'str, default “utf-8”',\n",
       "       'param_desc': 'Set character encoding.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_html.html#pandas.DataFrame.to_html'}]},\n",
       "  {'name': 'XML',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#xml',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_xml.html#pandas.read_xml',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_xml.html#pandas.DataFrame.to_xml'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_xml',\n",
       "     'full_function': \"pandas.read_xml(path_or_buffer, *, xpath='./*', namespaces=None, elems_only=False, attrs_only=False, names=None, dtype=None, converters=None, parse_dates=None, encoding='utf-8', parser='lxml', stylesheet=None, iterparse=None, compression='infer', storage_options=None, dtype_backend=_NoDefault.no_default)\",\n",
       "     'function_text': 'Read XML document into a DataFrame object.',\n",
       "     'parameter_names_desc': [{'param_name': 'path_or_buffer',\n",
       "       'param_type': 'str, path object, or file-like object',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a read() function. The string can be any valid XML\\nstring or a path. The string can further be a URL. Valid URL schemes\\ninclude http, ftp, s3, and file.\\n\\nDeprecated since version 2.1.0: Passing xml literal strings is deprecated.\\nWrap literal xml input in io.StringIO or io.BytesIO instead.\\n\\n'},\n",
       "      {'param_name': 'xpath',\n",
       "       'param_type': 'str, optional, default ‘./*’',\n",
       "       'param_desc': 'The XPath to parse required set of nodes for migration to\\nDataFrame.``XPath`` should return a collection of elements\\nand not a single element. Note: The etree parser supports limited XPath\\nexpressions. For more complex XPath, use lxml which requires\\ninstallation.\\n'},\n",
       "      {'param_name': 'namespaces',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'The namespaces defined in XML document as dicts with key being\\nnamespace prefix and value the URI. There is no need to include all\\nnamespaces in XML, only the ones used in xpath expression.\\nNote: if XML document uses default namespace denoted as\\nxmlns=’<URI>’ without a prefix, you must assign any temporary\\nnamespace prefix such as ‘doc’ to the URI in order to parse\\nunderlying nodes and/or attributes. For example,\\nnamespaces = {\"doc\": \"https://example.com\"}\\n\\n\\n'},\n",
       "      {'param_name': 'elems_only',\n",
       "       'param_type': 'bool, optional, default False',\n",
       "       'param_desc': 'Parse only the child elements at the specified xpath. By default,\\nall child elements and non-empty text nodes are returned.\\n'},\n",
       "      {'param_name': 'attrs_only',\n",
       "       'param_type': 'bool, optional, default False',\n",
       "       'param_desc': 'Parse only the attributes at the specified xpath.\\nBy default, all attributes are returned.\\n'},\n",
       "      {'param_name': 'names',\n",
       "       'param_type': 'list-like, optional',\n",
       "       'param_desc': 'Column names for DataFrame of parsed XML data. Use this parameter to\\nrename original element names and distinguish same named elements and\\nattributes.\\n'},\n",
       "      {'param_name': 'dtype',\n",
       "       'param_type': 'Type name or dict of column -> type, optional',\n",
       "       'param_desc': 'Data type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32,\\n‘c’: ‘Int64’}\\nUse str or object together with suitable na_values settings\\nto preserve and not interpret dtype.\\nIf converters are specified, they will be applied INSTEAD\\nof dtype conversion.\\n\\nNew in version 1.5.0.\\n\\n'},\n",
       "      {'param_name': 'converters',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Dict of functions for converting values in certain columns. Keys can either\\nbe integers or column labels.\\n\\nNew in version 1.5.0.\\n\\n'},\n",
       "      {'param_name': 'parse_dates',\n",
       "       'param_type': 'bool or list of int or names or list of lists or dict, default False',\n",
       "       'param_desc': 'Identifiers to parse index or columns to datetime. The behavior is as follows:\\n\\nboolean. If True -> try parsing the index.\\nlist of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\\neach as a separate date column.\\nlist of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as\\na single date column.\\ndict, e.g. {‘foo’ : [1, 3]} -> parse columns 1, 3 as date and call\\nresult ‘foo’\\n\\n\\nNew in version 1.5.0.\\n\\n'},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'str, optional, default ‘utf-8’',\n",
       "       'param_desc': 'Encoding of XML document.\\n'},\n",
       "      {'param_name': 'parser',\n",
       "       'param_type': '{‘lxml’,’etree’}, default ‘lxml’',\n",
       "       'param_desc': 'Parser module to use for retrieval of data. Only ‘lxml’ and\\n‘etree’ are supported. With ‘lxml’ more complex XPath searches\\nand ability to use XSLT stylesheet are supported.\\n'},\n",
       "      {'param_name': 'stylesheet',\n",
       "       'param_type': 'str, path object or file-like object',\n",
       "       'param_desc': 'A URL, file-like object, or a raw string containing an XSLT script.\\nThis stylesheet should flatten complex, deeply nested XML documents\\nfor easier parsing. To use this feature you must have lxml module\\ninstalled and specify ‘lxml’ as parser. The xpath must\\nreference nodes of transformed XML document generated after XSLT\\ntransformation and not the original XML document. Only XSLT 1.0\\nscripts and not later versions is currently supported.\\n'},\n",
       "      {'param_name': 'iterparse',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'The nodes or attributes to retrieve in iterparsing of XML document\\nas a dict with key being the name of repeating element and value being\\nlist of elements or attribute names that are descendants of the repeated\\nelement. Note: If this option is used, it will replace xpath parsing\\nand unlike xpath, descendants do not need to relate to each other but can\\nexist any where in document under the repeating element. This memory-\\nefficient method should be used for very large XML files (500MB, 1GB, or 5GB+).\\nFor example,\\niterparse = {\"row_element\": [\"child_elem\", \"attr\", \"grandchild_elem\"]}\\n\\n\\n\\nNew in version 1.5.0.\\n\\n'},\n",
       "      {'param_name': 'compression',\n",
       "       'param_type': 'str or dict, default ‘infer’',\n",
       "       'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘path_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_xml.html#pandas.read_xml'},\n",
       "    {'function_name': 'pandas.DataFrame.to_xml',\n",
       "     'full_function': \"DataFrame.to_xml(path_or_buffer=None, *, index=True, root_name='data', row_name='row', na_rep=None, attr_cols=None, elem_cols=None, namespaces=None, prefix=None, encoding='utf-8', xml_declaration=True, pretty_print=True, parser='lxml', stylesheet=None, compression='infer', storage_options=None)\",\n",
       "     'function_text': 'Render a DataFrame to an XML document.',\n",
       "     'parameter_names_desc': [{'param_name': 'path_or_buffer',\n",
       "       'param_type': 'str, path object, file-like object, or None, default None',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a write() function. If None, the result is returned\\nas a string.\\n'},\n",
       "      {'param_name': 'index',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Whether to include index in XML document.\\n'},\n",
       "      {'param_name': 'root_name',\n",
       "       'param_type': 'str, default ‘data’',\n",
       "       'param_desc': 'The name of root element in XML document.\\n'},\n",
       "      {'param_name': 'row_name',\n",
       "       'param_type': 'str, default ‘row’',\n",
       "       'param_desc': 'The name of row element in XML document.\\n'},\n",
       "      {'param_name': 'na_rep',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Missing data representation.\\n'},\n",
       "      {'param_name': 'attr_cols',\n",
       "       'param_type': 'list-like, optional',\n",
       "       'param_desc': 'List of columns to write as attributes in row element.\\nHierarchical columns will be flattened with underscore\\ndelimiting the different levels.\\n'},\n",
       "      {'param_name': 'elem_cols',\n",
       "       'param_type': 'list-like, optional',\n",
       "       'param_desc': 'List of columns to write as children in row element. By default,\\nall columns output as children of row element. Hierarchical\\ncolumns will be flattened with underscore delimiting the\\ndifferent levels.\\n'},\n",
       "      {'param_name': 'namespaces',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'All namespaces to be defined in root element. Keys of dict\\nshould be prefix names and values of dict corresponding URIs.\\nDefault namespaces should be given empty string key. For\\nexample,\\nnamespaces = {\"\": \"https://example.com\"}\\n\\n\\n'},\n",
       "      {'param_name': 'prefix',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Namespace prefix to be used for every element and/or attribute\\nin document. This should be one of the keys in namespaces\\ndict.\\n'},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'str, default ‘utf-8’',\n",
       "       'param_desc': 'Encoding of the resulting document.\\n'},\n",
       "      {'param_name': 'xml_declaration',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Whether to include the XML declaration at start of document.\\n'},\n",
       "      {'param_name': 'pretty_print',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Whether output should be pretty printed with indentation and\\nline breaks.\\n'},\n",
       "      {'param_name': 'parser',\n",
       "       'param_type': '{‘lxml’,’etree’}, default ‘lxml’',\n",
       "       'param_desc': 'Parser module to use for building of tree. Only ‘lxml’ and\\n‘etree’ are supported. With ‘lxml’, the ability to use XSLT\\nstylesheet is supported.\\n'},\n",
       "      {'param_name': 'stylesheet',\n",
       "       'param_type': 'str, path object or file-like object, optional',\n",
       "       'param_desc': 'A URL, file-like object, or a raw string containing an XSLT\\nscript used to transform the raw XML output. Script should use\\nlayout of elements and attributes from original output. This\\nargument requires lxml to be installed. Only XSLT 1.0\\nscripts and not later versions is currently supported.\\n'},\n",
       "      {'param_name': 'compression',\n",
       "       'param_type': 'str or dict, default ‘infer’',\n",
       "       'param_desc': \"For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nSet to None for no compression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for faster compression and to create\\na reproducible gzip archive:\\ncompression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_xml.html#pandas.DataFrame.to_xml'}]},\n",
       "  {'name': 'Latex',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#latex',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_latex.html#pandas.DataFrame.to_latex',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.to_latex.html#pandas.io.formats.style.Styler.to_latex'],\n",
       "   'function_definitions': [{'function_name': 'pandas.DataFrame.to_latex',\n",
       "     'full_function': \"DataFrame.to_latex(buf=None, *, columns=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, bold_rows=False, column_format=None, longtable=None, escape=None, encoding=None, decimal='.', multicolumn=None, multicolumn_format=None, multirow=None, caption=None, label=None, position=None)\",\n",
       "     'function_text': 'Render object to a LaTeX tabular, longtable, or nested table.',\n",
       "     'parameter_names_desc': [{'param_name': 'buf',\n",
       "       'param_type': 'str, Path or StringIO-like, optional, default None',\n",
       "       'param_desc': 'Buffer to write to. If None, the output is returned as a string.\\n'},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'list of label, optional',\n",
       "       'param_desc': 'The subset of columns to write. Writes all columns by default.\\n'},\n",
       "      {'param_name': 'header',\n",
       "       'param_type': 'bool or list of str, default True',\n",
       "       'param_desc': 'Write out the column names. If a list of strings is given,\\nit is assumed to be aliases for the column names.\\n'},\n",
       "      {'param_name': 'index',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Write row names (index).\\n'},\n",
       "      {'param_name': 'na_rep',\n",
       "       'param_type': 'str, default ‘NaN’',\n",
       "       'param_desc': 'Missing data representation.\\n'},\n",
       "      {'param_name': 'formatters',\n",
       "       'param_type': 'list of functions or dict of {{str: function}}, optional',\n",
       "       'param_desc': 'Formatter functions to apply to columns’ elements by position or\\nname. The result of each function must be a unicode string.\\nList must be of length equal to the number of columns.\\n'},\n",
       "      {'param_name': 'float_format',\n",
       "       'param_type': 'one-parameter function or str, optional, default None',\n",
       "       'param_desc': 'Formatter for floating point numbers. For example\\nfloat_format=\"%.2f\" and float_format=\"{{:0.2f}}\".format will\\nboth result in 0.1234 being formatted as 0.12.\\n'},\n",
       "      {'param_name': 'sparsify',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'Set to False for a DataFrame with a hierarchical index to print\\nevery multiindex key at each row. By default, the value will be\\nread from the config module.\\n'},\n",
       "      {'param_name': 'index_names',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Prints the names of the indexes.\\n'},\n",
       "      {'param_name': 'bold_rows',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Make the row labels bold in the output.\\n'},\n",
       "      {'param_name': 'column_format',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'The columns format as specified in LaTeX table format e.g. ‘rcl’ for 3\\ncolumns. By default, ‘l’ will be used for all columns except\\ncolumns of numbers, which default to ‘r’.\\n'},\n",
       "      {'param_name': 'longtable',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'Use a longtable environment instead of tabular. Requires\\nadding a usepackage{{longtable}} to your LaTeX preamble.\\nBy default, the value will be read from the pandas config\\nmodule, and set to True if the option styler.latex.environment is\\n“longtable”.\\n\\nChanged in version 2.0.0: The pandas option affecting this argument has changed.\\n\\n'},\n",
       "      {'param_name': 'escape',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'By default, the value will be read from the pandas config\\nmodule and set to True if the option styler.format.escape is\\n“latex”. When set to False prevents from escaping latex special\\ncharacters in column names.\\n\\nChanged in version 2.0.0: The pandas option affecting this argument has changed, as has the\\ndefault value to False.\\n\\n'},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'A string representing the encoding to use in the output file,\\ndefaults to ‘utf-8’.\\n'},\n",
       "      {'param_name': 'decimal',\n",
       "       'param_type': 'str, default ‘.’',\n",
       "       'param_desc': 'Character recognized as decimal separator, e.g. ‘,’ in Europe.\\n'},\n",
       "      {'param_name': 'multicolumn',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Use multicolumn to enhance MultiIndex columns.\\nThe default will be read from the config module, and is set\\nas the option styler.sparse.columns.\\n\\nChanged in version 2.0.0: The pandas option affecting this argument has changed.\\n\\n'},\n",
       "      {'param_name': 'multicolumn_format',\n",
       "       'param_type': 'str, default ‘r’',\n",
       "       'param_desc': 'The alignment for multicolumns, similar to column_format\\nThe default will be read from the config module, and is set as the option\\nstyler.latex.multicol_align.\\n\\nChanged in version 2.0.0: The pandas option affecting this argument has changed, as has the\\ndefault value to “r”.\\n\\n'},\n",
       "      {'param_name': 'multirow',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Use multirow to enhance MultiIndex rows. Requires adding a\\nusepackage{{multirow}} to your LaTeX preamble. Will print\\ncentered labels (instead of top-aligned) across the contained\\nrows, separating groups via clines. The default will be read\\nfrom the pandas config module, and is set as the option\\nstyler.sparse.index.\\n\\nChanged in version 2.0.0: The pandas option affecting this argument has changed, as has the\\ndefault value to True.\\n\\n'},\n",
       "      {'param_name': 'caption',\n",
       "       'param_type': 'str or tuple, optional',\n",
       "       'param_desc': 'Tuple (full_caption, short_caption),\\nwhich results in \\\\caption[short_caption]{{full_caption}};\\nif a single string is passed, no short caption will be set.\\n'},\n",
       "      {'param_name': 'label',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'The LaTeX label to be placed inside \\\\label{{}} in the output.\\nThis is used with \\\\ref{{}} in the main .tex file.\\n'},\n",
       "      {'param_name': 'position',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'The LaTeX positional argument for tables, to be placed after\\n\\\\begin{{}} in the output.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_latex.html#pandas.DataFrame.to_latex'},\n",
       "    {'function_name': 'pandas.io.formats.style.Styler.to_latex',\n",
       "     'full_function': 'Styler.to_latex(buf=None, *, column_format=None, position=None, position_float=None, hrules=None, clines=None, label=None, caption=None, sparse_index=None, sparse_columns=None, multirow_align=None, multicol_align=None, siunitx=False, environment=None, encoding=None, convert_css=False)',\n",
       "     'function_text': 'Write Styler to a file, buffer or string in LaTeX format.',\n",
       "     'parameter_names_desc': [{'param_name': 'buf',\n",
       "       'param_type': 'str, path object, file-like object, or None, default None',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a string write() function. If None, the result is\\nreturned as a string.\\n'},\n",
       "      {'param_name': 'column_format',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'The LaTeX column specification placed in location:\\n\\\\begin{tabular}{<column_format>}\\nDefaults to ‘l’ for index and\\nnon-numeric data columns, and, for numeric data columns,\\nto ‘r’ by default, or ‘S’ if siunitx is True.\\n'},\n",
       "      {'param_name': 'position',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'The LaTeX positional argument (e.g. ‘h!’) for tables, placed in location:\\n\\\\\\\\begin{table}[<position>].\\n'},\n",
       "      {'param_name': 'position_float',\n",
       "       'param_type': '{“centering”, “raggedleft”, “raggedright”}, optional',\n",
       "       'param_desc': 'The LaTeX float command placed in location:\\n\\\\begin{table}[<position>]\\n\\\\<position_float>\\nCannot be used if environment is “longtable”.\\n'},\n",
       "      {'param_name': 'hrules',\n",
       "       'param_type': 'bool',\n",
       "       'param_desc': 'Set to True to add \\\\toprule, \\\\midrule and \\\\bottomrule from the\\n{booktabs} LaTeX package.\\nDefaults to pandas.options.styler.latex.hrules, which is False.\\n\\nChanged in version 1.4.0.\\n\\n'},\n",
       "      {'param_name': 'clines',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Use to control adding \\\\cline commands for the index labels separation.\\nPossible values are:\\n\\n\\nNone: no cline commands are added (default).\\n“all;data”: a cline is added for every index value extending the\\nwidth of the table, including data entries.\\n“all;index”: as above with lines extending only the width of the\\nindex entries.\\n“skip-last;data”: a cline is added for each index value except the\\nlast level (which is never sparsified), extending the widtn of the\\ntable.\\n“skip-last;index”: as above with lines extending only the width of the\\nindex entries.\\n\\n\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "      {'param_name': 'label',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'The LaTeX label included as: \\\\label{<label>}.\\nThis is used with \\\\ref{<label>} in the main .tex file.\\n'},\n",
       "      {'param_name': 'caption',\n",
       "       'param_type': 'str, tuple, optional',\n",
       "       'param_desc': 'If string, the LaTeX table caption included as: \\\\caption{<caption>}.\\nIf tuple, i.e (“full caption”, “short caption”), the caption included\\nas: \\\\caption[<caption[1]>]{<caption[0]>}.\\n'},\n",
       "      {'param_name': 'sparse_index',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'Whether to sparsify the display of a hierarchical index. Setting to False\\nwill display each explicit level element in a hierarchical key for each row.\\nDefaults to pandas.options.styler.sparse.index, which is True.\\n'},\n",
       "      {'param_name': 'sparse_columns',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'Whether to sparsify the display of a hierarchical index. Setting to False\\nwill display each explicit level element in a hierarchical key for each\\ncolumn. Defaults to pandas.options.styler.sparse.columns, which\\nis True.\\n'},\n",
       "      {'param_name': 'multirow_align',\n",
       "       'param_type': '{“c”, “t”, “b”, “naive”}, optional',\n",
       "       'param_desc': 'If sparsifying hierarchical MultiIndexes whether to align text centrally,\\nat the top or bottom using the multirow package. If not given defaults to\\npandas.options.styler.latex.multirow_align, which is “c”.\\nIf “naive” is given renders without multirow.\\n\\nChanged in version 1.4.0.\\n\\n'},\n",
       "      {'param_name': 'multicol_align',\n",
       "       'param_type': '{“r”, “c”, “l”, “naive-l”, “naive-r”}, optional',\n",
       "       'param_desc': 'If sparsifying hierarchical MultiIndex columns whether to align text at\\nthe left, centrally, or at the right. If not given defaults to\\npandas.options.styler.latex.multicol_align, which is “r”.\\nIf a naive option is given renders without multicol.\\nPipe decorators can also be added to non-naive values to draw vertical\\nrules, e.g. “|r” will draw a rule on the left side of right aligned merged\\ncells.\\n\\nChanged in version 1.4.0.\\n\\n'},\n",
       "      {'param_name': 'siunitx',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Set to True to structure LaTeX compatible with the {siunitx} package.\\n'},\n",
       "      {'param_name': 'environment',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'If given, the environment that will replace ‘table’ in \\\\\\\\begin{table}.\\nIf ‘longtable’ is specified then a more suitable template is\\nrendered. If not given defaults to\\npandas.options.styler.latex.environment, which is None.\\n\\nNew in version 1.4.0.\\n\\n'},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Character encoding setting. Defaults\\nto pandas.options.styler.render.encoding, which is “utf-8”.\\n'},\n",
       "      {'param_name': 'convert_css',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Convert simple cell-styles from CSS to LaTeX format. Any CSS not found in\\nconversion table is dropped. A style can be forced by adding option\\n–latex. See notes.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.to_latex.html#pandas.io.formats.style.Styler.to_latex'}]},\n",
       "  {'name': 'HDFStore: PyTables (HDF5)',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#hdfstore-pytables-hdf5',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_hdf.html#pandas.read_hdf',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.append.html#pandas.HDFStore.append',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.select.html#pandas.HDFStore.select',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.keys.html#pandas.HDFStore.keys',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.walk.html#pandas.HDFStore.walk',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.put.html#pandas.HDFStore.put',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.get.html#pandas.HDFStore.get',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.info.html#pandas.HDFStore.info',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.groups.html#pandas.HDFStore.groups'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_hdf',\n",
       "     'full_function': \"pandas.read_hdf(path_or_buf, key=None, mode='r', errors='strict', where=None, start=None, stop=None, columns=None, iterator=False, chunksize=None, **kwargs)\",\n",
       "     'function_text': 'Read from the store, close it if we opened it.',\n",
       "     'parameter_names_desc': [{'param_name': 'path_or_buf',\n",
       "       'param_type': 'str, path object, pandas.HDFStore',\n",
       "       'param_desc': 'Any valid string path is acceptable. Only supports the local file system,\\nremote URLs and file-like objects are not supported.\\nIf you want to pass in a path object, pandas accepts any\\nos.PathLike.\\nAlternatively, pandas accepts an open pandas.HDFStore object.\\n'},\n",
       "      {'param_name': 'key',\n",
       "       'param_type': 'object, optional',\n",
       "       'param_desc': 'The group identifier in the store. Can be omitted if the HDF file\\ncontains a single pandas object.\\n'},\n",
       "      {'param_name': 'mode',\n",
       "       'param_type': '{‘r’, ‘r+’, ‘a’}, default ‘r’',\n",
       "       'param_desc': 'Mode to use when opening the file. Ignored if path_or_buf is a\\npandas.HDFStore. Default is ‘r’.\\n'},\n",
       "      {'param_name': 'errors',\n",
       "       'param_type': 'str, default ‘strict’',\n",
       "       'param_desc': 'Specifies how encoding and decoding errors are to be handled.\\nSee the errors argument for open() for a full list\\nof options.\\n'},\n",
       "      {'param_name': 'where',\n",
       "       'param_type': 'list, optional',\n",
       "       'param_desc': 'A list of Term (or convertible) objects.\\n'},\n",
       "      {'param_name': 'start',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'Row number to start selection.\\n'},\n",
       "      {'param_name': 'stop',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'Row number to stop selection.\\n'},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'list, optional',\n",
       "       'param_desc': 'A list of columns names to return.\\n'},\n",
       "      {'param_name': 'iterator',\n",
       "       'param_type': 'bool, optional',\n",
       "       'param_desc': 'Return an iterator object.\\n'},\n",
       "      {'param_name': 'chunksize',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'Number of rows to include in an iteration when using an iterator.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_hdf.html#pandas.read_hdf'},\n",
       "    {'function_name': 'pandas.HDFStore.append',\n",
       "     'full_function': \"HDFStore.append(key, value, format=None, axes=None, index=True, append=True, complib=None, complevel=None, columns=None, min_itemsize=None, nan_rep=None, chunksize=None, expectedrows=None, dropna=None, data_columns=None, encoding=None, errors='strict')\",\n",
       "     'function_text': 'Append to Table in file.',\n",
       "     'parameter_names_desc': [{'param_name': 'key',\n",
       "       'param_type': 'str',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'value',\n",
       "       'param_type': '{Series, DataFrame}',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'format',\n",
       "       'param_type': '‘table’ is the default',\n",
       "       'param_desc': \"Format to use when storing object in HDFStore. Value can be one of:\\n\\n'table'Table format. Write as a PyTables Table structure which may perform\\nworse but allow more flexible operations like searching / selecting\\nsubsets of the data.\\n\\n\\n\"},\n",
       "      {'param_name': 'index',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Write DataFrame index as a column.\\n'},\n",
       "      {'param_name': 'append',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Append the input data to the existing.\\n'},\n",
       "      {'param_name': 'data_columns',\n",
       "       'param_type': 'list of columns, or True, default None',\n",
       "       'param_desc': 'List of columns to create as indexed data columns for on-disk\\nqueries, or True to use all columns. By default only the axes\\nof the object are indexed. See here.\\n'},\n",
       "      {'param_name': 'min_itemsize',\n",
       "       'param_type': 'dict of columns that specify minimum str sizes',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'nan_rep',\n",
       "       'param_type': 'str to use as str nan representation',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'chunksize',\n",
       "       'param_type': 'size to chunk the writing',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'expectedrows',\n",
       "       'param_type': 'expected TOTAL row size of this table',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'default None, provide an encoding for str',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'dropna',\n",
       "       'param_type': 'bool, default False, optional',\n",
       "       'param_desc': 'Do not write an ALL nan row to the store settable\\nby the option ‘io.hdf.dropna_table’.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.append.html#pandas.HDFStore.append'},\n",
       "    {'function_name': 'pandas.HDFStore.select',\n",
       "     'full_function': 'HDFStore.select(key, where=None, start=None, stop=None, columns=None, iterator=False, chunksize=None, auto_close=False)',\n",
       "     'function_text': 'Retrieve pandas object stored in file, optionally based on where criteria.',\n",
       "     'parameter_names_desc': [{'param_name': 'key',\n",
       "       'param_type': 'str',\n",
       "       'param_desc': 'Object being retrieved from file.\\n'},\n",
       "      {'param_name': 'where',\n",
       "       'param_type': 'list or None',\n",
       "       'param_desc': 'List of Term (or convertible) objects, optional.\\n'},\n",
       "      {'param_name': 'start',\n",
       "       'param_type': 'int or None',\n",
       "       'param_desc': 'Row number to start selection.\\n'},\n",
       "      {'param_name': 'stop',\n",
       "       'param_type': 'int, default None',\n",
       "       'param_desc': 'Row number to stop selection.\\n'},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'list or None',\n",
       "       'param_desc': 'A list of columns that if not None, will limit the return columns.\\n'},\n",
       "      {'param_name': 'iterator',\n",
       "       'param_type': 'bool or False',\n",
       "       'param_desc': 'Returns an iterator.\\n'},\n",
       "      {'param_name': 'chunksize',\n",
       "       'param_type': 'int or None',\n",
       "       'param_desc': 'Number or rows to include in iteration, return an iterator.\\n'},\n",
       "      {'param_name': 'auto_close',\n",
       "       'param_type': 'bool or False',\n",
       "       'param_desc': 'Should automatically close the store when finished.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.select.html#pandas.HDFStore.select'},\n",
       "    {'function_name': 'pandas.HDFStore.keys',\n",
       "     'full_function': \"HDFStore.keys(include='pandas')\",\n",
       "     'function_text': 'Return a list of keys corresponding to objects stored in HDFStore.',\n",
       "     'parameter_names_desc': [],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.keys.html#pandas.HDFStore.keys'},\n",
       "    {'function_name': 'pandas.HDFStore.walk',\n",
       "     'full_function': \"HDFStore.walk(where='/')\",\n",
       "     'function_text': 'Walk the pytables group hierarchy for pandas objects.',\n",
       "     'parameter_names_desc': [{'param_name': 'where',\n",
       "       'param_type': 'str, default “/”',\n",
       "       'param_desc': 'Group where to start walking.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.walk.html#pandas.HDFStore.walk'},\n",
       "    {'function_name': 'pandas.HDFStore.put',\n",
       "     'full_function': \"HDFStore.put(key, value, format=None, index=True, append=False, complib=None, complevel=None, min_itemsize=None, nan_rep=None, data_columns=None, encoding=None, errors='strict', track_times=True, dropna=False)\",\n",
       "     'function_text': 'Store object in HDFStore.',\n",
       "     'parameter_names_desc': [{'param_name': 'key',\n",
       "       'param_type': 'str',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'value',\n",
       "       'param_type': '{Series, DataFrame}',\n",
       "       'param_desc': ''},\n",
       "      {'param_name': 'format',\n",
       "       'param_type': '‘fixed(f)|table(t)’, default is ‘fixed’',\n",
       "       'param_desc': \"Format to use when storing object in HDFStore. Value can be one of:\\n\\n'fixed'Fixed format. Fast writing/reading. Not-appendable, nor searchable.\\n\\n'table'Table format. Write as a PyTables Table structure which may perform\\nworse but allow more flexible operations like searching / selecting\\nsubsets of the data.\\n\\n\\n\"},\n",
       "      {'param_name': 'index',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Write DataFrame index as a column.\\n'},\n",
       "      {'param_name': 'append',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'This will force Table format, append the input data to the existing.\\n'},\n",
       "      {'param_name': 'data_columns',\n",
       "       'param_type': 'list of columns or True, default None',\n",
       "       'param_desc': 'List of columns to create as data columns, or True to use all columns.\\nSee here.\\n'},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'Provide an encoding for strings.\\n'},\n",
       "      {'param_name': 'track_times',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Parameter is propagated to ‘create_table’ method of ‘PyTables’.\\nIf set to False it enables to have the same h5 files (same hashes)\\nindependent on creation time.\\n'},\n",
       "      {'param_name': 'dropna',\n",
       "       'param_type': 'bool, default False, optional',\n",
       "       'param_desc': 'Remove missing values.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.put.html#pandas.HDFStore.put'},\n",
       "    {'function_name': 'pandas.HDFStore.get',\n",
       "     'full_function': 'HDFStore.get(key)',\n",
       "     'function_text': 'Retrieve pandas object stored in file.',\n",
       "     'parameter_names_desc': [{'param_name': 'key',\n",
       "       'param_type': 'str',\n",
       "       'param_desc': ''}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.get.html#pandas.HDFStore.get'},\n",
       "    {'function_name': 'pandas.HDFStore.info',\n",
       "     'full_function': 'HDFStore.info()',\n",
       "     'function_text': 'Print detailed information on the store.',\n",
       "     'parameter_names_desc': [],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.info.html#pandas.HDFStore.info'},\n",
       "    {'function_name': 'pandas.HDFStore.groups',\n",
       "     'full_function': 'HDFStore.groups()',\n",
       "     'function_text': 'Return a list of all the top-level nodes.',\n",
       "     'parameter_names_desc': [],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.HDFStore.groups.html#pandas.HDFStore.groups'}]},\n",
       "  {'name': 'Feather',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#feather',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_feather.html#pandas.read_feather',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_feather.html#pandas.DataFrame.to_feather'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_feather',\n",
       "     'full_function': 'pandas.read_feather(path, columns=None, use_threads=True, storage_options=None, dtype_backend=_NoDefault.no_default)',\n",
       "     'function_text': 'Load a feather-format object from the file path.',\n",
       "     'parameter_names_desc': [{'param_name': 'path',\n",
       "       'param_type': 'str, path object, or file-like object',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary read() function. The string could be a URL.\\nValid URL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be: file://localhost/path/to/table.feather.\\n'},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'sequence, default None',\n",
       "       'param_desc': 'If not provided, all columns are read.\\n'},\n",
       "      {'param_name': 'use_threads',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Whether to parallelize reading using multiple threads.\\n'},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_feather.html#pandas.read_feather'},\n",
       "    {'function_name': 'pandas.DataFrame.to_feather',\n",
       "     'full_function': 'DataFrame.to_feather(path, **kwargs)',\n",
       "     'function_text': 'Write a DataFrame to the binary Feather format.',\n",
       "     'parameter_names_desc': [{'param_name': 'path',\n",
       "       'param_type': 'str, path object, file-like object',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary write() function. If a string or a path,\\nit will be used as Root Directory path when writing a partitioned dataset.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_feather.html#pandas.DataFrame.to_feather'}]},\n",
       "  {'name': 'Parquet',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#parquet',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html#pandas.read_parquet',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_parquet',\n",
       "     'full_function': \"pandas.read_parquet(path, engine='auto', columns=None, storage_options=None, use_nullable_dtypes=_NoDefault.no_default, dtype_backend=_NoDefault.no_default, filesystem=None, filters=None, **kwargs)\",\n",
       "     'function_text': 'Load a parquet object from the file path, returning a DataFrame.',\n",
       "     'parameter_names_desc': [{'param_name': 'path',\n",
       "       'param_type': 'str, path object or file-like object',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary read() function.\\nThe string could be a URL. Valid URL schemes include http, ftp, s3,\\ngs, and file. For file URLs, a host is expected. A local file could be:\\nfile://localhost/path/to/table.parquet.\\nA file URL can also be a path to a directory that contains multiple\\npartitioned parquet files. Both pyarrow and fastparquet support\\npaths to directories as well as file URLs. A directory path could be:\\nfile://localhost/path/to/tables or s3://bucket/partition_dir.\\n'},\n",
       "      {'param_name': 'engine',\n",
       "       'param_type': '{‘auto’, ‘pyarrow’, ‘fastparquet’}, default ‘auto’',\n",
       "       'param_desc': \"Parquet library to use. If ‘auto’, then the option\\nio.parquet.engine is used. The default io.parquet.engine\\nbehavior is to try ‘pyarrow’, falling back to ‘fastparquet’ if\\n‘pyarrow’ is unavailable.\\nWhen using the 'pyarrow' engine and no storage options are provided\\nand a filesystem is implemented by both pyarrow.fs and fsspec\\n(e.g. “s3://”), then the pyarrow.fs filesystem is attempted first.\\nUse the filesystem keyword with an instantiated fsspec filesystem\\nif you wish to use its implementation.\\n\"},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'list, default=None',\n",
       "       'param_desc': 'If not None, only these columns will be read from the file.\\n'},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n\\nNew in version 1.3.0.\\n\\n'},\n",
       "      {'param_name': 'use_nullable_dtypes',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'If True, use dtypes that use pd.NA as missing value indicator\\nfor the resulting DataFrame. (only applicable for the pyarrow\\nengine)\\nAs new dtypes are added that support pd.NA in the future, the\\noutput with this option will change to use those dtypes.\\nNote: this is an experimental option, and behaviour (e.g. additional\\nsupport dtypes) may change without notice.\\n\\nDeprecated since version 2.0.\\n\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "      {'param_name': 'filesystem',\n",
       "       'param_type': 'fsspec or pyarrow filesystem, default None',\n",
       "       'param_desc': 'Filesystem object to use when reading the parquet file. Only implemented\\nfor engine=\"pyarrow\".\\n\\nNew in version 2.1.0.\\n\\n'},\n",
       "      {'param_name': 'filters',\n",
       "       'param_type': 'List[Tuple] or List[List[Tuple]], default None',\n",
       "       'param_desc': 'To filter out data.\\nFilter syntax: [[(column, op, val), …],…]\\nwhere op is [==, =, >, >=, <, <=, !=, in, not in]\\nThe innermost tuples are transposed into a set of filters applied\\nthrough an AND operation.\\nThe outer list combines these sets of filters through an OR\\noperation.\\nA single list of tuples can also be used, meaning that no OR\\noperation between set of filters is to be conducted.\\nUsing this argument will NOT result in row-wise filtering of the final\\npartitions unless engine=\"pyarrow\" is also specified. For\\nother engines, filtering is only performed at the partition level, that is,\\nto prevent the loading of some row-groups and/or files.\\n\\nNew in version 2.1.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html#pandas.read_parquet'},\n",
       "    {'function_name': 'pandas.DataFrame.to_parquet',\n",
       "     'full_function': \"DataFrame.to_parquet(path=None, *, engine='auto', compression='snappy', index=None, partition_cols=None, storage_options=None, **kwargs)\",\n",
       "     'function_text': 'Write a DataFrame to the binary parquet format.',\n",
       "     'parameter_names_desc': [{'param_name': 'path',\n",
       "       'param_type': 'str, path object, file-like object, or None, default None',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary write() function. If None, the result is\\nreturned as bytes. If a string or path, it will be used as Root Directory\\npath when writing a partitioned dataset.\\n'},\n",
       "      {'param_name': 'engine',\n",
       "       'param_type': '{‘auto’, ‘pyarrow’, ‘fastparquet’}, default ‘auto’',\n",
       "       'param_desc': 'Parquet library to use. If ‘auto’, then the option\\nio.parquet.engine is used. The default io.parquet.engine\\nbehavior is to try ‘pyarrow’, falling back to ‘fastparquet’ if\\n‘pyarrow’ is unavailable.\\n'},\n",
       "      {'param_name': 'compression',\n",
       "       'param_type': 'str or None, default ‘snappy’',\n",
       "       'param_desc': 'Name of the compression to use. Use None for no compression.\\nSupported options: ‘snappy’, ‘gzip’, ‘brotli’, ‘lz4’, ‘zstd’.\\n'},\n",
       "      {'param_name': 'index',\n",
       "       'param_type': 'bool, default None',\n",
       "       'param_desc': 'If True, include the dataframe’s index(es) in the file output.\\nIf False, they will not be written to the file.\\nIf None, similar to True the dataframe’s index(es)\\nwill be saved. However, instead of being saved as values,\\nthe RangeIndex will be stored as a range in the metadata so it\\ndoesn’t require much space and is faster. Other indexes will\\nbe included as columns in the file output.\\n'},\n",
       "      {'param_name': 'partition_cols',\n",
       "       'param_type': 'list, optional, default None',\n",
       "       'param_desc': 'Column names by which to partition the dataset.\\nColumns are partitioned in the order they are given.\\nMust be None if path is not a string.\\n'},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet'}]},\n",
       "  {'name': 'ORC',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#orc',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_orc.html#pandas.read_orc',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_orc',\n",
       "     'full_function': 'pandas.read_orc(path, columns=None, dtype_backend=_NoDefault.no_default, filesystem=None, **kwargs)',\n",
       "     'function_text': 'Load an ORC object from the file path, returning a DataFrame.',\n",
       "     'parameter_names_desc': [{'param_name': 'path',\n",
       "       'param_type': 'str, path object, or file-like object',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary read() function. The string could be a URL.\\nValid URL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be:\\nfile://localhost/path/to/table.orc.\\n'},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'list, default None',\n",
       "       'param_desc': 'If not None, only these columns will be read from the file.\\nOutput always follows the ordering of the file and not the columns list.\\nThis mirrors the original behaviour of\\npyarrow.orc.ORCFile.read().\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "      {'param_name': 'filesystem',\n",
       "       'param_type': 'fsspec or pyarrow filesystem, default None',\n",
       "       'param_desc': 'Filesystem object to use when reading the parquet file.\\n\\nNew in version 2.1.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_orc.html#pandas.read_orc'},\n",
       "    {'function_name': 'pandas.DataFrame.to_orc',\n",
       "     'full_function': \"DataFrame.to_orc(path=None, *, engine='pyarrow', index=None, engine_kwargs=None)\",\n",
       "     'function_text': 'Write a DataFrame to the ORC format.',\n",
       "     'parameter_names_desc': [],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc'}]},\n",
       "  {'name': 'SAS',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#sas',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_sas.html#pandas.read_sas'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_sas',\n",
       "     'full_function': \"pandas.read_sas(filepath_or_buffer, *, format=None, index=None, encoding=None, chunksize=None, iterator=False, compression='infer')\",\n",
       "     'function_text': 'Read SAS files stored as either XPORT or SAS7BDAT format files.',\n",
       "     'parameter_names_desc': [{'param_name': 'filepath_or_buffer',\n",
       "       'param_type': 'str, path object, or file-like object',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary read() function. The string could be a URL.\\nValid URL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be:\\nfile://localhost/path/to/table.sas7bdat.\\n'},\n",
       "      {'param_name': 'format',\n",
       "       'param_type': 'str {‘xport’, ‘sas7bdat’} or None',\n",
       "       'param_desc': 'If None, file format is inferred from file extension. If ‘xport’ or\\n‘sas7bdat’, uses the corresponding format.\\n'},\n",
       "      {'param_name': 'index',\n",
       "       'param_type': 'identifier of index column, defaults to None',\n",
       "       'param_desc': 'Identifier of column that should be used as index of the DataFrame.\\n'},\n",
       "      {'param_name': 'encoding',\n",
       "       'param_type': 'str, default is None',\n",
       "       'param_desc': 'Encoding for text data. If None, text data are stored as raw bytes.\\n'},\n",
       "      {'param_name': 'chunksize',\n",
       "       'param_type': 'int',\n",
       "       'param_desc': 'Read file chunksize lines at a time, returns iterator.\\n'},\n",
       "      {'param_name': 'iterator',\n",
       "       'param_type': 'bool, defaults to False',\n",
       "       'param_desc': 'If True, returns an iterator for reading the file incrementally.\\n'},\n",
       "      {'param_name': 'compression',\n",
       "       'param_type': 'str or dict, default ‘infer’',\n",
       "       'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\"}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_sas.html#pandas.read_sas'}]},\n",
       "  {'name': 'SPSS',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#spss',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_spss.html#pandas.read_spss'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_spss',\n",
       "     'full_function': 'pandas.read_spss(path, usecols=None, convert_categoricals=True, dtype_backend=_NoDefault.no_default)',\n",
       "     'function_text': 'Load an SPSS file from the file path, returning a DataFrame.',\n",
       "     'parameter_names_desc': [{'param_name': 'path',\n",
       "       'param_type': 'str or Path',\n",
       "       'param_desc': 'File path.\\n'},\n",
       "      {'param_name': 'usecols',\n",
       "       'param_type': 'list-like, optional',\n",
       "       'param_desc': 'Return a subset of the columns. If None, return all columns.\\n'},\n",
       "      {'param_name': 'convert_categoricals',\n",
       "       'param_type': 'bool, default is True',\n",
       "       'param_desc': 'Convert categorical columns into pd.Categorical.\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_spss.html#pandas.read_spss'}]},\n",
       "  {'name': 'SQL',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#sql',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html#pandas.read_sql_table',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html#pandas.read_sql',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html#pandas.read_sql_query',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_sql_table',\n",
       "     'full_function': 'pandas.read_sql_table(table_name, con, schema=None, index_col=None, coerce_float=True, parse_dates=None, columns=None, chunksize=None, dtype_backend=_NoDefault.no_default)',\n",
       "     'function_text': 'Read SQL database table into a DataFrame.',\n",
       "     'parameter_names_desc': [{'param_name': 'table_name',\n",
       "       'param_type': 'str',\n",
       "       'param_desc': 'Name of SQL table in database.\\n'},\n",
       "      {'param_name': 'con',\n",
       "       'param_type': 'SQLAlchemy connectable or str',\n",
       "       'param_desc': 'A database URI could be provided as str.\\nSQLite DBAPI connection mode not supported.\\n'},\n",
       "      {'param_name': 'schema',\n",
       "       'param_type': 'str, default None',\n",
       "       'param_desc': 'Name of SQL schema in database to query (if database flavor\\nsupports this). Uses default schema if None (default).\\n'},\n",
       "      {'param_name': 'index_col',\n",
       "       'param_type': 'str or list of str, optional, default: None',\n",
       "       'param_desc': 'Column(s) to set as index(MultiIndex).\\n'},\n",
       "      {'param_name': 'coerce_float',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Attempts to convert values of non-string, non-numeric objects (like\\ndecimal.Decimal) to floating point. Can result in loss of Precision.\\n'},\n",
       "      {'param_name': 'parse_dates',\n",
       "       'param_type': 'list or dict, default None',\n",
       "       'param_desc': '\\nList of column names to parse as dates.\\nDict of {column_name: format string} where format string is\\nstrftime compatible in case of parsing string times or is one of\\n(D, s, ns, ms, us) in case of parsing integer timestamps.\\nDict of {column_name: arg dict}, where the arg dict corresponds\\nto the keyword arguments of pandas.to_datetime()\\nEspecially useful with databases without native Datetime support,\\nsuch as SQLite.\\n\\n'},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'list, default None',\n",
       "       'param_desc': 'List of column names to select from SQL table.\\n'},\n",
       "      {'param_name': 'chunksize',\n",
       "       'param_type': 'int, default None',\n",
       "       'param_desc': 'If specified, returns an iterator where chunksize is the number of\\nrows to include in each chunk.\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html#pandas.read_sql_table'},\n",
       "    {'function_name': 'pandas.read_sql',\n",
       "     'full_function': 'pandas.read_sql(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, columns=None, chunksize=None, dtype_backend=_NoDefault.no_default, dtype=None)',\n",
       "     'function_text': 'Read SQL query or database table into a DataFrame.',\n",
       "     'parameter_names_desc': [{'param_name': 'sql',\n",
       "       'param_type': 'str or SQLAlchemy Selectable (select or text object)',\n",
       "       'param_desc': 'SQL query to be executed or a table name.\\n'},\n",
       "      {'param_name': 'con',\n",
       "       'param_type': 'ADBC Connection, SQLAlchemy connectable, str, or sqlite3 connection',\n",
       "       'param_desc': 'ADBC provides high performance I/O with native type support, where available.\\nUsing SQLAlchemy makes it possible to use any DB supported by that\\nlibrary. If a DBAPI2 object, only sqlite3 is supported. The user is responsible\\nfor engine disposal and connection closure for the ADBC connection and\\nSQLAlchemy connectable; str connections are closed automatically. See\\nhere.\\n'},\n",
       "      {'param_name': 'index_col',\n",
       "       'param_type': 'str or list of str, optional, default: None',\n",
       "       'param_desc': 'Column(s) to set as index(MultiIndex).\\n'},\n",
       "      {'param_name': 'coerce_float',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Attempts to convert values of non-string, non-numeric objects (like\\ndecimal.Decimal) to floating point, useful for SQL result sets.\\n'},\n",
       "      {'param_name': 'params',\n",
       "       'param_type': 'list, tuple or dict, optional, default: None',\n",
       "       'param_desc': 'List of parameters to pass to execute method. The syntax used\\nto pass parameters is database driver dependent. Check your\\ndatabase driver documentation for which of the five syntax styles,\\ndescribed in PEP 249’s paramstyle, is supported.\\nEg. for psycopg2, uses %(name)s so use params={‘name’ : ‘value’}.\\n'},\n",
       "      {'param_name': 'parse_dates',\n",
       "       'param_type': 'list or dict, default: None',\n",
       "       'param_desc': '\\nList of column names to parse as dates.\\nDict of {column_name: format string} where format string is\\nstrftime compatible in case of parsing string times, or is one of\\n(D, s, ns, ms, us) in case of parsing integer timestamps.\\nDict of {column_name: arg dict}, where the arg dict corresponds\\nto the keyword arguments of pandas.to_datetime()\\nEspecially useful with databases without native Datetime support,\\nsuch as SQLite.\\n\\n'},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'list, default: None',\n",
       "       'param_desc': 'List of column names to select from SQL table (only used when reading\\na table).\\n'},\n",
       "      {'param_name': 'chunksize',\n",
       "       'param_type': 'int, default None',\n",
       "       'param_desc': 'If specified, return an iterator where chunksize is the\\nnumber of rows to include in each chunk.\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'},\n",
       "      {'param_name': 'dtype',\n",
       "       'param_type': 'Type name or dict of columns',\n",
       "       'param_desc': 'Data type for data or columns. E.g. np.float64 or\\n{‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’}.\\nThe argument is ignored if a table is passed instead of a query.\\n\\nNew in version 2.0.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html#pandas.read_sql'},\n",
       "    {'function_name': 'pandas.read_sql_query',\n",
       "     'full_function': 'pandas.read_sql_query(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, chunksize=None, dtype=None, dtype_backend=_NoDefault.no_default)',\n",
       "     'function_text': 'Read SQL query into a DataFrame.',\n",
       "     'parameter_names_desc': [{'param_name': 'sql',\n",
       "       'param_type': 'str SQL query or SQLAlchemy Selectable (select or text object)',\n",
       "       'param_desc': 'SQL query to be executed.\\n'},\n",
       "      {'param_name': 'con',\n",
       "       'param_type': 'SQLAlchemy connectable, str, or sqlite3 connection',\n",
       "       'param_desc': 'Using SQLAlchemy makes it possible to use any DB supported by that\\nlibrary. If a DBAPI2 object, only sqlite3 is supported.\\n'},\n",
       "      {'param_name': 'index_col',\n",
       "       'param_type': 'str or list of str, optional, default: None',\n",
       "       'param_desc': 'Column(s) to set as index(MultiIndex).\\n'},\n",
       "      {'param_name': 'coerce_float',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Attempts to convert values of non-string, non-numeric objects (like\\ndecimal.Decimal) to floating point. Useful for SQL result sets.\\n'},\n",
       "      {'param_name': 'params',\n",
       "       'param_type': 'list, tuple or mapping, optional, default: None',\n",
       "       'param_desc': 'List of parameters to pass to execute method. The syntax used\\nto pass parameters is database driver dependent. Check your\\ndatabase driver documentation for which of the five syntax styles,\\ndescribed in PEP 249’s paramstyle, is supported.\\nEg. for psycopg2, uses %(name)s so use params={‘name’ : ‘value’}.\\n'},\n",
       "      {'param_name': 'parse_dates',\n",
       "       'param_type': 'list or dict, default: None',\n",
       "       'param_desc': '\\nList of column names to parse as dates.\\nDict of {column_name: format string} where format string is\\nstrftime compatible in case of parsing string times, or is one of\\n(D, s, ns, ms, us) in case of parsing integer timestamps.\\nDict of {column_name: arg dict}, where the arg dict corresponds\\nto the keyword arguments of pandas.to_datetime()\\nEspecially useful with databases without native Datetime support,\\nsuch as SQLite.\\n\\n'},\n",
       "      {'param_name': 'chunksize',\n",
       "       'param_type': 'int, default None',\n",
       "       'param_desc': 'If specified, return an iterator where chunksize is the number of\\nrows to include in each chunk.\\n'},\n",
       "      {'param_name': 'dtype',\n",
       "       'param_type': 'Type name or dict of columns',\n",
       "       'param_desc': 'Data type for data or columns. E.g. np.float64 or\\n{‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’}.\\n\\nNew in version 1.3.0.\\n\\n'},\n",
       "      {'param_name': 'dtype_backend',\n",
       "       'param_type': '{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’',\n",
       "       'param_desc': 'Back-end data type applied to the resultant DataFrame\\n(still experimental). Behaviour is as follows:\\n\\n\"numpy_nullable\": returns nullable-dtype-backed DataFrame\\n(default).\\n\"pyarrow\": returns pyarrow-backed nullable ArrowDtype\\nDataFrame.\\n\\n\\nNew in version 2.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html#pandas.read_sql_query'},\n",
       "    {'function_name': 'pandas.DataFrame.to_sql',\n",
       "     'full_function': \"DataFrame.to_sql(name, con, *, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None, method=None)\",\n",
       "     'function_text': 'Write records stored in a DataFrame to a SQL database.',\n",
       "     'parameter_names_desc': [],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql'}]},\n",
       "  {'name': 'Google BigQuery',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#google-bigquery',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html#pandas.read_gbq'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_gbq',\n",
       "     'full_function': 'pandas.read_gbq(query, project_id=None, index_col=None, col_order=None, reauth=False, auth_local_webserver=True, dialect=None, location=None, configuration=None, credentials=None, use_bqstorage_api=None, max_results=None, progress_bar_type=None)',\n",
       "     'function_text': 'Load data from Google BigQuery.',\n",
       "     'parameter_names_desc': [{'param_name': 'query',\n",
       "       'param_type': 'str',\n",
       "       'param_desc': 'SQL-Like Query to return data values.\\n'},\n",
       "      {'param_name': 'project_id',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Google BigQuery Account project ID. Optional when available from\\nthe environment.\\n'},\n",
       "      {'param_name': 'index_col',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Name of result column to use for index in results DataFrame.\\n'},\n",
       "      {'param_name': 'col_order',\n",
       "       'param_type': 'list(str), optional',\n",
       "       'param_desc': 'List of BigQuery column names in the desired order for results\\nDataFrame.\\n'},\n",
       "      {'param_name': 'reauth',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Force Google BigQuery to re-authenticate the user. This is useful\\nif multiple accounts are used.\\n'},\n",
       "      {'param_name': 'auth_local_webserver',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Use the local webserver flow instead of the console flow\\nwhen getting user credentials.\\nNew in version 0.2.0 of pandas-gbq.\\n\\nChanged in version 1.5.0: Default value is changed to True. Google has deprecated the\\nauth_local_webserver = False “out of band” (copy-paste)\\nflow.\\n\\n'},\n",
       "      {'param_name': 'dialect',\n",
       "       'param_type': 'str, default ‘legacy’',\n",
       "       'param_desc': \"Note: The default value is changing to ‘standard’ in a future version.\\nSQL syntax dialect to use. Value can be one of:\\n\\n'legacy'Use BigQuery’s legacy SQL dialect. For more information see\\nBigQuery Legacy SQL Reference.\\n\\n'standard'Use BigQuery’s standard SQL, which is\\ncompliant with the SQL 2011 standard. For more information\\nsee BigQuery Standard SQL Reference.\\n\\n\\n\"},\n",
       "      {'param_name': 'location',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Location where the query job should run. See the BigQuery locations\\ndocumentation for a\\nlist of available locations. The location must match that of any\\ndatasets used in the query.\\nNew in version 0.5.0 of pandas-gbq.\\n'},\n",
       "      {'param_name': 'configuration',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Query config parameters for job processing.\\nFor example:\\n\\nconfiguration = {‘query’: {‘useQueryCache’: False}}\\n\\nFor more information see BigQuery REST API Reference.\\n'},\n",
       "      {'param_name': 'credentials',\n",
       "       'param_type': 'google.auth.credentials.Credentials, optional',\n",
       "       'param_desc': 'Credentials for accessing Google APIs. Use this parameter to override\\ndefault credentials, such as to use Compute Engine\\ngoogle.auth.compute_engine.Credentials or Service Account\\ngoogle.oauth2.service_account.Credentials directly.\\nNew in version 0.8.0 of pandas-gbq.\\n'},\n",
       "      {'param_name': 'use_bqstorage_api',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Use the BigQuery Storage API to\\ndownload query results quickly, but at an increased cost. To use this\\nAPI, first enable it in the Cloud Console.\\nYou must also have the bigquery.readsessions.create\\npermission on the project you are billing queries to.\\nThis feature requires version 0.10.0 or later of the pandas-gbq\\npackage. It also requires the google-cloud-bigquery-storage and\\nfastavro packages.\\n'},\n",
       "      {'param_name': 'max_results',\n",
       "       'param_type': 'int, optional',\n",
       "       'param_desc': 'If set, limit the maximum number of rows to fetch from the query\\nresults.\\n'},\n",
       "      {'param_name': 'progress_bar_type',\n",
       "       'param_type': 'Optional, str',\n",
       "       'param_desc': \"If set, use the tqdm library to\\ndisplay a progress bar while the data downloads. Install the\\ntqdm package to use this feature.\\nPossible values of progress_bar_type include:\\n\\nNoneNo progress bar.\\n\\n'tqdm'Use the tqdm.tqdm() function to print a progress bar\\nto sys.stderr.\\n\\n'tqdm_notebook'Use the tqdm.tqdm_notebook() function to display a\\nprogress bar as a Jupyter notebook widget.\\n\\n'tqdm_gui'Use the tqdm.tqdm_gui() function to display a\\nprogress bar as a graphical dialog box.\\n\\n\\n\"}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html#pandas.read_gbq'}]},\n",
       "  {'name': 'STATA',\n",
       "   'url': 'https://pandas.pydata.org/docs/reference/io.html#stata',\n",
       "   'function_urls': ['https://pandas.pydata.org/docs/reference/api/pandas.read_stata.html#pandas.read_stata',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.io.stata.StataReader.data_label.html#pandas.io.stata.StataReader.data_label',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.io.stata.StataReader.variable_labels.html#pandas.io.stata.StataReader.variable_labels',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_stata.html#pandas.DataFrame.to_stata',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.io.stata.StataReader.value_labels.html#pandas.io.stata.StataReader.value_labels',\n",
       "    'https://pandas.pydata.org/docs/reference/api/pandas.io.stata.StataWriter.write_file.html#pandas.io.stata.StataWriter.write_file'],\n",
       "   'function_definitions': [{'function_name': 'pandas.read_stata',\n",
       "     'full_function': \"pandas.read_stata(filepath_or_buffer, *, convert_dates=True, convert_categoricals=True, index_col=None, convert_missing=False, preserve_dtypes=True, columns=None, order_categoricals=True, chunksize=None, iterator=False, compression='infer', storage_options=None)\",\n",
       "     'function_text': 'Read Stata file into DataFrame.',\n",
       "     'parameter_names_desc': [{'param_name': 'filepath_or_buffer',\n",
       "       'param_type': 'str, path object or file-like object',\n",
       "       'param_desc': 'Any valid string path is acceptable. The string could be a URL. Valid\\nURL schemes include http, ftp, s3, and file. For file URLs, a host is\\nexpected. A local file could be: file://localhost/path/to/table.dta.\\nIf you want to pass in a path object, pandas accepts any os.PathLike.\\nBy file-like object, we refer to objects with a read() method,\\nsuch as a file handle (e.g. via builtin open function)\\nor StringIO.\\n'},\n",
       "      {'param_name': 'convert_dates',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Convert date variables to DataFrame time values.\\n'},\n",
       "      {'param_name': 'convert_categoricals',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Read value labels and convert columns to Categorical/Factor variables.\\n'},\n",
       "      {'param_name': 'index_col',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'Column to set as index.\\n'},\n",
       "      {'param_name': 'convert_missing',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Flag indicating whether to convert missing values to their Stata\\nrepresentations. If False, missing values are replaced with nan.\\nIf True, columns containing missing values are returned with\\nobject data types and missing values are represented by\\nStataMissingValue objects.\\n'},\n",
       "      {'param_name': 'preserve_dtypes',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Preserve Stata datatypes. If False, numeric data are upcast to pandas\\ndefault types for foreign data (float64 or int64).\\n'},\n",
       "      {'param_name': 'columns',\n",
       "       'param_type': 'list or None',\n",
       "       'param_desc': 'Columns to retain. Columns will be returned in the given order. None\\nreturns all columns.\\n'},\n",
       "      {'param_name': 'order_categoricals',\n",
       "       'param_type': 'bool, default True',\n",
       "       'param_desc': 'Flag indicating whether converted categorical data are ordered.\\n'},\n",
       "      {'param_name': 'chunksize',\n",
       "       'param_type': 'int, default None',\n",
       "       'param_desc': 'Return StataReader object for iterations, returns chunks with\\ngiven number of lines.\\n'},\n",
       "      {'param_name': 'iterator',\n",
       "       'param_type': 'bool, default False',\n",
       "       'param_desc': 'Return StataReader object.\\n'},\n",
       "      {'param_name': 'compression',\n",
       "       'param_type': 'str or dict, default ‘infer’',\n",
       "       'param_desc': \"For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nIf using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.\\nSet to None for no decompression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for Zstandard decompression using a\\ncustom compression dictionary:\\ncompression={'method': 'zstd', 'dict_data': my_compression_dict}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\"},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.read_stata.html#pandas.read_stata'},\n",
       "    {'function_name': 'pandas.io.stata.StataReader.data_label',\n",
       "     'full_function': 'property StataReader.data_label',\n",
       "     'function_text': 'Return data label of Stata file.',\n",
       "     'parameter_names_desc': [],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.io.stata.StataReader.data_label.html#pandas.io.stata.StataReader.data_label'},\n",
       "    {'function_name': 'pandas.io.stata.StataReader.variable_labels',\n",
       "     'full_function': 'StataReader.variable_labels()',\n",
       "     'function_text': 'Return a dict associating each variable name with corresponding label.',\n",
       "     'parameter_names_desc': [],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.io.stata.StataReader.variable_labels.html#pandas.io.stata.StataReader.variable_labels'},\n",
       "    {'function_name': 'pandas.DataFrame.to_stata',\n",
       "     'full_function': \"DataFrame.to_stata(path, *, convert_dates=None, write_index=True, byteorder=None, time_stamp=None, data_label=None, variable_labels=None, version=114, convert_strl=None, compression='infer', storage_options=None, value_labels=None)\",\n",
       "     'function_text': 'Export DataFrame object to Stata dta format.',\n",
       "     'parameter_names_desc': [{'param_name': 'path',\n",
       "       'param_type': 'str, path object, or buffer',\n",
       "       'param_desc': 'String, path object (implementing os.PathLike[str]), or file-like\\nobject implementing a binary write() function.\\n'},\n",
       "      {'param_name': 'convert_dates',\n",
       "       'param_type': 'dict',\n",
       "       'param_desc': 'Dictionary mapping columns containing datetime types to stata\\ninternal format to use when writing the dates. Options are ‘tc’,\\n‘td’, ‘tm’, ‘tw’, ‘th’, ‘tq’, ‘ty’. Column can be either an integer\\nor a name. Datetime columns that do not have a conversion type\\nspecified will be converted to ‘tc’. Raises NotImplementedError if\\na datetime column has timezone information.\\n'},\n",
       "      {'param_name': 'write_index',\n",
       "       'param_type': 'bool',\n",
       "       'param_desc': 'Write the index to Stata dataset.\\n'},\n",
       "      {'param_name': 'byteorder',\n",
       "       'param_type': 'str',\n",
       "       'param_desc': 'Can be “>”, “<”, “little”, or “big”. default is sys.byteorder.\\n'},\n",
       "      {'param_name': 'time_stamp',\n",
       "       'param_type': 'datetime',\n",
       "       'param_desc': 'A datetime to use as file creation date. Default is the current\\ntime.\\n'},\n",
       "      {'param_name': 'data_label',\n",
       "       'param_type': 'str, optional',\n",
       "       'param_desc': 'A label for the data set. Must be 80 characters or smaller.\\n'},\n",
       "      {'param_name': 'variable_labels',\n",
       "       'param_type': 'dict',\n",
       "       'param_desc': 'Dictionary containing columns as keys and variable labels as\\nvalues. Each label must be 80 characters or smaller.\\n'},\n",
       "      {'param_name': 'version',\n",
       "       'param_type': '{114, 117, 118, 119, None}, default 114',\n",
       "       'param_desc': 'Version to use in the output dta file. Set to None to let pandas\\ndecide between 118 or 119 formats depending on the number of\\ncolumns in the frame. Version 114 can be read by Stata 10 and\\nlater. Version 117 can be read by Stata 13 or later. Version 118\\nis supported in Stata 14 and later. Version 119 is supported in\\nStata 15 and later. Version 114 limits string variables to 244\\ncharacters or fewer while versions 117 and later allow strings\\nwith lengths up to 2,000,000 characters. Versions 118 and 119\\nsupport Unicode characters, and version 119 supports more than\\n32,767 variables.\\nVersion 119 should usually only be used when the number of\\nvariables exceeds the capacity of dta format 118. Exporting\\nsmaller datasets in format 119 may have unintended consequences,\\nand, as of November 2020, Stata SE cannot read version 119 files.\\n'},\n",
       "      {'param_name': 'convert_strl',\n",
       "       'param_type': 'list, optional',\n",
       "       'param_desc': 'List of column names to convert to string columns to Stata StrL\\nformat. Only available if version is 117. Storing strings in the\\nStrL format can produce smaller dta files if strings have more than\\n8 characters and values are repeated.\\n'},\n",
       "      {'param_name': 'compression',\n",
       "       'param_type': 'str or dict, default ‘infer’',\n",
       "       'param_desc': \"For on-the-fly compression of the output data. If ‘infer’ and ‘path’ is\\npath-like, then detect compression from the following extensions: ‘.gz’,\\n‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’\\n(otherwise no compression).\\nSet to None for no compression.\\nCan also be a dict with key 'method' set\\nto one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and\\nother key-value pairs are forwarded to\\nzipfile.ZipFile, gzip.GzipFile,\\nbz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or\\ntarfile.TarFile, respectively.\\nAs an example, the following could be passed for faster compression and to create\\na reproducible gzip archive:\\ncompression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.\\n\\nNew in version 1.5.0: Added support for .tar files.\\n\\n\\nChanged in version 1.4.0: Zstandard support.\\n\\n\"},\n",
       "      {'param_name': 'storage_options',\n",
       "       'param_type': 'dict, optional',\n",
       "       'param_desc': 'Extra options that make sense for a particular storage connection, e.g.\\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\\nare forwarded to urllib.request.Request as header options. For other\\nURLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are\\nforwarded to fsspec.open. Please see fsspec and urllib for more\\ndetails, and for more examples on storage options refer here.\\n'},\n",
       "      {'param_name': 'value_labels',\n",
       "       'param_type': 'dict of dicts',\n",
       "       'param_desc': 'Dictionary containing columns as keys and dictionaries of column value\\nto labels as values. Labels for a single variable must be 32,000\\ncharacters or smaller.\\n\\nNew in version 1.4.0.\\n\\n'}],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_stata.html#pandas.DataFrame.to_stata'},\n",
       "    {'function_name': 'pandas.io.stata.StataReader.value_labels',\n",
       "     'full_function': 'StataReader.value_labels()',\n",
       "     'function_text': 'Return a nested dict associating each variable name to its value and label.',\n",
       "     'parameter_names_desc': [],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.io.stata.StataReader.value_labels.html#pandas.io.stata.StataReader.value_labels'},\n",
       "    {'function_name': 'pandas.io.stata.StataWriter.write_file',\n",
       "     'full_function': 'StataWriter.write_file()',\n",
       "     'function_text': 'Export DataFrame object to Stata dta format.',\n",
       "     'parameter_names_desc': [],\n",
       "     'function_url': 'https://pandas.pydata.org/docs/reference/api/pandas.io.stata.StataWriter.write_file.html#pandas.io.stata.StataWriter.write_file'}]}],\n",
       " 'name': 'Input/output',\n",
       " 'url': 'https://pandas.pydata.org/docs/reference/io.html'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['io.html']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGENDA FOR TODAY \n",
    "1. Check the validity of the data\n",
    "2. Build openai functions\n",
    "3. Build the graph\n",
    "4. Build the Agent framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD THE OPENAI FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"{0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_text_to_req(s):\n",
    "    star_idx = s.find(\" *,\")\n",
    "    if star_idx == -1:\n",
    "        return []\n",
    "    req_str = s[s.find(\"(\")+1:s.find(\"*\")].strip()[:-1]\n",
    "    req_list = [i.strip() for i in req_str.split(\",\")]\n",
    "    return req_list\n",
    "\n",
    "for parent in data:\n",
    "    for sub_level in data[parent]['functions']:\n",
    "        for func in sub_level['function_definitions']:\n",
    "            func_name = \"#\".join(func['function_name'].split(\".\"))\n",
    "            function_calling = {\"name\":func_name,\"descriptions\":func['function_text']}\n",
    "            # \n",
    "            if func['parameter_names_desc']!=[]:\n",
    "                properties_dict = {}\n",
    "                for params in func['parameter_names_desc']:\n",
    "                    type = params['param_type']\n",
    "                    if \"int\" in type:\n",
    "                        type = \"integer\"\n",
    "                        \n",
    "                    elif (\n",
    "                        \"Union\" in type or \"list\" in type\n",
    "                    ) and \"str\" in type:\n",
    "                        type = \"string\"\n",
    "                        type_dict = {\"type\":type}\n",
    "                    elif \"str\" in type:\n",
    "                        type = \"string\"\n",
    "                        type_dict = {\"type\":type}\n",
    "                    elif \"bool\" in type:\n",
    "                        type = \"boolean\"\n",
    "                        type_dict = {\"type\":type}\n",
    "                    elif \"dict\" in type:\n",
    "                        type = \"dictionary\"\n",
    "                        type_dict = {\"type\":type}\n",
    "                    elif \"{\" in type and \"}\" in type and \"’\" in type and \"‘\" in type:\n",
    "                        list_params = type[type.find(\"{\")+1:type.find(\"}\")]\n",
    "                        list_params = list_params.replace(\"’\",\"\").replace(\"‘\",\"\").split(\",\")\n",
    "                        type_dict = {\"type\":\"string\",\"enum\":list_params}\n",
    "                    else:\n",
    "                        type_dict = {\"type\":type}\n",
    "                    type_dict.update({'description':params['param_desc']})\n",
    "                \n",
    "                    properties_dict.update({params['param_name']:type_dict})\n",
    "                \n",
    "                function_calling.update({\"parameters\":{\"type\":\"object\",\"properties\":properties_dict,\"required\":function_text_to_req(func['full_function'])}})\n",
    "            func.update({\"openai_function\":function_calling})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to pandas_function_openai.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "filename = \"pandas_function_openai.json\"\n",
    "\n",
    "# Serialize the dictionary to a JSON string\n",
    "json_data = json.dumps(data, ensure_ascii=False)\n",
    "\n",
    "# Write the JSON string to a file with UTF-8 encoding\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "\n",
    "print(\"Data has been successfully written to\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['io.html', 'general_functions.html', 'series.html', 'frame.html', 'arrays.html', 'indexing.html', 'offset_frequency.html', 'window.html', 'groupby.html', 'resampling.html', 'style.html', 'plotting.html', 'options.html', 'extensions.html', 'testing.html', 'missing_value.html'])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "pandas_graph = nx.DiGraph()\n",
    "\n",
    "parent_names = [(data[d]['name'],{\"url\":data[d]['url'],\"url\":data[d]['url'],\"type\":\"parent_node\"}) for d in data]\n",
    "pandas_graph.add_nodes_from(parent_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parent in data:\n",
    "    parent_name = data[parent]['name']\n",
    "    for sub_level in data[parent]['functions']:\n",
    "        for func in sub_level['function_definitions']:\n",
    "            func_name = func['function_name']\n",
    "            # parent_trail = parent.split(\".\")[0]\n",
    "            pandas_graph.add_nodes_from([(func_name,{'metadata':func,'trail':f\"{parent_name}\",\"type\":\"function_node\"})])\n",
    "            pandas_graph.add_edge(parent_name,func_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from networkx.drawing.nx_agraph import graphviz_layout\n",
    "# import matplotlib.pyplot as plt\n",
    "# import networkx as nx\n",
    "\n",
    "# pos = graphviz_layout(pandas_graph, prog=\"dot\")\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# nx.draw(pandas_graph, pos, with_labels=True, node_size=500, node_color=\"skyblue\", font_size=5, font_weight=\"bold\", arrows=True)\n",
    "# # plt.title(f\"{router.upper()}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView(('Input/output', 'General functions', 'Series', 'DataFrame', 'pandas arrays, scalars, and data types', 'Index objects', 'Date offsets', 'Window', 'GroupBy', 'Resampling', 'Style', 'Plotting', 'Options and settings', 'Extensions', 'Testing', 'Missing values', 'pandas.read_pickle', 'pandas.DataFrame.to_pickle', 'pandas.read_table', 'pandas.DataFrame.to_csv', 'pandas.read_csv', 'pandas.read_fwf', 'pandas.read_clipboard', 'pandas.DataFrame.to_clipboard', 'pandas.read_excel', 'pandas.ExcelFile', 'pandas.io.formats.style.Styler.to_excel', 'pandas.ExcelWriter', 'pandas.DataFrame.to_excel', 'pandas.ExcelFile.parse', 'pandas.read_json', 'pandas.DataFrame.to_json', 'pandas.io.json.build_table_schema', 'pandas.json_normalize', 'pandas.read_html', 'pandas.io.formats.style.Styler.to_html', 'pandas.DataFrame.to_html', 'pandas.read_xml', 'pandas.DataFrame.to_xml', 'pandas.DataFrame.to_latex', 'pandas.io.formats.style.Styler.to_latex', 'pandas.read_hdf', 'pandas.HDFStore.append', 'pandas.HDFStore.select', 'pandas.HDFStore.keys', 'pandas.HDFStore.walk', 'pandas.HDFStore.put', 'pandas.HDFStore.get', 'pandas.HDFStore.info', 'pandas.HDFStore.groups', 'pandas.read_feather', 'pandas.DataFrame.to_feather', 'pandas.read_parquet', 'pandas.DataFrame.to_parquet', 'pandas.read_orc', 'pandas.DataFrame.to_orc', 'pandas.read_sas', 'pandas.read_spss', 'pandas.read_sql_table', 'pandas.read_sql', 'pandas.read_sql_query', 'pandas.DataFrame.to_sql', 'pandas.read_gbq', 'pandas.read_stata', 'pandas.io.stata.StataReader.data_label', 'pandas.io.stata.StataReader.variable_labels', 'pandas.DataFrame.to_stata', 'pandas.io.stata.StataReader.value_labels', 'pandas.io.stata.StataWriter.write_file', 'pandas.melt', 'pandas.pivot_table', 'pandas.cut', 'pandas.merge', 'pandas.merge_asof', 'pandas.get_dummies', 'pandas.factorize', 'pandas.lreshape', 'pandas.pivot', 'pandas.crosstab', 'pandas.qcut', 'pandas.merge_ordered', 'pandas.concat', 'pandas.from_dummies', 'pandas.unique', 'pandas.wide_to_long', 'pandas.isna', 'pandas.notna', 'pandas.isnull', 'pandas.notnull', 'pandas.to_numeric', 'pandas.to_datetime', 'pandas.date_range', 'pandas.period_range', 'pandas.infer_freq', 'pandas.to_timedelta', 'pandas.bdate_range', 'pandas.timedelta_range', 'pandas.interval_range', 'pandas.eval', 'pandas.tseries.api.guess_datetime_format', 'pandas.util.hash_array', 'pandas.util.hash_pandas_object', 'pandas.api.interchange.from_dataframe', 'pandas.Series', 'pandas.Series.index', 'pandas.Series.values', 'pandas.Series.shape', 'pandas.Series.ndim', 'pandas.Series.T', 'pandas.Series.hasnans', 'pandas.Series.dtypes', 'pandas.Series.flags', 'pandas.Series.array', 'pandas.Series.dtype', 'pandas.Series.nbytes', 'pandas.Series.size', 'pandas.Series.memory_usage', 'pandas.Series.empty', 'pandas.Series.name', 'pandas.Series.set_flags', 'pandas.Series.astype', 'pandas.Series.infer_objects', 'pandas.Series.bool', 'pandas.Series.to_period', 'pandas.Series.to_list', 'pandas.Series.convert_dtypes', 'pandas.Series.copy', 'pandas.Series.to_numpy', 'pandas.Series.to_timestamp', 'pandas.Series.__array__', 'pandas.Series.get', 'pandas.Series.iat', 'pandas.Series.iloc', 'pandas.Series.items', 'pandas.Series.pop', 'pandas.Series.xs', 'pandas.Series.at', 'pandas.Series.loc', 'pandas.Series.__iter__', 'pandas.Series.keys', 'pandas.Series.item', 'pandas.Series.add', 'pandas.Series.mul', 'pandas.Series.truediv', 'pandas.Series.mod', 'pandas.Series.radd', 'pandas.Series.rmul', 'pandas.Series.rtruediv', 'pandas.Series.rmod', 'pandas.Series.combine', 'pandas.Series.round', 'pandas.Series.gt', 'pandas.Series.ge', 'pandas.Series.eq', 'pandas.Series.dot', 'pandas.Series.sub', 'pandas.Series.div', 'pandas.Series.floordiv', 'pandas.Series.pow', 'pandas.Series.rsub', 'pandas.Series.rdiv', 'pandas.Series.rfloordiv', 'pandas.Series.rpow', 'pandas.Series.combine_first', 'pandas.Series.lt', 'pandas.Series.le', 'pandas.Series.ne', 'pandas.Series.product', 'pandas.Series.apply', 'pandas.Series.aggregate', 'pandas.Series.map', 'pandas.Series.rolling', 'pandas.Series.ewm', 'pandas.Series.agg', 'pandas.Series.transform', 'pandas.Series.groupby', 'pandas.Series.expanding', 'pandas.Series.pipe', 'pandas.Series.abs', 'pandas.Series.any', 'pandas.Series.between', 'pandas.Series.corr', 'pandas.Series.cov', 'pandas.Series.cummin', 'pandas.Series.cumsum', 'pandas.Series.diff', 'pandas.Series.kurt', 'pandas.Series.mean', 'pandas.Series.min', 'pandas.Series.nlargest', 'pandas.Series.pct_change', 'pandas.Series.quantile', 'pandas.Series.sem', 'pandas.Series.std', 'pandas.Series.var', 'pandas.Series.unique', 'pandas.Series.is_unique', 'pandas.Series.is_monotonic_decreasing', 'pandas.Series.all', 'pandas.Series.autocorr', 'pandas.Series.clip', 'pandas.Series.count', 'pandas.Series.cummax', 'pandas.Series.cumprod', 'pandas.Series.describe', 'pandas.Series.factorize', 'pandas.Series.max', 'pandas.Series.median', 'pandas.Series.mode', 'pandas.Series.nsmallest', 'pandas.Series.prod', 'pandas.Series.rank', 'pandas.Series.skew', 'pandas.Series.sum', 'pandas.Series.kurtosis', 'pandas.Series.nunique', 'pandas.Series.is_monotonic_increasing', 'pandas.Series.value_counts', 'pandas.Series.align', 'pandas.Series.drop', 'pandas.Series.drop_duplicates', 'pandas.Series.equals', 'pandas.Series.head', 'pandas.Series.idxmin', 'pandas.Series.last', 'pandas.Series.reindex_like', 'pandas.Series.rename_axis', 'pandas.Series.sample', 'pandas.Series.take', 'pandas.Series.truncate', 'pandas.Series.mask', 'pandas.Series.add_suffix', 'pandas.Series.case_when', 'pandas.Series.droplevel', 'pandas.Series.duplicated', 'pandas.Series.first', 'pandas.Series.idxmax', 'pandas.Series.isin', 'pandas.Series.reindex', 'pandas.Series.rename', 'pandas.Series.reset_index', 'pandas.Series.set_axis', 'pandas.Series.tail', 'pandas.Series.where', 'pandas.Series.add_prefix', 'pandas.Series.filter', 'pandas.Series.backfill', 'pandas.Series.dropna', 'pandas.Series.fillna', 'pandas.Series.isna', 'pandas.Series.notna', 'pandas.Series.pad', 'pandas.Series.bfill', 'pandas.Series.ffill', 'pandas.Series.interpolate', 'pandas.Series.isnull', 'pandas.Series.notnull', 'pandas.Series.replace', 'pandas.Series.argsort', 'pandas.Series.argmax', 'pandas.Series.sort_values', 'pandas.Series.swaplevel', 'pandas.Series.explode', 'pandas.Series.ravel', 'pandas.Series.squeeze', 'pandas.Series.argmin', 'pandas.Series.reorder_levels', 'pandas.Series.sort_index', 'pandas.Series.unstack', 'pandas.Series.searchsorted', 'pandas.Series.repeat', 'pandas.Series.view', 'pandas.Series.compare', 'pandas.Series.update', 'pandas.Series.asfreq', 'pandas.Series.shift', 'pandas.Series.last_valid_index', 'pandas.Series.tz_convert', 'pandas.Series.at_time', 'pandas.Series.asof', 'pandas.Series.first_valid_index', 'pandas.Series.resample', 'pandas.Series.tz_localize', 'pandas.Series.between_time', 'pandas.Series.str', 'pandas.Series.dt', 'pandas.DataFrame.sparse', 'pandas.Series.dt.date', 'pandas.Series.dt.timetz', 'pandas.Series.dt.month', 'pandas.Series.dt.hour', 'pandas.Series.dt.second', 'pandas.Series.dt.nanosecond', 'pandas.Series.dt.day_of_week', 'pandas.Series.dt.dayofyear', 'pandas.Series.dt.days_in_month', 'pandas.Series.dt.is_month_start', 'pandas.Series.dt.is_quarter_start', 'pandas.Series.dt.is_year_start', 'pandas.Series.dt.is_leap_year', 'pandas.Series.dt.isocalendar', 'pandas.Series.dt.to_pydatetime', 'pandas.Series.dt.tz_convert', 'pandas.Series.dt.strftime', 'pandas.Series.dt.floor', 'pandas.Series.dt.month_name', 'pandas.Series.dt.end_time', 'pandas.Series.dt.days', 'pandas.Series.dt.microseconds', 'pandas.Series.dt.components', 'pandas.Series.dt.to_pytimedelta', 'pandas.Series.str.capitalize', 'pandas.Series.str.cat', 'pandas.Series.str.contains', 'pandas.Series.str.decode', 'pandas.Series.str.endswith', 'pandas.Series.str.extractall', 'pandas.Series.str.findall', 'pandas.Series.str.get', 'pandas.Series.str.join', 'pandas.Series.str.ljust', 'pandas.Series.str.lstrip', 'pandas.Series.str.normalize', 'pandas.Series.str.partition', 'pandas.Series.str.removesuffix', 'pandas.Series.str.replace', 'pandas.Series.str.rindex', 'pandas.Series.str.rpartition', 'pandas.Series.str.slice', 'pandas.Series.str.split', 'pandas.Series.str.startswith', 'pandas.Series.str.swapcase', 'pandas.Series.str.translate', 'pandas.Series.str.wrap', 'pandas.Series.str.isalnum', 'pandas.Series.str.isdigit', 'pandas.Series.str.islower', 'pandas.Series.str.istitle', 'pandas.Series.str.isdecimal', 'pandas.Series.cat.categories', 'pandas.Series.cat.codes', 'pandas.Series.cat.rename_categories', 'pandas.Series.cat.add_categories', 'pandas.Series.cat.remove_unused_categories', 'pandas.Series.cat.as_ordered', 'pandas.Series.sparse.npoints', 'pandas.Series.sparse.fill_value', 'pandas.Series.sparse.from_coo', 'pandas.Series.list.flatten', 'pandas.Series.list.__getitem__', 'pandas.Series.struct.dtypes', 'pandas.Series.struct.field', 'pandas.Flags', 'pandas.Series.attrs', 'pandas.Series.cat', 'pandas.Series.sparse', 'pandas.Index.str', 'pandas.Series.dt.time', 'pandas.Series.dt.year', 'pandas.Series.dt.day', 'pandas.Series.dt.minute', 'pandas.Series.dt.microsecond', 'pandas.Series.dt.dayofweek', 'pandas.Series.dt.weekday', 'pandas.Series.dt.day_of_year', 'pandas.Series.dt.quarter', 'pandas.Series.dt.is_month_end', 'pandas.Series.dt.is_quarter_end', 'pandas.Series.dt.is_year_end', 'pandas.Series.dt.daysinmonth', 'pandas.Series.dt.tz', 'pandas.Series.dt.to_period', 'pandas.Series.dt.tz_localize', 'pandas.Series.dt.normalize', 'pandas.Series.dt.round', 'pandas.Series.dt.ceil', 'pandas.Series.dt.day_name', 'pandas.Series.dt.start_time', 'pandas.Series.dt.seconds', 'pandas.Series.dt.nanoseconds', 'pandas.Series.dt.total_seconds', 'pandas.Series.str.casefold', 'pandas.Series.str.center', 'pandas.Series.str.count', 'pandas.Series.str.encode', 'pandas.Series.str.extract', 'pandas.Series.str.find', 'pandas.Series.str.fullmatch', 'pandas.Series.str.index', 'pandas.Series.str.len', 'pandas.Series.str.lower', 'pandas.Series.str.match', 'pandas.Series.str.pad', 'pandas.Series.str.removeprefix', 'pandas.Series.str.repeat', 'pandas.Series.str.rfind', 'pandas.Series.str.rjust', 'pandas.Series.str.rstrip', 'pandas.Series.str.slice_replace', 'pandas.Series.str.rsplit', 'pandas.Series.str.strip', 'pandas.Series.str.title', 'pandas.Series.str.upper', 'pandas.Series.str.zfill', 'pandas.Series.str.isalpha', 'pandas.Series.str.isspace', 'pandas.Series.str.isupper', 'pandas.Series.str.isnumeric', 'pandas.Series.str.get_dummies', 'pandas.Series.cat.ordered', 'pandas.Series.cat.reorder_categories', 'pandas.Series.cat.remove_categories', 'pandas.Series.cat.set_categories', 'pandas.Series.cat.as_unordered', 'pandas.Series.sparse.density', 'pandas.Series.sparse.sp_values', 'pandas.Series.sparse.to_coo', 'pandas.Series.list.len', 'pandas.Series.struct.explode', 'pandas.Series.plot', 'pandas.Series.plot.area', 'pandas.Series.plot.barh', 'pandas.Series.plot.density', 'pandas.Series.plot.kde', 'pandas.Series.plot.pie', 'pandas.Series.hist', 'pandas.Series.plot.bar', 'pandas.Series.plot.box', 'pandas.Series.plot.hist', 'pandas.Series.plot.line', 'pandas.Series.to_pickle', 'pandas.Series.to_dict', 'pandas.Series.to_frame', 'pandas.Series.to_hdf', 'pandas.Series.to_json', 'pandas.Series.to_clipboard', 'pandas.Series.to_markdown', 'pandas.Series.to_csv', 'pandas.Series.to_excel', 'pandas.Series.to_xarray', 'pandas.Series.to_sql', 'pandas.Series.to_string', 'pandas.Series.to_latex', 'pandas.DataFrame', 'pandas.DataFrame.index', 'pandas.DataFrame.dtypes', 'pandas.DataFrame.select_dtypes', 'pandas.DataFrame.axes', 'pandas.DataFrame.size', 'pandas.DataFrame.memory_usage', 'pandas.DataFrame.set_flags', 'pandas.DataFrame.columns', 'pandas.DataFrame.info', 'pandas.DataFrame.values', 'pandas.DataFrame.ndim', 'pandas.DataFrame.shape', 'pandas.DataFrame.empty', 'pandas.DataFrame.astype', 'pandas.DataFrame.infer_objects', 'pandas.DataFrame.bool', 'pandas.DataFrame.convert_dtypes', 'pandas.DataFrame.copy', 'pandas.DataFrame.to_numpy', 'pandas.DataFrame.head', 'pandas.DataFrame.iat', 'pandas.DataFrame.iloc', 'pandas.DataFrame.__iter__', 'pandas.DataFrame.keys', 'pandas.DataFrame.itertuples', 'pandas.DataFrame.tail', 'pandas.DataFrame.get', 'pandas.DataFrame.where', 'pandas.DataFrame.query', 'pandas.DataFrame.at', 'pandas.DataFrame.loc', 'pandas.DataFrame.insert', 'pandas.DataFrame.items', 'pandas.DataFrame.iterrows', 'pandas.DataFrame.pop', 'pandas.DataFrame.xs', 'pandas.DataFrame.isin', 'pandas.DataFrame.mask', 'pandas.DataFrame.__add__', 'pandas.DataFrame.sub', 'pandas.DataFrame.div', 'pandas.DataFrame.floordiv', 'pandas.DataFrame.pow', 'pandas.DataFrame.radd', 'pandas.DataFrame.rmul', 'pandas.DataFrame.rtruediv', 'pandas.DataFrame.rmod', 'pandas.DataFrame.lt', 'pandas.DataFrame.le', 'pandas.DataFrame.ne', 'pandas.DataFrame.combine', 'pandas.DataFrame.add', 'pandas.DataFrame.mul', 'pandas.DataFrame.truediv', 'pandas.DataFrame.mod', 'pandas.DataFrame.dot', 'pandas.DataFrame.rsub', 'pandas.DataFrame.rdiv', 'pandas.DataFrame.rfloordiv', 'pandas.DataFrame.rpow', 'pandas.DataFrame.gt', 'pandas.DataFrame.ge', 'pandas.DataFrame.eq', 'pandas.DataFrame.combine_first', 'pandas.DataFrame.apply', 'pandas.DataFrame.applymap', 'pandas.DataFrame.agg', 'pandas.DataFrame.transform', 'pandas.DataFrame.rolling', 'pandas.DataFrame.ewm', 'pandas.DataFrame.map', 'pandas.DataFrame.pipe', 'pandas.DataFrame.aggregate', 'pandas.DataFrame.groupby', 'pandas.DataFrame.expanding', 'pandas.DataFrame.abs', 'pandas.DataFrame.any', 'pandas.DataFrame.corr', 'pandas.DataFrame.count', 'pandas.DataFrame.cummax', 'pandas.DataFrame.cumprod', 'pandas.DataFrame.describe', 'pandas.DataFrame.eval', 'pandas.DataFrame.kurtosis', 'pandas.DataFrame.mean', 'pandas.DataFrame.min', 'pandas.DataFrame.pct_change', 'pandas.DataFrame.product', 'pandas.DataFrame.rank', 'pandas.DataFrame.sem', 'pandas.DataFrame.sum', 'pandas.DataFrame.var', 'pandas.DataFrame.value_counts', 'pandas.DataFrame.all', 'pandas.DataFrame.clip', 'pandas.DataFrame.corrwith', 'pandas.DataFrame.cov', 'pandas.DataFrame.cummin', 'pandas.DataFrame.cumsum', 'pandas.DataFrame.diff', 'pandas.DataFrame.kurt', 'pandas.DataFrame.max', 'pandas.DataFrame.median', 'pandas.DataFrame.mode', 'pandas.DataFrame.prod', 'pandas.DataFrame.quantile', 'pandas.DataFrame.round', 'pandas.DataFrame.skew', 'pandas.DataFrame.std', 'pandas.DataFrame.nunique', 'pandas.DataFrame.add_prefix', 'pandas.DataFrame.align', 'pandas.DataFrame.between_time', 'pandas.DataFrame.drop_duplicates', 'pandas.DataFrame.equals', 'pandas.DataFrame.first', 'pandas.DataFrame.idxmax', 'pandas.DataFrame.last', 'pandas.DataFrame.reindex_like', 'pandas.DataFrame.rename_axis', 'pandas.DataFrame.sample', 'pandas.DataFrame.set_index', 'pandas.DataFrame.take', 'pandas.DataFrame.add_suffix', 'pandas.DataFrame.at_time', 'pandas.DataFrame.drop', 'pandas.DataFrame.duplicated', 'pandas.DataFrame.filter', 'pandas.DataFrame.idxmin', 'pandas.DataFrame.reindex', 'pandas.DataFrame.rename', 'pandas.DataFrame.reset_index', 'pandas.DataFrame.set_axis', 'pandas.DataFrame.truncate', 'pandas.DataFrame.backfill', 'pandas.DataFrame.dropna', 'pandas.DataFrame.fillna', 'pandas.DataFrame.isna', 'pandas.DataFrame.notna', 'pandas.DataFrame.pad', 'pandas.DataFrame.bfill', 'pandas.DataFrame.ffill', 'pandas.DataFrame.interpolate', 'pandas.DataFrame.isnull', 'pandas.DataFrame.notnull', 'pandas.DataFrame.replace', 'pandas.DataFrame.droplevel', 'pandas.DataFrame.pivot_table', 'pandas.DataFrame.sort_values', 'pandas.DataFrame.nlargest', 'pandas.DataFrame.swaplevel', 'pandas.DataFrame.unstack', 'pandas.DataFrame.melt', 'pandas.DataFrame.squeeze', 'pandas.DataFrame.T', 'pandas.DataFrame.pivot', 'pandas.DataFrame.reorder_levels', 'pandas.DataFrame.sort_index', 'pandas.DataFrame.nsmallest', 'pandas.DataFrame.stack', 'pandas.DataFrame.swapaxes', 'pandas.DataFrame.explode', 'pandas.DataFrame.to_xarray', 'pandas.DataFrame.transpose', 'pandas.DataFrame.assign', 'pandas.DataFrame.join', 'pandas.DataFrame.update', 'pandas.DataFrame.compare', 'pandas.DataFrame.merge', 'pandas.DataFrame.asfreq', 'pandas.DataFrame.shift', 'pandas.DataFrame.last_valid_index', 'pandas.DataFrame.to_period', 'pandas.DataFrame.tz_convert', 'pandas.DataFrame.asof', 'pandas.DataFrame.first_valid_index', 'pandas.DataFrame.resample', 'pandas.DataFrame.to_timestamp', 'pandas.DataFrame.tz_localize', 'pandas.DataFrame.attrs', 'pandas.DataFrame.plot', 'pandas.DataFrame.plot.area', 'pandas.DataFrame.plot.barh', 'pandas.DataFrame.plot.density', 'pandas.DataFrame.plot.hist', 'pandas.DataFrame.plot.line', 'pandas.DataFrame.plot.scatter', 'pandas.DataFrame.boxplot', 'pandas.DataFrame.plot.bar', 'pandas.DataFrame.plot.box', 'pandas.DataFrame.plot.hexbin', 'pandas.DataFrame.plot.kde', 'pandas.DataFrame.plot.pie', 'pandas.DataFrame.hist', 'pandas.DataFrame.sparse.density', 'pandas.DataFrame.sparse.from_spmatrix', 'pandas.DataFrame.sparse.to_dense', 'pandas.DataFrame.sparse.to_coo', 'pandas.DataFrame.from_dict', 'pandas.DataFrame.to_hdf', 'pandas.DataFrame.to_dict', 'pandas.DataFrame.to_records', 'pandas.DataFrame.style', 'pandas.DataFrame.from_records', 'pandas.DataFrame.to_gbq', 'pandas.DataFrame.to_string', 'pandas.DataFrame.to_markdown', 'pandas.DataFrame.__dataframe__', 'pandas.Timedelta', 'pandas.IntervalDtype', 'pandas.Float64Dtype', 'pandas.SparseDtype', 'pandas.BooleanDtype', 'pandas.array', 'pandas.arrays.ArrowExtensionArray', 'pandas.ArrowDtype', 'pandas.Timestamp', 'pandas.Timestamp.asm8', 'pandas.Timestamp.dayofweek', 'pandas.Timestamp.dayofyear', 'pandas.Timestamp.days_in_month', 'pandas.Timestamp.is_leap_year', 'pandas.Timestamp.is_month_start', 'pandas.Timestamp.is_quarter_start', 'pandas.Timestamp.is_year_start', 'pandas.Timestamp.tz', 'pandas.Timestamp.unit', 'pandas.Timestamp.week', 'pandas.Timestamp.as_unit', 'pandas.Timestamp.ceil', 'pandas.Timestamp.ctime', 'pandas.Timestamp.day_name', 'pandas.Timestamp.floor', 'pandas.Timestamp.fromtimestamp', 'pandas.Timestamp.isoformat', 'pandas.Timestamp.month_name', 'pandas.Timestamp.now', 'pandas.Timestamp.round', 'pandas.Timestamp.strptime', 'pandas.Timestamp.timestamp', 'pandas.Timestamp.timetz', 'pandas.Timestamp.to_numpy', 'pandas.Timestamp.to_period', 'pandas.Timestamp.today', 'pandas.Timestamp.tz_convert', 'pandas.Timestamp.tzname', 'pandas.Timestamp.utcnow', 'pandas.Timestamp.utctimetuple', 'pandas.arrays.DatetimeArray', 'pandas.DatetimeTZDtype', 'pandas.Timedelta.asm8', 'pandas.Timedelta.days', 'pandas.Timedelta.nanoseconds', 'pandas.Timedelta.seconds', 'pandas.Timedelta.as_unit', 'pandas.Timedelta.floor', 'pandas.Timedelta.round', 'pandas.Timedelta.to_timedelta64', 'pandas.Timedelta.total_seconds', 'pandas.arrays.TimedeltaArray', 'pandas.Period', 'pandas.Period.day', 'pandas.Period.day_of_week', 'pandas.Period.day_of_year', 'pandas.Period.daysinmonth', 'pandas.Period.hour', 'pandas.Period.minute', 'pandas.Period.qyear', 'pandas.Period.start_time', 'pandas.Period.weekday', 'pandas.Period.year', 'pandas.Period.asfreq', 'pandas.Period.strftime', 'pandas.arrays.PeriodArray', 'pandas.PeriodDtype', 'pandas.Interval', 'pandas.Interval.closed', 'pandas.Interval.closed_right', 'pandas.Interval.left', 'pandas.Interval.mid', 'pandas.Interval.open_right', 'pandas.Interval.right', 'pandas.arrays.IntervalArray', 'pandas.arrays.IntegerArray', 'pandas.Int8Dtype', 'pandas.Int32Dtype', 'pandas.UInt8Dtype', 'pandas.UInt32Dtype', 'pandas.arrays.FloatingArray', 'pandas.Float32Dtype', 'pandas.CategoricalDtype', 'pandas.CategoricalDtype.categories', 'pandas.Categorical', 'pandas.Categorical.from_codes', 'pandas.Categorical.dtype', 'pandas.Categorical.ordered', 'pandas.Categorical.__array__', 'pandas.arrays.SparseArray', 'pandas.arrays.StringArray', 'pandas.StringDtype', 'pandas.arrays.BooleanArray', 'pandas.Int64Dtype', 'pandas.Timestamp.day_of_week', 'pandas.Timestamp.day_of_year', 'pandas.Timestamp.daysinmonth', 'pandas.Timestamp.is_month_end', 'pandas.Timestamp.is_quarter_end', 'pandas.Timestamp.is_year_end', 'pandas.Timestamp.quarter', 'pandas.Timestamp.weekofyear', 'pandas.Timestamp.astimezone', 'pandas.Timestamp.combine', 'pandas.Timestamp.date', 'pandas.Timestamp.dst', 'pandas.Timestamp.fromordinal', 'pandas.Timestamp.isocalendar', 'pandas.Timestamp.isoweekday', 'pandas.Timestamp.normalize', 'pandas.Timestamp.replace', 'pandas.Timestamp.strftime', 'pandas.Timestamp.time', 'pandas.Timestamp.timetuple', 'pandas.Timestamp.to_datetime64', 'pandas.Timestamp.to_julian_date', 'pandas.Timestamp.to_pydatetime', 'pandas.Timestamp.toordinal', 'pandas.Timestamp.tz_localize', 'pandas.Timestamp.utcfromtimestamp', 'pandas.Timestamp.utcoffset', 'pandas.Timestamp.weekday', 'pandas.Timedelta.components', 'pandas.Timedelta.view', 'pandas.Timedelta.ceil', 'pandas.Timedelta.isoformat', 'pandas.Timedelta.to_pytimedelta', 'pandas.Timedelta.to_numpy', 'pandas.Period.dayofweek', 'pandas.Period.dayofyear', 'pandas.Period.days_in_month', 'pandas.Period.end_time', 'pandas.Period.freqstr', 'pandas.Period.is_leap_year', 'pandas.Period.month', 'pandas.Period.quarter', 'pandas.Period.second', 'pandas.Period.week', 'pandas.Period.weekofyear', 'pandas.Period.now', 'pandas.Period.to_timestamp', 'pandas.Interval.closed_left', 'pandas.Interval.is_empty', 'pandas.Interval.length', 'pandas.Interval.open_left', 'pandas.Interval.overlaps', 'pandas.Int16Dtype', 'pandas.UInt16Dtype', 'pandas.UInt64Dtype', 'pandas.CategoricalDtype.ordered', 'pandas.Categorical.categories', 'pandas.Categorical.codes', 'pandas.arrays.ArrowStringArray', 'pandas.api.types.union_categoricals', 'pandas.api.types.pandas_dtype', 'pandas.api.types.is_any_real_numeric_dtype', 'pandas.api.types.is_categorical_dtype', 'pandas.api.types.is_datetime64_any_dtype', 'pandas.api.types.is_datetime64_ns_dtype', 'pandas.api.types.is_extension_array_dtype', 'pandas.api.types.is_int64_dtype', 'pandas.api.types.is_interval_dtype', 'pandas.api.types.is_object_dtype', 'pandas.api.types.is_signed_integer_dtype', 'pandas.api.types.is_timedelta64_dtype', 'pandas.api.types.is_unsigned_integer_dtype', 'pandas.api.types.is_dict_like', 'pandas.api.types.is_list_like', 'pandas.api.types.is_iterator', 'pandas.api.types.is_bool', 'pandas.api.types.is_float', 'pandas.api.types.is_integer', 'pandas.api.types.is_number', 'pandas.api.types.is_re_compilable', 'pandas.api.types.infer_dtype', 'pandas.api.types.is_bool_dtype', 'pandas.api.types.is_complex_dtype', 'pandas.api.types.is_datetime64_dtype', 'pandas.api.types.is_datetime64tz_dtype', 'pandas.api.types.is_float_dtype', 'pandas.api.types.is_integer_dtype', 'pandas.api.types.is_numeric_dtype', 'pandas.api.types.is_period_dtype', 'pandas.api.types.is_string_dtype', 'pandas.api.types.is_timedelta64_ns_dtype', 'pandas.api.types.is_sparse', 'pandas.api.types.is_file_like', 'pandas.api.types.is_named_tuple', 'pandas.api.types.is_complex', 'pandas.api.types.is_hashable', 'pandas.api.types.is_re', 'pandas.api.types.is_scalar', 'pandas.Index', 'pandas.Index.values', 'pandas.Index.is_monotonic_decreasing', 'pandas.Index.has_duplicates', 'pandas.Index.dtype', 'pandas.Index.shape', 'pandas.Index.ndim', 'pandas.Index.memory_usage', 'pandas.Index.all', 'pandas.Index.argmin', 'pandas.Index.copy', 'pandas.Index.drop', 'pandas.Index.duplicated', 'pandas.Index.factorize', 'pandas.Index.insert', 'pandas.Index.is_boolean', 'pandas.Index.is_floating', 'pandas.Index.is_interval', 'pandas.Index.is_object', 'pandas.Index.max', 'pandas.Index.rename', 'pandas.Index.where', 'pandas.Index.putmask', 'pandas.Index.nunique', 'pandas.Index.set_names', 'pandas.Index.fillna', 'pandas.Index.isna', 'pandas.Index.astype', 'pandas.Index.map', 'pandas.Index.to_list', 'pandas.Index.to_frame', 'pandas.Index.argsort', 'pandas.Index.sort_values', 'pandas.Index.shift', 'pandas.Index.append', 'pandas.Index.intersection', 'pandas.Index.difference', 'pandas.Index.asof', 'pandas.Index.get_indexer', 'pandas.Index.get_indexer_non_unique', 'pandas.Index.get_loc', 'pandas.Index.isin', 'pandas.Index.slice_locs', 'pandas.Index.is_monotonic_increasing', 'pandas.Index.is_unique', 'pandas.Index.hasnans', 'pandas.Index.inferred_type', 'pandas.Index.name', 'pandas.Index.nbytes', 'pandas.Index.size', 'pandas.Index.T', 'pandas.Index.any', 'pandas.Index.argmax', 'pandas.Index.delete', 'pandas.Index.drop_duplicates', 'pandas.Index.equals', 'pandas.Index.identical', 'pandas.Index.is_', 'pandas.Index.is_categorical', 'pandas.Index.is_integer', 'pandas.Index.is_numeric', 'pandas.Index.min', 'pandas.Index.reindex', 'pandas.Index.repeat', 'pandas.Index.take', 'pandas.Index.unique', 'pandas.Index.value_counts', 'pandas.Index.droplevel', 'pandas.Index.dropna', 'pandas.Index.notna', 'pandas.Index.item', 'pandas.Index.ravel', 'pandas.Index.to_series', 'pandas.Index.searchsorted', 'pandas.Index.join', 'pandas.Index.union', 'pandas.Index.symmetric_difference', 'pandas.Index.asof_locs', 'pandas.Index.get_indexer_for', 'pandas.Index.get_level_values', 'pandas.Index.get_slice_bound', 'pandas.Index.slice_indexer', 'pandas.RangeIndex', 'pandas.RangeIndex.start', 'pandas.RangeIndex.step', 'pandas.RangeIndex.stop', 'pandas.RangeIndex.from_range', 'pandas.CategoricalIndex', 'pandas.CategoricalIndex.codes', 'pandas.CategoricalIndex.ordered', 'pandas.CategoricalIndex.reorder_categories', 'pandas.CategoricalIndex.remove_categories', 'pandas.CategoricalIndex.set_categories', 'pandas.CategoricalIndex.as_unordered', 'pandas.CategoricalIndex.map', 'pandas.CategoricalIndex.categories', 'pandas.CategoricalIndex.rename_categories', 'pandas.CategoricalIndex.add_categories', 'pandas.CategoricalIndex.remove_unused_categories', 'pandas.CategoricalIndex.as_ordered', 'pandas.CategoricalIndex.equals', 'pandas.IntervalIndex', 'pandas.IntervalIndex.from_arrays', 'pandas.IntervalIndex.from_breaks', 'pandas.IntervalIndex.closed', 'pandas.IntervalIndex.values', 'pandas.IntervalIndex.is_non_overlapping_monotonic', 'pandas.IntervalIndex.get_loc', 'pandas.IntervalIndex.set_closed', 'pandas.IntervalIndex.overlaps', 'pandas.IntervalIndex.from_tuples', 'pandas.IntervalIndex.is_empty', 'pandas.IntervalIndex.is_overlapping', 'pandas.IntervalIndex.get_indexer', 'pandas.IntervalIndex.contains', 'pandas.IntervalIndex.to_tuples', 'pandas.MultiIndex', 'pandas.MultiIndex.from_arrays', 'pandas.MultiIndex.from_product', 'pandas.MultiIndex.names', 'pandas.MultiIndex.levshape', 'pandas.MultiIndex.set_levels', 'pandas.MultiIndex.to_flat_index', 'pandas.MultiIndex.sortlevel', 'pandas.MultiIndex.swaplevel', 'pandas.MultiIndex.remove_unused_levels', 'pandas.MultiIndex.copy', 'pandas.MultiIndex.truncate', 'pandas.MultiIndex.get_loc', 'pandas.MultiIndex.get_loc_level', 'pandas.MultiIndex.get_level_values', 'pandas.IndexSlice', 'pandas.MultiIndex.from_tuples', 'pandas.MultiIndex.from_frame', 'pandas.MultiIndex.levels', 'pandas.MultiIndex.nlevels', 'pandas.MultiIndex.dtypes', 'pandas.MultiIndex.set_codes', 'pandas.MultiIndex.to_frame', 'pandas.MultiIndex.droplevel', 'pandas.MultiIndex.reorder_levels', 'pandas.MultiIndex.drop', 'pandas.MultiIndex.append', 'pandas.MultiIndex.get_locs', 'pandas.MultiIndex.get_indexer', 'pandas.DatetimeIndex', 'pandas.DatetimeIndex.year', 'pandas.DatetimeIndex.day', 'pandas.DatetimeIndex.minute', 'pandas.DatetimeIndex.microsecond', 'pandas.DatetimeIndex.date', 'pandas.DatetimeIndex.timetz', 'pandas.DatetimeIndex.day_of_year', 'pandas.DatetimeIndex.day_of_week', 'pandas.DatetimeIndex.quarter', 'pandas.DatetimeIndex.is_month_start', 'pandas.DatetimeIndex.is_quarter_start', 'pandas.DatetimeIndex.is_year_start', 'pandas.DatetimeIndex.is_leap_year', 'pandas.DatetimeIndex.indexer_at_time', 'pandas.DatetimeIndex.normalize', 'pandas.DatetimeIndex.snap', 'pandas.DatetimeIndex.tz_localize', 'pandas.DatetimeIndex.floor', 'pandas.DatetimeIndex.month_name', 'pandas.DatetimeIndex.as_unit', 'pandas.DatetimeIndex.to_pydatetime', 'pandas.DatetimeIndex.to_frame', 'pandas.DatetimeIndex.mean', 'pandas.DatetimeIndex.month', 'pandas.DatetimeIndex.hour', 'pandas.DatetimeIndex.second', 'pandas.DatetimeIndex.nanosecond', 'pandas.DatetimeIndex.time', 'pandas.DatetimeIndex.dayofyear', 'pandas.DatetimeIndex.dayofweek', 'pandas.DatetimeIndex.weekday', 'pandas.DatetimeIndex.tz', 'pandas.DatetimeIndex.freqstr', 'pandas.DatetimeIndex.is_month_end', 'pandas.DatetimeIndex.is_quarter_end', 'pandas.DatetimeIndex.is_year_end', 'pandas.DatetimeIndex.inferred_freq', 'pandas.DatetimeIndex.indexer_between_time', 'pandas.DatetimeIndex.strftime', 'pandas.DatetimeIndex.tz_convert', 'pandas.DatetimeIndex.round', 'pandas.DatetimeIndex.ceil', 'pandas.DatetimeIndex.day_name', 'pandas.DatetimeIndex.to_period', 'pandas.DatetimeIndex.to_series', 'pandas.DatetimeIndex.std', 'pandas.TimedeltaIndex', 'pandas.TimedeltaIndex.days', 'pandas.TimedeltaIndex.microseconds', 'pandas.TimedeltaIndex.components', 'pandas.TimedeltaIndex.as_unit', 'pandas.TimedeltaIndex.to_series', 'pandas.TimedeltaIndex.floor', 'pandas.TimedeltaIndex.to_frame', 'pandas.TimedeltaIndex.mean', 'pandas.TimedeltaIndex.seconds', 'pandas.TimedeltaIndex.nanoseconds', 'pandas.TimedeltaIndex.inferred_freq', 'pandas.TimedeltaIndex.to_pytimedelta', 'pandas.TimedeltaIndex.round', 'pandas.TimedeltaIndex.ceil', 'pandas.PeriodIndex', 'pandas.PeriodIndex.day', 'pandas.PeriodIndex.day_of_week', 'pandas.PeriodIndex.day_of_year', 'pandas.PeriodIndex.daysinmonth', 'pandas.PeriodIndex.hour', 'pandas.PeriodIndex.minute', 'pandas.PeriodIndex.quarter', 'pandas.PeriodIndex.second', 'pandas.PeriodIndex.week', 'pandas.PeriodIndex.weekofyear', 'pandas.PeriodIndex.asfreq', 'pandas.PeriodIndex.to_timestamp', 'pandas.PeriodIndex.dayofweek', 'pandas.PeriodIndex.dayofyear', 'pandas.PeriodIndex.days_in_month', 'pandas.PeriodIndex.end_time', 'pandas.PeriodIndex.freqstr', 'pandas.PeriodIndex.is_leap_year', 'pandas.PeriodIndex.month', 'pandas.PeriodIndex.start_time', 'pandas.PeriodIndex.weekday', 'pandas.PeriodIndex.year', 'pandas.PeriodIndex.strftime', 'pandas.tseries.offsets.DateOffset', 'pandas.tseries.offsets.DateOffset.freqstr', 'pandas.tseries.offsets.DateOffset.name', 'pandas.tseries.offsets.DateOffset.is_month_end', 'pandas.tseries.offsets.DateOffset.copy', 'pandas.tseries.offsets.DateOffset.is_on_offset', 'pandas.tseries.offsets.DateOffset.is_quarter_end', 'pandas.tseries.offsets.DateOffset.is_year_end', 'pandas.tseries.offsets.DateOffset.kwds', 'pandas.tseries.offsets.DateOffset.is_month_start', 'pandas.tseries.offsets.DateOffset.is_anchored', 'pandas.tseries.offsets.DateOffset.is_quarter_start', 'pandas.tseries.offsets.DateOffset.is_year_start', 'pandas.tseries.offsets.BusinessDay', 'pandas.tseries.offsets.BDay', 'pandas.tseries.offsets.BusinessDay.freqstr', 'pandas.tseries.offsets.BusinessDay.name', 'pandas.tseries.offsets.BusinessDay.copy', 'pandas.tseries.offsets.BusinessDay.is_on_offset', 'pandas.tseries.offsets.BusinessDay.is_month_end', 'pandas.tseries.offsets.BusinessDay.is_quarter_end', 'pandas.tseries.offsets.BusinessDay.is_year_end', 'pandas.tseries.offsets.BusinessDay.kwds', 'pandas.tseries.offsets.BusinessDay.is_anchored', 'pandas.tseries.offsets.BusinessDay.is_month_start', 'pandas.tseries.offsets.BusinessDay.is_quarter_start', 'pandas.tseries.offsets.BusinessDay.is_year_start', 'pandas.tseries.offsets.BusinessHour', 'pandas.tseries.offsets.BusinessHour.freqstr', 'pandas.tseries.offsets.BusinessHour.name', 'pandas.tseries.offsets.BusinessHour.copy', 'pandas.tseries.offsets.BusinessHour.is_on_offset', 'pandas.tseries.offsets.BusinessHour.is_month_end', 'pandas.tseries.offsets.BusinessHour.is_quarter_end', 'pandas.tseries.offsets.BusinessHour.is_year_end', 'pandas.tseries.offsets.BusinessHour.kwds', 'pandas.tseries.offsets.BusinessHour.is_anchored', 'pandas.tseries.offsets.BusinessHour.is_month_start', 'pandas.tseries.offsets.BusinessHour.is_quarter_start', 'pandas.tseries.offsets.BusinessHour.is_year_start', 'pandas.tseries.offsets.CustomBusinessDay', 'pandas.tseries.offsets.CDay', 'pandas.tseries.offsets.CustomBusinessDay.freqstr', 'pandas.tseries.offsets.CustomBusinessDay.name', 'pandas.tseries.offsets.CustomBusinessDay.copy', 'pandas.tseries.offsets.CustomBusinessDay.is_on_offset', 'pandas.tseries.offsets.CustomBusinessDay.is_month_end', 'pandas.tseries.offsets.CustomBusinessDay.is_quarter_end', 'pandas.tseries.offsets.CustomBusinessDay.is_year_end', 'pandas.tseries.offsets.CustomBusinessDay.kwds', 'pandas.tseries.offsets.CustomBusinessDay.is_anchored', 'pandas.tseries.offsets.CustomBusinessDay.is_month_start', 'pandas.tseries.offsets.CustomBusinessDay.is_quarter_start', 'pandas.tseries.offsets.CustomBusinessDay.is_year_start', 'pandas.tseries.offsets.CustomBusinessHour', 'pandas.tseries.offsets.CustomBusinessHour.freqstr', 'pandas.tseries.offsets.CustomBusinessHour.name', 'pandas.tseries.offsets.CustomBusinessHour.copy', 'pandas.tseries.offsets.CustomBusinessHour.is_on_offset', 'pandas.tseries.offsets.CustomBusinessHour.is_month_end', 'pandas.tseries.offsets.CustomBusinessHour.is_quarter_end', 'pandas.tseries.offsets.CustomBusinessHour.is_year_end', 'pandas.tseries.offsets.CustomBusinessHour.kwds', 'pandas.tseries.offsets.CustomBusinessHour.is_anchored', 'pandas.tseries.offsets.CustomBusinessHour.is_month_start', 'pandas.tseries.offsets.CustomBusinessHour.is_quarter_start', 'pandas.tseries.offsets.CustomBusinessHour.is_year_start', 'pandas.tseries.offsets.MonthEnd', 'pandas.tseries.offsets.MonthEnd.freqstr', 'pandas.tseries.offsets.MonthEnd.name', 'pandas.tseries.offsets.MonthEnd.copy', 'pandas.tseries.offsets.MonthEnd.is_on_offset', 'pandas.tseries.offsets.MonthEnd.is_month_end', 'pandas.tseries.offsets.MonthEnd.is_quarter_end', 'pandas.tseries.offsets.MonthEnd.is_year_end', 'pandas.tseries.offsets.MonthEnd.kwds', 'pandas.tseries.offsets.MonthEnd.is_anchored', 'pandas.tseries.offsets.MonthEnd.is_month_start', 'pandas.tseries.offsets.MonthEnd.is_quarter_start', 'pandas.tseries.offsets.MonthEnd.is_year_start', 'pandas.tseries.offsets.MonthBegin', 'pandas.tseries.offsets.MonthBegin.freqstr', 'pandas.tseries.offsets.MonthBegin.name', 'pandas.tseries.offsets.MonthBegin.copy', 'pandas.tseries.offsets.MonthBegin.is_on_offset', 'pandas.tseries.offsets.MonthBegin.is_month_end', 'pandas.tseries.offsets.MonthBegin.is_quarter_end', 'pandas.tseries.offsets.MonthBegin.is_year_end', 'pandas.tseries.offsets.MonthBegin.kwds', 'pandas.tseries.offsets.MonthBegin.is_anchored', 'pandas.tseries.offsets.MonthBegin.is_month_start', 'pandas.tseries.offsets.MonthBegin.is_quarter_start', 'pandas.tseries.offsets.MonthBegin.is_year_start', 'pandas.tseries.offsets.BusinessMonthEnd', 'pandas.tseries.offsets.BMonthEnd', 'pandas.tseries.offsets.BusinessMonthEnd.freqstr', 'pandas.tseries.offsets.BusinessMonthEnd.name', 'pandas.tseries.offsets.BusinessMonthEnd.copy', 'pandas.tseries.offsets.BusinessMonthEnd.is_on_offset', 'pandas.tseries.offsets.BusinessMonthEnd.is_month_end', 'pandas.tseries.offsets.BusinessMonthEnd.is_quarter_end', 'pandas.tseries.offsets.BusinessMonthEnd.is_year_end', 'pandas.tseries.offsets.BusinessMonthEnd.kwds', 'pandas.tseries.offsets.BusinessMonthEnd.is_anchored', 'pandas.tseries.offsets.BusinessMonthEnd.is_month_start', 'pandas.tseries.offsets.BusinessMonthEnd.is_quarter_start', 'pandas.tseries.offsets.BusinessMonthEnd.is_year_start', 'pandas.tseries.offsets.BusinessMonthBegin', 'pandas.tseries.offsets.BMonthBegin', 'pandas.tseries.offsets.BusinessMonthBegin.freqstr', 'pandas.tseries.offsets.BusinessMonthBegin.name', 'pandas.tseries.offsets.BusinessMonthBegin.copy', 'pandas.tseries.offsets.BusinessMonthBegin.is_on_offset', 'pandas.tseries.offsets.BusinessMonthBegin.is_month_end', 'pandas.tseries.offsets.BusinessMonthBegin.is_quarter_end', 'pandas.tseries.offsets.BusinessMonthBegin.is_year_end', 'pandas.tseries.offsets.BusinessMonthBegin.kwds', 'pandas.tseries.offsets.BusinessMonthBegin.is_anchored', 'pandas.tseries.offsets.BusinessMonthBegin.is_month_start', 'pandas.tseries.offsets.BusinessMonthBegin.is_quarter_start', 'pandas.tseries.offsets.BusinessMonthBegin.is_year_start', 'pandas.tseries.offsets.CustomBusinessMonthEnd', 'pandas.tseries.offsets.CBMonthEnd', 'pandas.tseries.offsets.CustomBusinessMonthEnd.freqstr', 'pandas.tseries.offsets.CustomBusinessMonthEnd.copy', 'pandas.tseries.offsets.CustomBusinessMonthEnd.is_on_offset', 'pandas.tseries.offsets.CustomBusinessMonthEnd.is_month_end', 'pandas.tseries.offsets.CustomBusinessMonthEnd.is_quarter_end', 'pandas.tseries.offsets.CustomBusinessMonthEnd.is_year_end', 'pandas.tseries.offsets.CustomBusinessMonthEnd.kwds', 'pandas.tseries.offsets.CustomBusinessMonthEnd.name', 'pandas.tseries.offsets.CustomBusinessMonthEnd.is_anchored', 'pandas.tseries.offsets.CustomBusinessMonthEnd.is_month_start', 'pandas.tseries.offsets.CustomBusinessMonthEnd.is_quarter_start', 'pandas.tseries.offsets.CustomBusinessMonthEnd.is_year_start', 'pandas.tseries.offsets.CustomBusinessMonthBegin', 'pandas.tseries.offsets.CBMonthBegin', 'pandas.tseries.offsets.CustomBusinessMonthBegin.freqstr', 'pandas.tseries.offsets.CustomBusinessMonthBegin.copy', 'pandas.tseries.offsets.CustomBusinessMonthBegin.is_on_offset', 'pandas.tseries.offsets.CustomBusinessMonthBegin.is_month_end', 'pandas.tseries.offsets.CustomBusinessMonthBegin.is_quarter_end', 'pandas.tseries.offsets.CustomBusinessMonthBegin.is_year_end', 'pandas.tseries.offsets.CustomBusinessMonthBegin.kwds', 'pandas.tseries.offsets.CustomBusinessMonthBegin.name', 'pandas.tseries.offsets.CustomBusinessMonthBegin.is_anchored', 'pandas.tseries.offsets.CustomBusinessMonthBegin.is_month_start', 'pandas.tseries.offsets.CustomBusinessMonthBegin.is_quarter_start', 'pandas.tseries.offsets.CustomBusinessMonthBegin.is_year_start', 'pandas.tseries.offsets.SemiMonthEnd', 'pandas.tseries.offsets.SemiMonthEnd.freqstr', 'pandas.tseries.offsets.SemiMonthEnd.name', 'pandas.tseries.offsets.SemiMonthEnd.copy', 'pandas.tseries.offsets.SemiMonthEnd.is_on_offset', 'pandas.tseries.offsets.SemiMonthEnd.is_month_end', 'pandas.tseries.offsets.SemiMonthEnd.is_quarter_end', 'pandas.tseries.offsets.SemiMonthEnd.is_year_end', 'pandas.tseries.offsets.SemiMonthEnd.kwds', 'pandas.tseries.offsets.SemiMonthEnd.is_anchored', 'pandas.tseries.offsets.SemiMonthEnd.is_month_start', 'pandas.tseries.offsets.SemiMonthEnd.is_quarter_start', 'pandas.tseries.offsets.SemiMonthEnd.is_year_start', 'pandas.tseries.offsets.SemiMonthBegin', 'pandas.tseries.offsets.SemiMonthBegin.freqstr', 'pandas.tseries.offsets.SemiMonthBegin.name', 'pandas.tseries.offsets.SemiMonthBegin.copy', 'pandas.tseries.offsets.SemiMonthBegin.is_on_offset', 'pandas.tseries.offsets.SemiMonthBegin.is_month_end', 'pandas.tseries.offsets.SemiMonthBegin.is_quarter_end', 'pandas.tseries.offsets.SemiMonthBegin.is_year_end', 'pandas.tseries.offsets.SemiMonthBegin.kwds', 'pandas.tseries.offsets.SemiMonthBegin.is_anchored', 'pandas.tseries.offsets.SemiMonthBegin.is_month_start', 'pandas.tseries.offsets.SemiMonthBegin.is_quarter_start', 'pandas.tseries.offsets.SemiMonthBegin.is_year_start', 'pandas.tseries.offsets.Week', 'pandas.tseries.offsets.Week.freqstr', 'pandas.tseries.offsets.Week.name', 'pandas.tseries.offsets.Week.copy', 'pandas.tseries.offsets.Week.is_on_offset', 'pandas.tseries.offsets.Week.is_month_end', 'pandas.tseries.offsets.Week.is_quarter_end', 'pandas.tseries.offsets.Week.is_year_end', 'pandas.tseries.offsets.Week.kwds', 'pandas.tseries.offsets.Week.is_anchored', 'pandas.tseries.offsets.Week.is_month_start', 'pandas.tseries.offsets.Week.is_quarter_start', 'pandas.tseries.offsets.Week.is_year_start', 'pandas.tseries.offsets.WeekOfMonth', 'pandas.tseries.offsets.WeekOfMonth.freqstr', 'pandas.tseries.offsets.WeekOfMonth.name', 'pandas.tseries.offsets.WeekOfMonth.copy', 'pandas.tseries.offsets.WeekOfMonth.is_on_offset', 'pandas.tseries.offsets.WeekOfMonth.is_month_start', 'pandas.tseries.offsets.WeekOfMonth.is_quarter_start', 'pandas.tseries.offsets.WeekOfMonth.is_year_start', 'pandas.tseries.offsets.WeekOfMonth.kwds', 'pandas.tseries.offsets.WeekOfMonth.is_anchored', 'pandas.tseries.offsets.WeekOfMonth.is_month_end', 'pandas.tseries.offsets.WeekOfMonth.is_quarter_end', 'pandas.tseries.offsets.WeekOfMonth.is_year_end', 'pandas.tseries.offsets.LastWeekOfMonth', 'pandas.tseries.offsets.LastWeekOfMonth.freqstr', 'pandas.tseries.offsets.LastWeekOfMonth.name', 'pandas.tseries.offsets.LastWeekOfMonth.copy', 'pandas.tseries.offsets.LastWeekOfMonth.is_on_offset', 'pandas.tseries.offsets.LastWeekOfMonth.is_month_end', 'pandas.tseries.offsets.LastWeekOfMonth.is_quarter_end', 'pandas.tseries.offsets.LastWeekOfMonth.is_year_end', 'pandas.tseries.offsets.LastWeekOfMonth.kwds', 'pandas.tseries.offsets.LastWeekOfMonth.is_anchored', 'pandas.tseries.offsets.LastWeekOfMonth.is_month_start', 'pandas.tseries.offsets.LastWeekOfMonth.is_quarter_start', 'pandas.tseries.offsets.LastWeekOfMonth.is_year_start', 'pandas.tseries.offsets.BQuarterEnd', 'pandas.tseries.offsets.BQuarterEnd.freqstr', 'pandas.tseries.offsets.BQuarterEnd.name', 'pandas.tseries.offsets.BQuarterEnd.copy', 'pandas.tseries.offsets.BQuarterEnd.is_on_offset', 'pandas.tseries.offsets.BQuarterEnd.is_month_end', 'pandas.tseries.offsets.BQuarterEnd.is_quarter_end', 'pandas.tseries.offsets.BQuarterEnd.is_year_end', 'pandas.tseries.offsets.BQuarterEnd.kwds', 'pandas.tseries.offsets.BQuarterEnd.is_anchored', 'pandas.tseries.offsets.BQuarterEnd.is_month_start', 'pandas.tseries.offsets.BQuarterEnd.is_quarter_start', 'pandas.tseries.offsets.BQuarterEnd.is_year_start', 'pandas.tseries.offsets.BQuarterBegin', 'pandas.tseries.offsets.BQuarterBegin.freqstr', 'pandas.tseries.offsets.BQuarterBegin.name', 'pandas.tseries.offsets.BQuarterBegin.copy', 'pandas.tseries.offsets.BQuarterBegin.is_on_offset', 'pandas.tseries.offsets.BQuarterBegin.is_month_end', 'pandas.tseries.offsets.BQuarterBegin.is_quarter_end', 'pandas.tseries.offsets.BQuarterBegin.is_year_end', 'pandas.tseries.offsets.BQuarterBegin.kwds', 'pandas.tseries.offsets.BQuarterBegin.is_anchored', 'pandas.tseries.offsets.BQuarterBegin.is_month_start', 'pandas.tseries.offsets.BQuarterBegin.is_quarter_start', 'pandas.tseries.offsets.BQuarterBegin.is_year_start', 'pandas.tseries.offsets.QuarterEnd', 'pandas.tseries.offsets.QuarterEnd.freqstr', 'pandas.tseries.offsets.QuarterEnd.name', 'pandas.tseries.offsets.QuarterEnd.copy', 'pandas.tseries.offsets.QuarterEnd.is_on_offset', 'pandas.tseries.offsets.QuarterEnd.is_month_end', 'pandas.tseries.offsets.QuarterEnd.is_quarter_end', 'pandas.tseries.offsets.QuarterEnd.is_year_end', 'pandas.tseries.offsets.QuarterEnd.kwds', 'pandas.tseries.offsets.QuarterEnd.is_anchored', 'pandas.tseries.offsets.QuarterEnd.is_month_start', 'pandas.tseries.offsets.QuarterEnd.is_quarter_start', 'pandas.tseries.offsets.QuarterEnd.is_year_start', 'pandas.tseries.offsets.QuarterBegin', 'pandas.tseries.offsets.QuarterBegin.freqstr', 'pandas.tseries.offsets.QuarterBegin.name', 'pandas.tseries.offsets.QuarterBegin.copy', 'pandas.tseries.offsets.QuarterBegin.is_on_offset', 'pandas.tseries.offsets.QuarterBegin.is_month_end', 'pandas.tseries.offsets.QuarterBegin.is_quarter_end', 'pandas.tseries.offsets.QuarterBegin.is_year_end', 'pandas.tseries.offsets.QuarterBegin.kwds', 'pandas.tseries.offsets.QuarterBegin.is_anchored', 'pandas.tseries.offsets.QuarterBegin.is_month_start', 'pandas.tseries.offsets.QuarterBegin.is_quarter_start', 'pandas.tseries.offsets.QuarterBegin.is_year_start', 'pandas.tseries.offsets.BYearEnd', 'pandas.tseries.offsets.BYearEnd.freqstr', 'pandas.tseries.offsets.BYearEnd.name', 'pandas.tseries.offsets.BYearEnd.copy', 'pandas.tseries.offsets.BYearEnd.is_on_offset', 'pandas.tseries.offsets.BYearEnd.is_month_end', 'pandas.tseries.offsets.BYearEnd.is_quarter_end', 'pandas.tseries.offsets.BYearEnd.is_year_end', 'pandas.tseries.offsets.BYearEnd.kwds', 'pandas.tseries.offsets.BYearEnd.is_anchored', 'pandas.tseries.offsets.BYearEnd.is_month_start', 'pandas.tseries.offsets.BYearEnd.is_quarter_start', 'pandas.tseries.offsets.BYearEnd.is_year_start', 'pandas.tseries.offsets.BYearBegin', 'pandas.tseries.offsets.BYearBegin.freqstr', 'pandas.tseries.offsets.BYearBegin.name', 'pandas.tseries.offsets.BYearBegin.copy', 'pandas.tseries.offsets.BYearBegin.is_on_offset', 'pandas.tseries.offsets.BYearBegin.is_month_end', 'pandas.tseries.offsets.BYearBegin.is_quarter_end', 'pandas.tseries.offsets.BYearBegin.is_year_end', 'pandas.tseries.offsets.BYearBegin.kwds', 'pandas.tseries.offsets.BYearBegin.is_anchored', 'pandas.tseries.offsets.BYearBegin.is_month_start', 'pandas.tseries.offsets.BYearBegin.is_quarter_start', 'pandas.tseries.offsets.BYearBegin.is_year_start', 'pandas.tseries.offsets.YearEnd', 'pandas.tseries.offsets.YearEnd.freqstr', 'pandas.tseries.offsets.YearEnd.name', 'pandas.tseries.offsets.YearEnd.copy', 'pandas.tseries.offsets.YearEnd.is_on_offset', 'pandas.tseries.offsets.YearEnd.is_month_end', 'pandas.tseries.offsets.YearEnd.is_quarter_end', 'pandas.tseries.offsets.YearEnd.is_year_end', 'pandas.tseries.offsets.YearEnd.kwds', 'pandas.tseries.offsets.YearEnd.is_anchored', 'pandas.tseries.offsets.YearEnd.is_month_start', 'pandas.tseries.offsets.YearEnd.is_quarter_start', 'pandas.tseries.offsets.YearEnd.is_year_start', 'pandas.tseries.offsets.YearBegin', 'pandas.tseries.offsets.YearBegin.freqstr', 'pandas.tseries.offsets.YearBegin.name', 'pandas.tseries.offsets.YearBegin.copy', 'pandas.tseries.offsets.YearBegin.is_on_offset', 'pandas.tseries.offsets.YearBegin.is_month_end', 'pandas.tseries.offsets.YearBegin.is_quarter_end', 'pandas.tseries.offsets.YearBegin.is_year_end', 'pandas.tseries.offsets.YearBegin.kwds', 'pandas.tseries.offsets.YearBegin.is_anchored', 'pandas.tseries.offsets.YearBegin.is_month_start', 'pandas.tseries.offsets.YearBegin.is_quarter_start', 'pandas.tseries.offsets.YearBegin.is_year_start', 'pandas.tseries.offsets.FY5253', 'pandas.tseries.offsets.FY5253.freqstr', 'pandas.tseries.offsets.FY5253.name', 'pandas.tseries.offsets.FY5253.copy', 'pandas.tseries.offsets.FY5253.is_on_offset', 'pandas.tseries.offsets.FY5253.is_month_end', 'pandas.tseries.offsets.FY5253.is_quarter_end', 'pandas.tseries.offsets.FY5253.is_year_end', 'pandas.tseries.offsets.FY5253.kwds', 'pandas.tseries.offsets.FY5253.is_anchored', 'pandas.tseries.offsets.FY5253.is_month_start', 'pandas.tseries.offsets.FY5253.is_quarter_start', 'pandas.tseries.offsets.FY5253.is_year_start', 'pandas.tseries.offsets.FY5253Quarter', 'pandas.tseries.offsets.FY5253Quarter.freqstr', 'pandas.tseries.offsets.FY5253Quarter.name', 'pandas.tseries.offsets.FY5253Quarter.copy', 'pandas.tseries.offsets.FY5253Quarter.is_on_offset', 'pandas.tseries.offsets.FY5253Quarter.is_month_start', 'pandas.tseries.offsets.FY5253Quarter.is_quarter_start', 'pandas.tseries.offsets.FY5253Quarter.is_year_start', 'pandas.tseries.offsets.FY5253Quarter.kwds', 'pandas.tseries.offsets.FY5253Quarter.is_anchored', 'pandas.tseries.offsets.FY5253Quarter.is_month_end', 'pandas.tseries.offsets.FY5253Quarter.is_quarter_end', 'pandas.tseries.offsets.FY5253Quarter.is_year_end', 'pandas.tseries.offsets.Easter', 'pandas.tseries.offsets.Easter.freqstr', 'pandas.tseries.offsets.Easter.name', 'pandas.tseries.offsets.Easter.copy', 'pandas.tseries.offsets.Easter.is_on_offset', 'pandas.tseries.offsets.Easter.is_month_end', 'pandas.tseries.offsets.Easter.is_quarter_end', 'pandas.tseries.offsets.Easter.is_year_end', 'pandas.tseries.offsets.Easter.kwds', 'pandas.tseries.offsets.Easter.is_anchored', 'pandas.tseries.offsets.Easter.is_month_start', 'pandas.tseries.offsets.Easter.is_quarter_start', 'pandas.tseries.offsets.Easter.is_year_start', 'pandas.tseries.offsets.Tick', 'pandas.tseries.offsets.Tick.kwds', 'pandas.tseries.offsets.Tick.nanos', 'pandas.tseries.offsets.Tick.copy', 'pandas.tseries.offsets.Tick.is_on_offset', 'pandas.tseries.offsets.Tick.is_month_end', 'pandas.tseries.offsets.Tick.is_quarter_end', 'pandas.tseries.offsets.Tick.is_year_end', 'pandas.tseries.offsets.Tick.freqstr', 'pandas.tseries.offsets.Tick.name', 'pandas.tseries.offsets.Tick.is_anchored', 'pandas.tseries.offsets.Tick.is_month_start', 'pandas.tseries.offsets.Tick.is_quarter_start', 'pandas.tseries.offsets.Tick.is_year_start', 'pandas.tseries.offsets.Day', 'pandas.tseries.offsets.Day.kwds', 'pandas.tseries.offsets.Day.nanos', 'pandas.tseries.offsets.Day.copy', 'pandas.tseries.offsets.Day.is_on_offset', 'pandas.tseries.offsets.Day.is_month_end', 'pandas.tseries.offsets.Day.is_quarter_end', 'pandas.tseries.offsets.Day.is_year_end', 'pandas.tseries.offsets.Day.freqstr', 'pandas.tseries.offsets.Day.name', 'pandas.tseries.offsets.Day.is_anchored', 'pandas.tseries.offsets.Day.is_month_start', 'pandas.tseries.offsets.Day.is_quarter_start', 'pandas.tseries.offsets.Day.is_year_start', 'pandas.tseries.offsets.Hour', 'pandas.tseries.offsets.Hour.kwds', 'pandas.tseries.offsets.Hour.nanos', 'pandas.tseries.offsets.Hour.copy', 'pandas.tseries.offsets.Hour.is_on_offset', 'pandas.tseries.offsets.Hour.is_month_end', 'pandas.tseries.offsets.Hour.is_quarter_end', 'pandas.tseries.offsets.Hour.is_year_end', 'pandas.tseries.offsets.Hour.freqstr', 'pandas.tseries.offsets.Hour.name', 'pandas.tseries.offsets.Hour.is_anchored', 'pandas.tseries.offsets.Hour.is_month_start', 'pandas.tseries.offsets.Hour.is_quarter_start', 'pandas.tseries.offsets.Hour.is_year_start', 'pandas.tseries.offsets.Minute', 'pandas.tseries.offsets.Minute.kwds', 'pandas.tseries.offsets.Minute.nanos', 'pandas.tseries.offsets.Minute.copy', 'pandas.tseries.offsets.Minute.is_on_offset', 'pandas.tseries.offsets.Minute.is_month_end', 'pandas.tseries.offsets.Minute.is_quarter_end', 'pandas.tseries.offsets.Minute.is_year_end', 'pandas.tseries.offsets.Minute.freqstr', 'pandas.tseries.offsets.Minute.name', 'pandas.tseries.offsets.Minute.is_anchored', 'pandas.tseries.offsets.Minute.is_month_start', 'pandas.tseries.offsets.Minute.is_quarter_start', 'pandas.tseries.offsets.Minute.is_year_start', 'pandas.tseries.offsets.Second', 'pandas.tseries.offsets.Second.kwds', 'pandas.tseries.offsets.Second.nanos', 'pandas.tseries.offsets.Second.copy', 'pandas.tseries.offsets.Second.is_on_offset', 'pandas.tseries.offsets.Second.is_month_end', 'pandas.tseries.offsets.Second.is_quarter_end', 'pandas.tseries.offsets.Second.is_year_end', 'pandas.tseries.offsets.Second.freqstr', 'pandas.tseries.offsets.Second.name', 'pandas.tseries.offsets.Second.is_anchored', 'pandas.tseries.offsets.Second.is_month_start', 'pandas.tseries.offsets.Second.is_quarter_start', 'pandas.tseries.offsets.Second.is_year_start', 'pandas.tseries.offsets.Milli', 'pandas.tseries.offsets.Milli.kwds', 'pandas.tseries.offsets.Milli.nanos', 'pandas.tseries.offsets.Milli.copy', 'pandas.tseries.offsets.Milli.is_on_offset', 'pandas.tseries.offsets.Milli.is_month_end', 'pandas.tseries.offsets.Milli.is_quarter_end', 'pandas.tseries.offsets.Milli.is_year_end', 'pandas.tseries.offsets.Milli.freqstr', 'pandas.tseries.offsets.Milli.name', 'pandas.tseries.offsets.Milli.is_anchored', 'pandas.tseries.offsets.Milli.is_month_start', 'pandas.tseries.offsets.Milli.is_quarter_start', 'pandas.tseries.offsets.Milli.is_year_start', 'pandas.tseries.offsets.Micro', 'pandas.tseries.offsets.Micro.kwds', 'pandas.tseries.offsets.Micro.nanos', 'pandas.tseries.offsets.Micro.copy', 'pandas.tseries.offsets.Micro.is_on_offset', 'pandas.tseries.offsets.Micro.is_month_end', 'pandas.tseries.offsets.Micro.is_quarter_end', 'pandas.tseries.offsets.Micro.is_year_end', 'pandas.tseries.offsets.Micro.freqstr', 'pandas.tseries.offsets.Micro.name', 'pandas.tseries.offsets.Micro.is_anchored', 'pandas.tseries.offsets.Micro.is_month_start', 'pandas.tseries.offsets.Micro.is_quarter_start', 'pandas.tseries.offsets.Micro.is_year_start', 'pandas.tseries.offsets.Nano', 'pandas.tseries.offsets.Nano.kwds', 'pandas.tseries.offsets.Nano.nanos', 'pandas.tseries.offsets.Nano.copy', 'pandas.tseries.offsets.Nano.is_on_offset', 'pandas.tseries.offsets.Nano.is_month_end', 'pandas.tseries.offsets.Nano.is_quarter_end', 'pandas.tseries.offsets.Nano.is_year_end', 'pandas.tseries.offsets.Nano.freqstr', 'pandas.tseries.offsets.Nano.name', 'pandas.tseries.offsets.Nano.is_anchored', 'pandas.tseries.offsets.Nano.is_month_start', 'pandas.tseries.offsets.Nano.is_quarter_start', 'pandas.tseries.offsets.Nano.is_year_start', 'pandas.tseries.frequencies.to_offset', 'pandas.core.window.rolling.Rolling.count', 'pandas.core.window.rolling.Rolling.mean', 'pandas.core.window.rolling.Rolling.var', 'pandas.core.window.rolling.Rolling.min', 'pandas.core.window.rolling.Rolling.corr', 'pandas.core.window.rolling.Rolling.skew', 'pandas.core.window.rolling.Rolling.apply', 'pandas.core.window.rolling.Rolling.quantile', 'pandas.core.window.rolling.Rolling.rank', 'pandas.core.window.rolling.Rolling.sum', 'pandas.core.window.rolling.Rolling.median', 'pandas.core.window.rolling.Rolling.std', 'pandas.core.window.rolling.Rolling.max', 'pandas.core.window.rolling.Rolling.cov', 'pandas.core.window.rolling.Rolling.kurt', 'pandas.core.window.rolling.Rolling.aggregate', 'pandas.core.window.rolling.Rolling.sem', 'pandas.core.window.rolling.Window.mean', 'pandas.core.window.rolling.Window.var', 'pandas.core.window.rolling.Window.sum', 'pandas.core.window.rolling.Window.std', 'pandas.core.window.expanding.Expanding.count', 'pandas.core.window.expanding.Expanding.mean', 'pandas.core.window.expanding.Expanding.var', 'pandas.core.window.expanding.Expanding.min', 'pandas.core.window.expanding.Expanding.corr', 'pandas.core.window.expanding.Expanding.skew', 'pandas.core.window.expanding.Expanding.apply', 'pandas.core.window.expanding.Expanding.quantile', 'pandas.core.window.expanding.Expanding.rank', 'pandas.core.window.expanding.Expanding.sum', 'pandas.core.window.expanding.Expanding.median', 'pandas.core.window.expanding.Expanding.std', 'pandas.core.window.expanding.Expanding.max', 'pandas.core.window.expanding.Expanding.cov', 'pandas.core.window.expanding.Expanding.kurt', 'pandas.core.window.expanding.Expanding.aggregate', 'pandas.core.window.expanding.Expanding.sem', 'pandas.core.window.ewm.ExponentialMovingWindow.mean', 'pandas.core.window.ewm.ExponentialMovingWindow.std', 'pandas.core.window.ewm.ExponentialMovingWindow.corr', 'pandas.core.window.ewm.ExponentialMovingWindow.sum', 'pandas.core.window.ewm.ExponentialMovingWindow.var', 'pandas.core.window.ewm.ExponentialMovingWindow.cov', 'pandas.api.indexers.BaseIndexer', 'pandas.api.indexers.VariableOffsetWindowIndexer', 'pandas.api.indexers.FixedForwardWindowIndexer', 'pandas.core.groupby.DataFrameGroupBy.__iter__', 'pandas.core.groupby.DataFrameGroupBy.groups', 'pandas.core.groupby.DataFrameGroupBy.indices', 'pandas.core.groupby.DataFrameGroupBy.get_group', 'pandas.Grouper', 'pandas.core.groupby.SeriesGroupBy.__iter__', 'pandas.core.groupby.SeriesGroupBy.groups', 'pandas.core.groupby.SeriesGroupBy.indices', 'pandas.core.groupby.SeriesGroupBy.get_group', 'pandas.NamedAgg', 'pandas.core.groupby.SeriesGroupBy.apply', 'pandas.core.groupby.SeriesGroupBy.agg', 'pandas.core.groupby.SeriesGroupBy.aggregate', 'pandas.core.groupby.SeriesGroupBy.transform', 'pandas.core.groupby.SeriesGroupBy.pipe', 'pandas.core.groupby.DataFrameGroupBy.filter', 'pandas.core.groupby.DataFrameGroupBy.apply', 'pandas.core.groupby.DataFrameGroupBy.agg', 'pandas.core.groupby.DataFrameGroupBy.aggregate', 'pandas.core.groupby.DataFrameGroupBy.transform', 'pandas.core.groupby.DataFrameGroupBy.pipe', 'pandas.core.groupby.SeriesGroupBy.filter', 'pandas.core.groupby.DataFrameGroupBy.all', 'pandas.core.groupby.DataFrameGroupBy.bfill', 'pandas.core.groupby.DataFrameGroupBy.corrwith', 'pandas.core.groupby.DataFrameGroupBy.cov', 'pandas.core.groupby.DataFrameGroupBy.cummax', 'pandas.core.groupby.DataFrameGroupBy.cumprod', 'pandas.core.groupby.DataFrameGroupBy.describe', 'pandas.core.groupby.DataFrameGroupBy.ffill', 'pandas.core.groupby.DataFrameGroupBy.first', 'pandas.core.groupby.DataFrameGroupBy.idxmax', 'pandas.core.groupby.DataFrameGroupBy.last', 'pandas.core.groupby.DataFrameGroupBy.mean', 'pandas.core.groupby.DataFrameGroupBy.min', 'pandas.core.groupby.DataFrameGroupBy.nth', 'pandas.core.groupby.DataFrameGroupBy.ohlc', 'pandas.core.groupby.DataFrameGroupBy.prod', 'pandas.core.groupby.DataFrameGroupBy.rank', 'pandas.core.groupby.DataFrameGroupBy.rolling', 'pandas.core.groupby.DataFrameGroupBy.sem', 'pandas.core.groupby.DataFrameGroupBy.size', 'pandas.core.groupby.DataFrameGroupBy.std', 'pandas.core.groupby.DataFrameGroupBy.var', 'pandas.core.groupby.DataFrameGroupBy.take', 'pandas.core.groupby.DataFrameGroupBy.any', 'pandas.core.groupby.DataFrameGroupBy.corr', 'pandas.core.groupby.DataFrameGroupBy.count', 'pandas.core.groupby.DataFrameGroupBy.cumcount', 'pandas.core.groupby.DataFrameGroupBy.cummin', 'pandas.core.groupby.DataFrameGroupBy.cumsum', 'pandas.core.groupby.DataFrameGroupBy.diff', 'pandas.core.groupby.DataFrameGroupBy.fillna', 'pandas.core.groupby.DataFrameGroupBy.head', 'pandas.core.groupby.DataFrameGroupBy.idxmin', 'pandas.core.groupby.DataFrameGroupBy.max', 'pandas.core.groupby.DataFrameGroupBy.median', 'pandas.core.groupby.DataFrameGroupBy.ngroup', 'pandas.core.groupby.DataFrameGroupBy.nunique', 'pandas.core.groupby.DataFrameGroupBy.pct_change', 'pandas.core.groupby.DataFrameGroupBy.quantile', 'pandas.core.groupby.DataFrameGroupBy.resample', 'pandas.core.groupby.DataFrameGroupBy.sample', 'pandas.core.groupby.DataFrameGroupBy.shift', 'pandas.core.groupby.DataFrameGroupBy.skew', 'pandas.core.groupby.DataFrameGroupBy.sum', 'pandas.core.groupby.DataFrameGroupBy.tail', 'pandas.core.groupby.DataFrameGroupBy.value_counts', 'pandas.core.groupby.SeriesGroupBy.all', 'pandas.core.groupby.SeriesGroupBy.bfill', 'pandas.core.groupby.SeriesGroupBy.count', 'pandas.core.groupby.SeriesGroupBy.cumcount', 'pandas.core.groupby.SeriesGroupBy.cummin', 'pandas.core.groupby.SeriesGroupBy.cumsum', 'pandas.core.groupby.SeriesGroupBy.diff', 'pandas.core.groupby.SeriesGroupBy.fillna', 'pandas.core.groupby.SeriesGroupBy.head', 'pandas.core.groupby.SeriesGroupBy.idxmax', 'pandas.core.groupby.SeriesGroupBy.is_monotonic_increasing', 'pandas.core.groupby.SeriesGroupBy.max', 'pandas.core.groupby.SeriesGroupBy.median', 'pandas.core.groupby.SeriesGroupBy.ngroup', 'pandas.core.groupby.SeriesGroupBy.nsmallest', 'pandas.core.groupby.SeriesGroupBy.nunique', 'pandas.core.groupby.SeriesGroupBy.ohlc', 'pandas.core.groupby.SeriesGroupBy.prod', 'pandas.core.groupby.SeriesGroupBy.rank', 'pandas.core.groupby.SeriesGroupBy.rolling', 'pandas.core.groupby.SeriesGroupBy.sem', 'pandas.core.groupby.SeriesGroupBy.size', 'pandas.core.groupby.SeriesGroupBy.std', 'pandas.core.groupby.SeriesGroupBy.var', 'pandas.core.groupby.SeriesGroupBy.take', 'pandas.core.groupby.SeriesGroupBy.any', 'pandas.core.groupby.SeriesGroupBy.corr', 'pandas.core.groupby.SeriesGroupBy.cov', 'pandas.core.groupby.SeriesGroupBy.cummax', 'pandas.core.groupby.SeriesGroupBy.cumprod', 'pandas.core.groupby.SeriesGroupBy.describe', 'pandas.core.groupby.SeriesGroupBy.ffill', 'pandas.core.groupby.SeriesGroupBy.first', 'pandas.core.groupby.SeriesGroupBy.last', 'pandas.core.groupby.SeriesGroupBy.idxmin', 'pandas.core.groupby.SeriesGroupBy.is_monotonic_decreasing', 'pandas.core.groupby.SeriesGroupBy.mean', 'pandas.core.groupby.SeriesGroupBy.min', 'pandas.core.groupby.SeriesGroupBy.nlargest', 'pandas.core.groupby.SeriesGroupBy.nth', 'pandas.core.groupby.SeriesGroupBy.unique', 'pandas.core.groupby.SeriesGroupBy.pct_change', 'pandas.core.groupby.SeriesGroupBy.quantile', 'pandas.core.groupby.SeriesGroupBy.resample', 'pandas.core.groupby.SeriesGroupBy.sample', 'pandas.core.groupby.SeriesGroupBy.shift', 'pandas.core.groupby.SeriesGroupBy.skew', 'pandas.core.groupby.SeriesGroupBy.sum', 'pandas.core.groupby.SeriesGroupBy.tail', 'pandas.core.groupby.DataFrameGroupBy.boxplot', 'pandas.core.groupby.SeriesGroupBy.hist', 'pandas.core.groupby.SeriesGroupBy.plot', 'pandas.core.groupby.DataFrameGroupBy.hist', 'pandas.core.groupby.DataFrameGroupBy.plot', 'pandas.core.resample.Resampler.__iter__', 'pandas.core.resample.Resampler.indices', 'pandas.core.resample.Resampler.groups', 'pandas.core.resample.Resampler.get_group', 'pandas.core.resample.Resampler.apply', 'pandas.core.resample.Resampler.transform', 'pandas.core.resample.Resampler.aggregate', 'pandas.core.resample.Resampler.pipe', 'pandas.core.resample.Resampler.ffill', 'pandas.core.resample.Resampler.nearest', 'pandas.core.resample.Resampler.asfreq', 'pandas.core.resample.Resampler.bfill', 'pandas.core.resample.Resampler.fillna', 'pandas.core.resample.Resampler.interpolate', 'pandas.core.resample.Resampler.count', 'pandas.core.resample.Resampler.first', 'pandas.core.resample.Resampler.max', 'pandas.core.resample.Resampler.median', 'pandas.core.resample.Resampler.ohlc', 'pandas.core.resample.Resampler.size', 'pandas.core.resample.Resampler.std', 'pandas.core.resample.Resampler.var', 'pandas.core.resample.Resampler.nunique', 'pandas.core.resample.Resampler.last', 'pandas.core.resample.Resampler.mean', 'pandas.core.resample.Resampler.min', 'pandas.core.resample.Resampler.prod', 'pandas.core.resample.Resampler.sem', 'pandas.core.resample.Resampler.sum', 'pandas.core.resample.Resampler.quantile', 'pandas.io.formats.style.Styler', 'pandas.io.formats.style.Styler.from_custom_template', 'pandas.io.formats.style.Styler.apply', 'pandas.io.formats.style.Styler.apply_index', 'pandas.io.formats.style.Styler.format', 'pandas.io.formats.style.Styler.relabel_index', 'pandas.io.formats.style.Styler.concat', 'pandas.io.formats.style.Styler.set_table_styles', 'pandas.io.formats.style.Styler.set_tooltips', 'pandas.io.formats.style.Styler.set_sticky', 'pandas.io.formats.style.Styler.set_uuid', 'pandas.io.formats.style.Styler.pipe', 'pandas.io.formats.style.Styler.map', 'pandas.io.formats.style.Styler.map_index', 'pandas.io.formats.style.Styler.format_index', 'pandas.io.formats.style.Styler.hide', 'pandas.io.formats.style.Styler.set_td_classes', 'pandas.io.formats.style.Styler.set_table_attributes', 'pandas.io.formats.style.Styler.set_caption', 'pandas.io.formats.style.Styler.set_properties', 'pandas.io.formats.style.Styler.clear', 'pandas.io.formats.style.Styler.highlight_null', 'pandas.io.formats.style.Styler.highlight_min', 'pandas.io.formats.style.Styler.highlight_quantile', 'pandas.io.formats.style.Styler.text_gradient', 'pandas.io.formats.style.Styler.highlight_max', 'pandas.io.formats.style.Styler.highlight_between', 'pandas.io.formats.style.Styler.background_gradient', 'pandas.io.formats.style.Styler.bar', 'pandas.io.formats.style.Styler.export', 'pandas.io.formats.style.Styler.to_string', 'pandas.io.formats.style.Styler.use', 'pandas.plotting.andrews_curves', 'pandas.plotting.autocorrelation_plot', 'pandas.plotting.bootstrap_plot', 'pandas.plotting.boxplot', 'pandas.plotting.deregister_matplotlib_converters', 'pandas.plotting.lag_plot', 'pandas.plotting.parallel_coordinates', 'pandas.plotting.plot_params', 'pandas.plotting.radviz', 'pandas.plotting.register_matplotlib_converters', 'pandas.plotting.scatter_matrix', 'pandas.plotting.table', 'pandas.describe_option', 'pandas.get_option', 'pandas.option_context', 'pandas.reset_option', 'pandas.set_option', 'pandas.set_eng_float_format', 'pandas.api.extensions.register_extension_dtype', 'pandas.api.extensions.register_dataframe_accessor', 'pandas.api.extensions.register_series_accessor', 'pandas.api.extensions.register_index_accessor', 'pandas.api.extensions.ExtensionDtype', 'pandas.api.extensions.ExtensionArray', 'pandas.arrays.NumpyExtensionArray', 'pandas.api.indexers.check_array_indexer', 'pandas.testing.assert_frame_equal', 'pandas.testing.assert_index_equal', 'pandas.testing.assert_series_equal', 'pandas.testing.assert_extension_array_equal', 'pandas.errors.AbstractMethodError', 'pandas.errors.CategoricalConversionWarning', 'pandas.errors.ClosedFileError', 'pandas.errors.DatabaseError', 'pandas.errors.DtypeWarning', 'pandas.errors.EmptyDataError', 'pandas.errors.IndexingError', 'pandas.errors.InvalidComparison', 'pandas.errors.InvalidVersion', 'pandas.errors.LossySetitemError', 'pandas.errors.NoBufferPresent', 'pandas.errors.NumbaUtilError', 'pandas.errors.OptionError', 'pandas.errors.OutOfBoundsTimedelta', 'pandas.errors.ParserWarning', 'pandas.errors.PossibleDataLossError', 'pandas.errors.PyperclipException', 'pandas.errors.SettingWithCopyError', 'pandas.errors.SpecificationError', 'pandas.errors.UnsortedIndexError', 'pandas.errors.ValueLabelTypeMismatch', 'pandas.errors.AttributeConflictWarning', 'pandas.errors.ChainedAssignmentError', 'pandas.errors.CSSWarning', 'pandas.errors.DataError', 'pandas.errors.DuplicateLabelError', 'pandas.errors.IncompatibilityWarning', 'pandas.errors.InvalidColumnName', 'pandas.errors.InvalidIndexError', 'pandas.errors.IntCastingNaNError', 'pandas.errors.MergeError', 'pandas.errors.NullFrequencyError', 'pandas.errors.NumExprClobberingError', 'pandas.errors.OutOfBoundsDatetime', 'pandas.errors.ParserError', 'pandas.errors.PerformanceWarning', 'pandas.errors.PossiblePrecisionLoss', 'pandas.errors.PyperclipWindowsException', 'pandas.errors.SettingWithCopyWarning', 'pandas.errors.UndefinedVariableError', 'pandas.errors.UnsupportedFunctionCall', 'pandas.show_versions', 'pandas.test', 'pandas.NA', 'pandas.NaT'))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_nodes = [node for node, attributes in pandas_graph.nodes(data=True) if attributes['type']=='parent_node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for node, attributes in pandas_graph.nodes(data=True):\n",
    "#     if attributes['type']=='function_node':\n",
    "#         print(node,attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_text_dict = {k:[] for k in parent_nodes}\n",
    "for node, attributes in pandas_graph.nodes(data=True):\n",
    "    if attributes['type']=='function_node':\n",
    "        # parent_text_dict[attributes['trail']] += attributes['metadata']['function_text'] + \"\\n\"\n",
    "        parent_text_dict[attributes['trail']].append(attributes['metadata']['function_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Generate a matplotlib plot for visualizing clusters of multivariate data.',\n",
       " 'Autocorrelation plot for time series.',\n",
       " 'Bootstrap plot on mean, median and mid-range statistics.',\n",
       " 'Make a box plot from DataFrame columns.',\n",
       " 'Remove pandas formatters and converters.',\n",
       " 'Lag plot for time series.',\n",
       " 'Parallel coordinates plotting.',\n",
       " 'Stores pandas plotting options.',\n",
       " 'Plot a multidimensional dataset in 2D.',\n",
       " 'Register pandas formatters and converters with matplotlib.',\n",
       " 'Draw a matrix of scatter plots.',\n",
       " 'Helper function to convert DataFrame and Series to matplotlib.table.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_text_dict['Plotting']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dspy\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationGeneration(dspy.Signature):\n",
    "    \"\"\"You are given a list of descriptions of different functions separated by newline. \n",
    "    Your task is to summarize all the text into coherent summary that covers all the functions descriptions.\n",
    "    Make sure that no function description is left out of the final summary. Provide a detailed summary and cover all the function descriptions\"\"\"\n",
    "    \n",
    "    function_descriptions = dspy.InputField(prefix=\"List of function descriptions: \",desc='list of function descriptions to be summarized')\n",
    "    summary = dspy.OutputField(prefix=\"Summary: \",desc='summary of all the function descriptions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_llm = dspy.OpenAI(model=\"gpt-3.5-turbo\",max_tokens=4096)\n",
    "dspy.settings.configure(lm=summarization_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"parent_summary_dict.json\"\n",
    "\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        # Load the JSON data from the file\n",
    "        parent_summary_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Input/output': 'The pandas library in Python provides a wide range of functions for reading and writing data in various formats. These functions include loading pickled objects, reading delimited files, CSV files, fixed-width formatted lines, Excel files, JSON strings, HTML tables, XML documents, and more. There are also functions for working with HDFStore, feather-format objects, parquet objects, ORC objects, SAS files, SPSS files, SQL databases, Google BigQuery, and Stata files. Additionally, there are functions for creating table schemas, normalizing JSON data, rendering DataFrames to XML documents, and exporting DataFrames to Stata dta format. These functions allow for seamless data manipulation and analysis across different data sources and formats. ',\n",
       " 'General functions': 'The functions provided cover a wide range of data manipulation tasks in pandas. These include unpivoting a DataFrame from wide to long format, creating pivot tables, binning values, merging DataFrames, converting categorical variables, reshaping data, computing cross tabulations, discretizing data based on quantiles, concatenating pandas objects, detecting missing or non-missing values, converting data types, working with datetime data, inferring frequencies, evaluating expressions, guessing datetime formats, generating deterministic integers, hashing data, and building DataFrames from compatible sources. These functions provide a comprehensive toolkit for data analysis and manipulation in pandas. ',\n",
       " 'Series': 'The pandas library provides a wide range of functions for working with one-dimensional ndarrays with axis labels, such as time series. These functions allow for operations like returning the Series as an ndarray, checking for NaN values, getting the dtype object of the data, accessing specific elements, converting between different types, performing mathematical operations like addition, multiplication, division, and modulo, rounding values, comparing values, computing dot products, and more. Additionally, there are functions for memory usage, empty checks, copying data, and accessing specific rows and columns based on labels or positions. Overall, these functions offer comprehensive functionality for manipulating and analyzing one-dimensional data structures in pandas. The functions provided cover a wide range of operations on Series and DataFrames in Python. These functions include basic arithmetic operations such as division, subtraction, and exponentiation, as well as more complex operations like rolling window calculations, exponentially weighted calculations, and expanding window calculations. There are also functions for aggregation, mapping, and applying functions to Series. Additionally, there are functions for calculating statistics such as mean, median, mode, standard deviation, variance, and correlation. Other functions include checking for unique values, monotonicity, and boolean conditions. Overall, these functions provide a comprehensive toolkit for data manipulation and analysis in Python. The functions provided cover a wide range of operations that can be performed on Series and DataFrames in pandas. These functions include returning the number of unique elements, checking for monotonically increasing values, counting unique values, aligning objects, removing duplicate values, testing for equality between objects, selecting rows based on conditions, setting axis names, sampling items, truncating data, replacing values based on conditions, suffixing and prefixing labels, removing missing values, sorting, transforming, squeezing, unstacking, finding indices, repeating elements, comparing differences, modifying in place, and converting time series data to specified frequencies. These functions provide a comprehensive set of tools for data manipulation and analysis in pandas. The functions provided cover a wide range of operations on time-series data and string manipulation. These functions include shifting the index by a desired number of periods, selecting values at specific times of the day, resampling time-series data, localizing time zones, accessing datetime properties, and converting datetime objects to different formats. Additionally, there are functions for identifying specific date attributes such as month, day of the week, and leap year status. \\n\\nOn the string manipulation side, there are functions for capitalizing strings, concatenating strings with a separator, testing for patterns or regex, decoding strings, extracting elements based on position or key, padding strings, removing characters, splitting strings, replacing patterns, and more. These functions provide a comprehensive toolkit for working with time-series data and manipulating strings efficiently. The functions provided cover a wide range of operations that can be performed on Series and Index data in Python. These include converting strings to different cases, checking for specific character types within strings, working with categorical data, handling sparse values, manipulating datetime objects, dealing with time zones, and performing various string operations such as padding, counting occurrences, encoding, and regex matching. Additionally, there are functions for extracting specific elements from data structures, accessing global attributes, and working with different data types like PeriodArray and SparseSparse. Overall, these functions offer a comprehensive set of tools for data manipulation and analysis in Python. ',\n",
       " 'DataFrame': 'The pandas library provides a wide range of functions for working with tabular data. These functions include serialization to various file formats such as CSV, Excel, JSON, Feather, parquet, ORC, SQL databases, and Stata dta format. Additionally, there are functions for rendering data as HTML tables, LaTeX tables, and nested tables. The library also offers functions for manipulating data within DataFrames, such as accessing specific rows and columns, inferring data types, and performing mathematical operations like addition, subtraction, division, and exponentiation. Other functions include memory usage analysis, copying data, querying, and replacing values based on conditions. Overall, pandas provides a comprehensive set of tools for data manipulation and analysis. The functions available for DataFrame manipulation include getting the modulo, less than, less than or equal to, not equal to, addition, multiplication, floating division, matrix multiplication, subtraction, integer division, exponential power, greater than, greater than or equal to, equal to, updating null elements, applying functions along an axis, aggregating using operations, rolling window calculations, exponentially weighted calculations, absolute numeric values, checking if any element is True, computing pairwise correlation, counting non-NA cells, cumulative maximum, cumulative product, generating descriptive statistics, evaluating string operations, unbiased kurtosis, mean, minimum, fractional change, product of values, numerical data ranks, unbiased standard error of the mean, sum of values, unbiased variance, frequency of distinct rows, checking if all elements are True, trimming values, computing pairwise covariance, cumulative minimum, cumulative sum, discrete difference. The functions provided cover a wide range of operations that can be performed on a DataFrame in Python. These include returning the maximum, median, mode, product, and quantile values over a specified axis, as well as rounding values to a specific number of decimal places. Other functions involve calculating skew, standard deviation, and counting distinct elements. There are also functions for aligning, selecting, and removing duplicate rows, as well as testing for equality between objects. Time-based functions allow for selecting values at specific times of the day, while index-related functions involve setting, resetting, or renaming indices. Missing values can be handled by filling with the next valid observation, interpolation, or specified method. Additionally, there are functions for detecting, removing, and replacing missing values. Pivot tables, sorting, and reshaping operations are also available, along with functions for transposing, squeezing, and swapping levels in a MultiIndex. ',\n",
       " 'pandas arrays, scalars, and data types': 'The functions provided cover a wide range of functionalities related to handling dates, times, durations, and time zones in Pandas. These functions include creating arrays, working with different data types, checking for specific dates (such as first day of the month, quarter, or year), converting timestamps, and manipulating time zones. Additionally, there are functions for extracting specific components of dates and times, such as day of the week, day of the year, and hour of the day. The functions also allow for conversions between different time units, rounding timestamps, and working with timedelta objects. Overall, these functions provide comprehensive support for managing and manipulating date and time data within Pandas. The Pandas library provides a wide range of functions for handling different types of data. This includes functions for working with Period data, such as returning a formatted string representation of the Period.Pandas ExtensionArray, checking if an interval is open or closed on either side, and returning the midpoint of an interval. There are also functions for working with integer and floating point data, categorical data, sparse data, string data, and boolean data. Additionally, there are functions for working with dates and times, such as returning the day of the week, the day of the year, the number of days in a month, and checking if a date is the last day of the month, quarter, or year. Functions are also available for converting between different time zones, combining dates and times, and working with timedeltas. The functions provided in the list cover a wide range of functionalities related to handling different data types and structures in pandas. These functions include getting timestamps, checking leap years, extracting components of periods, working with intervals, categorical data, extension data types, and checking the data type of arrays or objects. Additionally, there are functions to check for specific data types such as real numbers, integers, booleans, floats, strings, and more. Overall, these functions provide a comprehensive set of tools for data manipulation and validation in pandas. ',\n",
       " 'Index objects': 'The functions provided in the list are related to manipulating and analyzing Index objects in Python. These functions include operations such as checking for duplicate values, data types, memory usage, truthiness, and specific value conditions. Other functions involve sorting, shifting, appending, intersecting, and computing indexes. Additionally, there are functions for handling missing values, mapping values, and creating new Index objects. Overall, these functions offer a wide range of capabilities for working with Index objects efficiently and effectively in Python. The functions provided cover a wide range of functionalities related to Index and Series manipulation in pandas. These include returning the minimum value of an Index, creating a new Index with selected values, finding unique values, counting unique values, removing NA/NaN values, detecting existing values, creating a Series with index keys as both index and values, finding indices for maintaining order, computing join_index and indexers, forming unions and symmetric differences of Index objects, returning locations of labels, calculating slice bounds, creating RangeIndex, working with Categorical Index, mapping values, renaming categories, adding/removing categories, checking equality of CategoricalIndex objects, working with IntervalIndex, checking for overlapping intervals, and creating MultiIndex objects. These functions provide a comprehensive set of tools for managing and manipulating index and categorical data structures in pandas. The functions provided in the list cover a wide range of operations related to MultiIndex manipulation, datetime handling, and timezone localization. \\n\\nFor MultiIndex operations, functions include converting a MultiIndex to an Index of Tuples, sorting at a specific level, swapping levels, creating a new MultiIndex with unused levels removed, slicing between labels or tuples, getting locations for labels or tuples, returning label values for a level, creating an object for multi-index slicing, converting lists of tuples to MultiIndex, creating a MultiIndex from a DataFrame, getting the levels and number of levels in a MultiIndex, returning dtypes as a Series, setting new codes, creating a DataFrame with MultiIndex levels as columns, removing levels, rearranging levels, deleting codes, appending Index options, and more.\\n\\nFor datetime operations, functions include getting the year, day, minutes, microseconds, date objects, time objects with timezones, ordinal day of the year, day of the week, quarter, first day of the month, quarter, year, leap year indicator, index locations of values at a specific time of day, converting times to midnight, localizing timezones, performing floor operations, returning month names, converting to a specific dtype resolution, returning datetime objects, creating a DataFrame with datetime Index, returning mean values, month, hours, seconds, nanoseconds, time objects, day of the week, timezone, frequency object, last day of the month, quarter, year, infer_freq string, index locations of values between specific times of day, and converting to Index using a specified date format. ',\n",
       " 'Date offsets': 'The functions described are related to date increments and frequencies. They include returning a string representing the frequency, base frequency, and extra parameters for the offset. There are functions to check if a timestamp occurs on the month, quarter, or year end or start. Additionally, there are functions to determine if a timestamp intersects with a specific frequency and if the frequency is a unit frequency. There are also DateOffset subclasses for business days, business hours, and custom business days, each with similar functions for frequency representation and timestamp occurrence checks. The functions provided in the list are related to determining various aspects of timestamps and frequencies. These functions include checking if a timestamp intersects with a specific frequency, if it occurs on the month, quarter, or year end or start, returning extra parameters for the offset, checking if the frequency is a unit frequency, and providing date offsets for different points in time. Additionally, there are functions to return strings representing the frequency and base frequency, as well as making copies of the frequency. Some functions are aliases for specific business days within a month. Overall, these functions offer a comprehensive set of tools for working with timestamps and frequencies in a program. The functions provided in the list are related to date offsets and frequencies. They include functions to determine if a timestamp occurs on the start, end, or specific points of a month, quarter, or year. There are also functions to return a dictionary of extra parameters for the offset, check if the frequency is a unit frequency, and provide a string representation of the frequency. Additionally, there are functions for custom business months and two DateOffset options per month repeating on the first and last days of the month. These functions cover a wide range of date-related operations and can be used to manipulate and analyze time series data effectively. The functions described include checking if a frequency is a unit frequency, determining if a timestamp occurs at the start or end of a month, quarter, or year, describing monthly dates, finding the last business day of each quarter, and checking if a timestamp intersects with a specific frequency. These functions also involve returning strings representing the frequency and base frequency, making copies of the frequency, and providing extra parameters for the offset. The functions provided are related to DateOffset increments between different time periods such as Quarter start dates, Quarter end dates, month start, month end, year start, and year end. These functions return boolean values to determine if a timestamp occurs on a specific date within these time periods. They also provide information about the frequency, base frequency, and extra parameters for the offset. Additionally, they check if the frequency is a unit frequency and if a timestamp intersects with the specified frequency. Overall, these functions help in analyzing and manipulating time-related data efficiently. The functions provided in the list describe various operations related to timestamps and frequencies. These functions include determining if a timestamp intersects with a specific frequency, checking if a timestamp occurs on the start or end of a month, quarter, or year, returning extra parameters for the offset, identifying unit frequencies, and providing information about different types of fiscal years such as the 52-53 week fiscal year and the 4-4-5 calendar. Additionally, the functions involve working with business quarter dates and incrementing between calendar year begin or end dates. The functions also include returning strings representing the frequency, base frequency, and making copies of the frequency. The functions provided include returning a boolean value indicating whether a timestamp occurs on the start of the year, month, or quarter, as well as on the end of the year, month, or quarter. There are functions to return a dictionary of extra parameters for an offset, a string representing the frequency, and a copy of the frequency. Additionally, there are functions to determine if a timestamp intersects with a specific frequency, the total number of nanoseconds, and attributes related to the offset. Some functions involve calculating the Easter holiday date using logic defined in dateutil. Overall, these functions cover a range of operations related to timestamps, frequencies, and offsets. The functions provided are related to timestamp manipulation and frequency checking. They include functions to determine if a timestamp occurs on the start, end, or specific points of a month, quarter, or year. Additionally, there are functions to offset timestamps by different units such as seconds, milliseconds, microseconds, and nanoseconds. Each function returns specific information such as boolean values, dictionaries of extra parameters, total nanoseconds, and representations of frequency. The functions cover a wide range of timestamp-related operations and provide flexibility in handling time-based data. ',\n",
       " 'Window': 'The functions provided cover a wide range of rolling, expanding, and exponential weighted moment calculations for various statistical measures. These include calculating rolling counts, means, variances, minimums, correlations, skewness, custom aggregations, quantiles, ranks, sums, medians, standard deviations, maximums, sample covariances, Fisher’s definition of kurtosis without bias, and standard errors of the mean. Additionally, there are functions for weighted window calculations and expanding calculations for the same statistical measures. The ewm functions focus on exponential weighted moment calculations for means, standard deviations, sample correlations, sums, variances, and sample covariances. Lastly, there are functions for creating window boundaries based on non-fixed offsets and fixed-length windows that include the current row. ',\n",
       " 'GroupBy': 'The Groupby iterator is a powerful tool that allows the user to specify a groupby instruction for an object. It provides various functionalities such as constructing a DataFrame from a group with a provided name, helper for column-specific aggregation with control over output column names, applying a function group-wise and combining the results together, aggregating using one or more operations over the specified axis, calling a function producing a same-indexed Series or DataFrame on each group, filtering elements from groups that don’t satisfy a criterion, computing pairwise correlation and covariance of columns, calculating cumulative max, product, min, and sum for each group, generating descriptive statistics, filling values forward or backward, computing the first and last entry of each column within each group, computing mean, median, standard error, standard deviation, and variance of groups, providing the rank of values within each group, returning a rolling grouper for rolling functionality per group, computing group sizes, returning elements in given positional indices in each group, checking if all or any values in the group are truthful, computing count of group values, numbering each item in each group, calculating the first discrete difference of an element, filling NA/NaN values using a specified method within groups, returning the first n rows of each group, returning the index of the first occurrence of minimum or maximum over a requested axis, and calculating the pct_change of each value to the previous entry in the group. ',\n",
       " 'Resampling': 'The functions provided allow for grouping data by different criteria, such as group names or labels, and then performing various operations within each group. These operations include aggregating data using different functions, applying functions to each group, filling missing values, resampling data, interpolating values, computing statistics like count, mean, median, max, min, standard deviation, variance, sum, and more within each group. Additionally, functions are available to compute unique elements, quantiles, and other summary statistics within each group. ',\n",
       " 'Style': 'The Styler function provides a wide range of capabilities for styling DataFrames or Series in Python. It allows for styling with HTML and CSS, LaTeX, and text formats. Users can apply CSS-styling functions column-wise, row-wise, or table-wise, as well as to index or column headers. The function also enables users to format text display values, relabel index or column header keys, and combine multiple Stylers into a single table. Additional features include setting table styles, generating tooltips, adding CSS permanently, and applying styles elementwise. Users can also highlight missing values, minimum and maximum values, defined ranges, and values based on quantiles. The function supports gradient coloring for text and background, drawing bar charts in cell backgrounds, and exporting styles. Additionally, users can reset styles, set class attributes, table attributes, captions, and CSS properties for specific subsets. ',\n",
       " 'Plotting': '',\n",
       " 'Options and settings': 'The functions provided include one that prints the description for one or more registered options, another that retrieves the value of a specified option, a context manager for temporarily setting options in a with statement context, a function to reset one or more options to their default value, a function to set the value of a specified option, and a function to format float representation in a DataFrame with SI notation. These functions collectively offer a range of options for managing and manipulating settings and values within a program. ',\n",
       " 'Extensions': '',\n",
       " 'Testing': 'The functions in the list cover a wide range of error handling, warning messages, and exceptions that can occur when working with pandas. These include checking for equality between DataFrames, Indexes, Series, and ExtensionArrays, raising errors for bad syntax in SQL queries, encountering empty data or headers in CSV files, mismatched dimensions during indexing, invalid comparisons, unsupported operations on np.ndarrays, unsupported engine routines, unsupported clipboard functionality, and more. Additionally, warnings are raised for issues such as reading partial labeled Stata files, different dtypes in columns, timedelta values that cannot be represented, conflicts in index attributes, and more. Overall, these functions aim to provide informative messages to users when encountering various issues while working with pandas. ',\n",
       " 'Missing values': ''}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "class SummarizationPipeline(dspy.Module):\n",
    "    def __init__(self,parent_node,parent_text,MAX_WORDS:int=500):\n",
    "        self.parent_node = parent_node\n",
    "        self.parent_text = parent_text\n",
    "        self.summarization = dspy.Predict(SummarizationGeneration)\n",
    "        self.MAX_WORDS = MAX_WORDS\n",
    "    def __call__(self,*args,**kwargs):\n",
    "        return self.forward(*args,**kwargs)\n",
    "\n",
    "    def split_description(self):\n",
    "        split_s = []\n",
    "        running_num_words = 0\n",
    "        curr_func_string = \"\"\n",
    "        for txt in self.parent_text:\n",
    "            num_words = len(txt.split(\" \"))\n",
    "            running_num_words+=num_words\n",
    "            if running_num_words > self.MAX_WORDS:\n",
    "                running_num_words = num_words\n",
    "                split_s.append(curr_func_string)\n",
    "                curr_func_string = txt\n",
    "            else:\n",
    "                curr_func_string+=txt+\"\\n\"\n",
    "        if split_s == []:\n",
    "            split_s.append(curr_func_string)\n",
    "        return split_s\n",
    "    def forward(self):\n",
    "        if len(self.parent_text) == 0:\n",
    "            return \"\"\n",
    "        split_s = self.split_description()\n",
    "\n",
    "        summaries = \"\"\n",
    "        pbar = tqdm(total=len(split_s),desc=f\"For {self.parent_node}\")\n",
    "        for idx,desc in enumerate(split_s):\n",
    "            summaries+=self.summarization(function_descriptions=desc).summary\n",
    "            summaries+=\" \"\n",
    "            if idx%3 == 0 and idx>0:\n",
    "                print(\"Sleeping for 60 seconds\")\n",
    "                time.sleep(60)\n",
    "            pbar.update(1)\n",
    "        return summaries\n",
    "\n",
    "# parent_node = \"General functions\"\n",
    "# summ = SummarizationPipeline(parent_node,parent_text_dict[parent_node])\n",
    "# summary = summ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Plotting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Plotting: 100%|██████████| 1/1 [00:02<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Extensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Extensions: 100%|██████████| 1/1 [00:01<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing for Missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For Missing values: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "# parent_summary_dict = {k:\"\" for k in parent_text_dict}\n",
    "\n",
    "for parent in parent_text_dict:\n",
    "    if parent_summary_dict[parent]==\"\":\n",
    "        print(f\"Summarizing for {parent}\")\n",
    "        summ = SummarizationPipeline(parent,parent_text_dict[parent])\n",
    "        summary = summ()\n",
    "        parent_summary_dict[parent] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Serialize data into file:\n",
    "json.dump(parent_summary_dict, open( \"parent_summary_dict.json\", 'w' ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
