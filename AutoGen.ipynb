{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_agent import load_database as sklearn_load_database\n",
    "from pandas_agent import load_database as pandas_load_database\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pandas_database = pandas_load_database(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_database = sklearn_load_database(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_agent import SklearnAgentChroma\n",
    "from pandas_agent import PandasAgentChroma\n",
    "\n",
    "sklearn_chroma = SklearnAgentChroma(sklearn_database)\n",
    "pandas_chroma = PandasAgentChroma(pandas_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.isotonic: Section Navigation Determine whether y is monotonically correlated with x. y is found increasing or decreasing with respect to x based on a Spearman correlation test.\n",
      "\n",
      "sklearn.cross_decomposition: Section Navigation Canonical Correlation Analysis, also known as “Mode B” PLS. For a comparison between other cross decomposition algorithms, see Compare cross decomposition methods. Read more in the User Guide.\n",
      "\n",
      "sklearn.feature_selection: Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure. The cross correlation between each regressor and the target is computed as: For more on usage see the User Guide. Added in version 1.0.\n",
      "\n",
      " sklearn.feature_selection\n",
      "sklearn.feature_selection#defaults: Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure. The cross correlation between each regressor and the target is computed as: For more on usage see the User Guide. Added in version 1.0.\n",
      "\n",
      "sklearn.feature_selection#defaults: Section Navigation Univariate linear regression tests returning F-statistic and p-values. Quick linear model for testing the effect of a single regressor, sequentially for many regressors. This is done in 2 steps: The cross correlation between each regressor and the target is computed using r_regression as: It is converted to an F score and then to a p-value. f_regression is derived from r_regression and will rank features in the same order if all the features are positively correlated with the target. Note however that contrary to f_regression, r_regression values lie in [-1, 1] and can thus be negative. f_regression is therefore recommended as a feature selection criterion to identify potentially predictive feature for a downstream classifier, irrespective of the sign of the association with the target variable. Furthermore f_regression returns p-values while r_regression does not. Read more in the User Guide.\n",
      "\n",
      "sklearn.feature_selection#defaults: Section Navigation Compute the ANOVA F-value for the provided sample. Read more in the User Guide.\n",
      "\n",
      "\n",
      "sklearn.feature_selection#defaults\n",
      "{'function_trail': {'$eq': 'sklearn.feature_selection-->defaults'}}\n"
     ]
    }
   ],
   "source": [
    "funcs = sklearn_chroma(\"How to do correlation analysis?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sklearn.feature_selection.r_regression(X, y, *, center=True, force_finite=True)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs[0]['full_function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'name': 'r_regression', 'descriptions': 'Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors.\\\\nThis is a scoring function to be used in a feature selection procedure, not\\\\na free standing feature selection procedure. The cross correlation between each regressor and the target is computed\\\\nas: For more on usage see the User Guide. Added in version 1.0.', 'parameters': {'type': 'object', 'properties': {'X': {'type': 'array', 'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data matrix.\\\\n'}, 'y': {'type': 'array', 'description': 'array-like of shape (n_samples,). The target vector.\\\\n'}, 'center': {'type': 'boolean', 'description': 'bool, default=True. Whether or not to center the data matrix X and the target vector y.\\\\nBy default, X and y will be centered.\\\\n'}, 'force_finite': {'type': 'boolean', 'description': 'bool, default=True. Whether or not to force the Pearson’s R correlation to be finite.\\\\nIn the particular case where some features in X or the target y\\\\nare constant, the Pearson’s R correlation is not defined. When\\\\nforce_finite=False, a correlation of np.nan is returned to\\\\nacknowledge this case. When force_finite=True, this value will be\\\\nforced to a minimal correlation of 0.0.\\\\n\\\\nAdded in version 1.1.\\\\n\\\\n'}}, 'required': ['X', 'y']}}\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs[0]['function_calling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "\n",
    "sklearn_assistant = ConversableAgent(\n",
    "    name=\"SklearnAssistant\",\n",
    "    system_message=\"You are a helpful AI assistant that can write code. \"\n",
    "    \"You can help with writing code using the sklearn library. You will also be given a dummy full function that you need to refer when building the function\"\n",
    "    \"Return 'TERMINATE' when the task is done.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    ")\n",
    "\n",
    "user_proxy = ConversableAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import Tuple\n",
    "def sklearn_tool(query:str)->Tuple[str,dict]:\n",
    "    \"\"\"The sklearn tool will fetch the most relevant function definition that can answer the question that can be answered by the sklearn library. \n",
    "        It returns the full dummy function and function definition in JSON schema\n",
    "\n",
    "    Args:\n",
    "        query (str): query to the sklearn tool to fetch the most relevant function to answer the question\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str,dict]: The full dummy function and function definition in JSON schema\n",
    "    \"\"\"\n",
    "    sklearn_functions = sklearn_chroma(query)\n",
    "    if len(sklearn_functions) > 0:\n",
    "        return sklearn_functions[0]['full_function'],ast.literal_eval(sklearn_functions[0]['function_calling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.isotonic: Section Navigation Determine whether y is monotonically correlated with x. y is found increasing or decreasing with respect to x based on a Spearman correlation test.\n",
      "\n",
      "sklearn.cross_decomposition: Section Navigation Canonical Correlation Analysis, also known as “Mode B” PLS. For a comparison between other cross decomposition algorithms, see Compare cross decomposition methods. Read more in the User Guide.\n",
      "\n",
      "sklearn.feature_selection: Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure. The cross correlation between each regressor and the target is computed as: For more on usage see the User Guide. Added in version 1.0.\n",
      "\n",
      " sklearn.feature_selection\n",
      "sklearn.feature_selection#defaults: Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure. The cross correlation between each regressor and the target is computed as: For more on usage see the User Guide. Added in version 1.0.\n",
      "\n",
      "sklearn.feature_selection#defaults: Section Navigation Univariate linear regression tests returning F-statistic and p-values. Quick linear model for testing the effect of a single regressor, sequentially for many regressors. This is done in 2 steps: The cross correlation between each regressor and the target is computed using r_regression as: It is converted to an F score and then to a p-value. f_regression is derived from r_regression and will rank features in the same order if all the features are positively correlated with the target. Note however that contrary to f_regression, r_regression values lie in [-1, 1] and can thus be negative. f_regression is therefore recommended as a feature selection criterion to identify potentially predictive feature for a downstream classifier, irrespective of the sign of the association with the target variable. Furthermore f_regression returns p-values while r_regression does not. Read more in the User Guide.\n",
      "\n",
      "sklearn.feature_selection#defaults: Section Navigation Compute the ANOVA F-value for the provided sample. Read more in the User Guide.\n",
      "\n",
      "\n",
      "sklearn.feature_selection#defaults\n",
      "{'function_trail': {'$eq': 'sklearn.feature_selection-->defaults'}}\n"
     ]
    }
   ],
   "source": [
    "full_function,func_tool = sklearn_tool(\"How to do correlation analysis?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_assistant.llm_config['tools'] = func_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'r_regression',\n",
       " 'descriptions': 'Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors.\\nThis is a scoring function to be used in a feature selection procedure, not\\na free standing feature selection procedure. The cross correlation between each regressor and the target is computed\\nas: For more on usage see the User Guide. Added in version 1.0.',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'X': {'type': 'array',\n",
       "    'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data matrix.\\n'},\n",
       "   'y': {'type': 'array',\n",
       "    'description': 'array-like of shape (n_samples,). The target vector.\\n'},\n",
       "   'center': {'type': 'boolean',\n",
       "    'description': 'bool, default=True. Whether or not to center the data matrix X and the target vector y.\\nBy default, X and y will be centered.\\n'},\n",
       "   'force_finite': {'type': 'boolean',\n",
       "    'description': 'bool, default=True. Whether or not to force the Pearson’s R correlation to be finite.\\nIn the particular case where some features in X or the target y\\nare constant, the Pearson’s R correlation is not defined. When\\nforce_finite=False, a correlation of np.nan is returned to\\nacknowledge this case. When force_finite=True, this value will be\\nforced to a minimal correlation of 0.0.\\n\\nAdded in version 1.1.\\n\\n'}},\n",
       "  'required': ['X', 'y']}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_assistant.llm_config[\"tools\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "How to do correlation analysis?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_U8hjWEnnAO71s4TuiujuwVmW): sklearn_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"query\": \"correlation analysis in sklearn\"\n",
      "}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION sklearn_tool...\u001b[0m\n",
      "sklearn.feature_selection: Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure. The cross correlation between each regressor and the target is computed as: For more on usage see the User Guide. Added in version 1.0.\n",
      "\n",
      "sklearn.cross_decomposition: Section Navigation Canonical Correlation Analysis, also known as “Mode B” PLS. For a comparison between other cross decomposition algorithms, see Compare cross decomposition methods. Read more in the User Guide.\n",
      "\n",
      "sklearn.isotonic: Section Navigation Determine whether y is monotonically correlated with x. y is found increasing or decreasing with respect to x based on a Spearman correlation test.\n",
      "\n",
      " sklearn.feature_selection; sklearn.cross_decomposition\n",
      "sklearn.feature_selection#defaults: Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure. The cross correlation between each regressor and the target is computed as: For more on usage see the User Guide. Added in version 1.0.\n",
      "\n",
      "sklearn.feature_selection#defaults: Section Navigation Univariate linear regression tests returning F-statistic and p-values. Quick linear model for testing the effect of a single regressor, sequentially for many regressors. This is done in 2 steps: The cross correlation between each regressor and the target is computed using r_regression as: It is converted to an F score and then to a p-value. f_regression is derived from r_regression and will rank features in the same order if all the features are positively correlated with the target. Note however that contrary to f_regression, r_regression values lie in [-1, 1] and can thus be negative. f_regression is therefore recommended as a feature selection criterion to identify potentially predictive feature for a downstream classifier, irrespective of the sign of the association with the target variable. Furthermore f_regression returns p-values while r_regression does not. Read more in the User Guide.\n",
      "\n",
      "sklearn.feature_selection#defaults: Section Navigation Compute chi-squared stats between each non-negative feature and class. This score can be used to select the n_features features with the highest values for the test chi-squared statistic from X, which must contain only non-negative features such as booleans or frequencies (e.g., term counts in document classification), relative to the classes. Recall that the chi-square test measures dependence between stochastic variables, so using this function “weeds out” the features that are the most likely to be independent of class and therefore irrelevant for classification. Read more in the User Guide.\n",
      "\n",
      "\n",
      "sklearn.feature_selection#defaults\n",
      "{'$or': [{'function_trail': {'$eq': 'sklearn.feature_selection-->defaults'}}, {'function_trail': {'$eq': ' sklearn.cross_decomposition-->defaults'}}]}\n",
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_U8hjWEnnAO71s4TuiujuwVmW) *****\u001b[0m\n",
      "[\"sklearn.feature_selection.r_regression(X, y, *, center=True, force_finite=True)\", {\"name\": \"r_regression\", \"descriptions\": \"Section Navigation Compute Pearson\\u2019s r for each features and the target. Pearson\\u2019s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors.\\nThis is a scoring function to be used in a feature selection procedure, not\\na free standing feature selection procedure. The cross correlation between each regressor and the target is computed\\nas: For more on usage see the User Guide. Added in version 1.0.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"X\": {\"type\": \"array\", \"description\": \"{array-like, sparse matrix} of shape (n_samples, n_features). The data matrix.\\n\"}, \"y\": {\"type\": \"array\", \"description\": \"array-like of shape (n_samples,). The target vector.\\n\"}, \"center\": {\"type\": \"boolean\", \"description\": \"bool, default=True. Whether or not to center the data matrix X and the target vector y.\\nBy default, X and y will be centered.\\n\"}, \"force_finite\": {\"type\": \"boolean\", \"description\": \"bool, default=True. Whether or not to force the Pearson\\u2019s R correlation to be finite.\\nIn the particular case where some features in X or the target y\\nare constant, the Pearson\\u2019s R correlation is not defined. When\\nforce_finite=False, a correlation of np.nan is returned to\\nacknowledge this case. When force_finite=True, this value will be\\nforced to a minimal correlation of 0.0.\\n\\nAdded in version 1.1.\\n\\n\"}}, \"required\": [\"X\", \"y\"]}}]\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "To perform correlation analysis in sklearn, you can use the `sklearn.feature_selection.r_regression` function. This function computes Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. This is a scoring function to be used in a feature selection procedure.\n",
      "\n",
      "Here is an example of how to use this function:\n",
      "\n",
      "```python\n",
      "from sklearn.feature_selection import r_regression\n",
      "\n",
      "# X is your feature matrix, y is your target vector\n",
      "correlation_scores = r_regression(X, y, center=True, force_finite=True)\n",
      "```\n",
      "\n",
      "In the above function:\n",
      "\n",
      "- `X` is the data matrix. It should be array-like or sparse matrix of shape (n_samples, n_features).\n",
      "- `y` is the target vector. It should be array-like of shape (n_samples,).\n",
      "- `center` (default=True) indicates whether or not to center the data matrix `X` and the target vector `y`. By default, `X` and `y` will be centered.\n",
      "- `force_finite` (default=True) indicates whether or not to force the Pearson’s R correlation to be finite. When `force_finite=False`, a correlation of np.nan is returned to acknowledge this case. When `force_finite=True`, this value will be forced to a minimal correlation of 0.0. This parameter was added in version 1.1.\n",
      "\n",
      "After you call the above function, `correlation_scores` will contain the correlation scores for each feature in `X` with the target `y`.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat_result = user_proxy.initiate_chat(sklearn_assistant, message=\"How to do correlation analysis?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "How to do SVM with a large dataset?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "Here is a function that does SVM with a large dataset using `LinearSVC` from `sklearn.svm`. `LinearSVC` is more suitable than `SVC` with `kernel='linear'` for large datasets because it scales linearly with the number of data points and can be faster to run. \n",
      "\n",
      "In the code below, X is the features and y is the labels. They should both be numpy arrays or pandas dataframes. The function fits the SVM model to the data and returns the trained model.\n",
      "\n",
      "```python\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "def train_svm(X, y, C=1.0):\n",
      "    svm = LinearSVC(C=C)\n",
      "    svm.fit(X, y)\n",
      "    return svm\n",
      "```\n",
      "\n",
      "To use the model for prediction, you can use:\n",
      "\n",
      "```python\n",
      "def predict(svm, X):\n",
      "    return svm.predict(X)\n",
      "```\n",
      "\n",
      "Note that `C` is a regularization parameter. A smaller `C` encourages a larger margin, hence a simpler decision function, at the cost of training accuracy. In other words `C` behaves as the inverse of the regularization strength.\n",
      "Please keep in mind, though, that LinearSVC doesn't provide precise probability estimates as SVC does. If needed, you should consider a different approach such as using SGDClassifier with `loss='log'` that applies logistic regression, or `loss='modified_huber'` that enables to provides a poor man’s probability estimates.\n",
      "Make sure to scale your data before running SVM. SVMs are not scale invariant, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1.\n",
      "You can use `StandardScaler` from `sklearn.preprocessing` for standardization.\n",
      "\n",
      "```python\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler = StandardScaler()\n",
      "X = scaler.fit_transform(X)\n",
      "```\n",
      "Remember to scale the test set in the same way as the training set.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_Sv1DgktQbBrrxvyGrnBke2EQ): sklearn_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"query\": \"SVM with a large dataset\"\n",
      "}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION sklearn_tool...\u001b[0m\n",
      "sklearn.svm: Section Navigation C-Support Vector Classification. The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using LinearSVC or SGDClassifier instead, possibly after a Nystroem transformer or other Kernel Approximation. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions. To learn how to tune SVC’s hyperparameters, see the following example: Nested versus non-nested cross-validation Read more in the User Guide.\n",
      "\n",
      "sklearn.datasets: Section Navigation Dump the dataset in svmlight / libsvm file format. This format is a text-based format, with one sample per line. It does not store zero valued features hence is suitable for sparse dataset. The first element of each line can be used to store a target variable to predict.\n",
      "\n",
      "sklearn.svm: Section Navigation Epsilon-Support Vector Regression. The free parameters in the model are C and epsilon. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets consider using LinearSVR or SGDRegressor instead, possibly after a Nystroem transformer or other Kernel Approximation. Read more in the User Guide.\n",
      "\n",
      " sklearn.svm\n",
      "sklearn.svm#defaults: Section Navigation C-Support Vector Classification. The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using LinearSVC or SGDClassifier instead, possibly after a Nystroem transformer or other Kernel Approximation. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions. To learn how to tune SVC’s hyperparameters, see the following example: Nested versus non-nested cross-validation Read more in the User Guide.\n",
      "\n",
      "sklearn.svm#defaults: Section Navigation Epsilon-Support Vector Regression. The free parameters in the model are C and epsilon. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets consider using LinearSVR or SGDRegressor instead, possibly after a Nystroem transformer or other Kernel Approximation. Read more in the User Guide.\n",
      "\n",
      "sklearn.svm#defaults: Section Navigation Linear Support Vector Classification. Similar to SVC with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. The main differences between LinearSVC and SVC lie in the loss function used by default, and in the handling of intercept regularization between those two implementations. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme. Read more in the User Guide.\n",
      "\n",
      "\n",
      "sklearn.svm#defaults\n",
      "{'function_trail': {'$eq': 'sklearn.svm-->defaults'}}\n",
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_Sv1DgktQbBrrxvyGrnBke2EQ) *****\u001b[0m\n",
      "[\"class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\", {\"name\": \"SVC\", \"descriptions\": \"Section Navigation C-Support Vector Classification. The implementation is based on libsvm. The fit time scales at least\\nquadratically with the number of samples and may be impractical\\nbeyond tens of thousands of samples. For large datasets\\nconsider using LinearSVC or\\nSGDClassifier instead, possibly after a\\nNystroem transformer or\\nother Kernel Approximation. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided\\nkernel functions and how gamma, coef0 and degree affect each\\nother, see the corresponding section in the narrative documentation:\\nKernel functions. To learn how to tune SVC\\u2019s hyperparameters, see the following example:\\nNested versus non-nested cross-validation Read more in the User Guide.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"sample_weight\": {\"type\": \"string\", \"description\": \"str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n\"}}, \"required\": []}}]\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "When dealing with a large dataset, we should consider using `LinearSVC` or `SGDClassifier` instead of `SVC` because `SVC`'s fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. \n",
      "\n",
      "Here are functions to train a SVM model with `LinearSVC` and `SGDClassifier`. \n",
      "\n",
      "For `LinearSVC`:\n",
      "```python\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "def train_svm_lin(X, y, C=1.0):\n",
      "    scaler = StandardScaler()\n",
      "    X = scaler.fit_transform(X)\n",
      "    \n",
      "    svm = LinearSVC(C=C)\n",
      "    svm.fit(X, y)\n",
      "    return svm, scaler\n",
      "```\n",
      "For `SGDClassifier`:\n",
      "```python\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "def train_svm_sgd(X, y, alpha=0.0001, max_iter=1000, tol=0.001):\n",
      "    scaler = StandardScaler()\n",
      "    X = scaler.fit_transform(X)\n",
      "    \n",
      "    svm = SGDClassifier(alpha=alpha, max_iter=max_iter, tol=tol)\n",
      "    svm.fit(X, y)\n",
      "    return svm, scaler\n",
      "```\n",
      "Note:\n",
      "- `C` is a regularization parameter for `LinearSVC`. A smaller `C` encourages a larger margin, hence, a simpler decision function, at the cost of training accuracy. In other words, `C` behaves as the inverse of the regularization strength.\n",
      "- `alpha` is a regularization parameter for `SGDClassifier`, it works same as `C` but its role is opposite (Bigger alpha means more regularization).\n",
      "- `max_iter` and `tol` are stopping criteria parameters. The iterations will stop when `(loss > previous_loss - tol)` for `max_iter` consecutive epochs.\n",
      "- Always remember to scale your data before running SVM as SVMs are not scale invariant.\n",
      "- The same scaler that was used for training set should also be used for scaling the test set.\n",
      "- `LinearSVC` will not provide precise probability estimates as `SVC` with `kernel='linear'` does. If you need those, you might want to use `SGDClassifier` with `loss='log'` or `loss='modified_huber'`.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "To make predictions with the trained models, we can use the following functions:\n",
      "```python\n",
      "def predict(svm, scaler, X):\n",
      "    X = scaler.transform(X)\n",
      "    return svm.predict(X)\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "Let's summarize everything in one function: \n",
      "```python\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "def train_and_predict(X_train, y_train, X_test, model_type='sgd', C=1.0, alpha=0.0001, max_iter=1000, tol=0.001):\n",
      "    # Define the scaler\n",
      "    scaler = StandardScaler()\n",
      "    X_train = scaler.fit_transform(X_train)\n",
      "    \n",
      "    # Define the model\n",
      "    if model_type == 'sgd':\n",
      "        model = SGDClassifier(alpha=alpha, max_iter=max_iter, tol=tol)\n",
      "    elif model_type == 'linear':\n",
      "        model = LinearSVC(C=C)\n",
      "    \n",
      "    # Fit the model\n",
      "    model.fit(X_train, y_train)\n",
      "    \n",
      "    # Scale the test set\n",
      "    X_test = scaler.transform(X_test)\n",
      "    \n",
      "    # Make predictions\n",
      "    predictions = model.predict(X_test)\n",
      "    \n",
      "    # Return the predictions\n",
      "    return predictions\n",
      "\n",
      "# You can use this function like this:\n",
      "# predictions = train_and_predict(X_train, y_train, X_test, model_type='sgd')\n",
      "```\n",
      "\n",
      "In this function, you can specify the model type (`sgd` or `linear`) when calling the function. The function will train the specified model with the training data and make predictions on the test data. \n",
      "\n",
      "Here, we assume that the `X_train`, `y_train`, and `X_test` are pandas DataFrame or numpy arrays. `model_type` is a string indicating the model type, `C` is the regularization parameter for `LinearSVC`, `alpha` is the regularization parameter for `SGDClassifier`, `max_iter` is the maximum number of passes over the training data (aka epochs) and `tol` is the stopping criteria. \n",
      "\n",
      "Remember that SVM is not scale invariant. This means your test data has to be scaled in the same way as your training data. Here the same `scaler` is used to scale the test set.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "I'm glad to have been of assistance. Do you have any further questions on this matter?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat_result = user_proxy.initiate_chat(sklearn_assistant, message=\"How to do SVM with a large dataset?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def train_and_predict(X_train, y_train, X_test, model_type='sgd', C=1.0, alpha=0.0001, max_iter=1000, tol=0.001):\n",
    "    # Define the scaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Define the model\n",
    "    if model_type == 'sgd':\n",
    "        model = SGDClassifier(alpha=alpha, max_iter=max_iter, tol=tol)\n",
    "    elif model_type == 'linear':\n",
    "        model = LinearSVC(C=C)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Scale the test set\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Return the predictions\n",
    "    return predictions\n",
    "\n",
    "# You can use this function like this:\n",
    "# predictions = train_and_predict(X_train, y_train, X_test, model_type='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
