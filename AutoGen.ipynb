{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_agent import scrape_pandas_website\n",
    "# scrape_pandas_website()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PANDAS DATA VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"pandas_agent/data/pandas_function_openai.json\",\"r\") as f:\n",
    "#     data = json.load(f)\n",
    "# for parent in data:\n",
    "#     parent_data = data[parent]\n",
    "#     for sub_level in parent_data[\"functions\"]:\n",
    "#         for _, sub_level_funcs in sub_level.items():\n",
    "#             for sub_level_func in sub_level_funcs:\n",
    "#                 fcalling = sub_level_func['function_calling']\n",
    "#                 if 'parameters' in fcalling:\n",
    "#                     props = fcalling['parameters']['properties']\n",
    "#                     for k,v in props.items():\n",
    "#                         if v['type'] not in ['boolean', 'array', 'object', 'integer', 'string', 'number','null']:\n",
    "#                             if \"scalar\" in v['type'] or \"datetime\" in v['type'] or \"timedelta\" in v['type'] or \"function\" in v['type'] or \"label\" in v['type'] or \"tuple\" in v['type'] or \"Index\" in v['type'] or \"size \" in v['type'] or \"Any\" in v['type'] or v['type'] in [\"None\",\"optional\"] or \"name\" in v['type'] or 'name' in v['type']:\n",
    "#                                 pass\n",
    "#                             else:\n",
    "#                                 print(f\"{k}-->{v['type']}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLEARN DATA VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:35<00:00,  3.00s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sklearn.html': {'functions': [{'defaults': [{'func_name': 'config_context',\n",
       "      'func_desc': 'Context manager for global scikit-learn configuration.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.config_context.html#sklearn.config_context',\n",
       "      'function_definitions': {'function_name': 'config_context',\n",
       "       'full_function': 'sklearn.config_context(*, assume_finite=None, working_memory=None, print_changed_only=None, display=None, pairwise_dist_chunk_size=None, enable_cython_pairwise_dist=None, array_api_dispatch=None, transform_output=None, enable_metadata_routing=None, skip_parameter_validation=None)',\n",
       "       'function_text': 'Context manager for global scikit-learn configuration.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/array_api.html#array-api',\n",
       "       'parameter_names_desc': [{'param_name': 'assume_finite',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'If True, validation for finiteness will be skipped,\\nsaving time, but leading to potential crashes. If\\nFalse, validation for finiteness will be performed,\\navoiding error. If None, the existing value won’t change.\\nThe default value is False.\\n'},\n",
       "        {'param_name': 'working_memory',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'If set, scikit-learn will attempt to limit the size of temporary arrays\\nto this number of MiB (per job when parallelised), often saving both\\ncomputation time and memory on expensive operations that can be\\nperformed in chunks. If None, the existing value won’t change.\\nThe default value is 1024.\\n'},\n",
       "        {'param_name': 'print_changed_only',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'If True, only the parameters that were set to non-default\\nvalues will be printed when printing an estimator. For example,\\nprint(SVC()) while True will only print ‘SVC()’, but would print\\n‘SVC(C=1.0, cache_size=200, …)’ with all the non-changed parameters\\nwhen False. If None, the existing value won’t change.\\nThe default value is True.\\n\\nChanged in version 0.23: Default changed from False to True.\\n\\n'},\n",
       "        {'param_name': 'display',\n",
       "         'param_type': '{‘text’, ‘diagram’}, default=None',\n",
       "         'param_desc': 'If ‘diagram’, estimators will be displayed as a diagram in a Jupyter\\nlab or notebook context. If ‘text’, estimators will be displayed as\\ntext. If None, the existing value won’t change.\\nThe default value is ‘diagram’.\\n\\nAdded in version 0.23.\\n\\n'},\n",
       "        {'param_name': 'pairwise_dist_chunk_size',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of row vectors per chunk for the accelerated pairwise-\\ndistances reduction backend. Default is 256 (suitable for most of\\nmodern laptops’ caches and architectures).\\nIntended for easier benchmarking and testing of scikit-learn internals.\\nEnd users are not expected to benefit from customizing this configuration\\nsetting.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "        {'param_name': 'enable_cython_pairwise_dist',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'Use the accelerated pairwise-distances reduction backend when\\npossible. Global default: True.\\nIntended for easier benchmarking and testing of scikit-learn internals.\\nEnd users are not expected to benefit from customizing this configuration\\nsetting.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "        {'param_name': 'array_api_dispatch',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'Use Array API dispatching when inputs follow the Array API standard.\\nDefault is False.\\nSee the User Guide for more details.\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "        {'param_name': 'transform_output',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'Configure output of transform and fit_transform.\\nSee Introducing the set_output API\\nfor an example on how to use the API.\\n\\n\"default\": Default output format of a transformer\\n\"pandas\": DataFrame output\\n\"polars\": Polars output\\nNone: Transform configuration is unchanged\\n\\n\\nAdded in version 1.2.\\n\\n\\nAdded in version 1.4: \"polars\" option was added.\\n\\n'},\n",
       "        {'param_name': 'enable_metadata_routing',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'Enable metadata routing. By default this feature is disabled.\\nRefer to metadata routing user guide for more\\ndetails.\\n\\nTrue: Metadata routing is enabled\\nFalse: Metadata routing is disabled, use the old syntax.\\nNone: Configuration is unchanged\\n\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "        {'param_name': 'skip_parameter_validation',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'If True, disable the validation of the hyper-parameters’ types and values in\\nthe fit method of estimators and for arguments passed to public helper\\nfunctions. It can save time in some situations but can lead to low level\\ncrashes and exceptions with confusing error messages.\\nNote that for data parameters, such as X and y, only type validation is\\nskipped but validation with check_array will continue to run.\\n\\nAdded in version 1.3.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'config_context',\n",
       "       'descriptions': 'Context manager for global scikit-learn configuration.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'assume_finite': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. If True, validation for finiteness will be skipped,\\nsaving time, but leading to potential crashes. If\\nFalse, validation for finiteness will be performed,\\navoiding error. If None, the existing value won’t change.\\nThe default value is False.\\n'},\n",
       "         'working_memory': {'type': 'integer',\n",
       "          'description': 'int, default=None. If set, scikit-learn will attempt to limit the size of temporary arrays\\nto this number of MiB (per job when parallelised), often saving both\\ncomputation time and memory on expensive operations that can be\\nperformed in chunks. If None, the existing value won’t change.\\nThe default value is 1024.\\n'},\n",
       "         'print_changed_only': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. If True, only the parameters that were set to non-default\\nvalues will be printed when printing an estimator. For example,\\nprint(SVC()) while True will only print ‘SVC()’, but would print\\n‘SVC(C=1.0, cache_size=200, …)’ with all the non-changed parameters\\nwhen False. If None, the existing value won’t change.\\nThe default value is True.\\n\\nChanged in version 0.23: Default changed from False to True.\\n\\n'},\n",
       "         'display': {'type': 'string',\n",
       "          'enum': ['text', 'diagram'],\n",
       "          'description': '{‘text’, ‘diagram’}, default=None. If ‘diagram’, estimators will be displayed as a diagram in a Jupyter\\nlab or notebook context. If ‘text’, estimators will be displayed as\\ntext. If None, the existing value won’t change.\\nThe default value is ‘diagram’.\\n\\nAdded in version 0.23.\\n\\n'},\n",
       "         'pairwise_dist_chunk_size': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of row vectors per chunk for the accelerated pairwise-\\ndistances reduction backend. Default is 256 (suitable for most of\\nmodern laptops’ caches and architectures).\\nIntended for easier benchmarking and testing of scikit-learn internals.\\nEnd users are not expected to benefit from customizing this configuration\\nsetting.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "         'enable_cython_pairwise_dist': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. Use the accelerated pairwise-distances reduction backend when\\npossible. Global default: True.\\nIntended for easier benchmarking and testing of scikit-learn internals.\\nEnd users are not expected to benefit from customizing this configuration\\nsetting.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "         'array_api_dispatch': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. Use Array API dispatching when inputs follow the Array API standard.\\nDefault is False.\\nSee the User Guide for more details.\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "         'transform_output': {'type': 'string',\n",
       "          'description': 'str, default=None. Configure output of transform and fit_transform.\\nSee Introducing the set_output API\\nfor an example on how to use the API.\\n\\n\"default\": Default output format of a transformer\\n\"pandas\": DataFrame output\\n\"polars\": Polars output\\nNone: Transform configuration is unchanged\\n\\n\\nAdded in version 1.2.\\n\\n\\nAdded in version 1.4: \"polars\" option was added.\\n\\n'},\n",
       "         'enable_metadata_routing': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. Enable metadata routing. By default this feature is disabled.\\nRefer to metadata routing user guide for more\\ndetails.\\n\\nTrue: Metadata routing is enabled\\nFalse: Metadata routing is disabled, use the old syntax.\\nNone: Configuration is unchanged\\n\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "         'skip_parameter_validation': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. If True, disable the validation of the hyper-parameters’ types and values in\\nthe fit method of estimators and for arguments passed to public helper\\nfunctions. It can save time in some situations but can lead to low level\\ncrashes and exceptions with confusing error messages.\\nNote that for data parameters, such as X and y, only type validation is\\nskipped but validation with check_array will continue to run.\\n\\nAdded in version 1.3.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'set_config',\n",
       "      'func_desc': 'Set global scikit-learn configuration.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html#sklearn.set_config',\n",
       "      'function_definitions': {'function_name': 'set_config',\n",
       "       'full_function': 'sklearn.set_config(assume_finite=None, working_memory=None, print_changed_only=None, display=None, pairwise_dist_chunk_size=None, enable_cython_pairwise_dist=None, array_api_dispatch=None, transform_output=None, enable_metadata_routing=None, skip_parameter_validation=None)',\n",
       "       'function_text': 'Set global scikit-learn configuration. Added in version 0.19.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/array_api.html#array-api',\n",
       "       'parameter_names_desc': [{'param_name': 'assume_finite',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'If True, validation for finiteness will be skipped,\\nsaving time, but leading to potential crashes. If\\nFalse, validation for finiteness will be performed,\\navoiding error.  Global default: False.\\n\\nAdded in version 0.19.\\n\\n'},\n",
       "        {'param_name': 'working_memory',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'If set, scikit-learn will attempt to limit the size of temporary arrays\\nto this number of MiB (per job when parallelised), often saving both\\ncomputation time and memory on expensive operations that can be\\nperformed in chunks. Global default: 1024.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'print_changed_only',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'If True, only the parameters that were set to non-default\\nvalues will be printed when printing an estimator. For example,\\nprint(SVC()) while True will only print ‘SVC()’ while the default\\nbehaviour would be to print ‘SVC(C=1.0, cache_size=200, …)’ with\\nall the non-changed parameters.\\n\\nAdded in version 0.21.\\n\\n'},\n",
       "        {'param_name': 'display',\n",
       "         'param_type': '{‘text’, ‘diagram’}, default=None',\n",
       "         'param_desc': 'If ‘diagram’, estimators will be displayed as a diagram in a Jupyter\\nlab or notebook context. If ‘text’, estimators will be displayed as\\ntext. Default is ‘diagram’.\\n\\nAdded in version 0.23.\\n\\n'},\n",
       "        {'param_name': 'pairwise_dist_chunk_size',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of row vectors per chunk for the accelerated pairwise-\\ndistances reduction backend. Default is 256 (suitable for most of\\nmodern laptops’ caches and architectures).\\nIntended for easier benchmarking and testing of scikit-learn internals.\\nEnd users are not expected to benefit from customizing this configuration\\nsetting.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "        {'param_name': 'enable_cython_pairwise_dist',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'Use the accelerated pairwise-distances reduction backend when\\npossible. Global default: True.\\nIntended for easier benchmarking and testing of scikit-learn internals.\\nEnd users are not expected to benefit from customizing this configuration\\nsetting.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "        {'param_name': 'array_api_dispatch',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'Use Array API dispatching when inputs follow the Array API standard.\\nDefault is False.\\nSee the User Guide for more details.\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "        {'param_name': 'transform_output',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'Configure output of transform and fit_transform.\\nSee Introducing the set_output API\\nfor an example on how to use the API.\\n\\n\"default\": Default output format of a transformer\\n\"pandas\": DataFrame output\\n\"polars\": Polars output\\nNone: Transform configuration is unchanged\\n\\n\\nAdded in version 1.2.\\n\\n\\nAdded in version 1.4: \"polars\" option was added.\\n\\n'},\n",
       "        {'param_name': 'enable_metadata_routing',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'Enable metadata routing. By default this feature is disabled.\\nRefer to metadata routing user guide for more\\ndetails.\\n\\nTrue: Metadata routing is enabled\\nFalse: Metadata routing is disabled, use the old syntax.\\nNone: Configuration is unchanged\\n\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "        {'param_name': 'skip_parameter_validation',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'If True, disable the validation of the hyper-parameters’ types and values in\\nthe fit method of estimators and for arguments passed to public helper\\nfunctions. It can save time in some situations but can lead to low level\\ncrashes and exceptions with confusing error messages.\\nNote that for data parameters, such as X and y, only type validation is\\nskipped but validation with check_array will continue to run.\\n\\nAdded in version 1.3.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'set_config',\n",
       "       'descriptions': 'Set global scikit-learn configuration. Added in version 0.19.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'assume_finite': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. If True, validation for finiteness will be skipped,\\nsaving time, but leading to potential crashes. If\\nFalse, validation for finiteness will be performed,\\navoiding error.  Global default: False.\\n\\nAdded in version 0.19.\\n\\n'},\n",
       "         'working_memory': {'type': 'integer',\n",
       "          'description': 'int, default=None. If set, scikit-learn will attempt to limit the size of temporary arrays\\nto this number of MiB (per job when parallelised), often saving both\\ncomputation time and memory on expensive operations that can be\\nperformed in chunks. Global default: 1024.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'print_changed_only': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. If True, only the parameters that were set to non-default\\nvalues will be printed when printing an estimator. For example,\\nprint(SVC()) while True will only print ‘SVC()’ while the default\\nbehaviour would be to print ‘SVC(C=1.0, cache_size=200, …)’ with\\nall the non-changed parameters.\\n\\nAdded in version 0.21.\\n\\n'},\n",
       "         'display': {'type': 'string',\n",
       "          'enum': ['text', 'diagram'],\n",
       "          'description': '{‘text’, ‘diagram’}, default=None. If ‘diagram’, estimators will be displayed as a diagram in a Jupyter\\nlab or notebook context. If ‘text’, estimators will be displayed as\\ntext. Default is ‘diagram’.\\n\\nAdded in version 0.23.\\n\\n'},\n",
       "         'pairwise_dist_chunk_size': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of row vectors per chunk for the accelerated pairwise-\\ndistances reduction backend. Default is 256 (suitable for most of\\nmodern laptops’ caches and architectures).\\nIntended for easier benchmarking and testing of scikit-learn internals.\\nEnd users are not expected to benefit from customizing this configuration\\nsetting.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "         'enable_cython_pairwise_dist': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. Use the accelerated pairwise-distances reduction backend when\\npossible. Global default: True.\\nIntended for easier benchmarking and testing of scikit-learn internals.\\nEnd users are not expected to benefit from customizing this configuration\\nsetting.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "         'array_api_dispatch': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. Use Array API dispatching when inputs follow the Array API standard.\\nDefault is False.\\nSee the User Guide for more details.\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "         'transform_output': {'type': 'string',\n",
       "          'description': 'str, default=None. Configure output of transform and fit_transform.\\nSee Introducing the set_output API\\nfor an example on how to use the API.\\n\\n\"default\": Default output format of a transformer\\n\"pandas\": DataFrame output\\n\"polars\": Polars output\\nNone: Transform configuration is unchanged\\n\\n\\nAdded in version 1.2.\\n\\n\\nAdded in version 1.4: \"polars\" option was added.\\n\\n'},\n",
       "         'enable_metadata_routing': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. Enable metadata routing. By default this feature is disabled.\\nRefer to metadata routing user guide for more\\ndetails.\\n\\nTrue: Metadata routing is enabled\\nFalse: Metadata routing is disabled, use the old syntax.\\nNone: Configuration is unchanged\\n\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "         'skip_parameter_validation': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. If True, disable the validation of the hyper-parameters’ types and values in\\nthe fit method of estimators and for arguments passed to public helper\\nfunctions. It can save time in some situations but can lead to low level\\ncrashes and exceptions with confusing error messages.\\nNote that for data parameters, such as X and y, only type validation is\\nskipped but validation with check_array will continue to run.\\n\\nAdded in version 1.3.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'get_config',\n",
       "      'func_desc': 'Retrieve current values for configuration set by set_config.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.get_config.html#sklearn.get_config',\n",
       "      'function_definitions': {'function_name': 'get_config',\n",
       "       'full_function': 'sklearn.get_config()',\n",
       "       'function_text': 'Retrieve current values for configuration set by set_config.',\n",
       "       'func_text_user_guide': 'sklearn.set_config.html#sklearn.set_config',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'show_versions',\n",
       "      'func_desc': 'Print useful debugging information\"',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.show_versions.html#sklearn.show_versions',\n",
       "      'function_definitions': {'function_name': 'show_versions',\n",
       "       'full_function': 'sklearn.show_versions()',\n",
       "       'function_text': 'Print useful debugging information” Added in version 0.20. Examples',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}}]}],\n",
       "  'name': 'sklearn',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.html'},\n",
       " 'sklearn.base.html': {'functions': [{'defaults': [{'func_name': 'BaseEstimator',\n",
       "      'func_desc': 'Base class for all estimators in scikit-learn.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator',\n",
       "      'function_definitions': {'function_name': 'BaseEstimator',\n",
       "       'full_function': 'class sklearn.base.BaseEstimator',\n",
       "       'function_text': 'Base class for all estimators in scikit-learn. Inheriting from this class provides default implementations of: setting and getting parameters used by GridSearchCV and friends; textual and HTML representation displayed in terminals and IDEs; estimator serialization; parameters validation; data validation; feature names validation. Read more in the User Guide. Notes All estimators should specify all the parameters that can be set\\nat the class level in their __init__ as explicit keyword\\narguments (no *args or **kwargs). Examples Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/developers/develop.html#rolling-your-own-estimator',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'ClassNamePrefixFeaturesOutMixin',\n",
       "      'func_desc': 'Mixin class for transformers that generate their own names by prefixing.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.ClassNamePrefixFeaturesOutMixin.html#sklearn.base.ClassNamePrefixFeaturesOutMixin',\n",
       "      'function_definitions': {'function_name': 'ClassNamePrefixFeaturesOutMixin',\n",
       "       'full_function': 'class sklearn.base.ClassNamePrefixFeaturesOutMixin',\n",
       "       'function_text': 'Mixin class for transformers that generate their own names by prefixing. This mixin is useful when the transformer needs to generate its own feature\\nnames out, such as PCA. For example, if\\nPCA outputs 3 features, then the generated feature\\nnames out are: [\"pca0\", \"pca1\", \"pca2\"]. This mixin assumes that a _n_features_out attribute is defined when the\\ntransformer is fitted. _n_features_out is the number of output features\\nthat the transformer will return in transform of fit_transform. Examples Get output feature names for transformation. The feature names out will prefixed by the lowercased class name. For\\nexample, if the transformer outputs 3 features, then the feature names\\nout are: [\"class_name0\", \"class_name1\", \"class_name2\"].',\n",
       "       'func_text_user_guide': 'sklearn.decomposition.PCA.html#sklearn.decomposition.PCA',\n",
       "       'parameter_names_desc': [{'param_name': 'input_features',\n",
       "         'param_type': 'array-like of str or None, default=None',\n",
       "         'param_desc': 'Only used to validate feature names with the names seen in fit.\\n'}]},\n",
       "      'function_calling': {'name': 'ClassNamePrefixFeaturesOutMixin',\n",
       "       'descriptions': 'Mixin class for transformers that generate their own names by prefixing. This mixin is useful when the transformer needs to generate its own feature\\nnames out, such as PCA. For example, if\\nPCA outputs 3 features, then the generated feature\\nnames out are: [\"pca0\", \"pca1\", \"pca2\"]. This mixin assumes that a _n_features_out attribute is defined when the\\ntransformer is fitted. _n_features_out is the number of output features\\nthat the transformer will return in transform of fit_transform. Examples Get output feature names for transformation. The feature names out will prefixed by the lowercased class name. For\\nexample, if the transformer outputs 3 features, then the feature names\\nout are: [\"class_name0\", \"class_name1\", \"class_name2\"].',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'input_features': {'type': 'string',\n",
       "          'description': 'array-like of str or None, default=None. Only used to validate feature names with the names seen in fit.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'ClusterMixin',\n",
       "      'func_desc': 'Mixin class for all cluster estimators in scikit-learn.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.ClusterMixin.html#sklearn.base.ClusterMixin',\n",
       "      'function_definitions': {'function_name': 'ClusterMixin',\n",
       "       'full_function': 'class sklearn.base.ClusterMixin',\n",
       "       'function_text': 'Mixin class for all cluster estimators in scikit-learn. _estimator_type class attribute defaulting to \"clusterer\"; fit_predict method returning the cluster labels associated to each sample. Examples Perform clustering on X and returns cluster labels.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Input data.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'Ignored',\n",
       "         'param_desc': 'Not used, present for API consistency by convention.\\n'}]},\n",
       "      'function_calling': {'name': 'ClusterMixin',\n",
       "       'descriptions': 'Mixin class for all cluster estimators in scikit-learn. _estimator_type class attribute defaulting to \"clusterer\"; fit_predict method returning the cluster labels associated to each sample. Examples Perform clustering on X and returns cluster labels.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Input data.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'Ignored. Not used, present for API consistency by convention.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'MetaEstimatorMixin',\n",
       "      'func_desc': 'Mixin class for all meta estimators in scikit-learn.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.MetaEstimatorMixin.html#sklearn.base.MetaEstimatorMixin',\n",
       "      'function_definitions': {'function_name': 'MetaEstimatorMixin',\n",
       "       'full_function': 'class sklearn.base.MetaEstimatorMixin',\n",
       "       'function_text': 'Mixin class for all meta estimators in scikit-learn. This mixin defines the following functionality: define _required_parameters that specify the mandatory estimator parameter. Examples',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'OutlierMixin',\n",
       "      'func_desc': 'Mixin class for all outlier detection estimators in scikit-learn.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.OutlierMixin.html#sklearn.base.OutlierMixin',\n",
       "      'function_definitions': {'function_name': 'OutlierMixin',\n",
       "       'full_function': 'class sklearn.base.OutlierMixin',\n",
       "       'function_text': 'Mixin class for all outlier detection estimators in scikit-learn. This mixin defines the following functionality: _estimator_type class attribute defaulting to outlier_detector; fit_predict method that default to fit and predict. Examples Perform fit on X and returns labels for X. Returns -1 for outliers and 1 for inliers.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The input samples.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'Ignored',\n",
       "         'param_desc': 'Not used, present for API consistency by convention.\\n'}]},\n",
       "      'function_calling': {'name': 'OutlierMixin',\n",
       "       'descriptions': 'Mixin class for all outlier detection estimators in scikit-learn. This mixin defines the following functionality: _estimator_type class attribute defaulting to outlier_detector; fit_predict method that default to fit and predict. Examples Perform fit on X and returns labels for X. Returns -1 for outliers and 1 for inliers.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The input samples.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'Ignored. Not used, present for API consistency by convention.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'TransformerMixin',\n",
       "      'func_desc': 'Mixin class for all transformers in scikit-learn.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html#sklearn.base.TransformerMixin',\n",
       "      'function_definitions': {'function_name': 'TransformerMixin',\n",
       "       'full_function': 'class sklearn.base.TransformerMixin',\n",
       "       'function_text': 'Mixin class for all transformers in scikit-learn. This mixin defines the following functionality: a fit_transform method that delegates to fit and transform; a set_output method to output X as a specific container type. If get_feature_names_out is defined, then BaseEstimator will\\nautomatically wrap transform and fit_transform to follow the set_output\\nAPI. See the Developer API for set_output for details. OneToOneFeatureMixin and\\nClassNamePrefixFeaturesOutMixin are helpful mixins for\\ndefining get_feature_names_out. Examples Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params\\nand returns a transformed version of X.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-get_feature_names_out',\n",
       "       'parameter_names_desc': [{'param_name': 'transform',\n",
       "         'param_type': '{“default”, “pandas”, “polars”}, default=None',\n",
       "         'param_desc': 'Configure output of transform and fit_transform.\\n\\n\"default\": Default output format of a transformer\\n\"pandas\": DataFrame output\\n\"polars\": Polars output\\nNone: Transform configuration is unchanged\\n\\n\\nAdded in version 1.4: \"polars\" option was added.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'TransformerMixin',\n",
       "       'descriptions': 'Mixin class for all transformers in scikit-learn. This mixin defines the following functionality: a fit_transform method that delegates to fit and transform; a set_output method to output X as a specific container type. If get_feature_names_out is defined, then BaseEstimator will\\nautomatically wrap transform and fit_transform to follow the set_output\\nAPI. See the Developer API for set_output for details. OneToOneFeatureMixin and\\nClassNamePrefixFeaturesOutMixin are helpful mixins for\\ndefining get_feature_names_out. Examples Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params\\nand returns a transformed version of X.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'transform': {'type': 'string',\n",
       "          'enum': ['default', 'pandas', 'polars'],\n",
       "          'description': '{“default”, “pandas”, “polars”}, default=None. Configure output of transform and fit_transform.\\n\\n\"default\": Default output format of a transformer\\n\"pandas\": DataFrame output\\n\"polars\": Polars output\\nNone: Transform configuration is unchanged\\n\\n\\nAdded in version 1.4: \"polars\" option was added.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'is_classifier',\n",
       "      'func_desc': 'Return True if the given estimator is (probably) a classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.is_classifier.html#sklearn.base.is_classifier',\n",
       "      'function_definitions': {'function_name': 'is_classifier',\n",
       "       'full_function': 'sklearn.base.is_classifier(estimator)',\n",
       "       'function_text': 'Return True if the given estimator is (probably) a classifier.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'Estimator object to test.\\n'}]},\n",
       "      'function_calling': {'name': 'is_classifier',\n",
       "       'descriptions': 'Return True if the given estimator is (probably) a classifier.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'object. Estimator object to test.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'BiclusterMixin',\n",
       "      'func_desc': 'Mixin class for all bicluster estimators in scikit-learn.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.BiclusterMixin.html#sklearn.base.BiclusterMixin',\n",
       "      'function_definitions': {'function_name': 'BiclusterMixin',\n",
       "       'full_function': 'class sklearn.base.BiclusterMixin',\n",
       "       'function_text': 'Mixin class for all bicluster estimators in scikit-learn. This mixin defines the following functionality: biclusters_ property that returns the row and column indicators; get_indices method that returns the row and column indices of a bicluster; get_shape method that returns the shape of a bicluster; get_submatrix method that returns the submatrix corresponding to a bicluster. Examples Convenient way to get row and column indicators together. Returns the rows_ and columns_ members. Row and column indices of the i’th bicluster. Only works if rows_ and columns_ attributes exist.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'i',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'The index of the cluster.\\n'},\n",
       "        {'param_name': 'data',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data.\\n'}]},\n",
       "      'function_calling': {'name': 'BiclusterMixin',\n",
       "       'descriptions': 'Mixin class for all bicluster estimators in scikit-learn. This mixin defines the following functionality: biclusters_ property that returns the row and column indicators; get_indices method that returns the row and column indices of a bicluster; get_shape method that returns the shape of a bicluster; get_submatrix method that returns the submatrix corresponding to a bicluster. Examples Convenient way to get row and column indicators together. Returns the rows_ and columns_ members. Row and column indices of the i’th bicluster. Only works if rows_ and columns_ attributes exist.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'i': {'type': 'integer',\n",
       "          'description': 'int. The index of the cluster.\\n'},\n",
       "         'data': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). The data.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'ClassifierMixin',\n",
       "      'func_desc': 'Mixin class for all classifiers in scikit-learn.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html#sklearn.base.ClassifierMixin',\n",
       "      'function_definitions': {'function_name': 'ClassifierMixin',\n",
       "       'full_function': 'class sklearn.base.ClassifierMixin',\n",
       "       'function_text': 'Mixin class for all classifiers in scikit-learn. This mixin defines the following functionality: _estimator_type class attribute defaulting to \"classifier\"; score method that default to accuracy_score. enforce that fit requires y to be passed through the requires_y tag. Read more in the User Guide. Examples Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy\\nwhich is a harsh metric since you require for each sample that\\neach label set be correctly predicted.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Test samples.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'True labels for X.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'ClassifierMixin',\n",
       "       'descriptions': 'Mixin class for all classifiers in scikit-learn. This mixin defines the following functionality: _estimator_type class attribute defaulting to \"classifier\"; score method that default to accuracy_score. enforce that fit requires y to be passed through the requires_y tag. Read more in the User Guide. Examples Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy\\nwhich is a harsh metric since you require for each sample that\\neach label set be correctly predicted.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Test samples.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). True labels for X.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'DensityMixin',\n",
       "      'func_desc': 'Mixin class for all density estimators in scikit-learn.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.DensityMixin.html#sklearn.base.DensityMixin',\n",
       "      'function_definitions': {'function_name': 'DensityMixin',\n",
       "       'full_function': 'class sklearn.base.DensityMixin',\n",
       "       'function_text': 'Mixin class for all density estimators in scikit-learn. This mixin defines the following functionality: _estimator_type class attribute defaulting to \"DensityEstimator\"; score method that default that do no-op. Examples Return the score of the model on the data X.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Test samples.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'Ignored',\n",
       "         'param_desc': 'Not used, present for API consistency by convention.\\n'}]},\n",
       "      'function_calling': {'name': 'DensityMixin',\n",
       "       'descriptions': 'Mixin class for all density estimators in scikit-learn. This mixin defines the following functionality: _estimator_type class attribute defaulting to \"DensityEstimator\"; score method that default that do no-op. Examples Return the score of the model on the data X.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Test samples.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'Ignored. Not used, present for API consistency by convention.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'OneToOneFeatureMixin',\n",
       "      'func_desc': 'Provides get_feature_names_out for simple transformers.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.OneToOneFeatureMixin.html#sklearn.base.OneToOneFeatureMixin',\n",
       "      'function_definitions': {'function_name': 'OneToOneFeatureMixin',\n",
       "       'full_function': 'class sklearn.base.OneToOneFeatureMixin',\n",
       "       'function_text': 'Provides get_feature_names_out for simple transformers. This mixin assumes there’s a 1-to-1 correspondence between input features\\nand output features, such as StandardScaler. Examples Get output feature names for transformation.',\n",
       "       'func_text_user_guide': 'sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler',\n",
       "       'parameter_names_desc': [{'param_name': 'input_features',\n",
       "         'param_type': 'array-like of str or None, default=None',\n",
       "         'param_desc': 'Input features.\\n\\nIf input_features is None, then feature_names_in_ is\\nused as feature names in. If feature_names_in_ is not defined,\\nthen the following input feature names are generated:\\n[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"].\\nIf input_features is an array-like, then input_features must\\nmatch feature_names_in_ if feature_names_in_ is defined.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'OneToOneFeatureMixin',\n",
       "       'descriptions': 'Provides get_feature_names_out for simple transformers. This mixin assumes there’s a 1-to-1 correspondence between input features\\nand output features, such as StandardScaler. Examples Get output feature names for transformation.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'input_features': {'type': 'string',\n",
       "          'description': 'array-like of str or None, default=None. Input features.\\n\\nIf input_features is None, then feature_names_in_ is\\nused as feature names in. If feature_names_in_ is not defined,\\nthen the following input feature names are generated:\\n[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"].\\nIf input_features is an array-like, then input_features must\\nmatch feature_names_in_ if feature_names_in_ is defined.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'RegressorMixin',\n",
       "      'func_desc': 'Mixin class for all regression estimators in scikit-learn.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.RegressorMixin.html#sklearn.base.RegressorMixin',\n",
       "      'function_definitions': {'function_name': 'RegressorMixin',\n",
       "       'full_function': 'class sklearn.base.RegressorMixin',\n",
       "       'function_text': 'Mixin class for all regression estimators in scikit-learn. This mixin defines the following functionality: _estimator_type class attribute defaulting to \"regressor\"; score method that default to r2_score. enforce that fit requires y to be passed through the requires_y tag. Read more in the User Guide. Examples Return the coefficient of determination of the prediction. The coefficient of determination \\\\(R^2\\\\) is defined as\\n\\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual\\nsum of squares ((y_true - y_pred)** 2).sum() and \\\\(v\\\\)\\nis the total sum of squares ((y_true - y_true.mean()) ** 2).sum().\\nThe best possible score is 1.0 and it can be negative (because the\\nmodel can be arbitrarily worse). A constant model that always predicts\\nthe expected value of y, disregarding the input features, would get\\na \\\\(R^2\\\\) score of 0.0.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.r2_score.html#sklearn.metrics.r2_score',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Test samples. For some estimators this may be a precomputed\\nkernel matrix or a list of generic objects instead with shape\\n(n_samples, n_samples_fitted), where n_samples_fitted\\nis the number of samples used in the fitting for the estimator.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'True values for X.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'RegressorMixin',\n",
       "       'descriptions': 'Mixin class for all regression estimators in scikit-learn. This mixin defines the following functionality: _estimator_type class attribute defaulting to \"regressor\"; score method that default to r2_score. enforce that fit requires y to be passed through the requires_y tag. Read more in the User Guide. Examples Return the coefficient of determination of the prediction. The coefficient of determination \\\\(R^2\\\\) is defined as\\n\\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual\\nsum of squares ((y_true - y_pred)** 2).sum() and \\\\(v\\\\)\\nis the total sum of squares ((y_true - y_true.mean()) ** 2).sum().\\nThe best possible score is 1.0 and it can be negative (because the\\nmodel can be arbitrarily worse). A constant model that always predicts\\nthe expected value of y, disregarding the input features, would get\\na \\\\(R^2\\\\) score of 0.0.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Test samples. For some estimators this may be a precomputed\\nkernel matrix or a list of generic objects instead with shape\\n(n_samples, n_samples_fitted), where n_samples_fitted\\nis the number of samples used in the fitting for the estimator.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). True values for X.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'clone',\n",
       "      'func_desc': 'Construct a new unfitted estimator with the same parameters.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.clone.html#sklearn.base.clone',\n",
       "      'function_definitions': {'function_name': 'clone',\n",
       "       'full_function': 'sklearn.base.clone(estimator, *, safe=True)',\n",
       "       'function_text': 'Construct a new unfitted estimator with the same parameters. Clone does a deep copy of the model in an estimator\\nwithout actually copying attached data. It returns a new estimator\\nwith the same parameters that has not been fitted on any data. Changed in version 1.3: Delegates to estimator.__sklearn_clone__ if the method exists.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/common_pitfalls.html#randomness',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': '{list, tuple, set} of estimator instance or a single             estimator instance',\n",
       "         'param_desc': 'The estimator or group of estimators to be cloned.\\n'},\n",
       "        {'param_name': 'safe',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If safe is False, clone will fall back to a deep copy on objects\\nthat are not estimators. Ignored if estimator.__sklearn_clone__\\nexists.\\n'}]},\n",
       "      'function_calling': {'name': 'clone',\n",
       "       'descriptions': 'Construct a new unfitted estimator with the same parameters. Clone does a deep copy of the model in an estimator\\nwithout actually copying attached data. It returns a new estimator\\nwith the same parameters that has not been fitted on any data. Changed in version 1.3: Delegates to estimator.__sklearn_clone__ if the method exists.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'array',\n",
       "          'description': '{list, tuple, set} of estimator instance or a single             estimator instance. The estimator or group of estimators to be cloned.\\n'},\n",
       "         'safe': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If safe is False, clone will fall back to a deep copy on objects\\nthat are not estimators. Ignored if estimator.__sklearn_clone__\\nexists.\\n'}},\n",
       "        'required': ['estimator']}}},\n",
       "     {'func_name': 'is_regressor',\n",
       "      'func_desc': 'Return True if the given estimator is (probably) a regressor.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.base.is_regressor.html#sklearn.base.is_regressor',\n",
       "      'function_definitions': {'function_name': 'is_regressor',\n",
       "       'full_function': 'sklearn.base.is_regressor(estimator)',\n",
       "       'function_text': 'Return True if the given estimator is (probably) a regressor.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'estimator instance',\n",
       "         'param_desc': 'Estimator object to test.\\n'}]},\n",
       "      'function_calling': {'name': 'is_regressor',\n",
       "       'descriptions': 'Return True if the given estimator is (probably) a regressor.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'estimator instance. Estimator object to test.\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.base',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.base.html'},\n",
       " 'sklearn.calibration.html': {'functions': [{'defaults': [{'func_name': 'CalibratedClassifierCV',\n",
       "      'func_desc': 'Probability calibration with isotonic regression or logistic regression.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV',\n",
       "      'function_definitions': {'function_name': 'CalibratedClassifierCV',\n",
       "       'full_function': \"class sklearn.calibration.CalibratedClassifierCV(estimator=None, *, method='sigmoid', cv=None, n_jobs=None, ensemble=True)\",\n",
       "       'function_text': 'Probability calibration with isotonic regression or logistic regression. This class uses cross-validation to both estimate the parameters of a\\nclassifier and subsequently calibrate a classifier. With default\\nensemble=True, for each cv split it\\nfits a copy of the base estimator to the training subset, and calibrates it\\nusing the testing subset. For prediction, predicted probabilities are\\naveraged across these individual calibrated classifiers. When\\nensemble=False, cross-validation is used to obtain unbiased predictions,\\nvia cross_val_predict, which are then\\nused for calibration. For prediction, the base estimator, trained using all\\nthe data, is used. This is the prediction method implemented when\\nprobabilities=True for SVC and NuSVC\\nestimators (see User Guide for details). Already fitted classifiers can be calibrated via the parameter\\ncv=\"prefit\". In this case, no cross-validation is used and all provided\\ndata is used for calibration. The user has to take care manually that data\\nfor model fitting and calibration are disjoint. The calibration is based on the decision_function method of the\\nestimator if it exists, else on predict_proba. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'CalibratedClassifierCV',\n",
       "       'descriptions': 'Probability calibration with isotonic regression or logistic regression. This class uses cross-validation to both estimate the parameters of a\\nclassifier and subsequently calibrate a classifier. With default\\nensemble=True, for each cv split it\\nfits a copy of the base estimator to the training subset, and calibrates it\\nusing the testing subset. For prediction, predicted probabilities are\\naveraged across these individual calibrated classifiers. When\\nensemble=False, cross-validation is used to obtain unbiased predictions,\\nvia cross_val_predict, which are then\\nused for calibration. For prediction, the base estimator, trained using all\\nthe data, is used. This is the prediction method implemented when\\nprobabilities=True for SVC and NuSVC\\nestimators (see User Guide for details). Already fitted classifiers can be calibrated via the parameter\\ncv=\"prefit\". In this case, no cross-validation is used and all provided\\ndata is used for calibration. The user has to take care manually that data\\nfor model fitting and calibration are disjoint. The calibration is based on the decision_function method of the\\nestimator if it exists, else on predict_proba. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['estimator=None']}}},\n",
       "     {'func_name': 'calibration_curve',\n",
       "      'func_desc': 'Compute true and predicted probabilities for a calibration curve.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.calibration.calibration_curve.html#sklearn.calibration.calibration_curve',\n",
       "      'function_definitions': {'function_name': 'calibration_curve',\n",
       "       'full_function': \"sklearn.calibration.calibration_curve(y_true, y_prob, *, pos_label=None, n_bins=5, strategy='uniform')\",\n",
       "       'function_text': 'Compute true and predicted probabilities for a calibration curve. The method assumes the inputs come from a binary classifier, and\\ndiscretize the [0, 1] interval into bins. Calibration curves may also be referred to as reliability diagrams. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/calibration.html#calibration',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'True targets.\\n'},\n",
       "        {'param_name': 'y_prob',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Probabilities of the positive class.\\n'},\n",
       "        {'param_name': 'pos_label',\n",
       "         'param_type': 'int, float, bool or str, default=None',\n",
       "         'param_desc': 'The label of the positive class.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "        {'param_name': 'n_bins',\n",
       "         'param_type': 'int, default=5',\n",
       "         'param_desc': 'Number of bins to discretize the [0, 1] interval. A bigger number\\nrequires more data. Bins with no samples (i.e. without\\ncorresponding values in y_prob) will not be returned, thus the\\nreturned arrays may have less than n_bins values.\\n'},\n",
       "        {'param_name': 'strategy',\n",
       "         'param_type': '{‘uniform’, ‘quantile’}, default=’uniform’',\n",
       "         'param_desc': 'Strategy used to define the widths of the bins.\\n\\nuniformThe bins have identical widths.\\n\\nquantileThe bins have the same number of samples and depend on y_prob.\\n\\n\\n'}]},\n",
       "      'function_calling': {'name': 'calibration_curve',\n",
       "       'descriptions': 'Compute true and predicted probabilities for a calibration curve. The method assumes the inputs come from a binary classifier, and\\ndiscretize the [0, 1] interval into bins. Calibration curves may also be referred to as reliability diagrams. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). True targets.\\n'},\n",
       "         'y_prob': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Probabilities of the positive class.\\n'},\n",
       "         'pos_label': {'type': 'integer',\n",
       "          'description': 'int, float, bool or str, default=None. The label of the positive class.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "         'n_bins': {'type': 'integer',\n",
       "          'description': 'int, default=5. Number of bins to discretize the [0, 1] interval. A bigger number\\nrequires more data. Bins with no samples (i.e. without\\ncorresponding values in y_prob) will not be returned, thus the\\nreturned arrays may have less than n_bins values.\\n'},\n",
       "         'strategy': {'type': 'string',\n",
       "          'enum': ['uniform', 'quantile'],\n",
       "          'description': '{‘uniform’, ‘quantile’}, default=’uniform’. Strategy used to define the widths of the bins.\\n\\nuniformThe bins have identical widths.\\n\\nquantileThe bins have the same number of samples and depend on y_prob.\\n\\n\\n'}},\n",
       "        'required': ['y_true', 'y_prob']}}}]},\n",
       "   {'Visualization': [{'func_name': 'CalibrationDisplay',\n",
       "      'func_desc': 'Calibration curve (also known as reliability diagram) visualization.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibrationDisplay.html#sklearn.calibration.CalibrationDisplay',\n",
       "      'function_definitions': {'function_name': 'CalibrationDisplay',\n",
       "       'full_function': 'class sklearn.calibration.CalibrationDisplay(prob_true, prob_pred, y_prob, *, estimator_name=None, pos_label=None)',\n",
       "       'function_text': 'Calibration curve (also known as reliability diagram) visualization. It is recommended to use\\nfrom_estimator or\\nfrom_predictions\\nto create a CalibrationDisplay. All parameters are stored as attributes. Read more about calibration in the User Guide and\\nmore about the scikit-learn visualization API in Visualizations. Added in version 1.0.',\n",
       "       'func_text_user_guide': '#sklearn.calibration.CalibrationDisplay.from_estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'ax',\n",
       "         'param_type': 'Matplotlib Axes, default=None',\n",
       "         'param_desc': 'Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "        {'param_name': 'name',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'Name for labeling curve. If None, use estimator_name if\\nnot None, otherwise no labeling is shown.\\n'},\n",
       "        {'param_name': 'ref_line',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, plots a reference line representing a perfectly\\ncalibrated classifier.\\n'}]},\n",
       "      'function_calling': {'name': 'CalibrationDisplay',\n",
       "       'descriptions': 'Calibration curve (also known as reliability diagram) visualization. It is recommended to use\\nfrom_estimator or\\nfrom_predictions\\nto create a CalibrationDisplay. All parameters are stored as attributes. Read more about calibration in the User Guide and\\nmore about the scikit-learn visualization API in Visualizations. Added in version 1.0.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'ax': {'type': 'object',\n",
       "          'description': 'Matplotlib Axes, default=None. Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "         'name': {'type': 'string',\n",
       "          'description': 'str, default=None. Name for labeling curve. If None, use estimator_name if\\nnot None, otherwise no labeling is shown.\\n'},\n",
       "         'ref_line': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, plots a reference line representing a perfectly\\ncalibrated classifier.\\n'}},\n",
       "        'required': ['prob_true', 'prob_pred', 'y_prob']}}}]}],\n",
       "  'name': 'sklearn.calibration',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.calibration.html'},\n",
       " 'sklearn.cluster.html': {'functions': [{'defaults': [{'func_name': 'AffinityPropagation',\n",
       "      'func_desc': 'Perform Affinity Propagation Clustering of data.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation',\n",
       "      'function_definitions': {'function_name': 'AffinityPropagation',\n",
       "       'full_function': \"class sklearn.cluster.AffinityPropagation(*, damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity='euclidean', verbose=False, random_state=None)\",\n",
       "       'function_text': 'Perform Affinity Propagation Clustering of data. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'AffinityPropagation',\n",
       "       'descriptions': 'Perform Affinity Propagation Clustering of data. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object', 'properties': {}, 'required': []}}},\n",
       "     {'func_name': 'Birch',\n",
       "      'func_desc': 'Implements the BIRCH clustering algorithm.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html#sklearn.cluster.Birch',\n",
       "      'function_definitions': {'function_name': 'Birch',\n",
       "       'full_function': 'class sklearn.cluster.Birch(*, threshold=0.5, branching_factor=50, n_clusters=3, compute_labels=True, copy=True)',\n",
       "       'function_text': 'Implements the BIRCH clustering algorithm. It is a memory-efficient, online-learning algorithm provided as an\\nalternative to MiniBatchKMeans. It constructs a tree\\ndata structure with the cluster centroids being read off the leaf.\\nThese can be either the final cluster centroids or can be provided as input\\nto another clustering algorithm such as AgglomerativeClustering. Read more in the User Guide. Added in version 0.16.',\n",
       "       'func_text_user_guide': 'sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Input data.\\n'}]},\n",
       "      'function_calling': {'name': 'Birch',\n",
       "       'descriptions': 'Implements the BIRCH clustering algorithm. It is a memory-efficient, online-learning algorithm provided as an\\nalternative to MiniBatchKMeans. It constructs a tree\\ndata structure with the cluster centroids being read off the leaf.\\nThese can be either the final cluster centroids or can be provided as input\\nto another clustering algorithm such as AgglomerativeClustering. Read more in the User Guide. Added in version 0.16.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Input data.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'DBSCAN',\n",
       "      'func_desc': 'Perform DBSCAN clustering from vector array or distance matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN',\n",
       "      'function_definitions': {'function_name': 'DBSCAN',\n",
       "       'full_function': \"class sklearn.cluster.DBSCAN(eps=0.5, *, min_samples=5, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None)\",\n",
       "       'function_text': 'Perform DBSCAN clustering from vector array or distance matrix. DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\\nFinds core samples of high density and expands clusters from them.\\nGood for data which contains clusters of similar density. This implementation has a worst case memory complexity of \\\\(O({n}^2)\\\\),\\nwhich can occur when the eps param is large and min_samples is low,\\nwhile the original DBSCAN only uses linear memory.\\nFor further details, see the Notes below. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#dbscan',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'DBSCAN',\n",
       "       'descriptions': 'Perform DBSCAN clustering from vector array or distance matrix. DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\\nFinds core samples of high density and expands clusters from them.\\nGood for data which contains clusters of similar density. This implementation has a worst case memory complexity of \\\\(O({n}^2)\\\\),\\nwhich can occur when the eps param is large and min_samples is low,\\nwhile the original DBSCAN only uses linear memory.\\nFor further details, see the Notes below. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {},\n",
       "        'required': ['eps=0.5']}}},\n",
       "     {'func_name': 'HDBSCAN',\n",
       "      'func_desc': 'Cluster data using hierarchical density-based clustering.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html#sklearn.cluster.HDBSCAN',\n",
       "      'function_definitions': {'function_name': 'HDBSCAN',\n",
       "       'full_function': \"class sklearn.cluster.HDBSCAN(min_cluster_size=5, min_samples=None, cluster_selection_epsilon=0.0, max_cluster_size=None, metric='euclidean', metric_params=None, alpha=1.0, algorithm='auto', leaf_size=40, n_jobs=None, cluster_selection_method='eom', allow_single_cluster=False, store_centers=None, copy=False)\",\n",
       "       'function_text': 'Cluster data using hierarchical density-based clustering. HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications\\nwith Noise. Performs DBSCAN over varying epsilon\\nvalues and integrates the result to find a clustering that gives the best\\nstability over epsilon.\\nThis allows HDBSCAN to find clusters of varying densities (unlike\\nDBSCAN), and be more robust to parameter selection.\\nRead more in the User Guide. For an example of how to use HDBSCAN, as well as a comparison to\\nDBSCAN, please see the plotting demo. Added in version 1.3.',\n",
       "       'func_text_user_guide': 'sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'HDBSCAN',\n",
       "       'descriptions': 'Cluster data using hierarchical density-based clustering. HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications\\nwith Noise. Performs DBSCAN over varying epsilon\\nvalues and integrates the result to find a clustering that gives the best\\nstability over epsilon.\\nThis allows HDBSCAN to find clusters of varying densities (unlike\\nDBSCAN), and be more robust to parameter selection.\\nRead more in the User Guide. For an example of how to use HDBSCAN, as well as a comparison to\\nDBSCAN, please see the plotting demo. Added in version 1.3.',\n",
       "       'parameters': {'type': 'object', 'properties': {}, 'required': []}}},\n",
       "     {'func_name': 'MeanShift',\n",
       "      'func_desc': 'Mean shift clustering using a flat kernel.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift',\n",
       "      'function_definitions': {'function_name': 'MeanShift',\n",
       "       'full_function': 'class sklearn.cluster.MeanShift(*, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300)',\n",
       "       'function_text': 'Mean shift clustering using a flat kernel. Mean shift clustering aims to discover “blobs” in a smooth density of\\nsamples. It is a centroid-based algorithm, which works by updating\\ncandidates for centroids to be the mean of the points within a given\\nregion. These candidates are then filtered in a post-processing stage to\\neliminate near-duplicates to form the final set of centroids. Seeding is performed using a binning technique for scalability. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#mean-shift',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'MeanShift',\n",
       "       'descriptions': 'Mean shift clustering using a flat kernel. Mean shift clustering aims to discover “blobs” in a smooth density of\\nsamples. It is a centroid-based algorithm, which works by updating\\ncandidates for centroids to be the mean of the points within a given\\nregion. These candidates are then filtered in a post-processing stage to\\neliminate near-duplicates to form the final set of centroids. Seeding is performed using a binning technique for scalability. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object', 'properties': {}, 'required': []}}},\n",
       "     {'func_name': 'OPTICS',\n",
       "      'func_desc': 'Estimate clustering structure from vector array.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html#sklearn.cluster.OPTICS',\n",
       "      'function_definitions': {'function_name': 'OPTICS',\n",
       "       'full_function': \"class sklearn.cluster.OPTICS(*, min_samples=5, max_eps=inf, metric='minkowski', p=2, metric_params=None, cluster_method='xi', eps=None, xi=0.05, predecessor_correction=True, min_cluster_size=None, algorithm='auto', leaf_size=30, memory=None, n_jobs=None)\",\n",
       "       'function_text': 'Estimate clustering structure from vector array. OPTICS (Ordering Points To Identify the Clustering Structure), closely\\nrelated to DBSCAN, finds core sample of high density and expands clusters\\nfrom them [1]. Unlike DBSCAN, keeps cluster hierarchy for a variable\\nneighborhood radius. Better suited for usage on large datasets than the\\ncurrent sklearn implementation of DBSCAN. Clusters are then extracted using a DBSCAN-like method\\n(cluster_method = ‘dbscan’) or an automatic\\ntechnique proposed in [1] (cluster_method = ‘xi’). This implementation deviates from the original OPTICS by first performing\\nk-nearest-neighborhood searches on all points to identify core sizes, then\\ncomputing only the distances to unprocessed points when constructing the\\ncluster order. Note that we do not employ a heap to manage the expansion\\ncandidates, so the time complexity will be O(n^2). Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#r2c55e37003fe-1',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'OPTICS',\n",
       "       'descriptions': 'Estimate clustering structure from vector array. OPTICS (Ordering Points To Identify the Clustering Structure), closely\\nrelated to DBSCAN, finds core sample of high density and expands clusters\\nfrom them [1]. Unlike DBSCAN, keeps cluster hierarchy for a variable\\nneighborhood radius. Better suited for usage on large datasets than the\\ncurrent sklearn implementation of DBSCAN. Clusters are then extracted using a DBSCAN-like method\\n(cluster_method = ‘dbscan’) or an automatic\\ntechnique proposed in [1] (cluster_method = ‘xi’). This implementation deviates from the original OPTICS by first performing\\nk-nearest-neighborhood searches on all points to identify core sizes, then\\ncomputing only the distances to unprocessed points when constructing the\\ncluster order. Note that we do not employ a heap to manage the expansion\\ncandidates, so the time complexity will be O(n^2). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object', 'properties': {}, 'required': []}}},\n",
       "     {'func_name': 'SpectralClustering',\n",
       "      'func_desc': 'Apply clustering to a projection of the normalized Laplacian.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering',\n",
       "      'function_definitions': {'function_name': 'SpectralClustering',\n",
       "       'full_function': \"class sklearn.cluster.SpectralClustering(n_clusters=8, *, eigen_solver=None, n_components=None, random_state=None, n_init=10, gamma=1.0, affinity='rbf', n_neighbors=10, eigen_tol='auto', assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None, verbose=False)\",\n",
       "       'function_text': \"Apply clustering to a projection of the normalized Laplacian. In practice Spectral Clustering is very useful when the structure of\\nthe individual clusters is highly non-convex, or more generally when\\na measure of the center and spread of the cluster is not a suitable\\ndescription of the complete cluster, such as when clusters are\\nnested circles on the 2D plane. If the affinity matrix is the adjacency matrix of a graph, this method\\ncan be used to find normalized graph cuts [1], [2]. When calling fit, an affinity matrix is constructed using either\\na kernel function such the Gaussian (aka RBF) kernel with Euclidean\\ndistance d(X, X): or a k-nearest neighbors connectivity matrix. Alternatively, a user-provided affinity matrix can be specified by\\nsetting affinity='precomputed'. Read more in the User Guide.\",\n",
       "       'func_text_user_guide': '#r5f6cbeb1558e-1',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'SpectralClustering',\n",
       "       'descriptions': \"Apply clustering to a projection of the normalized Laplacian. In practice Spectral Clustering is very useful when the structure of\\nthe individual clusters is highly non-convex, or more generally when\\na measure of the center and spread of the cluster is not a suitable\\ndescription of the complete cluster, such as when clusters are\\nnested circles on the 2D plane. If the affinity matrix is the adjacency matrix of a graph, this method\\ncan be used to find normalized graph cuts [1], [2]. When calling fit, an affinity matrix is constructed using either\\na kernel function such the Gaussian (aka RBF) kernel with Euclidean\\ndistance d(X, X): or a k-nearest neighbors connectivity matrix. Alternatively, a user-provided affinity matrix can be specified by\\nsetting affinity='precomputed'. Read more in the User Guide.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {},\n",
       "        'required': ['n_clusters=8']}}},\n",
       "     {'func_name': 'affinity_propagation',\n",
       "      'func_desc': 'Perform Affinity Propagation Clustering of data.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.affinity_propagation.html#sklearn.cluster.affinity_propagation',\n",
       "      'function_definitions': {'function_name': 'affinity_propagation',\n",
       "       'full_function': 'sklearn.cluster.affinity_propagation(S, *, preference=None, convergence_iter=15, max_iter=200, damping=0.5, copy=True, verbose=False, return_n_iter=False, random_state=None)',\n",
       "       'function_text': 'Perform Affinity Propagation Clustering of data. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation',\n",
       "       'parameter_names_desc': [{'param_name': 'S',\n",
       "         'param_type': 'array-like of shape (n_samples, n_samples)',\n",
       "         'param_desc': 'Matrix of similarities between points.\\n'},\n",
       "        {'param_name': 'preference',\n",
       "         'param_type': 'array-like of shape (n_samples,) or float, default=None',\n",
       "         'param_desc': 'Preferences for each point - points with larger values of\\npreferences are more likely to be chosen as exemplars. The number of\\nexemplars, i.e. of clusters, is influenced by the input preferences\\nvalue. If the preferences are not passed as arguments, they will be\\nset to the median of the input similarities (resulting in a moderate\\nnumber of clusters). For a smaller amount of clusters, this can be set\\nto the minimum value of the similarities.\\n'},\n",
       "        {'param_name': 'convergence_iter',\n",
       "         'param_type': 'int, default=15',\n",
       "         'param_desc': 'Number of iterations with no change in the number\\nof estimated clusters that stops the convergence.\\n'},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=200',\n",
       "         'param_desc': 'Maximum number of iterations.\\n'},\n",
       "        {'param_name': 'damping',\n",
       "         'param_type': 'float, default=0.5',\n",
       "         'param_desc': 'Damping factor between 0.5 and 1.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If copy is False, the affinity matrix is modified inplace by the\\nalgorithm, for memory efficiency.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'The verbosity level.\\n'},\n",
       "        {'param_name': 'return_n_iter',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether or not to return the number of iterations.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Pseudo-random number generator to control the starting state.\\nUse an int for reproducible results across function calls.\\nSee the Glossary.\\n\\nAdded in version 0.23: this parameter was previously hardcoded as 0.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'affinity_propagation',\n",
       "       'descriptions': 'Perform Affinity Propagation Clustering of data. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'S': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_samples). Matrix of similarities between points.\\n'},\n",
       "         'preference': {'type': 'number',\n",
       "          'description': 'array-like of shape (n_samples,) or float, default=None. Preferences for each point - points with larger values of\\npreferences are more likely to be chosen as exemplars. The number of\\nexemplars, i.e. of clusters, is influenced by the input preferences\\nvalue. If the preferences are not passed as arguments, they will be\\nset to the median of the input similarities (resulting in a moderate\\nnumber of clusters). For a smaller amount of clusters, this can be set\\nto the minimum value of the similarities.\\n'},\n",
       "         'convergence_iter': {'type': 'integer',\n",
       "          'description': 'int, default=15. Number of iterations with no change in the number\\nof estimated clusters that stops the convergence.\\n'},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=200. Maximum number of iterations.\\n'},\n",
       "         'damping': {'type': 'number',\n",
       "          'description': 'float, default=0.5. Damping factor between 0.5 and 1.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If copy is False, the affinity matrix is modified inplace by the\\nalgorithm, for memory efficiency.\\n'},\n",
       "         'verbose': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. The verbosity level.\\n'},\n",
       "         'return_n_iter': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether or not to return the number of iterations.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Pseudo-random number generator to control the starting state.\\nUse an int for reproducible results across function calls.\\nSee the Glossary.\\n\\nAdded in version 0.23: this parameter was previously hardcoded as 0.\\n\\n'}},\n",
       "        'required': ['S']}}},\n",
       "     {'func_name': 'cluster_optics_xi',\n",
       "      'func_desc': 'Automatically extract clusters according to the Xi-steep method.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.cluster_optics_xi.html#sklearn.cluster.cluster_optics_xi',\n",
       "      'function_definitions': {'function_name': 'cluster_optics_xi',\n",
       "       'full_function': 'sklearn.cluster.cluster_optics_xi(*, reachability, predecessor, ordering, min_samples, min_cluster_size=None, xi=0.05, predecessor_correction=True)',\n",
       "       'function_text': 'Automatically extract clusters according to the Xi-steep method.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'reachability',\n",
       "         'param_type': 'ndarray of shape (n_samples,)',\n",
       "         'param_desc': 'Reachability distances calculated by OPTICS (reachability_).\\n'},\n",
       "        {'param_name': 'predecessor',\n",
       "         'param_type': 'ndarray of shape (n_samples,)',\n",
       "         'param_desc': 'Predecessors calculated by OPTICS.\\n'},\n",
       "        {'param_name': 'ordering',\n",
       "         'param_type': 'ndarray of shape (n_samples,)',\n",
       "         'param_desc': 'OPTICS ordered point indices (ordering_).\\n'},\n",
       "        {'param_name': 'min_samples',\n",
       "         'param_type': 'int > 1 or float between 0 and 1',\n",
       "         'param_desc': 'The same as the min_samples given to OPTICS. Up and down steep regions\\ncan’t have more then min_samples consecutive non-steep points.\\nExpressed as an absolute number or a fraction of the number of samples\\n(rounded to be at least 2).\\n'},\n",
       "        {'param_name': 'min_cluster_size',\n",
       "         'param_type': 'int > 1 or float between 0 and 1, default=None',\n",
       "         'param_desc': 'Minimum number of samples in an OPTICS cluster, expressed as an\\nabsolute number or a fraction of the number of samples (rounded to be\\nat least 2). If None, the value of min_samples is used instead.\\n'},\n",
       "        {'param_name': 'xi',\n",
       "         'param_type': 'float between 0 and 1, default=0.05',\n",
       "         'param_desc': 'Determines the minimum steepness on the reachability plot that\\nconstitutes a cluster boundary. For example, an upwards point in the\\nreachability plot is defined by the ratio from one point to its\\nsuccessor being at most 1-xi.\\n'},\n",
       "        {'param_name': 'predecessor_correction',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Correct clusters based on the calculated predecessors.\\n'}]},\n",
       "      'function_calling': {'name': 'cluster_optics_xi',\n",
       "       'descriptions': 'Automatically extract clusters according to the Xi-steep method.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'reachability': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples,). Reachability distances calculated by OPTICS (reachability_).\\n'},\n",
       "         'predecessor': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples,). Predecessors calculated by OPTICS.\\n'},\n",
       "         'ordering': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples,). OPTICS ordered point indices (ordering_).\\n'},\n",
       "         'min_samples': {'type': 'integer',\n",
       "          'description': 'int > 1 or float between 0 and 1. The same as the min_samples given to OPTICS. Up and down steep regions\\ncan’t have more then min_samples consecutive non-steep points.\\nExpressed as an absolute number or a fraction of the number of samples\\n(rounded to be at least 2).\\n'},\n",
       "         'min_cluster_size': {'type': 'integer',\n",
       "          'description': 'int > 1 or float between 0 and 1, default=None. Minimum number of samples in an OPTICS cluster, expressed as an\\nabsolute number or a fraction of the number of samples (rounded to be\\nat least 2). If None, the value of min_samples is used instead.\\n'},\n",
       "         'xi': {'type': 'number',\n",
       "          'description': 'float between 0 and 1, default=0.05. Determines the minimum steepness on the reachability plot that\\nconstitutes a cluster boundary. For example, an upwards point in the\\nreachability plot is defined by the ratio from one point to its\\nsuccessor being at most 1-xi.\\n'},\n",
       "         'predecessor_correction': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Correct clusters based on the calculated predecessors.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'dbscan',\n",
       "      'func_desc': 'Perform DBSCAN clustering from vector array or distance matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/dbscan-function.html#sklearn.cluster.dbscan',\n",
       "      'function_definitions': {'function_name': 'dbscan',\n",
       "       'full_function': \"sklearn.cluster.dbscan(X, eps=0.5, *, min_samples=5, metric='minkowski', metric_params=None, algorithm='auto', leaf_size=30, p=2, sample_weight=None, n_jobs=None)\",\n",
       "       'function_text': 'Perform DBSCAN clustering from vector array or distance matrix. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#dbscan',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or             (n_samples, n_samples)',\n",
       "         'param_desc': \"A feature array, or array of distances between samples if\\nmetric='precomputed'.\\n\"},\n",
       "        {'param_name': 'eps',\n",
       "         'param_type': 'float, default=0.5',\n",
       "         'param_desc': 'The maximum distance between two samples for one to be considered\\nas in the neighborhood of the other. This is not a maximum bound\\non the distances of points within a cluster. This is the most\\nimportant DBSCAN parameter to choose appropriately for your data set\\nand distance function.\\n'},\n",
       "        {'param_name': 'min_samples',\n",
       "         'param_type': 'int, default=5',\n",
       "         'param_desc': 'The number of samples (or total weight) in a neighborhood for a point\\nto be considered as a core point. This includes the point itself.\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'str or callable, default=’minkowski’',\n",
       "         'param_desc': 'The metric to use when calculating distance between instances in a\\nfeature array. If metric is a string or callable, it must be one of\\nthe options allowed by sklearn.metrics.pairwise_distances for\\nits metric parameter.\\nIf metric is “precomputed”, X is assumed to be a distance matrix and\\nmust be square during fit.\\nX may be a sparse graph,\\nin which case only “nonzero” elements may be considered neighbors.\\n'},\n",
       "        {'param_name': 'metric_params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Additional keyword arguments for the metric function.\\n\\nAdded in version 0.19.\\n\\n'},\n",
       "        {'param_name': 'algorithm',\n",
       "         'param_type': '{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’',\n",
       "         'param_desc': 'The algorithm to be used by the NearestNeighbors module\\nto compute pointwise distances and find nearest neighbors.\\nSee NearestNeighbors module documentation for details.\\n'},\n",
       "        {'param_name': 'leaf_size',\n",
       "         'param_type': 'int, default=30',\n",
       "         'param_desc': 'Leaf size passed to BallTree or cKDTree. This can affect the speed\\nof the construction and query, as well as the memory required\\nto store the tree. The optimal value depends\\non the nature of the problem.\\n'},\n",
       "        {'param_name': 'p',\n",
       "         'param_type': 'float, default=2',\n",
       "         'param_desc': 'The power of the Minkowski metric to be used to calculate distance\\nbetween points.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Weight of each sample, such that a sample with a weight of at least\\nmin_samples is by itself a core sample; a sample with negative\\nweight may inhibit its eps-neighbor from being core.\\nNote that weights are absolute, and default to 1.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of parallel jobs to run for neighbors search. None means\\n1 unless in a joblib.parallel_backend context. -1 means\\nusing all processors. See Glossary for more details.\\nIf precomputed distance are used, parallel execution is not available\\nand thus n_jobs will have no effect.\\n'}]},\n",
       "      'function_calling': {'name': 'dbscan',\n",
       "       'descriptions': 'Perform DBSCAN clustering from vector array or distance matrix. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': \"{array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or             (n_samples, n_samples). A feature array, or array of distances between samples if\\nmetric='precomputed'.\\n\"},\n",
       "         'eps': {'type': 'number',\n",
       "          'description': 'float, default=0.5. The maximum distance between two samples for one to be considered\\nas in the neighborhood of the other. This is not a maximum bound\\non the distances of points within a cluster. This is the most\\nimportant DBSCAN parameter to choose appropriately for your data set\\nand distance function.\\n'},\n",
       "         'min_samples': {'type': 'integer',\n",
       "          'description': 'int, default=5. The number of samples (or total weight) in a neighborhood for a point\\nto be considered as a core point. This includes the point itself.\\n'},\n",
       "         'metric': {'type': 'string',\n",
       "          'description': 'str or callable, default=’minkowski’. The metric to use when calculating distance between instances in a\\nfeature array. If metric is a string or callable, it must be one of\\nthe options allowed by sklearn.metrics.pairwise_distances for\\nits metric parameter.\\nIf metric is “precomputed”, X is assumed to be a distance matrix and\\nmust be square during fit.\\nX may be a sparse graph,\\nin which case only “nonzero” elements may be considered neighbors.\\n'},\n",
       "         'metric_params': {'type': 'array',\n",
       "          'description': 'dict, default=None. Additional keyword arguments for the metric function.\\n\\nAdded in version 0.19.\\n\\n'},\n",
       "         'algorithm': {'type': 'string',\n",
       "          'enum': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
       "          'description': '{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’. The algorithm to be used by the NearestNeighbors module\\nto compute pointwise distances and find nearest neighbors.\\nSee NearestNeighbors module documentation for details.\\n'},\n",
       "         'leaf_size': {'type': 'integer',\n",
       "          'description': 'int, default=30. Leaf size passed to BallTree or cKDTree. This can affect the speed\\nof the construction and query, as well as the memory required\\nto store the tree. The optimal value depends\\non the nature of the problem.\\n'},\n",
       "         'p': {'type': 'number',\n",
       "          'description': 'float, default=2. The power of the Minkowski metric to be used to calculate distance\\nbetween points.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Weight of each sample, such that a sample with a weight of at least\\nmin_samples is by itself a core sample; a sample with negative\\nweight may inhibit its eps-neighbor from being core.\\nNote that weights are absolute, and default to 1.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of parallel jobs to run for neighbors search. None means\\n1 unless in a joblib.parallel_backend context. -1 means\\nusing all processors. See Glossary for more details.\\nIf precomputed distance are used, parallel execution is not available\\nand thus n_jobs will have no effect.\\n'}},\n",
       "        'required': ['X', 'eps=0.5']}}},\n",
       "     {'func_name': 'k_means',\n",
       "      'func_desc': 'Perform K-means clustering algorithm.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.k_means.html#sklearn.cluster.k_means',\n",
       "      'function_definitions': {'function_name': 'k_means',\n",
       "       'full_function': \"sklearn.cluster.k_means(X, n_clusters, *, sample_weight=None, init='k-means++', n_init='auto', max_iter=300, verbose=False, tol=0.0001, random_state=None, copy_x=True, algorithm='lloyd', return_n_iter=False)\",\n",
       "       'function_text': 'Perform K-means clustering algorithm. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#k-means',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The observations to cluster. It must be noted that the data\\nwill be converted to C ordering, which will cause a memory copy\\nif the given data is not C-contiguous.\\n'},\n",
       "        {'param_name': 'n_clusters',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'The number of clusters to form as well as the number of\\ncentroids to generate.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'The weights for each observation in X. If None, all observations\\nare assigned equal weight. sample_weight is not used during\\ninitialization if init is a callable or a user provided array.\\n'},\n",
       "        {'param_name': 'init',\n",
       "         'param_type': '{‘k-means++’, ‘random’}, callable or array-like of shape             (n_clusters, n_features), default=’k-means++’',\n",
       "         'param_desc': \"Method for initialization:\\n\\n'k-means++' : selects initial cluster centers for k-mean\\nclustering in a smart way to speed up convergence. See section\\nNotes in k_init for more details.\\n'random': choose n_clusters observations (rows) at random from data\\nfor the initial centroids.\\nIf an array is passed, it should be of shape (n_clusters, n_features)\\nand gives the initial centers.\\nIf a callable is passed, it should take arguments X, n_clusters and a\\nrandom state and return an initialization.\\n\\n\"},\n",
       "        {'param_name': 'n_init',\n",
       "         'param_type': '‘auto’ or int, default=”auto”',\n",
       "         'param_desc': \"Number of time the k-means algorithm will be run with different\\ncentroid seeds. The final results will be the best output of\\nn_init consecutive runs in terms of inertia.\\nWhen n_init='auto', the number of runs depends on the value of init:\\n10 if using init='random' or init is a callable;\\n1 if using init='k-means++' or init is an array-like.\\n\\nAdded in version 1.2: Added ‘auto’ option for n_init.\\n\\n\\nChanged in version 1.4: Default value for n_init changed to 'auto'.\\n\\n\"},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=300',\n",
       "         'param_desc': 'Maximum number of iterations of the k-means algorithm to run.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Verbosity mode.\\n'},\n",
       "        {'param_name': 'tol',\n",
       "         'param_type': 'float, default=1e-4',\n",
       "         'param_desc': 'Relative tolerance with regards to Frobenius norm of the difference\\nin the cluster centers of two consecutive iterations to declare\\nconvergence.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for centroid initialization. Use\\nan int to make the randomness deterministic.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'copy_x',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'When pre-computing distances it is more numerically accurate to center\\nthe data first. If copy_x is True (default), then the original data is\\nnot modified. If False, the original data is modified, and put back\\nbefore the function returns, but small numerical differences may be\\nintroduced by subtracting and then adding the data mean. Note that if\\nthe original data is not C-contiguous, a copy will be made even if\\ncopy_x is False. If the original data is sparse, but not in CSR format,\\na copy will be made even if copy_x is False.\\n'},\n",
       "        {'param_name': 'algorithm',\n",
       "         'param_type': '{“lloyd”, “elkan”}, default=”lloyd”',\n",
       "         'param_desc': 'K-means algorithm to use. The classical EM-style algorithm is \"lloyd\".\\nThe \"elkan\" variation can be more efficient on some datasets with\\nwell-defined clusters, by using the triangle inequality. However it’s\\nmore memory intensive due to the allocation of an extra array of shape\\n(n_samples, n_clusters).\\n\\nChanged in version 0.18: Added Elkan algorithm\\n\\n\\nChanged in version 1.1: Renamed “full” to “lloyd”, and deprecated “auto” and “full”.\\nChanged “auto” to use “lloyd” instead of “elkan”.\\n\\n'},\n",
       "        {'param_name': 'return_n_iter',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether or not to return the number of iterations.\\n'}]},\n",
       "      'function_calling': {'name': 'k_means',\n",
       "       'descriptions': 'Perform K-means clustering algorithm. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The observations to cluster. It must be noted that the data\\nwill be converted to C ordering, which will cause a memory copy\\nif the given data is not C-contiguous.\\n'},\n",
       "         'n_clusters': {'type': 'integer',\n",
       "          'description': 'int. The number of clusters to form as well as the number of\\ncentroids to generate.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. The weights for each observation in X. If None, all observations\\nare assigned equal weight. sample_weight is not used during\\ninitialization if init is a callable or a user provided array.\\n'},\n",
       "         'init': {'type': 'object',\n",
       "          'description': \"{‘k-means++’, ‘random’}, callable or array-like of shape             (n_clusters, n_features), default=’k-means++’. Method for initialization:\\n\\n'k-means++' : selects initial cluster centers for k-mean\\nclustering in a smart way to speed up convergence. See section\\nNotes in k_init for more details.\\n'random': choose n_clusters observations (rows) at random from data\\nfor the initial centroids.\\nIf an array is passed, it should be of shape (n_clusters, n_features)\\nand gives the initial centers.\\nIf a callable is passed, it should take arguments X, n_clusters and a\\nrandom state and return an initialization.\\n\\n\"},\n",
       "         'n_init': {'type': 'integer',\n",
       "          'description': \"‘auto’ or int, default=”auto”. Number of time the k-means algorithm will be run with different\\ncentroid seeds. The final results will be the best output of\\nn_init consecutive runs in terms of inertia.\\nWhen n_init='auto', the number of runs depends on the value of init:\\n10 if using init='random' or init is a callable;\\n1 if using init='k-means++' or init is an array-like.\\n\\nAdded in version 1.2: Added ‘auto’ option for n_init.\\n\\n\\nChanged in version 1.4: Default value for n_init changed to 'auto'.\\n\\n\"},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=300. Maximum number of iterations of the k-means algorithm to run.\\n'},\n",
       "         'verbose': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Verbosity mode.\\n'},\n",
       "         'tol': {'type': 'number',\n",
       "          'description': 'float, default=1e-4. Relative tolerance with regards to Frobenius norm of the difference\\nin the cluster centers of two consecutive iterations to declare\\nconvergence.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for centroid initialization. Use\\nan int to make the randomness deterministic.\\nSee Glossary.\\n'},\n",
       "         'copy_x': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. When pre-computing distances it is more numerically accurate to center\\nthe data first. If copy_x is True (default), then the original data is\\nnot modified. If False, the original data is modified, and put back\\nbefore the function returns, but small numerical differences may be\\nintroduced by subtracting and then adding the data mean. Note that if\\nthe original data is not C-contiguous, a copy will be made even if\\ncopy_x is False. If the original data is sparse, but not in CSR format,\\na copy will be made even if copy_x is False.\\n'},\n",
       "         'algorithm': {'type': 'string',\n",
       "          'enum': ['lloyd', 'elkan'],\n",
       "          'description': '{“lloyd”, “elkan”}, default=”lloyd”. K-means algorithm to use. The classical EM-style algorithm is \"lloyd\".\\nThe \"elkan\" variation can be more efficient on some datasets with\\nwell-defined clusters, by using the triangle inequality. However it’s\\nmore memory intensive due to the allocation of an extra array of shape\\n(n_samples, n_clusters).\\n\\nChanged in version 0.18: Added Elkan algorithm\\n\\n\\nChanged in version 1.1: Renamed “full” to “lloyd”, and deprecated “auto” and “full”.\\nChanged “auto” to use “lloyd” instead of “elkan”.\\n\\n'},\n",
       "         'return_n_iter': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether or not to return the number of iterations.\\n'}},\n",
       "        'required': ['X', 'n_clusters']}}},\n",
       "     {'func_name': 'mean_shift',\n",
       "      'func_desc': 'Perform mean shift clustering of data using a flat kernel.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.mean_shift.html#sklearn.cluster.mean_shift',\n",
       "      'function_definitions': {'function_name': 'mean_shift',\n",
       "       'full_function': 'sklearn.cluster.mean_shift(X, *, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, max_iter=300, n_jobs=None)',\n",
       "       'function_text': 'Perform mean shift clustering of data using a flat kernel. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#mean-shift',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Input data.\\n'},\n",
       "        {'param_name': 'bandwidth',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': 'Kernel bandwidth. If not None, must be in the range [0, +inf).\\nIf None, the bandwidth is determined using a heuristic based on\\nthe median of all pairwise distances. This will take quadratic time in\\nthe number of samples. The sklearn.cluster.estimate_bandwidth function\\ncan be used to do this more efficiently.\\n'},\n",
       "        {'param_name': 'seeds',\n",
       "         'param_type': 'array-like of shape (n_seeds, n_features) or None',\n",
       "         'param_desc': 'Point used as initial kernel locations. If None and bin_seeding=False,\\neach data point is used as a seed. If None and bin_seeding=True,\\nsee bin_seeding.\\n'},\n",
       "        {'param_name': 'bin_seeding',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If true, initial kernel locations are not locations of all\\npoints, but rather the location of the discretized version of\\npoints, where points are binned onto a grid whose coarseness\\ncorresponds to the bandwidth. Setting this option to True will speed\\nup the algorithm because fewer seeds will be initialized.\\nIgnored if seeds argument is not None.\\n'},\n",
       "        {'param_name': 'min_bin_freq',\n",
       "         'param_type': 'int, default=1',\n",
       "         'param_desc': 'To speed up the algorithm, accept only those bins with at least\\nmin_bin_freq points as seeds.\\n'},\n",
       "        {'param_name': 'cluster_all',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If true, then all points are clustered, even those orphans that are\\nnot within any kernel. Orphans are assigned to the nearest kernel.\\nIf false, then orphans are given cluster label -1.\\n'},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=300',\n",
       "         'param_desc': 'Maximum number of iterations, per seed point before the clustering\\noperation terminates (for that seed point), if has not converged yet.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of jobs to use for the computation. The following tasks benefit\\nfrom the parallelization:\\n\\nThe search of nearest neighbors for bandwidth estimation and label\\nassignments. See the details in the docstring of the\\nNearestNeighbors class.\\nHill-climbing optimization for all seeds.\\n\\nSee Glossary for more details.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n\\nAdded in version 0.17: Parallel Execution using n_jobs.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'mean_shift',\n",
       "       'descriptions': 'Perform mean shift clustering of data using a flat kernel. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Input data.\\n'},\n",
       "         'bandwidth': {'type': 'number',\n",
       "          'description': 'float, default=None. Kernel bandwidth. If not None, must be in the range [0, +inf).\\nIf None, the bandwidth is determined using a heuristic based on\\nthe median of all pairwise distances. This will take quadratic time in\\nthe number of samples. The sklearn.cluster.estimate_bandwidth function\\ncan be used to do this more efficiently.\\n'},\n",
       "         'seeds': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_seeds, n_features) or None. Point used as initial kernel locations. If None and bin_seeding=False,\\neach data point is used as a seed. If None and bin_seeding=True,\\nsee bin_seeding.\\n'},\n",
       "         'bin_seeding': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If true, initial kernel locations are not locations of all\\npoints, but rather the location of the discretized version of\\npoints, where points are binned onto a grid whose coarseness\\ncorresponds to the bandwidth. Setting this option to True will speed\\nup the algorithm because fewer seeds will be initialized.\\nIgnored if seeds argument is not None.\\n'},\n",
       "         'min_bin_freq': {'type': 'integer',\n",
       "          'description': 'int, default=1. To speed up the algorithm, accept only those bins with at least\\nmin_bin_freq points as seeds.\\n'},\n",
       "         'cluster_all': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If true, then all points are clustered, even those orphans that are\\nnot within any kernel. Orphans are assigned to the nearest kernel.\\nIf false, then orphans are given cluster label -1.\\n'},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=300. Maximum number of iterations, per seed point before the clustering\\noperation terminates (for that seed point), if has not converged yet.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of jobs to use for the computation. The following tasks benefit\\nfrom the parallelization:\\n\\nThe search of nearest neighbors for bandwidth estimation and label\\nassignments. See the details in the docstring of the\\nNearestNeighbors class.\\nHill-climbing optimization for all seeds.\\n\\nSee Glossary for more details.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n\\nAdded in version 0.17: Parallel Execution using n_jobs.\\n\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'ward_tree',\n",
       "      'func_desc': 'Ward clustering based on a Feature matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.ward_tree.html#sklearn.cluster.ward_tree',\n",
       "      'function_definitions': {'function_name': 'ward_tree',\n",
       "       'full_function': 'sklearn.cluster.ward_tree(X, *, connectivity=None, n_clusters=None, return_distance=False)',\n",
       "       'function_text': 'Ward clustering based on a Feature matrix. Recursively merges the pair of clusters that minimally increases\\nwithin-cluster variance. The inertia matrix uses a Heapq-based representation. This is the structured version, that takes into account some topological\\nstructure between samples. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Feature matrix representing n_samples samples to be clustered.\\n'},\n",
       "        {'param_name': 'connectivity',\n",
       "         'param_type': '{array-like, sparse matrix}, default=None',\n",
       "         'param_desc': 'Connectivity matrix. Defines for each sample the neighboring samples\\nfollowing a given structure of the data. The matrix is assumed to\\nbe symmetric and only the upper triangular half is used.\\nDefault is None, i.e, the Ward algorithm is unstructured.\\n'},\n",
       "        {'param_name': 'n_clusters',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'n_clusters should be less than n_samples.  Stop early the\\nconstruction of the tree at n_clusters. This is useful to decrease\\ncomputation time if the number of clusters is not small compared to the\\nnumber of samples. In this case, the complete tree is not computed, thus\\nthe ‘children’ output is of limited use, and the ‘parents’ output should\\nrather be used. This option is valid only when specifying a connectivity\\nmatrix.\\n'},\n",
       "        {'param_name': 'return_distance',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, return the distance between the clusters.\\n'}]},\n",
       "      'function_calling': {'name': 'ward_tree',\n",
       "       'descriptions': 'Ward clustering based on a Feature matrix. Recursively merges the pair of clusters that minimally increases\\nwithin-cluster variance. The inertia matrix uses a Heapq-based representation. This is the structured version, that takes into account some topological\\nstructure between samples. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Feature matrix representing n_samples samples to be clustered.\\n'},\n",
       "         'connectivity': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix}, default=None. Connectivity matrix. Defines for each sample the neighboring samples\\nfollowing a given structure of the data. The matrix is assumed to\\nbe symmetric and only the upper triangular half is used.\\nDefault is None, i.e, the Ward algorithm is unstructured.\\n'},\n",
       "         'n_clusters': {'type': 'integer',\n",
       "          'description': 'int, default=None. n_clusters should be less than n_samples.  Stop early the\\nconstruction of the tree at n_clusters. This is useful to decrease\\ncomputation time if the number of clusters is not small compared to the\\nnumber of samples. In this case, the complete tree is not computed, thus\\nthe ‘children’ output is of limited use, and the ‘parents’ output should\\nrather be used. This option is valid only when specifying a connectivity\\nmatrix.\\n'},\n",
       "         'return_distance': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, return the distance between the clusters.\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'AgglomerativeClustering',\n",
       "      'func_desc': 'Agglomerative Clustering.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering',\n",
       "      'function_definitions': {'function_name': 'AgglomerativeClustering',\n",
       "       'full_function': \"class sklearn.cluster.AgglomerativeClustering(n_clusters=2, *, metric='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', distance_threshold=None, compute_distances=False)\",\n",
       "       'function_text': 'Agglomerative Clustering. Recursively merges pair of clusters of sample data; uses linkage distance. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'AgglomerativeClustering',\n",
       "       'descriptions': 'Agglomerative Clustering. Recursively merges pair of clusters of sample data; uses linkage distance. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {},\n",
       "        'required': ['n_clusters=2']}}},\n",
       "     {'func_name': 'BisectingKMeans',\n",
       "      'func_desc': 'Bisecting K-Means clustering.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.BisectingKMeans.html#sklearn.cluster.BisectingKMeans',\n",
       "      'function_definitions': {'function_name': 'BisectingKMeans',\n",
       "       'full_function': \"class sklearn.cluster.BisectingKMeans(n_clusters=8, *, init='random', n_init=1, random_state=None, max_iter=300, verbose=0, tol=0.0001, copy_x=True, algorithm='lloyd', bisecting_strategy='biggest_inertia')\",\n",
       "       'function_text': 'Bisecting K-Means clustering. Read more in the User Guide. Added in version 1.1.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#bisect-k-means',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'New data to transform.\\n'}]},\n",
       "      'function_calling': {'name': 'BisectingKMeans',\n",
       "       'descriptions': 'Bisecting K-Means clustering. Read more in the User Guide. Added in version 1.1.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). New data to transform.\\n'}},\n",
       "        'required': ['n_clusters=8']}}},\n",
       "     {'func_name': 'FeatureAgglomeration',\n",
       "      'func_desc': 'Agglomerate features.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration',\n",
       "      'function_definitions': {'function_name': 'FeatureAgglomeration',\n",
       "       'full_function': \"class sklearn.cluster.FeatureAgglomeration(n_clusters=2, *, metric='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', pooling_func=<function mean>, distance_threshold=None, compute_distances=False)\",\n",
       "       'function_text': 'Agglomerate features. Recursively merges pair of clusters of features. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features) or                 (n_samples, n_samples)',\n",
       "         'param_desc': 'A M by N array of M observations in N dimensions or a length\\nM array of M one-dimensional observations.\\n'}]},\n",
       "      'function_calling': {'name': 'FeatureAgglomeration',\n",
       "       'descriptions': 'Agglomerate features. Recursively merges pair of clusters of features. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features) or                 (n_samples, n_samples). A M by N array of M observations in N dimensions or a length\\nM array of M one-dimensional observations.\\n'}},\n",
       "        'required': ['n_clusters=2']}}},\n",
       "     {'func_name': 'KMeans',\n",
       "      'func_desc': 'K-Means clustering.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans',\n",
       "      'function_definitions': {'function_name': 'KMeans',\n",
       "       'full_function': \"class sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\",\n",
       "       'function_text': 'K-Means clustering. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#k-means',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'New data to transform.\\n'}]},\n",
       "      'function_calling': {'name': 'KMeans',\n",
       "       'descriptions': 'K-Means clustering. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). New data to transform.\\n'}},\n",
       "        'required': ['n_clusters=8']}}},\n",
       "     {'func_name': 'MiniBatchKMeans',\n",
       "      'func_desc': 'Mini-Batch K-Means clustering.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans',\n",
       "      'function_definitions': {'function_name': 'MiniBatchKMeans',\n",
       "       'full_function': \"class sklearn.cluster.MiniBatchKMeans(n_clusters=8, *, init='k-means++', max_iter=100, batch_size=1024, verbose=0, compute_labels=True, random_state=None, tol=0.0, max_no_improvement=10, init_size=None, n_init='auto', reassignment_ratio=0.01)\",\n",
       "       'function_text': 'Mini-Batch K-Means clustering. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'New data to transform.\\n'}]},\n",
       "      'function_calling': {'name': 'MiniBatchKMeans',\n",
       "       'descriptions': 'Mini-Batch K-Means clustering. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). New data to transform.\\n'}},\n",
       "        'required': ['n_clusters=8']}}},\n",
       "     {'func_name': 'SpectralBiclustering',\n",
       "      'func_desc': 'Spectral biclustering (Kluger, 2003).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralBiclustering.html#sklearn.cluster.SpectralBiclustering',\n",
       "      'function_definitions': {'function_name': 'SpectralBiclustering',\n",
       "       'full_function': \"class sklearn.cluster.SpectralBiclustering(n_clusters=3, *, method='bistochastic', n_components=6, n_best=3, svd_method='randomized', n_svd_vecs=None, mini_batch=False, init='k-means++', n_init=10, random_state=None)\",\n",
       "       'function_text': 'Spectral biclustering (Kluger, 2003). Partitions rows and columns under the assumption that the data has\\nan underlying checkerboard structure. For instance, if there are\\ntwo row partitions and three column partitions, each row will\\nbelong to three biclusters, and each column will belong to two\\nbiclusters. The outer product of the corresponding row and column\\nlabel vectors gives this checkerboard structure. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/biclustering.html#spectral-biclustering',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'SpectralBiclustering',\n",
       "       'descriptions': 'Spectral biclustering (Kluger, 2003). Partitions rows and columns under the assumption that the data has\\nan underlying checkerboard structure. For instance, if there are\\ntwo row partitions and three column partitions, each row will\\nbelong to three biclusters, and each column will belong to two\\nbiclusters. The outer product of the corresponding row and column\\nlabel vectors gives this checkerboard structure. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {},\n",
       "        'required': ['n_clusters=3']}}},\n",
       "     {'func_name': 'SpectralCoclustering',\n",
       "      'func_desc': 'Spectral Co-Clustering algorithm (Dhillon, 2001).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralCoclustering.html#sklearn.cluster.SpectralCoclustering',\n",
       "      'function_definitions': {'function_name': 'SpectralCoclustering',\n",
       "       'full_function': \"class sklearn.cluster.SpectralCoclustering(n_clusters=3, *, svd_method='randomized', n_svd_vecs=None, mini_batch=False, init='k-means++', n_init=10, random_state=None)\",\n",
       "       'function_text': 'Spectral Co-Clustering algorithm (Dhillon, 2001). Clusters rows and columns of an array X to solve the relaxed\\nnormalized cut of the bipartite graph created from X as follows:\\nthe edge between row vertex i and column vertex j has weight\\nX[i, j]. The resulting bicluster structure is block-diagonal, since each\\nrow and each column belongs to exactly one bicluster. Supports sparse matrices, as long as they are nonnegative. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/biclustering.html#spectral-coclustering',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'SpectralCoclustering',\n",
       "       'descriptions': 'Spectral Co-Clustering algorithm (Dhillon, 2001). Clusters rows and columns of an array X to solve the relaxed\\nnormalized cut of the bipartite graph created from X as follows:\\nthe edge between row vertex i and column vertex j has weight\\nX[i, j]. The resulting bicluster structure is block-diagonal, since each\\nrow and each column belongs to exactly one bicluster. Supports sparse matrices, as long as they are nonnegative. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {},\n",
       "        'required': ['n_clusters=3']}}},\n",
       "     {'func_name': 'cluster_optics_dbscan',\n",
       "      'func_desc': 'Perform DBSCAN extraction for an arbitrary epsilon.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.cluster_optics_dbscan.html#sklearn.cluster.cluster_optics_dbscan',\n",
       "      'function_definitions': {'function_name': 'cluster_optics_dbscan',\n",
       "       'full_function': 'sklearn.cluster.cluster_optics_dbscan(*, reachability, core_distances, ordering, eps)',\n",
       "       'function_text': 'Perform DBSCAN extraction for an arbitrary epsilon. Extracting the clusters runs in linear time. Note that this results in\\nlabels_ which are close to a DBSCAN with\\nsimilar settings and eps, only if eps is close to max_eps.',\n",
       "       'func_text_user_guide': 'sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN',\n",
       "       'parameter_names_desc': [{'param_name': 'reachability',\n",
       "         'param_type': 'ndarray of shape (n_samples,)',\n",
       "         'param_desc': 'Reachability distances calculated by OPTICS (reachability_).\\n'},\n",
       "        {'param_name': 'core_distances',\n",
       "         'param_type': 'ndarray of shape (n_samples,)',\n",
       "         'param_desc': 'Distances at which points become core (core_distances_).\\n'},\n",
       "        {'param_name': 'ordering',\n",
       "         'param_type': 'ndarray of shape (n_samples,)',\n",
       "         'param_desc': 'OPTICS ordered point indices (ordering_).\\n'},\n",
       "        {'param_name': 'eps',\n",
       "         'param_type': 'float',\n",
       "         'param_desc': 'DBSCAN eps parameter. Must be set to < max_eps. Results\\nwill be close to DBSCAN algorithm if eps and max_eps are close\\nto one another.\\n'}]},\n",
       "      'function_calling': {'name': 'cluster_optics_dbscan',\n",
       "       'descriptions': 'Perform DBSCAN extraction for an arbitrary epsilon. Extracting the clusters runs in linear time. Note that this results in\\nlabels_ which are close to a DBSCAN with\\nsimilar settings and eps, only if eps is close to max_eps.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'reachability': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples,). Reachability distances calculated by OPTICS (reachability_).\\n'},\n",
       "         'core_distances': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples,). Distances at which points become core (core_distances_).\\n'},\n",
       "         'ordering': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples,). OPTICS ordered point indices (ordering_).\\n'},\n",
       "         'eps': {'type': 'number',\n",
       "          'description': 'float. DBSCAN eps parameter. Must be set to < max_eps. Results\\nwill be close to DBSCAN algorithm if eps and max_eps are close\\nto one another.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'compute_optics_graph',\n",
       "      'func_desc': 'Compute the OPTICS reachability graph.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.compute_optics_graph.html#sklearn.cluster.compute_optics_graph',\n",
       "      'function_definitions': {'function_name': 'compute_optics_graph',\n",
       "       'full_function': 'sklearn.cluster.compute_optics_graph(X, *, min_samples, max_eps, metric, p, metric_params, algorithm, leaf_size, n_jobs)',\n",
       "       'function_text': 'Compute the OPTICS reachability graph. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#optics',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{ndarray, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples) if metric=’precomputed’',\n",
       "         'param_desc': 'A feature array, or array of distances between samples if\\nmetric=’precomputed’.\\n'},\n",
       "        {'param_name': 'min_samples',\n",
       "         'param_type': 'int > 1 or float between 0 and 1',\n",
       "         'param_desc': 'The number of samples in a neighborhood for a point to be considered\\nas a core point. Expressed as an absolute number or a fraction of the\\nnumber of samples (rounded to be at least 2).\\n'},\n",
       "        {'param_name': 'max_eps',\n",
       "         'param_type': 'float, default=np.inf',\n",
       "         'param_desc': 'The maximum distance between two samples for one to be considered as\\nin the neighborhood of the other. Default value of np.inf will\\nidentify clusters across all scales; reducing max_eps will result\\nin shorter run times.\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'str or callable, default=’minkowski’',\n",
       "         'param_desc': \"Metric to use for distance computation. Any metric from scikit-learn\\nor scipy.spatial.distance can be used.\\nIf metric is a callable function, it is called on each\\npair of instances (rows) and the resulting value recorded. The callable\\nshould take two arrays as input and return one value indicating the\\ndistance between them. This works for Scipy’s metrics, but is less\\nefficient than passing the metric name as a string. If metric is\\n“precomputed”, X is assumed to be a distance matrix and must be square.\\nValid values for metric are:\\n\\nfrom scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,\\n‘manhattan’]\\nfrom scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,\\n‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,\\n‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,\\n‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,\\n‘yule’]\\n\\nSee the documentation for scipy.spatial.distance for details on these\\nmetrics.\\n\\nNote\\n'kulsinski' is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\\n\\n\"},\n",
       "        {'param_name': 'p',\n",
       "         'param_type': 'float, default=2',\n",
       "         'param_desc': 'Parameter for the Minkowski metric from\\npairwise_distances. When p = 1, this is\\nequivalent to using manhattan_distance (l1), and euclidean_distance\\n(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\\n'},\n",
       "        {'param_name': 'metric_params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Additional keyword arguments for the metric function.\\n'},\n",
       "        {'param_name': 'algorithm',\n",
       "         'param_type': '{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’',\n",
       "         'param_desc': 'Algorithm used to compute the nearest neighbors:\\n\\n‘ball_tree’ will use BallTree.\\n‘kd_tree’ will use KDTree.\\n‘brute’ will use a brute-force search.\\n‘auto’ will attempt to decide the most appropriate algorithm\\nbased on the values passed to fit method. (default)\\n\\nNote: fitting on sparse input will override the setting of\\nthis parameter, using brute force.\\n'},\n",
       "        {'param_name': 'leaf_size',\n",
       "         'param_type': 'int, default=30',\n",
       "         'param_desc': 'Leaf size passed to BallTree or\\nKDTree. This can affect the speed of the\\nconstruction and query, as well as the memory required to store the\\ntree. The optimal value depends on the nature of the problem.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of parallel jobs to run for neighbors search.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'}]},\n",
       "      'function_calling': {'name': 'compute_optics_graph',\n",
       "       'descriptions': 'Compute the OPTICS reachability graph. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{ndarray, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples) if metric=’precomputed’. A feature array, or array of distances between samples if\\nmetric=’precomputed’.\\n'},\n",
       "         'min_samples': {'type': 'integer',\n",
       "          'description': 'int > 1 or float between 0 and 1. The number of samples in a neighborhood for a point to be considered\\nas a core point. Expressed as an absolute number or a fraction of the\\nnumber of samples (rounded to be at least 2).\\n'},\n",
       "         'max_eps': {'type': 'number',\n",
       "          'description': 'float, default=np.inf. The maximum distance between two samples for one to be considered as\\nin the neighborhood of the other. Default value of np.inf will\\nidentify clusters across all scales; reducing max_eps will result\\nin shorter run times.\\n'},\n",
       "         'metric': {'type': 'string',\n",
       "          'description': \"str or callable, default=’minkowski’. Metric to use for distance computation. Any metric from scikit-learn\\nor scipy.spatial.distance can be used.\\nIf metric is a callable function, it is called on each\\npair of instances (rows) and the resulting value recorded. The callable\\nshould take two arrays as input and return one value indicating the\\ndistance between them. This works for Scipy’s metrics, but is less\\nefficient than passing the metric name as a string. If metric is\\n“precomputed”, X is assumed to be a distance matrix and must be square.\\nValid values for metric are:\\n\\nfrom scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,\\n‘manhattan’]\\nfrom scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,\\n‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,\\n‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,\\n‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,\\n‘yule’]\\n\\nSee the documentation for scipy.spatial.distance for details on these\\nmetrics.\\n\\nNote\\n'kulsinski' is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\\n\\n\"},\n",
       "         'p': {'type': 'number',\n",
       "          'description': 'float, default=2. Parameter for the Minkowski metric from\\npairwise_distances. When p = 1, this is\\nequivalent to using manhattan_distance (l1), and euclidean_distance\\n(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\\n'},\n",
       "         'metric_params': {'type': 'array',\n",
       "          'description': 'dict, default=None. Additional keyword arguments for the metric function.\\n'},\n",
       "         'algorithm': {'type': 'string',\n",
       "          'enum': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
       "          'description': '{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’. Algorithm used to compute the nearest neighbors:\\n\\n‘ball_tree’ will use BallTree.\\n‘kd_tree’ will use KDTree.\\n‘brute’ will use a brute-force search.\\n‘auto’ will attempt to decide the most appropriate algorithm\\nbased on the values passed to fit method. (default)\\n\\nNote: fitting on sparse input will override the setting of\\nthis parameter, using brute force.\\n'},\n",
       "         'leaf_size': {'type': 'integer',\n",
       "          'description': 'int, default=30. Leaf size passed to BallTree or\\nKDTree. This can affect the speed of the\\nconstruction and query, as well as the memory required to store the\\ntree. The optimal value depends on the nature of the problem.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of parallel jobs to run for neighbors search.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'estimate_bandwidth',\n",
       "      'func_desc': 'Estimate the bandwidth to use with the mean-shift algorithm.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.estimate_bandwidth.html#sklearn.cluster.estimate_bandwidth',\n",
       "      'function_definitions': {'function_name': 'estimate_bandwidth',\n",
       "       'full_function': 'sklearn.cluster.estimate_bandwidth(X, *, quantile=0.3, n_samples=None, random_state=0, n_jobs=None)',\n",
       "       'function_text': 'Estimate the bandwidth to use with the mean-shift algorithm. This function takes time at least quadratic in n_samples. For large\\ndatasets, it is wise to subsample by setting n_samples. Alternatively,\\nthe parameter bandwidth can be set to a small value without estimating\\nit.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-random_state',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Input points.\\n'},\n",
       "        {'param_name': 'quantile',\n",
       "         'param_type': 'float, default=0.3',\n",
       "         'param_desc': 'Should be between [0, 1]\\n0.5 means that the median of all pairwise distances is used.\\n'},\n",
       "        {'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of samples to use. If not given, all samples are used.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance, default=None',\n",
       "         'param_desc': 'The generator used to randomly select the samples from input points\\nfor bandwidth estimation. Use an int to make the randomness\\ndeterministic.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of parallel jobs to run for neighbors search.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'}]},\n",
       "      'function_calling': {'name': 'estimate_bandwidth',\n",
       "       'descriptions': 'Estimate the bandwidth to use with the mean-shift algorithm. This function takes time at least quadratic in n_samples. For large\\ndatasets, it is wise to subsample by setting n_samples. Alternatively,\\nthe parameter bandwidth can be set to a small value without estimating\\nit.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Input points.\\n'},\n",
       "         'quantile': {'type': 'number',\n",
       "          'description': 'float, default=0.3. Should be between [0, 1]\\n0.5 means that the median of all pairwise distances is used.\\n'},\n",
       "         'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of samples to use. If not given, all samples are used.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance, default=None. The generator used to randomly select the samples from input points\\nfor bandwidth estimation. Use an int to make the randomness\\ndeterministic.\\nSee Glossary.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of parallel jobs to run for neighbors search.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'kmeans_plusplus',\n",
       "      'func_desc': 'Init n_clusters seeds according to k-means++.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.kmeans_plusplus.html#sklearn.cluster.kmeans_plusplus',\n",
       "      'function_definitions': {'function_name': 'kmeans_plusplus',\n",
       "       'full_function': 'sklearn.cluster.kmeans_plusplus(X, n_clusters, *, sample_weight=None, x_squared_norms=None, random_state=None, n_local_trials=None)',\n",
       "       'function_text': 'Init n_clusters seeds according to k-means++. Added in version 0.24.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-random_state',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to pick seeds from.\\n'},\n",
       "        {'param_name': 'n_clusters',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'The number of centroids to initialize.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'The weights for each observation in X. If None, all observations\\nare assigned equal weight. sample_weight is ignored if init\\nis a callable or a user provided array.\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "        {'param_name': 'x_squared_norms',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Squared Euclidean norm of each data point.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int or RandomState instance, default=None',\n",
       "         'param_desc': 'Determines random number generation for centroid initialization. Pass\\nan int for reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'n_local_trials',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of seeding trials for each center (except the first),\\nof which the one reducing inertia the most is greedily chosen.\\nSet to None to make the number of trials depend logarithmically\\non the number of seeds (2+log(k)) which is the recommended setting.\\nSetting to 1 disables the greedy cluster selection and recovers the\\nvanilla k-means++ algorithm which was empirically shown to work less\\nwell than its greedy variant.\\n'}]},\n",
       "      'function_calling': {'name': 'kmeans_plusplus',\n",
       "       'descriptions': 'Init n_clusters seeds according to k-means++. Added in version 0.24.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data to pick seeds from.\\n'},\n",
       "         'n_clusters': {'type': 'integer',\n",
       "          'description': 'int. The number of centroids to initialize.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. The weights for each observation in X. If None, all observations\\nare assigned equal weight. sample_weight is ignored if init\\nis a callable or a user provided array.\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "         'x_squared_norms': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Squared Euclidean norm of each data point.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int or RandomState instance, default=None. Determines random number generation for centroid initialization. Pass\\nan int for reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'n_local_trials': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of seeding trials for each center (except the first),\\nof which the one reducing inertia the most is greedily chosen.\\nSet to None to make the number of trials depend logarithmically\\non the number of seeds (2+log(k)) which is the recommended setting.\\nSetting to 1 disables the greedy cluster selection and recovers the\\nvanilla k-means++ algorithm which was empirically shown to work less\\nwell than its greedy variant.\\n'}},\n",
       "        'required': ['X', 'n_clusters']}}},\n",
       "     {'func_name': 'spectral_clustering',\n",
       "      'func_desc': 'Apply clustering to a projection of the normalized Laplacian.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.spectral_clustering.html#sklearn.cluster.spectral_clustering',\n",
       "      'function_definitions': {'function_name': 'spectral_clustering',\n",
       "       'full_function': \"sklearn.cluster.spectral_clustering(affinity, *, n_clusters=8, n_components=None, eigen_solver=None, random_state=None, n_init=10, eigen_tol='auto', assign_labels='kmeans', verbose=False)\",\n",
       "       'function_text': 'Apply clustering to a projection of the normalized Laplacian. In practice Spectral Clustering is very useful when the structure of\\nthe individual clusters is highly non-convex or more generally when\\na measure of the center and spread of the cluster is not a suitable\\ndescription of the complete cluster. For instance, when clusters are\\nnested circles on the 2D plane. If affinity is the adjacency matrix of a graph, this method can be\\nused to find normalized graph cuts [1], [2]. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#r89dec4780971-1',\n",
       "       'parameter_names_desc': [{'param_name': 'affinity',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_samples)',\n",
       "         'param_desc': 'The affinity matrix describing the relationship of the samples to\\nembed. Must be symmetric.\\n\\nPossible examples:\\nadjacency matrix of a graph,\\nheat kernel of the pairwise distance matrix of the samples,\\nsymmetric k-nearest neighbours connectivity matrix of the samples.\\n\\n\\n\\n'},\n",
       "        {'param_name': 'n_clusters',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of clusters to extract.\\n'},\n",
       "        {'param_name': 'n_components',\n",
       "         'param_type': 'int, default=n_clusters',\n",
       "         'param_desc': 'Number of eigenvectors to use for the spectral embedding.\\n'},\n",
       "        {'param_name': 'eigen_solver',\n",
       "         'param_type': '{None, ‘arpack’, ‘lobpcg’, or ‘amg’}',\n",
       "         'param_desc': \"The eigenvalue decomposition method. If None then 'arpack' is used.\\nSee [4] for more details regarding 'lobpcg'.\\nEigensolver 'amg' runs 'lobpcg' with optional\\nAlgebraic MultiGrid preconditioning and requires pyamg to be installed.\\nIt can be faster on very large sparse problems [6] and [7].\\n\"},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance, default=None',\n",
       "         'param_desc': \"A pseudo random number generator used for the initialization\\nof the lobpcg eigenvectors decomposition when eigen_solver ==\\n'amg', and for the K-Means initialization. Use an int to make\\nthe results deterministic across calls (See\\nGlossary).\\n\\nNote\\nWhen using eigen_solver == 'amg',\\nit is necessary to also fix the global numpy seed with\\nnp.random.seed(int) to get deterministic results. See\\npyamg/pyamg#139 for further\\ninformation.\\n\\n\"},\n",
       "        {'param_name': 'n_init',\n",
       "         'param_type': 'int, default=10',\n",
       "         'param_desc': \"Number of time the k-means algorithm will be run with different\\ncentroid seeds. The final results will be the best output of n_init\\nconsecutive runs in terms of inertia. Only used if\\nassign_labels='kmeans'.\\n\"},\n",
       "        {'param_name': 'eigen_tol',\n",
       "         'param_type': 'float, default=”auto”',\n",
       "         'param_desc': 'Stopping criterion for eigendecomposition of the Laplacian matrix.\\nIf eigen_tol=\"auto\" then the passed tolerance will depend on the\\neigen_solver:\\n\\nIf eigen_solver=\"arpack\", then eigen_tol=0.0;\\nIf eigen_solver=\"lobpcg\" or eigen_solver=\"amg\", then\\neigen_tol=None which configures the underlying lobpcg solver to\\nautomatically resolve the value according to their heuristics. See,\\nscipy.sparse.linalg.lobpcg for details.\\n\\nNote that when using eigen_solver=\"lobpcg\" or eigen_solver=\"amg\"\\nvalues of tol<1e-5 may lead to convergence issues and should be\\navoided.\\n\\nAdded in version 1.2: Added ‘auto’ option.\\n\\n'},\n",
       "        {'param_name': 'assign_labels',\n",
       "         'param_type': '{‘kmeans’, ‘discretize’, ‘cluster_qr’}, default=’kmeans’',\n",
       "         'param_desc': 'The strategy to use to assign labels in the embedding\\nspace.  There are three ways to assign labels after the Laplacian\\nembedding.  k-means can be applied and is a popular choice. But it can\\nalso be sensitive to initialization. Discretization is another\\napproach which is less sensitive to random initialization [3].\\nThe cluster_qr method [5] directly extracts clusters from eigenvectors\\nin spectral clustering. In contrast to k-means and discretization, cluster_qr\\nhas no tuning parameters and is not an iterative method, yet may outperform\\nk-means and discretization in terms of both quality and speed.\\n\\nChanged in version 1.1: Added new labeling method ‘cluster_qr’.\\n\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Verbosity mode.\\n\\nAdded in version 0.24.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'spectral_clustering',\n",
       "       'descriptions': 'Apply clustering to a projection of the normalized Laplacian. In practice Spectral Clustering is very useful when the structure of\\nthe individual clusters is highly non-convex or more generally when\\na measure of the center and spread of the cluster is not a suitable\\ndescription of the complete cluster. For instance, when clusters are\\nnested circles on the 2D plane. If affinity is the adjacency matrix of a graph, this method can be\\nused to find normalized graph cuts [1], [2]. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'affinity': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_samples). The affinity matrix describing the relationship of the samples to\\nembed. Must be symmetric.\\n\\nPossible examples:\\nadjacency matrix of a graph,\\nheat kernel of the pairwise distance matrix of the samples,\\nsymmetric k-nearest neighbours connectivity matrix of the samples.\\n\\n\\n\\n'},\n",
       "         'n_clusters': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of clusters to extract.\\n'},\n",
       "         'n_components': {'type': 'integer',\n",
       "          'description': 'int, default=n_clusters. Number of eigenvectors to use for the spectral embedding.\\n'},\n",
       "         'eigen_solver': {'type': 'string',\n",
       "          'enum': ['None', 'arpack', 'lobpcg', 'or amg'],\n",
       "          'description': \"{None, ‘arpack’, ‘lobpcg’, or ‘amg’}. The eigenvalue decomposition method. If None then 'arpack' is used.\\nSee [4] for more details regarding 'lobpcg'.\\nEigensolver 'amg' runs 'lobpcg' with optional\\nAlgebraic MultiGrid preconditioning and requires pyamg to be installed.\\nIt can be faster on very large sparse problems [6] and [7].\\n\"},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': \"int, RandomState instance, default=None. A pseudo random number generator used for the initialization\\nof the lobpcg eigenvectors decomposition when eigen_solver ==\\n'amg', and for the K-Means initialization. Use an int to make\\nthe results deterministic across calls (See\\nGlossary).\\n\\nNote\\nWhen using eigen_solver == 'amg',\\nit is necessary to also fix the global numpy seed with\\nnp.random.seed(int) to get deterministic results. See\\npyamg/pyamg#139 for further\\ninformation.\\n\\n\"},\n",
       "         'n_init': {'type': 'integer',\n",
       "          'description': \"int, default=10. Number of time the k-means algorithm will be run with different\\ncentroid seeds. The final results will be the best output of n_init\\nconsecutive runs in terms of inertia. Only used if\\nassign_labels='kmeans'.\\n\"},\n",
       "         'eigen_tol': {'type': 'number',\n",
       "          'description': 'float, default=”auto”. Stopping criterion for eigendecomposition of the Laplacian matrix.\\nIf eigen_tol=\"auto\" then the passed tolerance will depend on the\\neigen_solver:\\n\\nIf eigen_solver=\"arpack\", then eigen_tol=0.0;\\nIf eigen_solver=\"lobpcg\" or eigen_solver=\"amg\", then\\neigen_tol=None which configures the underlying lobpcg solver to\\nautomatically resolve the value according to their heuristics. See,\\nscipy.sparse.linalg.lobpcg for details.\\n\\nNote that when using eigen_solver=\"lobpcg\" or eigen_solver=\"amg\"\\nvalues of tol<1e-5 may lead to convergence issues and should be\\navoided.\\n\\nAdded in version 1.2: Added ‘auto’ option.\\n\\n'},\n",
       "         'assign_labels': {'type': 'string',\n",
       "          'enum': ['kmeans', 'discretize', 'cluster_qr'],\n",
       "          'description': '{‘kmeans’, ‘discretize’, ‘cluster_qr’}, default=’kmeans’. The strategy to use to assign labels in the embedding\\nspace.  There are three ways to assign labels after the Laplacian\\nembedding.  k-means can be applied and is a popular choice. But it can\\nalso be sensitive to initialization. Discretization is another\\napproach which is less sensitive to random initialization [3].\\nThe cluster_qr method [5] directly extracts clusters from eigenvectors\\nin spectral clustering. In contrast to k-means and discretization, cluster_qr\\nhas no tuning parameters and is not an iterative method, yet may outperform\\nk-means and discretization in terms of both quality and speed.\\n\\nChanged in version 1.1: Added new labeling method ‘cluster_qr’.\\n\\n'},\n",
       "         'verbose': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Verbosity mode.\\n\\nAdded in version 0.24.\\n\\n'}},\n",
       "        'required': ['affinity']}}}]}],\n",
       "  'name': 'sklearn.cluster',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.cluster.html'},\n",
       " 'sklearn.compose.html': {'functions': [{'defaults': [{'func_name': 'ColumnTransformer',\n",
       "      'func_desc': 'Applies transformers to columns of an array or pandas DataFrame.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer',\n",
       "      'function_definitions': {'function_name': 'ColumnTransformer',\n",
       "       'full_function': \"class sklearn.compose.ColumnTransformer(transformers, *, remainder='drop', sparse_threshold=0.3, n_jobs=None, transformer_weights=None, verbose=False, verbose_feature_names_out=True, force_int_remainder_cols=True)\",\n",
       "       'function_text': 'Applies transformers to columns of an array or pandas DataFrame. This estimator allows different columns or column subsets of the input\\nto be transformed separately and the features generated by each transformer\\nwill be concatenated to form a single feature space.\\nThis is useful for heterogeneous or columnar data, to combine several\\nfeature extraction mechanisms or transformations into a single transformer. Read more in the User Guide. Added in version 0.20.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/compose.html#column-transformer',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, dataframe} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to be transformed by subset.\\n'},\n",
       "        {'param_name': '**params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Parameters to be passed to the underlying transformers’ transform\\nmethod.\\nYou can only pass this if metadata routing is enabled, which you\\ncan enable using sklearn.set_config(enable_metadata_routing=True).\\n\\nAdded in version 1.4.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'ColumnTransformer',\n",
       "       'descriptions': 'Applies transformers to columns of an array or pandas DataFrame. This estimator allows different columns or column subsets of the input\\nto be transformed separately and the features generated by each transformer\\nwill be concatenated to form a single feature space.\\nThis is useful for heterogeneous or columnar data, to combine several\\nfeature extraction mechanisms or transformations into a single transformer. Read more in the User Guide. Added in version 0.20.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, dataframe} of shape (n_samples, n_features). The data to be transformed by subset.\\n'}},\n",
       "        'required': ['transformers']}}},\n",
       "     {'func_name': 'make_column_selector',\n",
       "      'func_desc': 'Create a callable to select columns to be used with ColumnTransformer.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html#sklearn.compose.make_column_selector',\n",
       "      'function_definitions': {'function_name': 'make_column_selector',\n",
       "       'full_function': 'class sklearn.compose.make_column_selector(pattern=None, *, dtype_include=None, dtype_exclude=None)',\n",
       "       'function_text': 'Create a callable to select columns to be used with\\nColumnTransformer. make_column_selector can select columns based on datatype or the\\ncolumns name with a regex. When using multiple selection criteria, all\\ncriteria must match for a column to be selected. For an example of how to use make_column_selector within a\\nColumnTransformer to select columns based on data type (i.e.\\ndtype), refer to\\nColumn Transformer with Mixed Types.',\n",
       "       'func_text_user_guide': 'sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer',\n",
       "       'parameter_names_desc': [{'param_name': 'df',\n",
       "         'param_type': 'dataframe of shape (n_features, n_samples)',\n",
       "         'param_desc': 'DataFrame to select columns from.\\n'}]},\n",
       "      'function_calling': {'name': 'make_column_selector',\n",
       "       'descriptions': 'Create a callable to select columns to be used with\\nColumnTransformer. make_column_selector can select columns based on datatype or the\\ncolumns name with a regex. When using multiple selection criteria, all\\ncriteria must match for a column to be selected. For an example of how to use make_column_selector within a\\nColumnTransformer to select columns based on data type (i.e.\\ndtype), refer to\\nColumn Transformer with Mixed Types.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'df': {'type': 'object',\n",
       "          'description': 'dataframe of shape (n_features, n_samples). DataFrame to select columns from.\\n'}},\n",
       "        'required': ['pattern=None']}}},\n",
       "     {'func_name': 'TransformedTargetRegressor',\n",
       "      'func_desc': 'Meta-estimator to regress on a transformed target.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html#sklearn.compose.TransformedTargetRegressor',\n",
       "      'function_definitions': {'function_name': 'TransformedTargetRegressor',\n",
       "       'full_function': 'class sklearn.compose.TransformedTargetRegressor(regressor=None, *, transformer=None, func=None, inverse_func=None, check_inverse=True)',\n",
       "       'function_text': 'Meta-estimator to regress on a transformed target. Useful for applying a non-linear transformation to the target y in\\nregression problems. This transformation can be given as a Transformer\\nsuch as the QuantileTransformer or as a\\nfunction and its inverse such as np.log and np.exp. The computation during fit is: or: The computation during predict is: or: Read more in the User Guide. Added in version 0.20.',\n",
       "       'func_text_user_guide': 'sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'TransformedTargetRegressor',\n",
       "       'descriptions': 'Meta-estimator to regress on a transformed target. Useful for applying a non-linear transformation to the target y in\\nregression problems. This transformation can be given as a Transformer\\nsuch as the QuantileTransformer or as a\\nfunction and its inverse such as np.log and np.exp. The computation during fit is: or: The computation during predict is: or: Read more in the User Guide. Added in version 0.20.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['regressor=None']}}},\n",
       "     {'func_name': 'make_column_transformer',\n",
       "      'func_desc': 'Construct a ColumnTransformer from the given transformers.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html#sklearn.compose.make_column_transformer',\n",
       "      'function_definitions': {'function_name': 'make_column_transformer',\n",
       "       'full_function': \"sklearn.compose.make_column_transformer(*transformers, remainder='drop', sparse_threshold=0.3, n_jobs=None, verbose=False, verbose_feature_names_out=True, force_int_remainder_cols=True)\",\n",
       "       'function_text': 'Construct a ColumnTransformer from the given transformers. This is a shorthand for the ColumnTransformer constructor; it does not\\nrequire, and does not permit, naming the transformers. Instead, they will\\nbe given names automatically based on their types. It also does not allow\\nweighting with transformer_weights. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/compose.html#make-column-transformer',\n",
       "       'parameter_names_desc': [{'param_name': '*transformers',\n",
       "         'param_type': 'tuples',\n",
       "         'param_desc': 'Tuples of the form (transformer, columns) specifying the\\ntransformer objects to be applied to subsets of the data.\\n\\ntransformer{‘drop’, ‘passthrough’} or estimatorEstimator must support fit and transform.\\nSpecial-cased strings ‘drop’ and ‘passthrough’ are accepted as\\nwell, to indicate to drop the columns or to pass them through\\nuntransformed, respectively.\\n\\ncolumnsstr,  array-like of str, int, array-like of int, slice,                 array-like of bool or callableIndexes the data on its second axis. Integers are interpreted as\\npositional columns, while strings can reference DataFrame columns\\nby name. A scalar string or int should be used where\\ntransformer expects X to be a 1d array-like (vector),\\notherwise a 2d array will be passed to the transformer.\\nA callable is passed the input data X and can return any of the\\nabove. To select multiple columns by name or dtype, you can use\\nmake_column_selector.\\n\\n\\n'},\n",
       "        {'param_name': 'remainder',\n",
       "         'param_type': '{‘drop’, ‘passthrough’} or estimator, default=’drop’',\n",
       "         'param_desc': \"By default, only the specified columns in transformers are\\ntransformed and combined in the output, and the non-specified\\ncolumns are dropped. (default of 'drop').\\nBy specifying remainder='passthrough', all remaining columns that\\nwere not specified in transformers will be automatically passed\\nthrough. This subset of columns is concatenated with the output of\\nthe transformers.\\nBy setting remainder to be an estimator, the remaining\\nnon-specified columns will use the remainder estimator. The\\nestimator must support fit and transform.\\n\"},\n",
       "        {'param_name': 'sparse_threshold',\n",
       "         'param_type': 'float, default=0.3',\n",
       "         'param_desc': 'If the transformed output consists of a mix of sparse and dense data,\\nit will be stacked as a sparse matrix if the density is lower than this\\nvalue. Use sparse_threshold=0 to always return dense.\\nWhen the transformed output consists of all sparse or all dense data,\\nthe stacked result will be sparse or dense, respectively, and this\\nkeyword will be ignored.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of jobs to run in parallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the time elapsed while fitting each transformer will be\\nprinted as it is completed.\\n'},\n",
       "        {'param_name': 'verbose_feature_names_out',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, ColumnTransformer.get_feature_names_out will prefix\\nall feature names with the name of the transformer that generated that\\nfeature.\\nIf False, ColumnTransformer.get_feature_names_out will not\\nprefix any feature names and will error if feature names are not\\nunique.\\n\\nAdded in version 1.0.\\n\\n'},\n",
       "        {'param_name': 'force_int_remainder_cols',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Force the columns of the last entry of transformers_, which\\ncorresponds to the “remainder” transformer, to always be stored as\\nindices (int) rather than column names (str). See description of the\\nColumnTransformer.transformers_ attribute for details.\\n\\nNote\\nIf you do not access the list of columns for the remainder columns\\nin the ColumnTransformer.transformers_ fitted attribute,\\nyou do not need to set this parameter.\\n\\n\\nAdded in version 1.5.\\n\\n\\nChanged in version 1.7: The default value for force_int_remainder_cols will change from\\nTrue to False in version 1.7.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'make_column_transformer',\n",
       "       'descriptions': 'Construct a ColumnTransformer from the given transformers. This is a shorthand for the ColumnTransformer constructor; it does not\\nrequire, and does not permit, naming the transformers. Instead, they will\\nbe given names automatically based on their types. It also does not allow\\nweighting with transformer_weights. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'*transformers': {'type': 'array',\n",
       "          'description': 'tuples. Tuples of the form (transformer, columns) specifying the\\ntransformer objects to be applied to subsets of the data.\\n\\ntransformer{‘drop’, ‘passthrough’} or estimatorEstimator must support fit and transform.\\nSpecial-cased strings ‘drop’ and ‘passthrough’ are accepted as\\nwell, to indicate to drop the columns or to pass them through\\nuntransformed, respectively.\\n\\ncolumnsstr,  array-like of str, int, array-like of int, slice,                 array-like of bool or callableIndexes the data on its second axis. Integers are interpreted as\\npositional columns, while strings can reference DataFrame columns\\nby name. A scalar string or int should be used where\\ntransformer expects X to be a 1d array-like (vector),\\notherwise a 2d array will be passed to the transformer.\\nA callable is passed the input data X and can return any of the\\nabove. To select multiple columns by name or dtype, you can use\\nmake_column_selector.\\n\\n\\n'},\n",
       "         'remainder': {'type': 'string',\n",
       "          'enum': ['drop', 'passthrough'],\n",
       "          'description': \"{‘drop’, ‘passthrough’} or estimator, default=’drop’. By default, only the specified columns in transformers are\\ntransformed and combined in the output, and the non-specified\\ncolumns are dropped. (default of 'drop').\\nBy specifying remainder='passthrough', all remaining columns that\\nwere not specified in transformers will be automatically passed\\nthrough. This subset of columns is concatenated with the output of\\nthe transformers.\\nBy setting remainder to be an estimator, the remaining\\nnon-specified columns will use the remainder estimator. The\\nestimator must support fit and transform.\\n\"},\n",
       "         'sparse_threshold': {'type': 'number',\n",
       "          'description': 'float, default=0.3. If the transformed output consists of a mix of sparse and dense data,\\nit will be stacked as a sparse matrix if the density is lower than this\\nvalue. Use sparse_threshold=0 to always return dense.\\nWhen the transformed output consists of all sparse or all dense data,\\nthe stacked result will be sparse or dense, respectively, and this\\nkeyword will be ignored.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of jobs to run in parallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "         'verbose': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the time elapsed while fitting each transformer will be\\nprinted as it is completed.\\n'},\n",
       "         'verbose_feature_names_out': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, ColumnTransformer.get_feature_names_out will prefix\\nall feature names with the name of the transformer that generated that\\nfeature.\\nIf False, ColumnTransformer.get_feature_names_out will not\\nprefix any feature names and will error if feature names are not\\nunique.\\n\\nAdded in version 1.0.\\n\\n'},\n",
       "         'force_int_remainder_cols': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Force the columns of the last entry of transformers_, which\\ncorresponds to the “remainder” transformer, to always be stored as\\nindices (int) rather than column names (str). See description of the\\nColumnTransformer.transformers_ attribute for details.\\n\\nNote\\nIf you do not access the list of columns for the remainder columns\\nin the ColumnTransformer.transformers_ fitted attribute,\\nyou do not need to set this parameter.\\n\\n\\nAdded in version 1.5.\\n\\n\\nChanged in version 1.7: The default value for force_int_remainder_cols will change from\\nTrue to False in version 1.7.\\n\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.compose',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.compose.html'},\n",
       " 'sklearn.covariance.html': {'functions': [{'defaults': [{'func_name': 'EllipticEnvelope',\n",
       "      'func_desc': 'An object for detecting outliers in a Gaussian distributed dataset.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope',\n",
       "      'function_definitions': {'function_name': 'EllipticEnvelope',\n",
       "       'full_function': 'class sklearn.covariance.EllipticEnvelope(*, store_precision=True, assume_centered=False, support_fraction=None, contamination=0.1, random_state=None)',\n",
       "       'function_text': 'An object for detecting outliers in a Gaussian distributed dataset. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'EllipticEnvelope',\n",
       "       'descriptions': 'An object for detecting outliers in a Gaussian distributed dataset. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'GraphicalLasso',\n",
       "      'func_desc': 'Sparse inverse covariance estimation with an l1-penalized estimator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLasso.html#sklearn.covariance.GraphicalLasso',\n",
       "      'function_definitions': {'function_name': 'GraphicalLasso',\n",
       "       'full_function': \"class sklearn.covariance.GraphicalLasso(alpha=0.01, *, mode='cd', covariance=None, tol=0.0001, enet_tol=0.0001, max_iter=100, verbose=False, eps=2.220446049250313e-16, assume_centered=False)\",\n",
       "       'function_text': 'Sparse inverse covariance estimation with an l1-penalized estimator. Read more in the User Guide. Changed in version v0.20: GraphLasso has been renamed to GraphicalLasso',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/covariance.html#sparse-inverse-covariance',\n",
       "       'parameter_names_desc': [{'param_name': 'X_test',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for X_test parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'GraphicalLasso',\n",
       "       'descriptions': 'Sparse inverse covariance estimation with an l1-penalized estimator. Read more in the User Guide. Changed in version v0.20: GraphLasso has been renamed to GraphicalLasso',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X_test': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for X_test parameter in score.\\n'}},\n",
       "        'required': ['alpha=0.01']}}},\n",
       "     {'func_name': 'LedoitWolf',\n",
       "      'func_desc': 'LedoitWolf Estimator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html#sklearn.covariance.LedoitWolf',\n",
       "      'function_definitions': {'function_name': 'LedoitWolf',\n",
       "       'full_function': 'class sklearn.covariance.LedoitWolf(*, store_precision=True, assume_centered=False, block_size=1000)',\n",
       "       'function_text': 'LedoitWolf Estimator. Ledoit-Wolf is a particular form of shrinkage, where the shrinkage\\ncoefficient is computed using O. Ledoit and M. Wolf’s formula as\\ndescribed in “A Well-Conditioned Estimator for Large-Dimensional\\nCovariance Matrices”, Ledoit and Wolf, Journal of Multivariate\\nAnalysis, Volume 88, Issue 2, February 2004, pages 365-411. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/covariance.html#shrunk-covariance',\n",
       "       'parameter_names_desc': [{'param_name': 'X_test',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for X_test parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'LedoitWolf',\n",
       "       'descriptions': 'LedoitWolf Estimator. Ledoit-Wolf is a particular form of shrinkage, where the shrinkage\\ncoefficient is computed using O. Ledoit and M. Wolf’s formula as\\ndescribed in “A Well-Conditioned Estimator for Large-Dimensional\\nCovariance Matrices”, Ledoit and Wolf, Journal of Multivariate\\nAnalysis, Volume 88, Issue 2, February 2004, pages 365-411. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X_test': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for X_test parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'OAS',\n",
       "      'func_desc': 'Oracle Approximating Shrinkage Estimator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.covariance.OAS.html#sklearn.covariance.OAS',\n",
       "      'function_definitions': {'function_name': 'OAS',\n",
       "       'full_function': 'class sklearn.covariance.OAS(*, store_precision=True, assume_centered=False)',\n",
       "       'function_text': 'Oracle Approximating Shrinkage Estimator. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/covariance.html#shrunk-covariance',\n",
       "       'parameter_names_desc': [{'param_name': 'X_test',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for X_test parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'OAS',\n",
       "       'descriptions': 'Oracle Approximating Shrinkage Estimator. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X_test': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for X_test parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'empirical_covariance',\n",
       "      'func_desc': 'Compute the Maximum likelihood covariance estimator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.covariance.empirical_covariance.html#sklearn.covariance.empirical_covariance',\n",
       "      'function_definitions': {'function_name': 'empirical_covariance',\n",
       "       'full_function': 'sklearn.covariance.empirical_covariance(X, *, assume_centered=False)',\n",
       "       'function_text': 'Compute the Maximum likelihood covariance estimator.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'ndarray of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Data from which to compute the covariance estimate.\\n'},\n",
       "        {'param_name': 'assume_centered',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, data will not be centered before computation.\\nUseful when working with data whose mean is almost, but not exactly\\nzero.\\nIf False, data will be centered before computation.\\n'}]},\n",
       "      'function_calling': {'name': 'empirical_covariance',\n",
       "       'descriptions': 'Compute the Maximum likelihood covariance estimator.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples, n_features). Data from which to compute the covariance estimate.\\n'},\n",
       "         'assume_centered': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, data will not be centered before computation.\\nUseful when working with data whose mean is almost, but not exactly\\nzero.\\nIf False, data will be centered before computation.\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'ledoit_wolf',\n",
       "      'func_desc': 'Estimate the shrunk Ledoit-Wolf covariance matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.covariance.ledoit_wolf.html#sklearn.covariance.ledoit_wolf',\n",
       "      'function_definitions': {'function_name': 'ledoit_wolf',\n",
       "       'full_function': 'sklearn.covariance.ledoit_wolf(X, *, assume_centered=False, block_size=1000)',\n",
       "       'function_text': 'Estimate the shrunk Ledoit-Wolf covariance matrix. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/covariance.html#shrunk-covariance',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Data from which to compute the covariance estimate.\\n'},\n",
       "        {'param_name': 'assume_centered',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, data will not be centered before computation.\\nUseful to work with data whose mean is significantly equal to\\nzero but is not exactly zero.\\nIf False, data will be centered before computation.\\n'},\n",
       "        {'param_name': 'block_size',\n",
       "         'param_type': 'int, default=1000',\n",
       "         'param_desc': 'Size of blocks into which the covariance matrix will be split.\\nThis is purely a memory optimization and does not affect results.\\n'}]},\n",
       "      'function_calling': {'name': 'ledoit_wolf',\n",
       "       'descriptions': 'Estimate the shrunk Ledoit-Wolf covariance matrix. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Data from which to compute the covariance estimate.\\n'},\n",
       "         'assume_centered': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, data will not be centered before computation.\\nUseful to work with data whose mean is significantly equal to\\nzero but is not exactly zero.\\nIf False, data will be centered before computation.\\n'},\n",
       "         'block_size': {'type': 'integer',\n",
       "          'description': 'int, default=1000. Size of blocks into which the covariance matrix will be split.\\nThis is purely a memory optimization and does not affect results.\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'oas',\n",
       "      'func_desc': 'Estimate covariance with the Oracle Approximating Shrinkage.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/oas-function.html#sklearn.covariance.oas',\n",
       "      'function_definitions': {'function_name': 'oas',\n",
       "       'full_function': 'sklearn.covariance.oas(X, *, assume_centered=False)',\n",
       "       'function_text': 'Estimate covariance with the Oracle Approximating Shrinkage. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/covariance.html#shrunk-covariance',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Data from which to compute the covariance estimate.\\n'},\n",
       "        {'param_name': 'assume_centered',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, data will not be centered before computation.\\nUseful to work with data whose mean is significantly equal to\\nzero but is not exactly zero.\\nIf False, data will be centered before computation.\\n'}]},\n",
       "      'function_calling': {'name': 'oas',\n",
       "       'descriptions': 'Estimate covariance with the Oracle Approximating Shrinkage. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Data from which to compute the covariance estimate.\\n'},\n",
       "         'assume_centered': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, data will not be centered before computation.\\nUseful to work with data whose mean is significantly equal to\\nzero but is not exactly zero.\\nIf False, data will be centered before computation.\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'EmpiricalCovariance',\n",
       "      'func_desc': 'Maximum likelihood covariance estimator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EmpiricalCovariance.html#sklearn.covariance.EmpiricalCovariance',\n",
       "      'function_definitions': {'function_name': 'EmpiricalCovariance',\n",
       "       'full_function': 'class sklearn.covariance.EmpiricalCovariance(*, store_precision=True, assume_centered=False)',\n",
       "       'function_text': 'Maximum likelihood covariance estimator. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/covariance.html#covariance',\n",
       "       'parameter_names_desc': [{'param_name': 'X_test',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for X_test parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'EmpiricalCovariance',\n",
       "       'descriptions': 'Maximum likelihood covariance estimator. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X_test': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for X_test parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'GraphicalLassoCV',\n",
       "      'func_desc': 'Sparse inverse covariance w/ cross-validated choice of the l1 penalty.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLassoCV.html#sklearn.covariance.GraphicalLassoCV',\n",
       "      'function_definitions': {'function_name': 'GraphicalLassoCV',\n",
       "       'full_function': \"class sklearn.covariance.GraphicalLassoCV(*, alphas=4, n_refinements=4, cv=None, tol=0.0001, enet_tol=0.0001, max_iter=100, mode='cd', n_jobs=None, verbose=False, eps=2.220446049250313e-16, assume_centered=False)\",\n",
       "       'function_text': 'Sparse inverse covariance w/ cross-validated choice of the l1 penalty. See glossary entry for cross-validation estimator. Read more in the User Guide. Changed in version v0.20: GraphLassoCV has been renamed to GraphicalLassoCV',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-cross-validation-estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'X_test',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for X_test parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'GraphicalLassoCV',\n",
       "       'descriptions': 'Sparse inverse covariance w/ cross-validated choice of the l1 penalty. See glossary entry for cross-validation estimator. Read more in the User Guide. Changed in version v0.20: GraphLassoCV has been renamed to GraphicalLassoCV',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X_test': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for X_test parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'MinCovDet',\n",
       "      'func_desc': 'Minimum Covariance Determinant (MCD): robust estimator of covariance.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.covariance.MinCovDet.html#sklearn.covariance.MinCovDet',\n",
       "      'function_definitions': {'function_name': 'MinCovDet',\n",
       "       'full_function': 'class sklearn.covariance.MinCovDet(*, store_precision=True, assume_centered=False, support_fraction=None, random_state=None)',\n",
       "       'function_text': 'Minimum Covariance Determinant (MCD): robust estimator of covariance. The Minimum Covariance Determinant covariance estimator is to be applied\\non Gaussian-distributed data, but could still be relevant on data\\ndrawn from a unimodal, symmetric distribution. It is not meant to be used\\nwith multi-modal data (the algorithm used to fit a MinCovDet object is\\nlikely to fail in such a case).\\nOne should consider projection pursuit methods to deal with multi-modal\\ndatasets. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/covariance.html#robust-covariance',\n",
       "       'parameter_names_desc': [{'param_name': 'X_test',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for X_test parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'MinCovDet',\n",
       "       'descriptions': 'Minimum Covariance Determinant (MCD): robust estimator of covariance. The Minimum Covariance Determinant covariance estimator is to be applied\\non Gaussian-distributed data, but could still be relevant on data\\ndrawn from a unimodal, symmetric distribution. It is not meant to be used\\nwith multi-modal data (the algorithm used to fit a MinCovDet object is\\nlikely to fail in such a case).\\nOne should consider projection pursuit methods to deal with multi-modal\\ndatasets. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X_test': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for X_test parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'ShrunkCovariance',\n",
       "      'func_desc': 'Covariance estimator with shrinkage.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.covariance.ShrunkCovariance.html#sklearn.covariance.ShrunkCovariance',\n",
       "      'function_definitions': {'function_name': 'ShrunkCovariance',\n",
       "       'full_function': 'class sklearn.covariance.ShrunkCovariance(*, store_precision=True, assume_centered=False, shrinkage=0.1)',\n",
       "       'function_text': 'Covariance estimator with shrinkage. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/covariance.html#shrunk-covariance',\n",
       "       'parameter_names_desc': [{'param_name': 'X_test',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for X_test parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'ShrunkCovariance',\n",
       "       'descriptions': 'Covariance estimator with shrinkage. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X_test': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for X_test parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'graphical_lasso',\n",
       "      'func_desc': 'L1-penalized covariance estimator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.covariance.graphical_lasso.html#sklearn.covariance.graphical_lasso',\n",
       "      'function_definitions': {'function_name': 'graphical_lasso',\n",
       "       'full_function': \"sklearn.covariance.graphical_lasso(emp_cov, alpha, *, mode='cd', tol=0.0001, enet_tol=0.0001, max_iter=100, verbose=False, return_costs=False, eps=2.220446049250313e-16, return_n_iter=False)\",\n",
       "       'function_text': 'L1-penalized covariance estimator. Read more in the User Guide. Changed in version v0.20: graph_lasso has been renamed to graphical_lasso',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/covariance.html#sparse-inverse-covariance',\n",
       "       'parameter_names_desc': [{'param_name': 'emp_cov',\n",
       "         'param_type': 'array-like of shape (n_features, n_features)',\n",
       "         'param_desc': 'Empirical covariance from which to compute the covariance estimate.\\n'},\n",
       "        {'param_name': 'alpha',\n",
       "         'param_type': 'float',\n",
       "         'param_desc': 'The regularization parameter: the higher alpha, the more\\nregularization, the sparser the inverse covariance.\\nRange is (0, inf].\\n'},\n",
       "        {'param_name': 'mode',\n",
       "         'param_type': '{‘cd’, ‘lars’}, default=’cd’',\n",
       "         'param_desc': 'The Lasso solver to use: coordinate descent or LARS. Use LARS for\\nvery sparse underlying graphs, where p > n. Elsewhere prefer cd\\nwhich is more numerically stable.\\n'},\n",
       "        {'param_name': 'tol',\n",
       "         'param_type': 'float, default=1e-4',\n",
       "         'param_desc': 'The tolerance to declare convergence: if the dual gap goes below\\nthis value, iterations are stopped. Range is (0, inf].\\n'},\n",
       "        {'param_name': 'enet_tol',\n",
       "         'param_type': 'float, default=1e-4',\n",
       "         'param_desc': 'The tolerance for the elastic net solver used to calculate the descent\\ndirection. This parameter controls the accuracy of the search direction\\nfor a given column update, not of the overall parameter estimate. Only\\nused for mode=’cd’. Range is (0, inf].\\n'},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The maximum number of iterations.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If verbose is True, the objective function and dual gap are\\nprinted at each iteration.\\n'},\n",
       "        {'param_name': 'return_costs',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If return_costs is True, the objective function and dual gap\\nat each iteration are returned.\\n'},\n",
       "        {'param_name': 'eps',\n",
       "         'param_type': 'float, default=eps',\n",
       "         'param_desc': 'The machine-precision regularization in the computation of the\\nCholesky diagonal factors. Increase this for very ill-conditioned\\nsystems. Default is np.finfo(np.float64).eps.\\n'},\n",
       "        {'param_name': 'return_n_iter',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether or not to return the number of iterations.\\n'}]},\n",
       "      'function_calling': {'name': 'graphical_lasso',\n",
       "       'descriptions': 'L1-penalized covariance estimator. Read more in the User Guide. Changed in version v0.20: graph_lasso has been renamed to graphical_lasso',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'emp_cov': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_features, n_features). Empirical covariance from which to compute the covariance estimate.\\n'},\n",
       "         'alpha': {'type': 'number',\n",
       "          'description': 'float. The regularization parameter: the higher alpha, the more\\nregularization, the sparser the inverse covariance.\\nRange is (0, inf].\\n'},\n",
       "         'mode': {'type': 'string',\n",
       "          'enum': ['cd', 'lars'],\n",
       "          'description': '{‘cd’, ‘lars’}, default=’cd’. The Lasso solver to use: coordinate descent or LARS. Use LARS for\\nvery sparse underlying graphs, where p > n. Elsewhere prefer cd\\nwhich is more numerically stable.\\n'},\n",
       "         'tol': {'type': 'number',\n",
       "          'description': 'float, default=1e-4. The tolerance to declare convergence: if the dual gap goes below\\nthis value, iterations are stopped. Range is (0, inf].\\n'},\n",
       "         'enet_tol': {'type': 'number',\n",
       "          'description': 'float, default=1e-4. The tolerance for the elastic net solver used to calculate the descent\\ndirection. This parameter controls the accuracy of the search direction\\nfor a given column update, not of the overall parameter estimate. Only\\nused for mode=’cd’. Range is (0, inf].\\n'},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=100. The maximum number of iterations.\\n'},\n",
       "         'verbose': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If verbose is True, the objective function and dual gap are\\nprinted at each iteration.\\n'},\n",
       "         'return_costs': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If return_costs is True, the objective function and dual gap\\nat each iteration are returned.\\n'},\n",
       "         'eps': {'type': 'number',\n",
       "          'description': 'float, default=eps. The machine-precision regularization in the computation of the\\nCholesky diagonal factors. Increase this for very ill-conditioned\\nsystems. Default is np.finfo(np.float64).eps.\\n'},\n",
       "         'return_n_iter': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether or not to return the number of iterations.\\n'}},\n",
       "        'required': ['emp_cov', 'alpha']}}},\n",
       "     {'func_name': 'ledoit_wolf_shrinkage',\n",
       "      'func_desc': 'Estimate the shrunk Ledoit-Wolf covariance matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.covariance.ledoit_wolf_shrinkage.html#sklearn.covariance.ledoit_wolf_shrinkage',\n",
       "      'function_definitions': {'function_name': 'ledoit_wolf_shrinkage',\n",
       "       'full_function': 'sklearn.covariance.ledoit_wolf_shrinkage(X, assume_centered=False, block_size=1000)',\n",
       "       'function_text': 'Estimate the shrunk Ledoit-Wolf covariance matrix. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/covariance.html#shrunk-covariance',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage.\\n'},\n",
       "        {'param_name': 'assume_centered',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, data will not be centered before computation.\\nUseful to work with data whose mean is significantly equal to\\nzero but is not exactly zero.\\nIf False, data will be centered before computation.\\n'},\n",
       "        {'param_name': 'block_size',\n",
       "         'param_type': 'int, default=1000',\n",
       "         'param_desc': 'Size of blocks into which the covariance matrix will be split.\\n'}]},\n",
       "      'function_calling': {'name': 'ledoit_wolf_shrinkage',\n",
       "       'descriptions': 'Estimate the shrunk Ledoit-Wolf covariance matrix. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage.\\n'},\n",
       "         'assume_centered': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, data will not be centered before computation.\\nUseful to work with data whose mean is significantly equal to\\nzero but is not exactly zero.\\nIf False, data will be centered before computation.\\n'},\n",
       "         'block_size': {'type': 'integer',\n",
       "          'description': 'int, default=1000. Size of blocks into which the covariance matrix will be split.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'shrunk_covariance',\n",
       "      'func_desc': 'Calculate covariance matrices shrunk on the diagonal.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.covariance.shrunk_covariance.html#sklearn.covariance.shrunk_covariance',\n",
       "      'function_definitions': {'function_name': 'shrunk_covariance',\n",
       "       'full_function': 'sklearn.covariance.shrunk_covariance(emp_cov, shrinkage=0.1)',\n",
       "       'function_text': 'Calculate covariance matrices shrunk on the diagonal. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/covariance.html#shrunk-covariance',\n",
       "       'parameter_names_desc': [{'param_name': 'emp_cov',\n",
       "         'param_type': 'array-like of shape (…, n_features, n_features)',\n",
       "         'param_desc': 'Covariance matrices to be shrunk, at least 2D ndarray.\\n'},\n",
       "        {'param_name': 'shrinkage',\n",
       "         'param_type': 'float, default=0.1',\n",
       "         'param_desc': 'Coefficient in the convex combination used for the computation\\nof the shrunk estimate. Range is [0, 1].\\n'}]},\n",
       "      'function_calling': {'name': 'shrunk_covariance',\n",
       "       'descriptions': 'Calculate covariance matrices shrunk on the diagonal. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'emp_cov': {'type': 'array',\n",
       "          'description': 'array-like of shape (…, n_features, n_features). Covariance matrices to be shrunk, at least 2D ndarray.\\n'},\n",
       "         'shrinkage': {'type': 'number',\n",
       "          'description': 'float, default=0.1. Coefficient in the convex combination used for the computation\\nof the shrunk estimate. Range is [0, 1].\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.covariance',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.covariance.html'},\n",
       " 'sklearn.cross_decomposition.html': {'functions': [{'defaults': [{'func_name': 'CCA',\n",
       "      'func_desc': 'Canonical Correlation Analysis, also known as \"Mode B\" PLS.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#sklearn.cross_decomposition.CCA',\n",
       "      'function_definitions': {'function_name': 'CCA',\n",
       "       'full_function': 'class sklearn.cross_decomposition.CCA(n_components=2, *, scale=True, max_iter=500, tol=1e-06, copy=True)',\n",
       "       'function_text': 'Canonical Correlation Analysis, also known as “Mode B” PLS. For a comparison between other cross decomposition algorithms, see\\nCompare cross decomposition methods. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/cross_decomposition/plot_compare_cross_decomposition.html#sphx-glr-auto-examples-cross-decomposition-plot-compare-cross-decomposition-py',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Samples to transform.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples, n_targets), default=None',\n",
       "         'param_desc': 'Target vectors.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': 'array-like of shape (n_samples, n_targets), default=None',\n",
       "         'param_desc': 'Target vectors.\\n\\nDeprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.\\n\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to copy X and Y, or perform in-place normalization.\\n'}]},\n",
       "      'function_calling': {'name': 'CCA',\n",
       "       'descriptions': 'Canonical Correlation Analysis, also known as “Mode B” PLS. For a comparison between other cross decomposition algorithms, see\\nCompare cross decomposition methods. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Samples to transform.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_targets), default=None. Target vectors.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_targets), default=None. Target vectors.\\n\\nDeprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.\\n\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to copy X and Y, or perform in-place normalization.\\n'}},\n",
       "        'required': ['n_components=2']}}},\n",
       "     {'func_name': 'PLSRegression',\n",
       "      'func_desc': 'PLS regression.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression',\n",
       "      'function_definitions': {'function_name': 'PLSRegression',\n",
       "       'full_function': 'class sklearn.cross_decomposition.PLSRegression(n_components=2, *, scale=True, max_iter=500, tol=1e-06, copy=True)',\n",
       "       'function_text': 'PLS regression. PLSRegression is also known as PLS2 or PLS1, depending on the number of\\ntargets. For a comparison between other cross decomposition algorithms, see\\nCompare cross decomposition methods. Read more in the User Guide. Added in version 0.8.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/cross_decomposition/plot_compare_cross_decomposition.html#sphx-glr-auto-examples-cross-decomposition-plot-compare-cross-decomposition-py',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Samples to transform.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples, n_targets), default=None',\n",
       "         'param_desc': 'Target vectors.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': 'array-like of shape (n_samples, n_targets), default=None',\n",
       "         'param_desc': 'Target vectors.\\n\\nDeprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.\\n\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to copy X and Y, or perform in-place normalization.\\n'}]},\n",
       "      'function_calling': {'name': 'PLSRegression',\n",
       "       'descriptions': 'PLS regression. PLSRegression is also known as PLS2 or PLS1, depending on the number of\\ntargets. For a comparison between other cross decomposition algorithms, see\\nCompare cross decomposition methods. Read more in the User Guide. Added in version 0.8.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Samples to transform.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_targets), default=None. Target vectors.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_targets), default=None. Target vectors.\\n\\nDeprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.\\n\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to copy X and Y, or perform in-place normalization.\\n'}},\n",
       "        'required': ['n_components=2']}}},\n",
       "     {'func_name': 'PLSCanonical',\n",
       "      'func_desc': 'Partial Least Squares transformer and regressor.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical',\n",
       "      'function_definitions': {'function_name': 'PLSCanonical',\n",
       "       'full_function': \"class sklearn.cross_decomposition.PLSCanonical(n_components=2, *, scale=True, algorithm='nipals', max_iter=500, tol=1e-06, copy=True)\",\n",
       "       'function_text': 'Partial Least Squares transformer and regressor. For a comparison between other cross decomposition algorithms, see\\nCompare cross decomposition methods. Read more in the User Guide. Added in version 0.8.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/cross_decomposition/plot_compare_cross_decomposition.html#sphx-glr-auto-examples-cross-decomposition-plot-compare-cross-decomposition-py',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Samples to transform.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples, n_targets), default=None',\n",
       "         'param_desc': 'Target vectors.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': 'array-like of shape (n_samples, n_targets), default=None',\n",
       "         'param_desc': 'Target vectors.\\n\\nDeprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.\\n\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to copy X and Y, or perform in-place normalization.\\n'}]},\n",
       "      'function_calling': {'name': 'PLSCanonical',\n",
       "       'descriptions': 'Partial Least Squares transformer and regressor. For a comparison between other cross decomposition algorithms, see\\nCompare cross decomposition methods. Read more in the User Guide. Added in version 0.8.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Samples to transform.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_targets), default=None. Target vectors.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_targets), default=None. Target vectors.\\n\\nDeprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.\\n\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to copy X and Y, or perform in-place normalization.\\n'}},\n",
       "        'required': ['n_components=2']}}},\n",
       "     {'func_name': 'PLSSVD',\n",
       "      'func_desc': 'Partial Least Square SVD.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSSVD.html#sklearn.cross_decomposition.PLSSVD',\n",
       "      'function_definitions': {'function_name': 'PLSSVD',\n",
       "       'full_function': 'class sklearn.cross_decomposition.PLSSVD(n_components=2, *, scale=True, copy=True)',\n",
       "       'function_text': \"Partial Least Square SVD. This transformer simply performs a SVD on the cross-covariance matrix\\nX'Y. It is able to project both the training data X and the targets\\nY. The training data X is projected on the left singular vectors, while\\nthe targets are projected on the right singular vectors. Read more in the User Guide. Added in version 0.8.\",\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_decomposition.html#cross-decomposition',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Samples to be transformed.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None',\n",
       "         'param_desc': 'Targets.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None',\n",
       "         'param_desc': 'Targets.\\n\\nDeprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'PLSSVD',\n",
       "       'descriptions': \"Partial Least Square SVD. This transformer simply performs a SVD on the cross-covariance matrix\\nX'Y. It is able to project both the training data X and the targets\\nY. The training data X is projected on the left singular vectors, while\\nthe targets are projected on the right singular vectors. Read more in the User Guide. Added in version 0.8.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Samples to be transformed.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None. Targets.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None. Targets.\\n\\nDeprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.\\n\\n'}},\n",
       "        'required': ['n_components=2']}}}]}],\n",
       "  'name': 'sklearn.cross_decomposition',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.cross_decomposition.html'},\n",
       " 'sklearn.datasets.html': {'functions': [{'Loaders': [{'func_name': 'clear_data_home',\n",
       "      'func_desc': 'Delete all the content of the data home cache.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.clear_data_home.html#sklearn.datasets.clear_data_home',\n",
       "      'function_definitions': {'function_name': 'clear_data_home',\n",
       "       'full_function': 'sklearn.datasets.clear_data_home(data_home=None)',\n",
       "       'function_text': 'Delete all the content of the data home cache.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'data_home',\n",
       "         'param_type': 'str or path-like, default=None',\n",
       "         'param_desc': 'The path to scikit-learn data directory. If None, the default path\\nis ~/scikit_learn_data.\\n'}]},\n",
       "      'function_calling': {'name': 'clear_data_home',\n",
       "       'descriptions': 'Delete all the content of the data home cache.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'data_home': {'type': 'string',\n",
       "          'description': 'str or path-like, default=None. The path to scikit-learn data directory. If None, the default path\\nis ~/scikit_learn_data.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'fetch_20newsgroups',\n",
       "      'func_desc': 'Load the filenames and data from the 20 newsgroups dataset (classification).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups',\n",
       "      'function_definitions': {'function_name': 'fetch_20newsgroups',\n",
       "       'full_function': \"sklearn.datasets.fetch_20newsgroups(*, data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True, return_X_y=False, n_retries=3, delay=1.0)\",\n",
       "       'function_text': 'Load the filenames and data from the 20 newsgroups dataset (classification). Download it if necessary. Classes 20 Samples total 18846 Dimensionality 1 Features text Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/real_world.html#newsgroups-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'data_home',\n",
       "         'param_type': 'str or path-like, default=None',\n",
       "         'param_desc': 'Specify a download and cache folder for the datasets. If None,\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "        {'param_name': 'subset',\n",
       "         'param_type': '{‘train’, ‘test’, ‘all’}, default=’train’',\n",
       "         'param_desc': 'Select the dataset to load: ‘train’ for the training set, ‘test’\\nfor the test set, ‘all’ for both, with shuffled ordering.\\n'},\n",
       "        {'param_name': 'categories',\n",
       "         'param_type': 'array-like, dtype=str, default=None',\n",
       "         'param_desc': 'If None (default), load all the categories.\\nIf not None, list of category names to load (other categories\\nignored).\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether or not to shuffle the data: might be important for models that\\nmake the assumption that the samples are independent and identically\\ndistributed (i.i.d.), such as stochastic gradient descent.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=42',\n",
       "         'param_desc': 'Determines random number generation for dataset shuffling. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'remove',\n",
       "         'param_type': 'tuple, default=()',\n",
       "         'param_desc': 'May contain any subset of (‘headers’, ‘footers’, ‘quotes’). Each of\\nthese are kinds of text that will be detected and removed from the\\nnewsgroup posts, preventing classifiers from overfitting on\\nmetadata.\\n‘headers’ removes newsgroup headers, ‘footers’ removes blocks at the\\nends of posts that look like signatures, and ‘quotes’ removes lines\\nthat appear to be quoting another post.\\n‘headers’ follows an exact standard; the other filters are not always\\ncorrect.\\n'},\n",
       "        {'param_name': 'download_if_missing',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "        {'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (data.data, data.target) instead of a Bunch\\nobject.\\n\\nAdded in version 0.22.\\n\\n'},\n",
       "        {'param_name': 'n_retries',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "        {'param_name': 'delay',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'fetch_20newsgroups',\n",
       "       'descriptions': 'Load the filenames and data from the 20 newsgroups dataset (classification). Download it if necessary. Classes 20 Samples total 18846 Dimensionality 1 Features text Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'data_home': {'type': 'string',\n",
       "          'description': 'str or path-like, default=None. Specify a download and cache folder for the datasets. If None,\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "         'subset': {'type': 'string',\n",
       "          'enum': ['train', 'test', 'all'],\n",
       "          'description': '{‘train’, ‘test’, ‘all’}, default=’train’. Select the dataset to load: ‘train’ for the training set, ‘test’\\nfor the test set, ‘all’ for both, with shuffled ordering.\\n'},\n",
       "         'categories': {'type': 'string',\n",
       "          'description': 'array-like, dtype=str, default=None. If None (default), load all the categories.\\nIf not None, list of category names to load (other categories\\nignored).\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether or not to shuffle the data: might be important for models that\\nmake the assumption that the samples are independent and identically\\ndistributed (i.i.d.), such as stochastic gradient descent.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=42. Determines random number generation for dataset shuffling. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'remove': {'type': 'array',\n",
       "          'description': 'tuple, default=(). May contain any subset of (‘headers’, ‘footers’, ‘quotes’). Each of\\nthese are kinds of text that will be detected and removed from the\\nnewsgroup posts, preventing classifiers from overfitting on\\nmetadata.\\n‘headers’ removes newsgroup headers, ‘footers’ removes blocks at the\\nends of posts that look like signatures, and ‘quotes’ removes lines\\nthat appear to be quoting another post.\\n‘headers’ follows an exact standard; the other filters are not always\\ncorrect.\\n'},\n",
       "         'download_if_missing': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "         'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (data.data, data.target) instead of a Bunch\\nobject.\\n\\nAdded in version 0.22.\\n\\n'},\n",
       "         'n_retries': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "         'delay': {'type': 'number',\n",
       "          'description': 'float, default=1.0. Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'fetch_california_housing',\n",
       "      'func_desc': 'Load the California housing dataset (regression).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing',\n",
       "      'function_definitions': {'function_name': 'fetch_california_housing',\n",
       "       'full_function': 'sklearn.datasets.fetch_california_housing(*, data_home=None, download_if_missing=True, return_X_y=False, as_frame=False, n_retries=3, delay=1.0)',\n",
       "       'function_text': 'Load the California housing dataset (regression). Samples total 20640 Dimensionality 8 Features real Target real 0.15 - 5. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/real_world.html#california-housing-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'data_home',\n",
       "         'param_type': 'str or path-like, default=None',\n",
       "         'param_desc': 'Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "        {'param_name': 'download_if_missing',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "        {'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (data.data, data.target) instead of a Bunch\\nobject.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'as_frame',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric, string or categorical). The target is\\na pandas DataFrame or Series depending on the number of target_columns.\\n\\nAdded in version 0.23.\\n\\n'},\n",
       "        {'param_name': 'n_retries',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "        {'param_name': 'delay',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'fetch_california_housing',\n",
       "       'descriptions': 'Load the California housing dataset (regression). Samples total 20640 Dimensionality 8 Features real Target real 0.15 - 5. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'data_home': {'type': 'string',\n",
       "          'description': 'str or path-like, default=None. Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "         'download_if_missing': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "         'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (data.data, data.target) instead of a Bunch\\nobject.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'as_frame': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric, string or categorical). The target is\\na pandas DataFrame or Series depending on the number of target_columns.\\n\\nAdded in version 0.23.\\n\\n'},\n",
       "         'n_retries': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "         'delay': {'type': 'number',\n",
       "          'description': 'float, default=1.0. Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'fetch_kddcup99',\n",
       "      'func_desc': 'Load the kddcup99 dataset (classification).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_kddcup99.html#sklearn.datasets.fetch_kddcup99',\n",
       "      'function_definitions': {'function_name': 'fetch_kddcup99',\n",
       "       'full_function': 'sklearn.datasets.fetch_kddcup99(*, subset=None, data_home=None, shuffle=False, random_state=None, percent10=True, download_if_missing=True, return_X_y=False, as_frame=False, n_retries=3, delay=1.0)',\n",
       "       'function_text': 'Load the kddcup99 dataset (classification). Download it if necessary. Classes 23 Samples total 4898431 Dimensionality 41 Features discrete (int) or continuous (float) Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/real_world.html#kddcup99-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'subset',\n",
       "         'param_type': '{‘SA’, ‘SF’, ‘http’, ‘smtp’}, default=None',\n",
       "         'param_desc': 'To return the corresponding classical subsets of kddcup 99.\\nIf None, return the entire kddcup 99 dataset.\\n'},\n",
       "        {'param_name': 'data_home',\n",
       "         'param_type': 'str or path-like, default=None',\n",
       "         'param_desc': 'Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n\\nAdded in version 0.19.\\n\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to shuffle dataset.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': \"Determines random number generation for dataset shuffling and for\\nselection of abnormal samples if subset='SA'. Pass an int for\\nreproducible output across multiple function calls.\\nSee Glossary.\\n\"},\n",
       "        {'param_name': 'percent10',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to load only 10 percent of the data.\\n'},\n",
       "        {'param_name': 'download_if_missing',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "        {'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (data, target) instead of a Bunch object. See\\nbelow for more information about the data and target object.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'as_frame',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns a pandas Dataframe for the data and target\\nobjects in the Bunch returned object; Bunch return object will also\\nhave a frame member.\\n\\nAdded in version 0.24.\\n\\n'},\n",
       "        {'param_name': 'n_retries',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "        {'param_name': 'delay',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'fetch_kddcup99',\n",
       "       'descriptions': 'Load the kddcup99 dataset (classification). Download it if necessary. Classes 23 Samples total 4898431 Dimensionality 41 Features discrete (int) or continuous (float) Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'subset': {'type': 'string',\n",
       "          'enum': ['SA', 'SF', 'http', 'smtp'],\n",
       "          'description': '{‘SA’, ‘SF’, ‘http’, ‘smtp’}, default=None. To return the corresponding classical subsets of kddcup 99.\\nIf None, return the entire kddcup 99 dataset.\\n'},\n",
       "         'data_home': {'type': 'string',\n",
       "          'description': 'str or path-like, default=None. Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n\\nAdded in version 0.19.\\n\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to shuffle dataset.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': \"int, RandomState instance or None, default=None. Determines random number generation for dataset shuffling and for\\nselection of abnormal samples if subset='SA'. Pass an int for\\nreproducible output across multiple function calls.\\nSee Glossary.\\n\"},\n",
       "         'percent10': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to load only 10 percent of the data.\\n'},\n",
       "         'download_if_missing': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "         'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (data, target) instead of a Bunch object. See\\nbelow for more information about the data and target object.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'as_frame': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns a pandas Dataframe for the data and target\\nobjects in the Bunch returned object; Bunch return object will also\\nhave a frame member.\\n\\nAdded in version 0.24.\\n\\n'},\n",
       "         'n_retries': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "         'delay': {'type': 'number',\n",
       "          'description': 'float, default=1.0. Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'fetch_lfw_people',\n",
       "      'func_desc': 'Load the Labeled Faces in the Wild (LFW) people dataset (classification).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html#sklearn.datasets.fetch_lfw_people',\n",
       "      'function_definitions': {'function_name': 'fetch_lfw_people',\n",
       "       'full_function': 'sklearn.datasets.fetch_lfw_people(*, data_home=None, funneled=True, resize=0.5, min_faces_per_person=0, color=False, slice_=(slice(70, 195, None), slice(78, 172, None)), download_if_missing=True, return_X_y=False, n_retries=3, delay=1.0)',\n",
       "       'function_text': 'Load the Labeled Faces in the Wild (LFW) people dataset (classification). Download it if necessary. Classes 5749 Samples total 13233 Dimensionality 5828 Features real, between 0 and 255 Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/real_world.html#labeled-faces-in-the-wild-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'data_home',\n",
       "         'param_type': 'str or path-like, default=None',\n",
       "         'param_desc': 'Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "        {'param_name': 'funneled',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Download and use the funneled variant of the dataset.\\n'},\n",
       "        {'param_name': 'resize',\n",
       "         'param_type': 'float or None, default=0.5',\n",
       "         'param_desc': 'Ratio used to resize the each face picture. If None, no resizing is\\nperformed.\\n'},\n",
       "        {'param_name': 'min_faces_per_person',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The extracted dataset will only retain pictures of people that have at\\nleast min_faces_per_person different pictures.\\n'},\n",
       "        {'param_name': 'color',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Keep the 3 RGB channels instead of averaging them to a single\\ngray level channel. If color is True the shape of the data has\\none more dimension than the shape with color = False.\\n'},\n",
       "        {'param_name': 'slice_',\n",
       "         'param_type': 'tuple of slice, default=(slice(70, 195), slice(78, 172))',\n",
       "         'param_desc': 'Provide a custom 2D slice (height, width) to extract the\\n‘interesting’ part of the jpeg files and avoid use statistical\\ncorrelation from the background.\\n'},\n",
       "        {'param_name': 'download_if_missing',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "        {'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (dataset.data, dataset.target) instead of a Bunch\\nobject. See below for more information about the dataset.data and\\ndataset.target object.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'n_retries',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "        {'param_name': 'delay',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'fetch_lfw_people',\n",
       "       'descriptions': 'Load the Labeled Faces in the Wild (LFW) people dataset (classification). Download it if necessary. Classes 5749 Samples total 13233 Dimensionality 5828 Features real, between 0 and 255 Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'data_home': {'type': 'string',\n",
       "          'description': 'str or path-like, default=None. Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "         'funneled': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Download and use the funneled variant of the dataset.\\n'},\n",
       "         'resize': {'type': 'number',\n",
       "          'description': 'float or None, default=0.5. Ratio used to resize the each face picture. If None, no resizing is\\nperformed.\\n'},\n",
       "         'min_faces_per_person': {'type': 'integer',\n",
       "          'description': 'int, default=None. The extracted dataset will only retain pictures of people that have at\\nleast min_faces_per_person different pictures.\\n'},\n",
       "         'color': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Keep the 3 RGB channels instead of averaging them to a single\\ngray level channel. If color is True the shape of the data has\\none more dimension than the shape with color = False.\\n'},\n",
       "         'slice_': {'type': 'array',\n",
       "          'description': 'tuple of slice, default=(slice(70, 195), slice(78, 172)). Provide a custom 2D slice (height, width) to extract the\\n‘interesting’ part of the jpeg files and avoid use statistical\\ncorrelation from the background.\\n'},\n",
       "         'download_if_missing': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "         'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (dataset.data, dataset.target) instead of a Bunch\\nobject. See below for more information about the dataset.data and\\ndataset.target object.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'n_retries': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "         'delay': {'type': 'number',\n",
       "          'description': 'float, default=1.0. Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'fetch_openml',\n",
       "      'func_desc': 'Fetch dataset from openml by name or dataset id.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml',\n",
       "      'function_definitions': {'function_name': 'fetch_openml',\n",
       "       'full_function': \"sklearn.datasets.fetch_openml(name: str | None = None, *, version: str | int = 'active', data_id: int | None = None, data_home: str | PathLike | None = None, target_column: str | List | None = 'default-target', cache: bool = True, return_X_y: bool = False, as_frame: str | bool = 'auto', n_retries: int = 3, delay: float = 1.0, parser: str = 'auto', read_csv_kwargs: Dict | None = None)\",\n",
       "       'function_text': 'Fetch dataset from openml by name or dataset id. Datasets are uniquely identified by either an integer ID or by a\\ncombination of name and version (i.e. there might be multiple\\nversions of the ‘iris’ dataset). Please give either name or data_id\\n(not both). In case a name is given, a version can also be\\nprovided. Read more in the User Guide. Added in version 0.20. Note EXPERIMENTAL The API is experimental (particularly the return value structure),\\nand might have small backward-incompatible changes without notice\\nor warning in future releases.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/loading_other_datasets.html#openml',\n",
       "       'parameter_names_desc': [{'param_name': 'name',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'String identifier of the dataset. Note that OpenML can have multiple\\ndatasets with the same name.\\n'},\n",
       "        {'param_name': 'version',\n",
       "         'param_type': 'int or ‘active’, default=’active’',\n",
       "         'param_desc': 'Version of the dataset. Can only be provided if also name is given.\\nIf ‘active’ the oldest version that’s still active is used. Since\\nthere may be more than one active version of a dataset, and those\\nversions may fundamentally be different from one another, setting an\\nexact version is highly recommended.\\n'},\n",
       "        {'param_name': 'data_id',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'OpenML ID of the dataset. The most specific way of retrieving a\\ndataset. If data_id is not given, name (and potential version) are\\nused to obtain a dataset.\\n'},\n",
       "        {'param_name': 'data_home',\n",
       "         'param_type': 'str or path-like, default=None',\n",
       "         'param_desc': 'Specify another download and cache folder for the data sets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "        {'param_name': 'target_column',\n",
       "         'param_type': 'str, list or None, default=’default-target’',\n",
       "         'param_desc': 'Specify the column name in the data to use as target. If\\n‘default-target’, the standard target column a stored on the server\\nis used. If None, all columns are returned as data and the\\ntarget is None. If list (of strings), all columns with these names\\nare returned as multi-target (Note: not all scikit-learn classifiers\\ncan handle all types of multi-output combinations).\\n'},\n",
       "        {'param_name': 'cache',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to cache the downloaded datasets into data_home.\\n'},\n",
       "        {'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (data, target) instead of a Bunch object. See\\nbelow for more information about the data and target objects.\\n'},\n",
       "        {'param_name': 'as_frame',\n",
       "         'param_type': 'bool or ‘auto’, default=’auto’',\n",
       "         'param_desc': 'If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric, string or categorical). The target is\\na pandas DataFrame or Series depending on the number of target_columns.\\nThe Bunch will contain a frame attribute with the target and the\\ndata. If return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as describe above.\\nIf as_frame is ‘auto’, the data and target will be converted to\\nDataFrame or Series as if as_frame is set to True, unless the dataset\\nis stored in sparse format.\\nIf as_frame is False, the data and target will be NumPy arrays and\\nthe data will only contain numerical values when parser=\"liac-arff\"\\nwhere the categories are provided in the attribute categories of the\\nBunch instance. When parser=\"pandas\", no ordinal encoding is made.\\n\\nChanged in version 0.24: The default value of as_frame changed from False to \\'auto\\'\\nin 0.24.\\n\\n'},\n",
       "        {'param_name': 'n_retries',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of retries when HTTP errors or network timeouts are encountered.\\nError with status code 412 won’t be retried as they represent OpenML\\ngeneric errors.\\n'},\n",
       "        {'param_name': 'delay',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'Number of seconds between retries.\\n'},\n",
       "        {'param_name': 'parser',\n",
       "         'param_type': '{“auto”, “pandas”, “liac-arff”}, default=”auto”',\n",
       "         'param_desc': 'Parser used to load the ARFF file. Two parsers are implemented:\\n\\n\"pandas\": this is the most efficient parser. However, it requires\\npandas to be installed and can only open dense datasets.\\n\"liac-arff\": this is a pure Python ARFF parser that is much less\\nmemory- and CPU-efficient. It deals with sparse ARFF datasets.\\n\\nIf \"auto\", the parser is chosen automatically such that \"liac-arff\"\\nis selected for sparse ARFF datasets, otherwise \"pandas\" is selected.\\n\\nAdded in version 1.2.\\n\\n\\nChanged in version 1.4: The default value of parser changes from \"liac-arff\" to\\n\"auto\".\\n\\n'},\n",
       "        {'param_name': 'read_csv_kwargs',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Keyword arguments passed to pandas.read_csv when loading the data\\nfrom a ARFF file and using the pandas parser. It can allow to\\noverwrite some default parameters.\\n\\nAdded in version 1.3.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'fetch_openml',\n",
       "       'descriptions': 'Fetch dataset from openml by name or dataset id. Datasets are uniquely identified by either an integer ID or by a\\ncombination of name and version (i.e. there might be multiple\\nversions of the ‘iris’ dataset). Please give either name or data_id\\n(not both). In case a name is given, a version can also be\\nprovided. Read more in the User Guide. Added in version 0.20. Note EXPERIMENTAL The API is experimental (particularly the return value structure),\\nand might have small backward-incompatible changes without notice\\nor warning in future releases.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'name': {'type': 'string',\n",
       "          'description': 'str, default=None. String identifier of the dataset. Note that OpenML can have multiple\\ndatasets with the same name.\\n'},\n",
       "         'version': {'type': 'integer',\n",
       "          'description': 'int or ‘active’, default=’active’. Version of the dataset. Can only be provided if also name is given.\\nIf ‘active’ the oldest version that’s still active is used. Since\\nthere may be more than one active version of a dataset, and those\\nversions may fundamentally be different from one another, setting an\\nexact version is highly recommended.\\n'},\n",
       "         'data_id': {'type': 'integer',\n",
       "          'description': 'int, default=None. OpenML ID of the dataset. The most specific way of retrieving a\\ndataset. If data_id is not given, name (and potential version) are\\nused to obtain a dataset.\\n'},\n",
       "         'data_home': {'type': 'string',\n",
       "          'description': 'str or path-like, default=None. Specify another download and cache folder for the data sets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "         'target_column': {'type': 'string',\n",
       "          'description': 'str, list or None, default=’default-target’. Specify the column name in the data to use as target. If\\n‘default-target’, the standard target column a stored on the server\\nis used. If None, all columns are returned as data and the\\ntarget is None. If list (of strings), all columns with these names\\nare returned as multi-target (Note: not all scikit-learn classifiers\\ncan handle all types of multi-output combinations).\\n'},\n",
       "         'cache': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to cache the downloaded datasets into data_home.\\n'},\n",
       "         'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (data, target) instead of a Bunch object. See\\nbelow for more information about the data and target objects.\\n'},\n",
       "         'as_frame': {'type': 'boolean',\n",
       "          'description': 'bool or ‘auto’, default=’auto’. If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric, string or categorical). The target is\\na pandas DataFrame or Series depending on the number of target_columns.\\nThe Bunch will contain a frame attribute with the target and the\\ndata. If return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as describe above.\\nIf as_frame is ‘auto’, the data and target will be converted to\\nDataFrame or Series as if as_frame is set to True, unless the dataset\\nis stored in sparse format.\\nIf as_frame is False, the data and target will be NumPy arrays and\\nthe data will only contain numerical values when parser=\"liac-arff\"\\nwhere the categories are provided in the attribute categories of the\\nBunch instance. When parser=\"pandas\", no ordinal encoding is made.\\n\\nChanged in version 0.24: The default value of as_frame changed from False to \\'auto\\'\\nin 0.24.\\n\\n'},\n",
       "         'n_retries': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of retries when HTTP errors or network timeouts are encountered.\\nError with status code 412 won’t be retried as they represent OpenML\\ngeneric errors.\\n'},\n",
       "         'delay': {'type': 'number',\n",
       "          'description': 'float, default=1.0. Number of seconds between retries.\\n'},\n",
       "         'parser': {'type': 'string',\n",
       "          'enum': ['auto', 'pandas', 'liac-arff'],\n",
       "          'description': '{“auto”, “pandas”, “liac-arff”}, default=”auto”. Parser used to load the ARFF file. Two parsers are implemented:\\n\\n\"pandas\": this is the most efficient parser. However, it requires\\npandas to be installed and can only open dense datasets.\\n\"liac-arff\": this is a pure Python ARFF parser that is much less\\nmemory- and CPU-efficient. It deals with sparse ARFF datasets.\\n\\nIf \"auto\", the parser is chosen automatically such that \"liac-arff\"\\nis selected for sparse ARFF datasets, otherwise \"pandas\" is selected.\\n\\nAdded in version 1.2.\\n\\n\\nChanged in version 1.4: The default value of parser changes from \"liac-arff\" to\\n\"auto\".\\n\\n'},\n",
       "         'read_csv_kwargs': {'type': 'array',\n",
       "          'description': 'dict, default=None. Keyword arguments passed to pandas.read_csv when loading the data\\nfrom a ARFF file and using the pandas parser. It can allow to\\noverwrite some default parameters.\\n\\nAdded in version 1.3.\\n\\n'}},\n",
       "        'required': ['name: str | None = None']}}},\n",
       "     {'func_name': 'fetch_species_distributions',\n",
       "      'func_desc': 'Loader for species distribution dataset from Phillips et.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_species_distributions.html#sklearn.datasets.fetch_species_distributions',\n",
       "      'function_definitions': {'function_name': 'fetch_species_distributions',\n",
       "       'full_function': 'sklearn.datasets.fetch_species_distributions(*, data_home=None, download_if_missing=True, n_retries=3, delay=1.0)',\n",
       "       'function_text': 'Loader for species distribution dataset from Phillips et. al. (2006). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/real_world.html#species-distribution-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'data_home',\n",
       "         'param_type': 'str or path-like, default=None',\n",
       "         'param_desc': 'Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "        {'param_name': 'download_if_missing',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "        {'param_name': 'n_retries',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "        {'param_name': 'delay',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'fetch_species_distributions',\n",
       "       'descriptions': 'Loader for species distribution dataset from Phillips et. al. (2006). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'data_home': {'type': 'string',\n",
       "          'description': 'str or path-like, default=None. Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "         'download_if_missing': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "         'n_retries': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "         'delay': {'type': 'number',\n",
       "          'description': 'float, default=1.0. Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'load_breast_cancer',\n",
       "      'func_desc': 'Load and return the breast cancer wisconsin dataset (classification).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer',\n",
       "      'function_definitions': {'function_name': 'load_breast_cancer',\n",
       "       'full_function': 'sklearn.datasets.load_breast_cancer(*, return_X_y=False, as_frame=False)',\n",
       "       'function_text': 'Load and return the breast cancer wisconsin dataset (classification). The breast cancer dataset is a classic and very easy binary classification\\ndataset. Classes 2 Samples per class 212(M),357(B) Samples total 569 Dimensionality 30 Features real, positive The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is\\ndownloaded from:\\nhttps://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/toy_dataset.html#breast-cancer-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (data, target) instead of a Bunch object.\\nSee below for more information about the data and target object.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "        {'param_name': 'as_frame',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric). The target is\\na pandas DataFrame or Series depending on the number of target columns.\\nIf return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as described below.\\n\\nAdded in version 0.23.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'load_breast_cancer',\n",
       "       'descriptions': 'Load and return the breast cancer wisconsin dataset (classification). The breast cancer dataset is a classic and very easy binary classification\\ndataset. Classes 2 Samples per class 212(M),357(B) Samples total 569 Dimensionality 30 Features real, positive The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is\\ndownloaded from:\\nhttps://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (data, target) instead of a Bunch object.\\nSee below for more information about the data and target object.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "         'as_frame': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric). The target is\\na pandas DataFrame or Series depending on the number of target columns.\\nIf return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as described below.\\n\\nAdded in version 0.23.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'load_digits',\n",
       "      'func_desc': 'Load and return the digits dataset (classification).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits',\n",
       "      'function_definitions': {'function_name': 'load_digits',\n",
       "       'full_function': 'sklearn.datasets.load_digits(*, n_class=10, return_X_y=False, as_frame=False)',\n",
       "       'function_text': 'Load and return the digits dataset (classification). Each datapoint is a 8x8 image of a digit. Classes 10 Samples per class ~180 Samples total 1797 Dimensionality 64 Features integers 0-16 This is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/toy_dataset.html#digits-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'n_class',\n",
       "         'param_type': 'int, default=10',\n",
       "         'param_desc': 'The number of classes to return. Between 0 and 10.\\n'},\n",
       "        {'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (data, target) instead of a Bunch object.\\nSee below for more information about the data and target object.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "        {'param_name': 'as_frame',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric). The target is\\na pandas DataFrame or Series depending on the number of target columns.\\nIf return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as described below.\\n\\nAdded in version 0.23.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'load_digits',\n",
       "       'descriptions': 'Load and return the digits dataset (classification). Each datapoint is a 8x8 image of a digit. Classes 10 Samples per class ~180 Samples total 1797 Dimensionality 64 Features integers 0-16 This is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_class': {'type': 'integer',\n",
       "          'description': 'int, default=10. The number of classes to return. Between 0 and 10.\\n'},\n",
       "         'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (data, target) instead of a Bunch object.\\nSee below for more information about the data and target object.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "         'as_frame': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric). The target is\\na pandas DataFrame or Series depending on the number of target columns.\\nIf return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as described below.\\n\\nAdded in version 0.23.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'load_iris',\n",
       "      'func_desc': 'Load and return the iris dataset (classification).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris',\n",
       "      'function_definitions': {'function_name': 'load_iris',\n",
       "       'full_function': 'sklearn.datasets.load_iris(*, return_X_y=False, as_frame=False)',\n",
       "       'function_text': 'Load and return the iris dataset (classification). The iris dataset is a classic and very easy multi-class classification\\ndataset. Classes 3 Samples per class 50 Samples total 150 Dimensionality 4 Features real, positive Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/toy_dataset.html#iris-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (data, target) instead of a Bunch object. See\\nbelow for more information about the data and target object.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "        {'param_name': 'as_frame',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric). The target is\\na pandas DataFrame or Series depending on the number of target columns.\\nIf return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as described below.\\n\\nAdded in version 0.23.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'load_iris',\n",
       "       'descriptions': 'Load and return the iris dataset (classification). The iris dataset is a classic and very easy multi-class classification\\ndataset. Classes 3 Samples per class 50 Samples total 150 Dimensionality 4 Features real, positive Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (data, target) instead of a Bunch object. See\\nbelow for more information about the data and target object.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "         'as_frame': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric). The target is\\na pandas DataFrame or Series depending on the number of target columns.\\nIf return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as described below.\\n\\nAdded in version 0.23.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'load_sample_image',\n",
       "      'func_desc': 'Load the numpy array of a single sample image.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_sample_image.html#sklearn.datasets.load_sample_image',\n",
       "      'function_definitions': {'function_name': 'load_sample_image',\n",
       "       'full_function': 'sklearn.datasets.load_sample_image(image_name)',\n",
       "       'function_text': 'Load the numpy array of a single sample image. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/loading_other_datasets.html#sample-images',\n",
       "       'parameter_names_desc': [{'param_name': 'image_name',\n",
       "         'param_type': '{china.jpg, flower.jpg}',\n",
       "         'param_desc': 'The name of the sample image loaded.\\n'}]},\n",
       "      'function_calling': {'name': 'load_sample_image',\n",
       "       'descriptions': 'Load the numpy array of a single sample image. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'image_name': {'type': 'string',\n",
       "          'enum': ['china.jpg', 'flower.jpg'],\n",
       "          'description': '{china.jpg, flower.jpg}. The name of the sample image loaded.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'load_svmlight_file',\n",
       "      'func_desc': 'Load datasets in the svmlight / libsvm format into sparse CSR matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_file.html#sklearn.datasets.load_svmlight_file',\n",
       "      'function_definitions': {'function_name': 'load_svmlight_file',\n",
       "       'full_function': \"sklearn.datasets.load_svmlight_file(f, *, n_features=None, dtype=<class 'numpy.float64'>, multilabel=False, zero_based='auto', query_id=False, offset=0, length=-1)\",\n",
       "       'function_text': 'Load datasets in the svmlight / libsvm format into sparse CSR matrix. This format is a text-based format, with one sample per line. It does\\nnot store zero valued features hence is suitable for sparse dataset. The first element of each line can be used to store a target variable\\nto predict. This format is used as the default format for both svmlight and the\\nlibsvm command line programs. Parsing a text based source can be expensive. When repeatedly\\nworking on the same dataset, it is recommended to wrap this\\nloader with joblib.Memory.cache to store a memmapped backup of the\\nCSR results of the first call and benefit from the near instantaneous\\nloading of memmapped structures for the subsequent calls. In case the file contains a pairwise preference constraint (known\\nas “qid” in the svmlight format) these are ignored unless the\\nquery_id parameter is set to True. These pairwise preference\\nconstraints can be used to constraint the combination of samples\\nwhen using pairwise loss functions (as is the case in some\\nlearning to rank problems) so that only pairs with the same\\nquery_id value are considered. This implementation is written in Cython and is reasonably fast.\\nHowever, a faster API-compatible loader is also available at: mblondel/svmlight-loader',\n",
       "       'func_text_user_guide': 'sklearn.datasets.load_svmlight_files.html#sklearn.datasets.load_svmlight_files',\n",
       "       'parameter_names_desc': [{'param_name': 'f',\n",
       "         'param_type': 'str, path-like, file-like or int',\n",
       "         'param_desc': '(Path to) a file to load. If a path ends in “.gz” or “.bz2”, it will\\nbe uncompressed on the fly. If an integer is passed, it is assumed to\\nbe a file descriptor. A file-like or file descriptor will not be closed\\nby this function. A file-like object must be opened in binary mode.\\n\\nChanged in version 1.2: Path-like objects are now accepted.\\n\\n'},\n",
       "        {'param_name': 'n_features',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of features to use. If None, it will be inferred. This\\nargument is useful to load several files that are subsets of a\\nbigger sliced dataset: each subset might not have examples of\\nevery feature, hence the inferred shape might vary from one\\nslice to another.\\nn_features is only required if offset or length are passed a\\nnon-default value.\\n'},\n",
       "        {'param_name': 'dtype',\n",
       "         'param_type': 'numpy data type, default=np.float64',\n",
       "         'param_desc': 'Data type of dataset to be loaded. This will be the data type of the\\noutput numpy arrays X and y.\\n'},\n",
       "        {'param_name': 'multilabel',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Samples may have several labels each (see\\nhttps://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html).\\n'},\n",
       "        {'param_name': 'zero_based',\n",
       "         'param_type': 'bool or “auto”, default=”auto”',\n",
       "         'param_desc': 'Whether column indices in f are zero-based (True) or one-based\\n(False). If column indices are one-based, they are transformed to\\nzero-based to match Python/NumPy conventions.\\nIf set to “auto”, a heuristic check is applied to determine this from\\nthe file contents. Both kinds of files occur “in the wild”, but they\\nare unfortunately not self-identifying. Using “auto” or True should\\nalways be safe when no offset or length is passed.\\nIf offset or length are passed, the “auto” mode falls back\\nto zero_based=True to avoid having the heuristic check yield\\ninconsistent results on different segments of the file.\\n'},\n",
       "        {'param_name': 'query_id',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, will return the query_id array for each file.\\n'},\n",
       "        {'param_name': 'offset',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Ignore the offset first bytes by seeking forward, then\\ndiscarding the following bytes up until the next new line\\ncharacter.\\n'},\n",
       "        {'param_name': 'length',\n",
       "         'param_type': 'int, default=-1',\n",
       "         'param_desc': 'If strictly positive, stop reading any new line of data once the\\nposition in the file has reached the (offset + length) bytes threshold.\\n'}]},\n",
       "      'function_calling': {'name': 'load_svmlight_file',\n",
       "       'descriptions': 'Load datasets in the svmlight / libsvm format into sparse CSR matrix. This format is a text-based format, with one sample per line. It does\\nnot store zero valued features hence is suitable for sparse dataset. The first element of each line can be used to store a target variable\\nto predict. This format is used as the default format for both svmlight and the\\nlibsvm command line programs. Parsing a text based source can be expensive. When repeatedly\\nworking on the same dataset, it is recommended to wrap this\\nloader with joblib.Memory.cache to store a memmapped backup of the\\nCSR results of the first call and benefit from the near instantaneous\\nloading of memmapped structures for the subsequent calls. In case the file contains a pairwise preference constraint (known\\nas “qid” in the svmlight format) these are ignored unless the\\nquery_id parameter is set to True. These pairwise preference\\nconstraints can be used to constraint the combination of samples\\nwhen using pairwise loss functions (as is the case in some\\nlearning to rank problems) so that only pairs with the same\\nquery_id value are considered. This implementation is written in Cython and is reasonably fast.\\nHowever, a faster API-compatible loader is also available at: mblondel/svmlight-loader',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'f': {'type': 'integer',\n",
       "          'description': 'str, path-like, file-like or int. (Path to) a file to load. If a path ends in “.gz” or “.bz2”, it will\\nbe uncompressed on the fly. If an integer is passed, it is assumed to\\nbe a file descriptor. A file-like or file descriptor will not be closed\\nby this function. A file-like object must be opened in binary mode.\\n\\nChanged in version 1.2: Path-like objects are now accepted.\\n\\n'},\n",
       "         'n_features': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of features to use. If None, it will be inferred. This\\nargument is useful to load several files that are subsets of a\\nbigger sliced dataset: each subset might not have examples of\\nevery feature, hence the inferred shape might vary from one\\nslice to another.\\nn_features is only required if offset or length are passed a\\nnon-default value.\\n'},\n",
       "         'dtype': {'type': 'number',\n",
       "          'description': 'numpy data type, default=np.float64. Data type of dataset to be loaded. This will be the data type of the\\noutput numpy arrays X and y.\\n'},\n",
       "         'multilabel': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Samples may have several labels each (see\\nhttps://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html).\\n'},\n",
       "         'zero_based': {'type': 'boolean',\n",
       "          'description': 'bool or “auto”, default=”auto”. Whether column indices in f are zero-based (True) or one-based\\n(False). If column indices are one-based, they are transformed to\\nzero-based to match Python/NumPy conventions.\\nIf set to “auto”, a heuristic check is applied to determine this from\\nthe file contents. Both kinds of files occur “in the wild”, but they\\nare unfortunately not self-identifying. Using “auto” or True should\\nalways be safe when no offset or length is passed.\\nIf offset or length are passed, the “auto” mode falls back\\nto zero_based=True to avoid having the heuristic check yield\\ninconsistent results on different segments of the file.\\n'},\n",
       "         'query_id': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, will return the query_id array for each file.\\n'},\n",
       "         'offset': {'type': 'integer',\n",
       "          'description': 'int, default=0. Ignore the offset first bytes by seeking forward, then\\ndiscarding the following bytes up until the next new line\\ncharacter.\\n'},\n",
       "         'length': {'type': 'integer',\n",
       "          'description': 'int, default=-1. If strictly positive, stop reading any new line of data once the\\nposition in the file has reached the (offset + length) bytes threshold.\\n'}},\n",
       "        'required': ['f']}}},\n",
       "     {'func_name': 'load_wine',\n",
       "      'func_desc': 'Load and return the wine dataset (classification).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine',\n",
       "      'function_definitions': {'function_name': 'load_wine',\n",
       "       'full_function': 'sklearn.datasets.load_wine(*, return_X_y=False, as_frame=False)',\n",
       "       'function_text': 'Load and return the wine dataset (classification). Added in version 0.18. The wine dataset is a classic and very easy multi-class classification\\ndataset. Classes 3 Samples per class [59,71,48] Samples total 178 Dimensionality 13 Features real, positive The copy of UCI ML Wine Data Set dataset is downloaded and modified to fit\\nstandard format from:\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/toy_dataset.html#wine-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (data, target) instead of a Bunch object.\\nSee below for more information about the data and target object.\\n'},\n",
       "        {'param_name': 'as_frame',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric). The target is\\na pandas DataFrame or Series depending on the number of target columns.\\nIf return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as described below.\\n\\nAdded in version 0.23.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'load_wine',\n",
       "       'descriptions': 'Load and return the wine dataset (classification). Added in version 0.18. The wine dataset is a classic and very easy multi-class classification\\ndataset. Classes 3 Samples per class [59,71,48] Samples total 178 Dimensionality 13 Features real, positive The copy of UCI ML Wine Data Set dataset is downloaded and modified to fit\\nstandard format from:\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (data, target) instead of a Bunch object.\\nSee below for more information about the data and target object.\\n'},\n",
       "         'as_frame': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric). The target is\\na pandas DataFrame or Series depending on the number of target columns.\\nIf return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as described below.\\n\\nAdded in version 0.23.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'dump_svmlight_file',\n",
       "      'func_desc': 'Dump the dataset in svmlight / libsvm file format.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.dump_svmlight_file.html#sklearn.datasets.dump_svmlight_file',\n",
       "      'function_definitions': {'function_name': 'dump_svmlight_file',\n",
       "       'full_function': 'sklearn.datasets.dump_svmlight_file(X, y, f, *, zero_based=True, comment=None, query_id=None, multilabel=False)',\n",
       "       'function_text': 'Dump the dataset in svmlight / libsvm file format. This format is a text-based format, with one sample per line. It does\\nnot store zero valued features hence is suitable for sparse dataset. The first element of each line can be used to store a target variable\\nto predict.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training vectors, where n_samples is the number of samples and\\nn_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': '{array-like, sparse matrix}, shape = (n_samples,) or (n_samples, n_labels)',\n",
       "         'param_desc': 'Target values. Class labels must be an\\ninteger or float, or array-like objects of integer or float for\\nmultilabel classifications.\\n'},\n",
       "        {'param_name': 'f',\n",
       "         'param_type': 'str or file-like in binary mode',\n",
       "         'param_desc': 'If string, specifies the path that will contain the data.\\nIf file-like, data will be written to f. f should be opened in binary\\nmode.\\n'},\n",
       "        {'param_name': 'zero_based',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether column indices should be written zero-based (True) or one-based\\n(False).\\n'},\n",
       "        {'param_name': 'comment',\n",
       "         'param_type': 'str or bytes, default=None',\n",
       "         'param_desc': 'Comment to insert at the top of the file. This should be either a\\nUnicode string, which will be encoded as UTF-8, or an ASCII byte\\nstring.\\nIf a comment is given, then it will be preceded by one that identifies\\nthe file as having been dumped by scikit-learn. Note that not all\\ntools grok comments in SVMlight files.\\n'},\n",
       "        {'param_name': 'query_id',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Array containing pairwise preference constraints (qid in svmlight\\nformat).\\n'},\n",
       "        {'param_name': 'multilabel',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Samples may have several labels each (see\\nhttps://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html).\\n\\nAdded in version 0.17: parameter multilabel to support multilabel datasets.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'dump_svmlight_file',\n",
       "       'descriptions': 'Dump the dataset in svmlight / libsvm file format. This format is a text-based format, with one sample per line. It does\\nnot store zero valued features hence is suitable for sparse dataset. The first element of each line can be used to store a target variable\\nto predict.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Training vectors, where n_samples is the number of samples and\\nn_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix}, shape = (n_samples,) or (n_samples, n_labels). Target values. Class labels must be an\\ninteger or float, or array-like objects of integer or float for\\nmultilabel classifications.\\n'},\n",
       "         'f': {'type': 'string',\n",
       "          'description': 'str or file-like in binary mode. If string, specifies the path that will contain the data.\\nIf file-like, data will be written to f. f should be opened in binary\\nmode.\\n'},\n",
       "         'zero_based': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether column indices should be written zero-based (True) or one-based\\n(False).\\n'},\n",
       "         'comment': {'type': 'string',\n",
       "          'description': 'str or bytes, default=None. Comment to insert at the top of the file. This should be either a\\nUnicode string, which will be encoded as UTF-8, or an ASCII byte\\nstring.\\nIf a comment is given, then it will be preceded by one that identifies\\nthe file as having been dumped by scikit-learn. Note that not all\\ntools grok comments in SVMlight files.\\n'},\n",
       "         'query_id': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Array containing pairwise preference constraints (qid in svmlight\\nformat).\\n'},\n",
       "         'multilabel': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Samples may have several labels each (see\\nhttps://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html).\\n\\nAdded in version 0.17: parameter multilabel to support multilabel datasets.\\n\\n'}},\n",
       "        'required': ['X', 'y', 'f']}}},\n",
       "     {'func_name': 'fetch_20newsgroups_vectorized',\n",
       "      'func_desc': 'Load and vectorize the 20 newsgroups dataset (classification).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized.html#sklearn.datasets.fetch_20newsgroups_vectorized',\n",
       "      'function_definitions': {'function_name': 'fetch_20newsgroups_vectorized',\n",
       "       'full_function': \"sklearn.datasets.fetch_20newsgroups_vectorized(*, subset='train', remove=(), data_home=None, download_if_missing=True, return_X_y=False, normalize=True, as_frame=False, n_retries=3, delay=1.0)\",\n",
       "       'function_text': 'Load and vectorize the 20 newsgroups dataset (classification). Download it if necessary. This is a convenience function; the transformation is done using the\\ndefault settings for\\nCountVectorizer. For more\\nadvanced usage (stopword filtering, n-gram extraction, etc.), combine\\nfetch_20newsgroups with a custom\\nCountVectorizer,\\nHashingVectorizer,\\nTfidfTransformer or\\nTfidfVectorizer. The resulting counts are normalized using\\nsklearn.preprocessing.normalize unless normalize is set to False. Classes 20 Samples total 18846 Dimensionality 130107 Features real Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer',\n",
       "       'parameter_names_desc': [{'param_name': 'subset',\n",
       "         'param_type': '{‘train’, ‘test’, ‘all’}, default=’train’',\n",
       "         'param_desc': 'Select the dataset to load: ‘train’ for the training set, ‘test’\\nfor the test set, ‘all’ for both, with shuffled ordering.\\n'},\n",
       "        {'param_name': 'remove',\n",
       "         'param_type': 'tuple, default=()',\n",
       "         'param_desc': 'May contain any subset of (‘headers’, ‘footers’, ‘quotes’). Each of\\nthese are kinds of text that will be detected and removed from the\\nnewsgroup posts, preventing classifiers from overfitting on\\nmetadata.\\n‘headers’ removes newsgroup headers, ‘footers’ removes blocks at the\\nends of posts that look like signatures, and ‘quotes’ removes lines\\nthat appear to be quoting another post.\\n'},\n",
       "        {'param_name': 'data_home',\n",
       "         'param_type': 'str or path-like, default=None',\n",
       "         'param_desc': 'Specify an download and cache folder for the datasets. If None,\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "        {'param_name': 'download_if_missing',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "        {'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (data.data, data.target) instead of a Bunch\\nobject.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'normalize',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, normalizes each document’s feature vector to unit norm using\\nsklearn.preprocessing.normalize.\\n\\nAdded in version 0.22.\\n\\n'},\n",
       "        {'param_name': 'as_frame',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric, string, or categorical). The target is\\na pandas DataFrame or Series depending on the number of\\ntarget_columns.\\n\\nAdded in version 0.24.\\n\\n'},\n",
       "        {'param_name': 'n_retries',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "        {'param_name': 'delay',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'fetch_20newsgroups_vectorized',\n",
       "       'descriptions': 'Load and vectorize the 20 newsgroups dataset (classification). Download it if necessary. This is a convenience function; the transformation is done using the\\ndefault settings for\\nCountVectorizer. For more\\nadvanced usage (stopword filtering, n-gram extraction, etc.), combine\\nfetch_20newsgroups with a custom\\nCountVectorizer,\\nHashingVectorizer,\\nTfidfTransformer or\\nTfidfVectorizer. The resulting counts are normalized using\\nsklearn.preprocessing.normalize unless normalize is set to False. Classes 20 Samples total 18846 Dimensionality 130107 Features real Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'subset': {'type': 'string',\n",
       "          'enum': ['train', 'test', 'all'],\n",
       "          'description': '{‘train’, ‘test’, ‘all’}, default=’train’. Select the dataset to load: ‘train’ for the training set, ‘test’\\nfor the test set, ‘all’ for both, with shuffled ordering.\\n'},\n",
       "         'remove': {'type': 'array',\n",
       "          'description': 'tuple, default=(). May contain any subset of (‘headers’, ‘footers’, ‘quotes’). Each of\\nthese are kinds of text that will be detected and removed from the\\nnewsgroup posts, preventing classifiers from overfitting on\\nmetadata.\\n‘headers’ removes newsgroup headers, ‘footers’ removes blocks at the\\nends of posts that look like signatures, and ‘quotes’ removes lines\\nthat appear to be quoting another post.\\n'},\n",
       "         'data_home': {'type': 'string',\n",
       "          'description': 'str or path-like, default=None. Specify an download and cache folder for the datasets. If None,\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "         'download_if_missing': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "         'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (data.data, data.target) instead of a Bunch\\nobject.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'normalize': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, normalizes each document’s feature vector to unit norm using\\nsklearn.preprocessing.normalize.\\n\\nAdded in version 0.22.\\n\\n'},\n",
       "         'as_frame': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric, string, or categorical). The target is\\na pandas DataFrame or Series depending on the number of\\ntarget_columns.\\n\\nAdded in version 0.24.\\n\\n'},\n",
       "         'n_retries': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "         'delay': {'type': 'number',\n",
       "          'description': 'float, default=1.0. Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'fetch_covtype',\n",
       "      'func_desc': 'Load the covertype dataset (classification).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype',\n",
       "      'function_definitions': {'function_name': 'fetch_covtype',\n",
       "       'full_function': 'sklearn.datasets.fetch_covtype(*, data_home=None, download_if_missing=True, random_state=None, shuffle=False, return_X_y=False, as_frame=False, n_retries=3, delay=1.0)',\n",
       "       'function_text': 'Load the covertype dataset (classification). Download it if necessary. Classes 7 Samples total 581012 Dimensionality 54 Features int Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/real_world.html#covtype-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'data_home',\n",
       "         'param_type': 'str or path-like, default=None',\n",
       "         'param_desc': 'Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "        {'param_name': 'download_if_missing',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset shuffling. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to shuffle dataset.\\n'},\n",
       "        {'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (data.data, data.target) instead of a Bunch\\nobject.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'as_frame',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric). The target is a pandas DataFrame or\\nSeries depending on the number of target columns. If return_X_y is\\nTrue, then (data, target) will be pandas DataFrames or Series as\\ndescribed below.\\n\\nAdded in version 0.24.\\n\\n'},\n",
       "        {'param_name': 'n_retries',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "        {'param_name': 'delay',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'fetch_covtype',\n",
       "       'descriptions': 'Load the covertype dataset (classification). Download it if necessary. Classes 7 Samples total 581012 Dimensionality 54 Features int Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'data_home': {'type': 'string',\n",
       "          'description': 'str or path-like, default=None. Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "         'download_if_missing': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset shuffling. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to shuffle dataset.\\n'},\n",
       "         'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (data.data, data.target) instead of a Bunch\\nobject.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'as_frame': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric). The target is a pandas DataFrame or\\nSeries depending on the number of target columns. If return_X_y is\\nTrue, then (data, target) will be pandas DataFrames or Series as\\ndescribed below.\\n\\nAdded in version 0.24.\\n\\n'},\n",
       "         'n_retries': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "         'delay': {'type': 'number',\n",
       "          'description': 'float, default=1.0. Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'fetch_lfw_pairs',\n",
       "      'func_desc': 'Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_pairs.html#sklearn.datasets.fetch_lfw_pairs',\n",
       "      'function_definitions': {'function_name': 'fetch_lfw_pairs',\n",
       "       'full_function': \"sklearn.datasets.fetch_lfw_pairs(*, subset='train', data_home=None, funneled=True, resize=0.5, color=False, slice_=(slice(70, 195, None), slice(78, 172, None)), download_if_missing=True, n_retries=3, delay=1.0)\",\n",
       "       'function_text': 'Load the Labeled Faces in the Wild (LFW) pairs dataset (classification). Download it if necessary. Classes 2 Samples total 13233 Dimensionality 5828 Features real, between 0 and 255 In the official README.txt this task is described as the\\n“Restricted” task.  As I am not sure as to implement the\\n“Unrestricted” variant correctly, I left it as unsupported for now. The original images are 250 x 250 pixels, but the default slice and resize\\narguments reduce them to 62 x 47. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/real_world.html#labeled-faces-in-the-wild-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'subset',\n",
       "         'param_type': '{‘train’, ‘test’, ‘10_folds’}, default=’train’',\n",
       "         'param_desc': 'Select the dataset to load: ‘train’ for the development training\\nset, ‘test’ for the development test set, and ‘10_folds’ for the\\nofficial evaluation set that is meant to be used with a 10-folds\\ncross validation.\\n'},\n",
       "        {'param_name': 'data_home',\n",
       "         'param_type': 'str or path-like, default=None',\n",
       "         'param_desc': 'Specify another download and cache folder for the datasets. By\\ndefault all scikit-learn data is stored in ‘~/scikit_learn_data’\\nsubfolders.\\n'},\n",
       "        {'param_name': 'funneled',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Download and use the funneled variant of the dataset.\\n'},\n",
       "        {'param_name': 'resize',\n",
       "         'param_type': 'float, default=0.5',\n",
       "         'param_desc': 'Ratio used to resize the each face picture.\\n'},\n",
       "        {'param_name': 'color',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Keep the 3 RGB channels instead of averaging them to a single\\ngray level channel. If color is True the shape of the data has\\none more dimension than the shape with color = False.\\n'},\n",
       "        {'param_name': 'slice_',\n",
       "         'param_type': 'tuple of slice, default=(slice(70, 195), slice(78, 172))',\n",
       "         'param_desc': 'Provide a custom 2D slice (height, width) to extract the\\n‘interesting’ part of the jpeg files and avoid use statistical\\ncorrelation from the background.\\n'},\n",
       "        {'param_name': 'download_if_missing',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "        {'param_name': 'n_retries',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "        {'param_name': 'delay',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'fetch_lfw_pairs',\n",
       "       'descriptions': 'Load the Labeled Faces in the Wild (LFW) pairs dataset (classification). Download it if necessary. Classes 2 Samples total 13233 Dimensionality 5828 Features real, between 0 and 255 In the official README.txt this task is described as the\\n“Restricted” task.  As I am not sure as to implement the\\n“Unrestricted” variant correctly, I left it as unsupported for now. The original images are 250 x 250 pixels, but the default slice and resize\\narguments reduce them to 62 x 47. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'subset': {'type': 'string',\n",
       "          'enum': ['train', 'test', '10_folds'],\n",
       "          'description': '{‘train’, ‘test’, ‘10_folds’}, default=’train’. Select the dataset to load: ‘train’ for the development training\\nset, ‘test’ for the development test set, and ‘10_folds’ for the\\nofficial evaluation set that is meant to be used with a 10-folds\\ncross validation.\\n'},\n",
       "         'data_home': {'type': 'string',\n",
       "          'description': 'str or path-like, default=None. Specify another download and cache folder for the datasets. By\\ndefault all scikit-learn data is stored in ‘~/scikit_learn_data’\\nsubfolders.\\n'},\n",
       "         'funneled': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Download and use the funneled variant of the dataset.\\n'},\n",
       "         'resize': {'type': 'number',\n",
       "          'description': 'float, default=0.5. Ratio used to resize the each face picture.\\n'},\n",
       "         'color': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Keep the 3 RGB channels instead of averaging them to a single\\ngray level channel. If color is True the shape of the data has\\none more dimension than the shape with color = False.\\n'},\n",
       "         'slice_': {'type': 'array',\n",
       "          'description': 'tuple of slice, default=(slice(70, 195), slice(78, 172)). Provide a custom 2D slice (height, width) to extract the\\n‘interesting’ part of the jpeg files and avoid use statistical\\ncorrelation from the background.\\n'},\n",
       "         'download_if_missing': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "         'n_retries': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "         'delay': {'type': 'number',\n",
       "          'description': 'float, default=1.0. Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'fetch_olivetti_faces',\n",
       "      'func_desc': 'Load the Olivetti faces data-set from AT&T (classification).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_olivetti_faces.html#sklearn.datasets.fetch_olivetti_faces',\n",
       "      'function_definitions': {'function_name': 'fetch_olivetti_faces',\n",
       "       'full_function': 'sklearn.datasets.fetch_olivetti_faces(*, data_home=None, shuffle=False, random_state=0, download_if_missing=True, return_X_y=False, n_retries=3, delay=1.0)',\n",
       "       'function_text': 'Load the Olivetti faces data-set from AT&T (classification). Download it if necessary. Classes 40 Samples total 400 Dimensionality 4096 Features real, between 0 and 1 Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/real_world.html#olivetti-faces-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'data_home',\n",
       "         'param_type': 'str or path-like, default=None',\n",
       "         'param_desc': 'Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True the order of the dataset is shuffled to avoid having\\nimages of the same person grouped.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=0',\n",
       "         'param_desc': 'Determines random number generation for dataset shuffling. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'download_if_missing',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "        {'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (data, target) instead of a Bunch object. See\\nbelow for more information about the data and target object.\\n\\nAdded in version 0.22.\\n\\n'},\n",
       "        {'param_name': 'n_retries',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "        {'param_name': 'delay',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'fetch_olivetti_faces',\n",
       "       'descriptions': 'Load the Olivetti faces data-set from AT&T (classification). Download it if necessary. Classes 40 Samples total 400 Dimensionality 4096 Features real, between 0 and 1 Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'data_home': {'type': 'string',\n",
       "          'description': 'str or path-like, default=None. Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True the order of the dataset is shuffled to avoid having\\nimages of the same person grouped.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=0. Determines random number generation for dataset shuffling. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'download_if_missing': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "         'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (data, target) instead of a Bunch object. See\\nbelow for more information about the data and target object.\\n\\nAdded in version 0.22.\\n\\n'},\n",
       "         'n_retries': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "         'delay': {'type': 'number',\n",
       "          'description': 'float, default=1.0. Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'fetch_rcv1',\n",
       "      'func_desc': 'Load the RCV1 multilabel dataset (classification).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_rcv1.html#sklearn.datasets.fetch_rcv1',\n",
       "      'function_definitions': {'function_name': 'fetch_rcv1',\n",
       "       'full_function': \"sklearn.datasets.fetch_rcv1(*, data_home=None, subset='all', download_if_missing=True, random_state=None, shuffle=False, return_X_y=False, n_retries=3, delay=1.0)\",\n",
       "       'function_text': 'Load the RCV1 multilabel dataset (classification). Download it if necessary. Version: RCV1-v2, vectors, full sets, topics multilabels. Classes 103 Samples total 804414 Dimensionality 47236 Features real, between 0 and 1 Read more in the User Guide. Added in version 0.17.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/real_world.html#rcv1-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'data_home',\n",
       "         'param_type': 'str or path-like, default=None',\n",
       "         'param_desc': 'Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "        {'param_name': 'subset',\n",
       "         'param_type': '{‘train’, ‘test’, ‘all’}, default=’all’',\n",
       "         'param_desc': 'Select the dataset to load: ‘train’ for the training set\\n(23149 samples), ‘test’ for the test set (781265 samples),\\n‘all’ for both, with the training samples first if shuffle is False.\\nThis follows the official LYRL2004 chronological split.\\n'},\n",
       "        {'param_name': 'download_if_missing',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset shuffling. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to shuffle dataset.\\n'},\n",
       "        {'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (dataset.data, dataset.target) instead of a Bunch\\nobject. See below for more information about the dataset.data and\\ndataset.target object.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'n_retries',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "        {'param_name': 'delay',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'fetch_rcv1',\n",
       "       'descriptions': 'Load the RCV1 multilabel dataset (classification). Download it if necessary. Version: RCV1-v2, vectors, full sets, topics multilabels. Classes 103 Samples total 804414 Dimensionality 47236 Features real, between 0 and 1 Read more in the User Guide. Added in version 0.17.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'data_home': {'type': 'string',\n",
       "          'description': 'str or path-like, default=None. Specify another download and cache folder for the datasets. By default\\nall scikit-learn data is stored in ‘~/scikit_learn_data’ subfolders.\\n'},\n",
       "         'subset': {'type': 'string',\n",
       "          'enum': ['train', 'test', 'all'],\n",
       "          'description': '{‘train’, ‘test’, ‘all’}, default=’all’. Select the dataset to load: ‘train’ for the training set\\n(23149 samples), ‘test’ for the test set (781265 samples),\\n‘all’ for both, with the training samples first if shuffle is False.\\nThis follows the official LYRL2004 chronological split.\\n'},\n",
       "         'download_if_missing': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, raise an OSError if the data is not locally available\\ninstead of trying to download the data from the source site.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset shuffling. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to shuffle dataset.\\n'},\n",
       "         'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (dataset.data, dataset.target) instead of a Bunch\\nobject. See below for more information about the dataset.data and\\ndataset.target object.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'n_retries': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of retries when HTTP errors are encountered.\\n\\nAdded in version 1.5.\\n\\n'},\n",
       "         'delay': {'type': 'number',\n",
       "          'description': 'float, default=1.0. Number of seconds between retries.\\n\\nAdded in version 1.5.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'get_data_home',\n",
       "      'func_desc': 'Return the path of the scikit-learn data directory.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.get_data_home.html#sklearn.datasets.get_data_home',\n",
       "      'function_definitions': {'function_name': 'get_data_home',\n",
       "       'full_function': 'sklearn.datasets.get_data_home(data_home=None) → str',\n",
       "       'function_text': 'Return the path of the scikit-learn data directory. This folder is used by some large dataset loaders to avoid downloading the\\ndata several times. By default the data directory is set to a folder named ‘scikit_learn_data’ in the\\nuser home folder. Alternatively, it can be set by the ‘SCIKIT_LEARN_DATA’ environment\\nvariable or programmatically by giving an explicit folder path. The ‘~’\\nsymbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'data_home',\n",
       "         'param_type': 'str or path-like, default=None',\n",
       "         'param_desc': 'The path to scikit-learn data directory. If None, the default path\\nis ~/scikit_learn_data.\\n'}]},\n",
       "      'function_calling': {'name': 'get_data_home',\n",
       "       'descriptions': 'Return the path of the scikit-learn data directory. This folder is used by some large dataset loaders to avoid downloading the\\ndata several times. By default the data directory is set to a folder named ‘scikit_learn_data’ in the\\nuser home folder. Alternatively, it can be set by the ‘SCIKIT_LEARN_DATA’ environment\\nvariable or programmatically by giving an explicit folder path. The ‘~’\\nsymbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'data_home': {'type': 'string',\n",
       "          'description': 'str or path-like, default=None. The path to scikit-learn data directory. If None, the default path\\nis ~/scikit_learn_data.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'load_diabetes',\n",
       "      'func_desc': 'Load and return the diabetes dataset (regression).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes',\n",
       "      'function_definitions': {'function_name': 'load_diabetes',\n",
       "       'full_function': 'sklearn.datasets.load_diabetes(*, return_X_y=False, as_frame=False, scaled=True)',\n",
       "       'function_text': 'Load and return the diabetes dataset (regression). Samples total 442 Dimensionality 10 Features real, -.2 < x < .2 Targets integer 25 - 346 Note The meaning of each feature (i.e. feature_names) might be unclear\\n(especially for ltg) as the documentation of the original dataset is\\nnot explicit. We provide information that seems correct in regard with\\nthe scientific literature in this field of research. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/toy_dataset.html#diabetes-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (data, target) instead of a Bunch object.\\nSee below for more information about the data and target object.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "        {'param_name': 'as_frame',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric). The target is\\na pandas DataFrame or Series depending on the number of target columns.\\nIf return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as described below.\\n\\nAdded in version 0.23.\\n\\n'},\n",
       "        {'param_name': 'scaled',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, the feature variables are mean centered and scaled by the\\nstandard deviation times the square root of n_samples.\\nIf False, raw data is returned for the feature variables.\\n\\nAdded in version 1.1.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'load_diabetes',\n",
       "       'descriptions': 'Load and return the diabetes dataset (regression). Samples total 442 Dimensionality 10 Features real, -.2 < x < .2 Targets integer 25 - 346 Note The meaning of each feature (i.e. feature_names) might be unclear\\n(especially for ltg) as the documentation of the original dataset is\\nnot explicit. We provide information that seems correct in regard with\\nthe scientific literature in this field of research. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (data, target) instead of a Bunch object.\\nSee below for more information about the data and target object.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "         'as_frame': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric). The target is\\na pandas DataFrame or Series depending on the number of target columns.\\nIf return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as described below.\\n\\nAdded in version 0.23.\\n\\n'},\n",
       "         'scaled': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, the feature variables are mean centered and scaled by the\\nstandard deviation times the square root of n_samples.\\nIf False, raw data is returned for the feature variables.\\n\\nAdded in version 1.1.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'load_files',\n",
       "      'func_desc': 'Load text files with categories as subfolder names.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html#sklearn.datasets.load_files',\n",
       "      'function_definitions': {'function_name': 'load_files',\n",
       "       'full_function': \"sklearn.datasets.load_files(container_path, *, description=None, categories=None, load_content=True, shuffle=True, encoding=None, decode_error='strict', random_state=0, allowed_extensions=None)\",\n",
       "       'function_text': 'Load text files with categories as subfolder names. Individual samples are assumed to be files stored a two levels folder\\nstructure such as the following: file_1.txt\\nfile_2.txt\\n…\\nfile_42.txt file_43.txt\\nfile_44.txt\\n… The folder names are used as supervised signal label names. The individual\\nfile names are not important. This function does not try to extract features into a numpy array or scipy\\nsparse matrix. In addition, if load_content is false it does not try to\\nload the files in memory. To use text files in a scikit-learn classification or clustering algorithm,\\nyou will need to use the text module to\\nbuild a feature extraction transformer that suits your problem. If you set load_content=True, you should also specify the encoding of the\\ntext using the ‘encoding’ parameter. For many modern text files, ‘utf-8’\\nwill be the correct encoding. If you leave encoding equal to None, then the\\ncontent will be made of bytes instead of Unicode, and you will not be able\\nto use most functions in text. Similar feature extractors should be built for other kind of unstructured\\ndata input such as images, audio, video, … If you want files with a specific file extension (e.g. .txt) then you\\ncan pass a list of those file extensions to allowed_extensions. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/api/sklearn.feature_extraction.html#module-sklearn.feature_extraction.text',\n",
       "       'parameter_names_desc': [{'param_name': 'container_path',\n",
       "         'param_type': 'str',\n",
       "         'param_desc': 'Path to the main folder holding one subfolder per category.\\n'},\n",
       "        {'param_name': 'description',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'A paragraph describing the characteristic of the dataset: its source,\\nreference, etc.\\n'},\n",
       "        {'param_name': 'categories',\n",
       "         'param_type': 'list of str, default=None',\n",
       "         'param_desc': 'If None (default), load all the categories. If not None, list of\\ncategory names to load (other categories ignored).\\n'},\n",
       "        {'param_name': 'load_content',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to load or not the content of the different files. If true a\\n‘data’ attribute containing the text information is present in the data\\nstructure returned. If not, a filenames attribute gives the path to the\\nfiles.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether or not to shuffle the data: might be important for models that\\nmake the assumption that the samples are independent and identically\\ndistributed (i.i.d.), such as stochastic gradient descent.\\n'},\n",
       "        {'param_name': 'encoding',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'If None, do not try to decode the content of the files (e.g. for images\\nor other non-text content). If not None, encoding to use to decode text\\nfiles to Unicode if load_content is True.\\n'},\n",
       "        {'param_name': 'decode_error',\n",
       "         'param_type': '{‘strict’, ‘ignore’, ‘replace’}, default=’strict’',\n",
       "         'param_desc': 'Instruction on what to do if a byte sequence is given to analyze that\\ncontains characters not of the given encoding. Passed as keyword\\nargument ‘errors’ to bytes.decode.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=0',\n",
       "         'param_desc': 'Determines random number generation for dataset shuffling. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'allowed_extensions',\n",
       "         'param_type': 'list of str, default=None',\n",
       "         'param_desc': 'List of desired file extensions to filter the files to be loaded.\\n'}]},\n",
       "      'function_calling': {'name': 'load_files',\n",
       "       'descriptions': 'Load text files with categories as subfolder names. Individual samples are assumed to be files stored a two levels folder\\nstructure such as the following: file_1.txt\\nfile_2.txt\\n…\\nfile_42.txt file_43.txt\\nfile_44.txt\\n… The folder names are used as supervised signal label names. The individual\\nfile names are not important. This function does not try to extract features into a numpy array or scipy\\nsparse matrix. In addition, if load_content is false it does not try to\\nload the files in memory. To use text files in a scikit-learn classification or clustering algorithm,\\nyou will need to use the text module to\\nbuild a feature extraction transformer that suits your problem. If you set load_content=True, you should also specify the encoding of the\\ntext using the ‘encoding’ parameter. For many modern text files, ‘utf-8’\\nwill be the correct encoding. If you leave encoding equal to None, then the\\ncontent will be made of bytes instead of Unicode, and you will not be able\\nto use most functions in text. Similar feature extractors should be built for other kind of unstructured\\ndata input such as images, audio, video, … If you want files with a specific file extension (e.g. .txt) then you\\ncan pass a list of those file extensions to allowed_extensions. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'container_path': {'type': 'string',\n",
       "          'description': 'str. Path to the main folder holding one subfolder per category.\\n'},\n",
       "         'description': {'type': 'string',\n",
       "          'description': 'str, default=None. A paragraph describing the characteristic of the dataset: its source,\\nreference, etc.\\n'},\n",
       "         'categories': {'type': 'string',\n",
       "          'description': 'list of str, default=None. If None (default), load all the categories. If not None, list of\\ncategory names to load (other categories ignored).\\n'},\n",
       "         'load_content': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to load or not the content of the different files. If true a\\n‘data’ attribute containing the text information is present in the data\\nstructure returned. If not, a filenames attribute gives the path to the\\nfiles.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether or not to shuffle the data: might be important for models that\\nmake the assumption that the samples are independent and identically\\ndistributed (i.i.d.), such as stochastic gradient descent.\\n'},\n",
       "         'encoding': {'type': 'string',\n",
       "          'description': 'str, default=None. If None, do not try to decode the content of the files (e.g. for images\\nor other non-text content). If not None, encoding to use to decode text\\nfiles to Unicode if load_content is True.\\n'},\n",
       "         'decode_error': {'type': 'string',\n",
       "          'description': '{‘strict’, ‘ignore’, ‘replace’}, default=’strict’. Instruction on what to do if a byte sequence is given to analyze that\\ncontains characters not of the given encoding. Passed as keyword\\nargument ‘errors’ to bytes.decode.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=0. Determines random number generation for dataset shuffling. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'allowed_extensions': {'type': 'string',\n",
       "          'description': 'list of str, default=None. List of desired file extensions to filter the files to be loaded.\\n'}},\n",
       "        'required': ['container_path']}}},\n",
       "     {'func_name': 'load_linnerud',\n",
       "      'func_desc': 'Load and return the physical exercise Linnerud dataset.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_linnerud.html#sklearn.datasets.load_linnerud',\n",
       "      'function_definitions': {'function_name': 'load_linnerud',\n",
       "       'full_function': 'sklearn.datasets.load_linnerud(*, return_X_y=False, as_frame=False)',\n",
       "       'function_text': 'Load and return the physical exercise Linnerud dataset. This dataset is suitable for multi-output regression tasks. Samples total 20 Dimensionality 3 (for both data and target) Features integer Targets integer Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/toy_dataset.html#linnerrud-dataset',\n",
       "       'parameter_names_desc': [{'param_name': 'return_X_y',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns (data, target) instead of a Bunch object.\\nSee below for more information about the data and target object.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "        {'param_name': 'as_frame',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric, string or categorical). The target is\\na pandas DataFrame or Series depending on the number of target columns.\\nIf return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as described below.\\n\\nAdded in version 0.23.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'load_linnerud',\n",
       "       'descriptions': 'Load and return the physical exercise Linnerud dataset. This dataset is suitable for multi-output regression tasks. Samples total 20 Dimensionality 3 (for both data and target) Features integer Targets integer Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'return_X_y': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns (data, target) instead of a Bunch object.\\nSee below for more information about the data and target object.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "         'as_frame': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the data is a pandas DataFrame including columns with\\nappropriate dtypes (numeric, string or categorical). The target is\\na pandas DataFrame or Series depending on the number of target columns.\\nIf return_X_y is True, then (data, target) will be pandas\\nDataFrames or Series as described below.\\n\\nAdded in version 0.23.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'load_sample_images',\n",
       "      'func_desc': 'Load sample images for image manipulation.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_sample_images.html#sklearn.datasets.load_sample_images',\n",
       "      'function_definitions': {'function_name': 'load_sample_images',\n",
       "       'full_function': 'sklearn.datasets.load_sample_images()',\n",
       "       'function_text': 'Load sample images for image manipulation. Loads both, china and flower. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/loading_other_datasets.html#sample-images',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'load_svmlight_files',\n",
       "      'func_desc': 'Load dataset from multiple files in SVMlight format.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_files.html#sklearn.datasets.load_svmlight_files',\n",
       "      'function_definitions': {'function_name': 'load_svmlight_files',\n",
       "       'full_function': \"sklearn.datasets.load_svmlight_files(files, *, n_features=None, dtype=<class 'numpy.float64'>, multilabel=False, zero_based='auto', query_id=False, offset=0, length=-1)\",\n",
       "       'function_text': 'Load dataset from multiple files in SVMlight format. This function is equivalent to mapping load_svmlight_file over a list of\\nfiles, except that the results are concatenated into a single, flat list\\nand the samples vectors are constrained to all have the same number of\\nfeatures. In case the file contains a pairwise preference constraint (known\\nas “qid” in the svmlight format) these are ignored unless the\\nquery_id parameter is set to True. These pairwise preference\\nconstraints can be used to constraint the combination of samples\\nwhen using pairwise loss functions (as is the case in some\\nlearning to rank problems) so that only pairs with the same\\nquery_id value are considered.',\n",
       "       'func_text_user_guide': 'sklearn.datasets.load_svmlight_file.html#sklearn.datasets.load_svmlight_file',\n",
       "       'parameter_names_desc': [{'param_name': 'files',\n",
       "         'param_type': 'array-like, dtype=str, path-like, file-like or int',\n",
       "         'param_desc': '(Paths of) files to load. If a path ends in “.gz” or “.bz2”, it will\\nbe uncompressed on the fly. If an integer is passed, it is assumed to\\nbe a file descriptor. File-likes and file descriptors will not be\\nclosed by this function. File-like objects must be opened in binary\\nmode.\\n\\nChanged in version 1.2: Path-like objects are now accepted.\\n\\n'},\n",
       "        {'param_name': 'n_features',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of features to use. If None, it will be inferred from the\\nmaximum column index occurring in any of the files.\\nThis can be set to a higher value than the actual number of features\\nin any of the input files, but setting it to a lower value will cause\\nan exception to be raised.\\n'},\n",
       "        {'param_name': 'dtype',\n",
       "         'param_type': 'numpy data type, default=np.float64',\n",
       "         'param_desc': 'Data type of dataset to be loaded. This will be the data type of the\\noutput numpy arrays X and y.\\n'},\n",
       "        {'param_name': 'multilabel',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Samples may have several labels each (see\\nhttps://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html).\\n'},\n",
       "        {'param_name': 'zero_based',\n",
       "         'param_type': 'bool or “auto”, default=”auto”',\n",
       "         'param_desc': 'Whether column indices in f are zero-based (True) or one-based\\n(False). If column indices are one-based, they are transformed to\\nzero-based to match Python/NumPy conventions.\\nIf set to “auto”, a heuristic check is applied to determine this from\\nthe file contents. Both kinds of files occur “in the wild”, but they\\nare unfortunately not self-identifying. Using “auto” or True should\\nalways be safe when no offset or length is passed.\\nIf offset or length are passed, the “auto” mode falls back\\nto zero_based=True to avoid having the heuristic check yield\\ninconsistent results on different segments of the file.\\n'},\n",
       "        {'param_name': 'query_id',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, will return the query_id array for each file.\\n'},\n",
       "        {'param_name': 'offset',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Ignore the offset first bytes by seeking forward, then\\ndiscarding the following bytes up until the next new line\\ncharacter.\\n'},\n",
       "        {'param_name': 'length',\n",
       "         'param_type': 'int, default=-1',\n",
       "         'param_desc': 'If strictly positive, stop reading any new line of data once the\\nposition in the file has reached the (offset + length) bytes threshold.\\n'}]},\n",
       "      'function_calling': {'name': 'load_svmlight_files',\n",
       "       'descriptions': 'Load dataset from multiple files in SVMlight format. This function is equivalent to mapping load_svmlight_file over a list of\\nfiles, except that the results are concatenated into a single, flat list\\nand the samples vectors are constrained to all have the same number of\\nfeatures. In case the file contains a pairwise preference constraint (known\\nas “qid” in the svmlight format) these are ignored unless the\\nquery_id parameter is set to True. These pairwise preference\\nconstraints can be used to constraint the combination of samples\\nwhen using pairwise loss functions (as is the case in some\\nlearning to rank problems) so that only pairs with the same\\nquery_id value are considered.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'files': {'type': 'integer',\n",
       "          'description': 'array-like, dtype=str, path-like, file-like or int. (Paths of) files to load. If a path ends in “.gz” or “.bz2”, it will\\nbe uncompressed on the fly. If an integer is passed, it is assumed to\\nbe a file descriptor. File-likes and file descriptors will not be\\nclosed by this function. File-like objects must be opened in binary\\nmode.\\n\\nChanged in version 1.2: Path-like objects are now accepted.\\n\\n'},\n",
       "         'n_features': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of features to use. If None, it will be inferred from the\\nmaximum column index occurring in any of the files.\\nThis can be set to a higher value than the actual number of features\\nin any of the input files, but setting it to a lower value will cause\\nan exception to be raised.\\n'},\n",
       "         'dtype': {'type': 'number',\n",
       "          'description': 'numpy data type, default=np.float64. Data type of dataset to be loaded. This will be the data type of the\\noutput numpy arrays X and y.\\n'},\n",
       "         'multilabel': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Samples may have several labels each (see\\nhttps://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html).\\n'},\n",
       "         'zero_based': {'type': 'boolean',\n",
       "          'description': 'bool or “auto”, default=”auto”. Whether column indices in f are zero-based (True) or one-based\\n(False). If column indices are one-based, they are transformed to\\nzero-based to match Python/NumPy conventions.\\nIf set to “auto”, a heuristic check is applied to determine this from\\nthe file contents. Both kinds of files occur “in the wild”, but they\\nare unfortunately not self-identifying. Using “auto” or True should\\nalways be safe when no offset or length is passed.\\nIf offset or length are passed, the “auto” mode falls back\\nto zero_based=True to avoid having the heuristic check yield\\ninconsistent results on different segments of the file.\\n'},\n",
       "         'query_id': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, will return the query_id array for each file.\\n'},\n",
       "         'offset': {'type': 'integer',\n",
       "          'description': 'int, default=0. Ignore the offset first bytes by seeking forward, then\\ndiscarding the following bytes up until the next new line\\ncharacter.\\n'},\n",
       "         'length': {'type': 'integer',\n",
       "          'description': 'int, default=-1. If strictly positive, stop reading any new line of data once the\\nposition in the file has reached the (offset + length) bytes threshold.\\n'}},\n",
       "        'required': ['files']}}}]},\n",
       "   {'Sample generators': [{'func_name': 'make_biclusters',\n",
       "      'func_desc': 'Generate a constant block diagonal structure array for biclustering.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_biclusters.html#sklearn.datasets.make_biclusters',\n",
       "      'function_definitions': {'function_name': 'make_biclusters',\n",
       "       'full_function': 'sklearn.datasets.make_biclusters(shape, n_clusters, *, noise=0.0, minval=10, maxval=100, shuffle=True, random_state=None)',\n",
       "       'function_text': 'Generate a constant block diagonal structure array for biclustering. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'shape',\n",
       "         'param_type': 'tuple of shape (n_rows, n_cols)',\n",
       "         'param_desc': 'The shape of the result.\\n'},\n",
       "        {'param_name': 'n_clusters',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'The number of biclusters.\\n'},\n",
       "        {'param_name': 'noise',\n",
       "         'param_type': 'float, default=0.0',\n",
       "         'param_desc': 'The standard deviation of the gaussian noise.\\n'},\n",
       "        {'param_name': 'minval',\n",
       "         'param_type': 'float, default=10',\n",
       "         'param_desc': 'Minimum value of a bicluster.\\n'},\n",
       "        {'param_name': 'maxval',\n",
       "         'param_type': 'float, default=100',\n",
       "         'param_desc': 'Maximum value of a bicluster.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Shuffle the samples.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_biclusters',\n",
       "       'descriptions': 'Generate a constant block diagonal structure array for biclustering. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'shape': {'type': 'array',\n",
       "          'description': 'tuple of shape (n_rows, n_cols). The shape of the result.\\n'},\n",
       "         'n_clusters': {'type': 'integer',\n",
       "          'description': 'int. The number of biclusters.\\n'},\n",
       "         'noise': {'type': 'number',\n",
       "          'description': 'float, default=0.0. The standard deviation of the gaussian noise.\\n'},\n",
       "         'minval': {'type': 'number',\n",
       "          'description': 'float, default=10. Minimum value of a bicluster.\\n'},\n",
       "         'maxval': {'type': 'number',\n",
       "          'description': 'float, default=100. Maximum value of a bicluster.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Shuffle the samples.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['shape', 'n_clusters']}}},\n",
       "     {'func_name': 'make_checkerboard',\n",
       "      'func_desc': 'Generate an array with block checkerboard structure for biclustering.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_checkerboard.html#sklearn.datasets.make_checkerboard',\n",
       "      'function_definitions': {'function_name': 'make_checkerboard',\n",
       "       'full_function': 'sklearn.datasets.make_checkerboard(shape, n_clusters, *, noise=0.0, minval=10, maxval=100, shuffle=True, random_state=None)',\n",
       "       'function_text': 'Generate an array with block checkerboard structure for biclustering. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'shape',\n",
       "         'param_type': 'tuple of shape (n_rows, n_cols)',\n",
       "         'param_desc': 'The shape of the result.\\n'},\n",
       "        {'param_name': 'n_clusters',\n",
       "         'param_type': 'int or array-like or shape (n_row_clusters, n_column_clusters)',\n",
       "         'param_desc': 'The number of row and column clusters.\\n'},\n",
       "        {'param_name': 'noise',\n",
       "         'param_type': 'float, default=0.0',\n",
       "         'param_desc': 'The standard deviation of the gaussian noise.\\n'},\n",
       "        {'param_name': 'minval',\n",
       "         'param_type': 'float, default=10',\n",
       "         'param_desc': 'Minimum value of a bicluster.\\n'},\n",
       "        {'param_name': 'maxval',\n",
       "         'param_type': 'float, default=100',\n",
       "         'param_desc': 'Maximum value of a bicluster.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Shuffle the samples.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_checkerboard',\n",
       "       'descriptions': 'Generate an array with block checkerboard structure for biclustering. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'shape': {'type': 'array',\n",
       "          'description': 'tuple of shape (n_rows, n_cols). The shape of the result.\\n'},\n",
       "         'n_clusters': {'type': 'integer',\n",
       "          'description': 'int or array-like or shape (n_row_clusters, n_column_clusters). The number of row and column clusters.\\n'},\n",
       "         'noise': {'type': 'number',\n",
       "          'description': 'float, default=0.0. The standard deviation of the gaussian noise.\\n'},\n",
       "         'minval': {'type': 'number',\n",
       "          'description': 'float, default=10. Minimum value of a bicluster.\\n'},\n",
       "         'maxval': {'type': 'number',\n",
       "          'description': 'float, default=100. Maximum value of a bicluster.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Shuffle the samples.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['shape', 'n_clusters']}}},\n",
       "     {'func_name': 'make_classification',\n",
       "      'func_desc': 'Generate a random n-class classification problem.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification',\n",
       "      'function_definitions': {'function_name': 'make_classification',\n",
       "       'full_function': 'sklearn.datasets.make_classification(n_samples=100, n_features=20, *, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)',\n",
       "       'function_text': 'Generate a random n-class classification problem. This initially creates clusters of points normally distributed (std=1)\\nabout vertices of an n_informative-dimensional hypercube with sides of\\nlength 2*class_sep and assigns an equal number of clusters to each\\nclass. It introduces interdependence between these features and adds\\nvarious types of further noise to the data. Without shuffling, X horizontally stacks features in the following\\norder: the primary n_informative features, followed by n_redundant\\nlinear combinations of the informative features, followed by n_repeated\\nduplicates, drawn randomly with replacement from the informative and\\nredundant features. The remaining features are filled with random noise.\\nThus, without shuffling, all useful features are contained in the columns\\nX[:, :n_informative + n_redundant + n_repeated]. For an example of usage, see\\nPlot randomly generated classification dataset. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/datasets/plot_random_dataset.html#sphx-glr-auto-examples-datasets-plot-random-dataset-py',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The number of samples.\\n'},\n",
       "        {'param_name': 'n_features',\n",
       "         'param_type': 'int, default=20',\n",
       "         'param_desc': 'The total number of features. These comprise n_informative\\ninformative features, n_redundant redundant features,\\nn_repeated duplicated features and\\nn_features-n_informative-n_redundant-n_repeated useless features\\ndrawn at random.\\n'},\n",
       "        {'param_name': 'n_informative',\n",
       "         'param_type': 'int, default=2',\n",
       "         'param_desc': 'The number of informative features. Each class is composed of a number\\nof gaussian clusters each located around the vertices of a hypercube\\nin a subspace of dimension n_informative. For each cluster,\\ninformative features are drawn independently from  N(0, 1) and then\\nrandomly linearly combined within each cluster in order to add\\ncovariance. The clusters are then placed on the vertices of the\\nhypercube.\\n'},\n",
       "        {'param_name': 'n_redundant',\n",
       "         'param_type': 'int, default=2',\n",
       "         'param_desc': 'The number of redundant features. These features are generated as\\nrandom linear combinations of the informative features.\\n'},\n",
       "        {'param_name': 'n_repeated',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'The number of duplicated features, drawn randomly from the informative\\nand the redundant features.\\n'},\n",
       "        {'param_name': 'n_classes',\n",
       "         'param_type': 'int, default=2',\n",
       "         'param_desc': 'The number of classes (or labels) of the classification problem.\\n'},\n",
       "        {'param_name': 'n_clusters_per_class',\n",
       "         'param_type': 'int, default=2',\n",
       "         'param_desc': 'The number of clusters per class.\\n'},\n",
       "        {'param_name': 'weights',\n",
       "         'param_type': 'array-like of shape (n_classes,) or (n_classes - 1,),              default=None',\n",
       "         'param_desc': 'The proportions of samples assigned to each class. If None, then\\nclasses are balanced. Note that if len(weights) == n_classes - 1,\\nthen the last class weight is automatically inferred.\\nMore than n_samples samples may be returned if the sum of\\nweights exceeds 1. Note that the actual class proportions will\\nnot exactly match weights when flip_y isn’t 0.\\n'},\n",
       "        {'param_name': 'flip_y',\n",
       "         'param_type': 'float, default=0.01',\n",
       "         'param_desc': 'The fraction of samples whose class is assigned randomly. Larger\\nvalues introduce noise in the labels and make the classification\\ntask harder. Note that the default setting flip_y > 0 might lead\\nto less than n_classes in y in some cases.\\n'},\n",
       "        {'param_name': 'class_sep',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'The factor multiplying the hypercube size.  Larger values spread\\nout the clusters/classes and make the classification task easier.\\n'},\n",
       "        {'param_name': 'hypercube',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, the clusters are put on the vertices of a hypercube. If\\nFalse, the clusters are put on the vertices of a random polytope.\\n'},\n",
       "        {'param_name': 'shift',\n",
       "         'param_type': 'float, ndarray of shape (n_features,) or None, default=0.0',\n",
       "         'param_desc': 'Shift features by the specified value. If None, then features\\nare shifted by a random value drawn in [-class_sep, class_sep].\\n'},\n",
       "        {'param_name': 'scale',\n",
       "         'param_type': 'float, ndarray of shape (n_features,) or None, default=1.0',\n",
       "         'param_desc': 'Multiply features by the specified value. If None, then features\\nare scaled by a random value drawn in [1, 100]. Note that scaling\\nhappens after shifting.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Shuffle the samples and the features.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_classification',\n",
       "       'descriptions': 'Generate a random n-class classification problem. This initially creates clusters of points normally distributed (std=1)\\nabout vertices of an n_informative-dimensional hypercube with sides of\\nlength 2*class_sep and assigns an equal number of clusters to each\\nclass. It introduces interdependence between these features and adds\\nvarious types of further noise to the data. Without shuffling, X horizontally stacks features in the following\\norder: the primary n_informative features, followed by n_redundant\\nlinear combinations of the informative features, followed by n_repeated\\nduplicates, drawn randomly with replacement from the informative and\\nredundant features. The remaining features are filled with random noise.\\nThus, without shuffling, all useful features are contained in the columns\\nX[:, :n_informative + n_redundant + n_repeated]. For an example of usage, see\\nPlot randomly generated classification dataset. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=100. The number of samples.\\n'},\n",
       "         'n_features': {'type': 'integer',\n",
       "          'description': 'int, default=20. The total number of features. These comprise n_informative\\ninformative features, n_redundant redundant features,\\nn_repeated duplicated features and\\nn_features-n_informative-n_redundant-n_repeated useless features\\ndrawn at random.\\n'},\n",
       "         'n_informative': {'type': 'integer',\n",
       "          'description': 'int, default=2. The number of informative features. Each class is composed of a number\\nof gaussian clusters each located around the vertices of a hypercube\\nin a subspace of dimension n_informative. For each cluster,\\ninformative features are drawn independently from  N(0, 1) and then\\nrandomly linearly combined within each cluster in order to add\\ncovariance. The clusters are then placed on the vertices of the\\nhypercube.\\n'},\n",
       "         'n_redundant': {'type': 'integer',\n",
       "          'description': 'int, default=2. The number of redundant features. These features are generated as\\nrandom linear combinations of the informative features.\\n'},\n",
       "         'n_repeated': {'type': 'integer',\n",
       "          'description': 'int, default=0. The number of duplicated features, drawn randomly from the informative\\nand the redundant features.\\n'},\n",
       "         'n_classes': {'type': 'integer',\n",
       "          'description': 'int, default=2. The number of classes (or labels) of the classification problem.\\n'},\n",
       "         'n_clusters_per_class': {'type': 'integer',\n",
       "          'description': 'int, default=2. The number of clusters per class.\\n'},\n",
       "         'weights': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_classes,) or (n_classes - 1,),              default=None. The proportions of samples assigned to each class. If None, then\\nclasses are balanced. Note that if len(weights) == n_classes - 1,\\nthen the last class weight is automatically inferred.\\nMore than n_samples samples may be returned if the sum of\\nweights exceeds 1. Note that the actual class proportions will\\nnot exactly match weights when flip_y isn’t 0.\\n'},\n",
       "         'flip_y': {'type': 'number',\n",
       "          'description': 'float, default=0.01. The fraction of samples whose class is assigned randomly. Larger\\nvalues introduce noise in the labels and make the classification\\ntask harder. Note that the default setting flip_y > 0 might lead\\nto less than n_classes in y in some cases.\\n'},\n",
       "         'class_sep': {'type': 'number',\n",
       "          'description': 'float, default=1.0. The factor multiplying the hypercube size.  Larger values spread\\nout the clusters/classes and make the classification task easier.\\n'},\n",
       "         'hypercube': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, the clusters are put on the vertices of a hypercube. If\\nFalse, the clusters are put on the vertices of a random polytope.\\n'},\n",
       "         'shift': {'type': 'number',\n",
       "          'description': 'float, ndarray of shape (n_features,) or None, default=0.0. Shift features by the specified value. If None, then features\\nare shifted by a random value drawn in [-class_sep, class_sep].\\n'},\n",
       "         'scale': {'type': 'number',\n",
       "          'description': 'float, ndarray of shape (n_features,) or None, default=1.0. Multiply features by the specified value. If None, then features\\nare scaled by a random value drawn in [1, 100]. Note that scaling\\nhappens after shifting.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Shuffle the samples and the features.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['n_samples=100', 'n_features=20']}}},\n",
       "     {'func_name': 'make_friedman2',\n",
       "      'func_desc': 'Generate the \"Friedman #2\" regression problem.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman2.html#sklearn.datasets.make_friedman2',\n",
       "      'function_definitions': {'function_name': 'make_friedman2',\n",
       "       'full_function': 'sklearn.datasets.make_friedman2(n_samples=100, *, noise=0.0, random_state=None)',\n",
       "       'function_text': 'Generate the “Friedman #2” regression problem. This dataset is described in Friedman [1] and Breiman [2]. Inputs X are 4 independent features uniformly distributed on the\\nintervals: The output y is created according to the formula: Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The number of samples.\\n'},\n",
       "        {'param_name': 'noise',\n",
       "         'param_type': 'float, default=0.0',\n",
       "         'param_desc': 'The standard deviation of the gaussian noise applied to the output.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset noise. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_friedman2',\n",
       "       'descriptions': 'Generate the “Friedman #2” regression problem. This dataset is described in Friedman [1] and Breiman [2]. Inputs X are 4 independent features uniformly distributed on the\\nintervals: The output y is created according to the formula: Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=100. The number of samples.\\n'},\n",
       "         'noise': {'type': 'number',\n",
       "          'description': 'float, default=0.0. The standard deviation of the gaussian noise applied to the output.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset noise. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['n_samples=100']}}},\n",
       "     {'func_name': 'make_gaussian_quantiles',\n",
       "      'func_desc': 'Generate isotropic Gaussian and label samples by quantile.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_gaussian_quantiles.html#sklearn.datasets.make_gaussian_quantiles',\n",
       "      'function_definitions': {'function_name': 'make_gaussian_quantiles',\n",
       "       'full_function': 'sklearn.datasets.make_gaussian_quantiles(*, mean=None, cov=1.0, n_samples=100, n_features=2, n_classes=3, shuffle=True, random_state=None)',\n",
       "       'function_text': 'Generate isotropic Gaussian and label samples by quantile. This classification dataset is constructed by taking a multi-dimensional\\nstandard normal distribution and defining classes separated by nested\\nconcentric multi-dimensional spheres such that roughly equal numbers of\\nsamples are in each class (quantiles of the \\\\(\\\\chi^2\\\\) distribution). For an example of usage, see\\nPlot randomly generated classification dataset. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/datasets/plot_random_dataset.html#sphx-glr-auto-examples-datasets-plot-random-dataset-py',\n",
       "       'parameter_names_desc': [{'param_name': 'mean',\n",
       "         'param_type': 'array-like of shape (n_features,), default=None',\n",
       "         'param_desc': 'The mean of the multi-dimensional normal distribution.\\nIf None then use the origin (0, 0, …).\\n'},\n",
       "        {'param_name': 'cov',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'The covariance matrix will be this value times the unit matrix. This\\ndataset only produces symmetric normal distributions.\\n'},\n",
       "        {'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The total number of points equally divided among classes.\\n'},\n",
       "        {'param_name': 'n_features',\n",
       "         'param_type': 'int, default=2',\n",
       "         'param_desc': 'The number of features for each sample.\\n'},\n",
       "        {'param_name': 'n_classes',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'The number of classes.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Shuffle the samples.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_gaussian_quantiles',\n",
       "       'descriptions': 'Generate isotropic Gaussian and label samples by quantile. This classification dataset is constructed by taking a multi-dimensional\\nstandard normal distribution and defining classes separated by nested\\nconcentric multi-dimensional spheres such that roughly equal numbers of\\nsamples are in each class (quantiles of the \\\\(\\\\chi^2\\\\) distribution). For an example of usage, see\\nPlot randomly generated classification dataset. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'mean': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_features,), default=None. The mean of the multi-dimensional normal distribution.\\nIf None then use the origin (0, 0, …).\\n'},\n",
       "         'cov': {'type': 'number',\n",
       "          'description': 'float, default=1.0. The covariance matrix will be this value times the unit matrix. This\\ndataset only produces symmetric normal distributions.\\n'},\n",
       "         'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=100. The total number of points equally divided among classes.\\n'},\n",
       "         'n_features': {'type': 'integer',\n",
       "          'description': 'int, default=2. The number of features for each sample.\\n'},\n",
       "         'n_classes': {'type': 'integer',\n",
       "          'description': 'int, default=3. The number of classes.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Shuffle the samples.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'make_low_rank_matrix',\n",
       "      'func_desc': 'Generate a mostly low rank matrix with bell-shaped singular values.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_low_rank_matrix.html#sklearn.datasets.make_low_rank_matrix',\n",
       "      'function_definitions': {'function_name': 'make_low_rank_matrix',\n",
       "       'full_function': 'sklearn.datasets.make_low_rank_matrix(n_samples=100, n_features=100, *, effective_rank=10, tail_strength=0.5, random_state=None)',\n",
       "       'function_text': 'Generate a mostly low rank matrix with bell-shaped singular values. Most of the variance can be explained by a bell-shaped curve of width\\neffective_rank: the low rank part of the singular values profile is: The remaining singular values’ tail is fat, decreasing as: The low rank part of the profile can be considered the structured\\nsignal part of the data while the tail can be considered the noisy\\npart of the data that cannot be summarized by a low number of linear\\ncomponents (singular vectors). gray level pictures of faces TF-IDF vectors of text documents crawled from the web Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The number of samples.\\n'},\n",
       "        {'param_name': 'n_features',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The number of features.\\n'},\n",
       "        {'param_name': 'effective_rank',\n",
       "         'param_type': 'int, default=10',\n",
       "         'param_desc': 'The approximate number of singular vectors required to explain most of\\nthe data by linear combinations.\\n'},\n",
       "        {'param_name': 'tail_strength',\n",
       "         'param_type': 'float, default=0.5',\n",
       "         'param_desc': 'The relative importance of the fat noisy tail of the singular values\\nprofile. The value should be between 0 and 1.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_low_rank_matrix',\n",
       "       'descriptions': 'Generate a mostly low rank matrix with bell-shaped singular values. Most of the variance can be explained by a bell-shaped curve of width\\neffective_rank: the low rank part of the singular values profile is: The remaining singular values’ tail is fat, decreasing as: The low rank part of the profile can be considered the structured\\nsignal part of the data while the tail can be considered the noisy\\npart of the data that cannot be summarized by a low number of linear\\ncomponents (singular vectors). gray level pictures of faces TF-IDF vectors of text documents crawled from the web Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=100. The number of samples.\\n'},\n",
       "         'n_features': {'type': 'integer',\n",
       "          'description': 'int, default=100. The number of features.\\n'},\n",
       "         'effective_rank': {'type': 'integer',\n",
       "          'description': 'int, default=10. The approximate number of singular vectors required to explain most of\\nthe data by linear combinations.\\n'},\n",
       "         'tail_strength': {'type': 'number',\n",
       "          'description': 'float, default=0.5. The relative importance of the fat noisy tail of the singular values\\nprofile. The value should be between 0 and 1.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['n_samples=100', 'n_features=100']}}},\n",
       "     {'func_name': 'make_multilabel_classification',\n",
       "      'func_desc': 'Generate a random multilabel classification problem.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_multilabel_classification.html#sklearn.datasets.make_multilabel_classification',\n",
       "      'function_definitions': {'function_name': 'make_multilabel_classification',\n",
       "       'full_function': \"sklearn.datasets.make_multilabel_classification(n_samples=100, n_features=20, *, n_classes=5, n_labels=2, length=50, allow_unlabeled=True, sparse=False, return_indicator='dense', return_distributions=False, random_state=None)\",\n",
       "       'function_text': 'Generate a random multilabel classification problem. pick the number of labels: n ~ Poisson(n_labels) n times, choose a class c: c ~ Multinomial(theta) pick the document length: k ~ Poisson(length) k times, choose a word: w ~ Multinomial(theta_c) In the above process, rejection sampling is used to make sure that\\nn is never zero or more than n_classes, and that the document length\\nis never zero. Likewise, we reject classes which have already been chosen. For an example of usage, see\\nPlot randomly generated multilabel dataset. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/datasets/plot_random_multilabel_dataset.html#sphx-glr-auto-examples-datasets-plot-random-multilabel-dataset-py',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The number of samples.\\n'},\n",
       "        {'param_name': 'n_features',\n",
       "         'param_type': 'int, default=20',\n",
       "         'param_desc': 'The total number of features.\\n'},\n",
       "        {'param_name': 'n_classes',\n",
       "         'param_type': 'int, default=5',\n",
       "         'param_desc': 'The number of classes of the classification problem.\\n'},\n",
       "        {'param_name': 'n_labels',\n",
       "         'param_type': 'int, default=2',\n",
       "         'param_desc': 'The average number of labels per instance. More precisely, the number\\nof labels per sample is drawn from a Poisson distribution with\\nn_labels as its expected value, but samples are bounded (using\\nrejection sampling) by n_classes, and must be nonzero if\\nallow_unlabeled is False.\\n'},\n",
       "        {'param_name': 'length',\n",
       "         'param_type': 'int, default=50',\n",
       "         'param_desc': 'The sum of the features (number of words if documents) is drawn from\\na Poisson distribution with this expected value.\\n'},\n",
       "        {'param_name': 'allow_unlabeled',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, some instances might not belong to any class.\\n'},\n",
       "        {'param_name': 'sparse',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, return a sparse feature matrix.\\n\\nAdded in version 0.17: parameter to allow sparse output.\\n\\n'},\n",
       "        {'param_name': 'return_indicator',\n",
       "         'param_type': '{‘dense’, ‘sparse’} or False, default=’dense’',\n",
       "         'param_desc': \"If 'dense' return Y in the dense binary indicator format. If\\n'sparse' return Y in the sparse binary indicator format.\\nFalse returns a list of lists of labels.\\n\"},\n",
       "        {'param_name': 'return_distributions',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, return the prior class probability and conditional\\nprobabilities of features given classes, from which the data was\\ndrawn.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_multilabel_classification',\n",
       "       'descriptions': 'Generate a random multilabel classification problem. pick the number of labels: n ~ Poisson(n_labels) n times, choose a class c: c ~ Multinomial(theta) pick the document length: k ~ Poisson(length) k times, choose a word: w ~ Multinomial(theta_c) In the above process, rejection sampling is used to make sure that\\nn is never zero or more than n_classes, and that the document length\\nis never zero. Likewise, we reject classes which have already been chosen. For an example of usage, see\\nPlot randomly generated multilabel dataset. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=100. The number of samples.\\n'},\n",
       "         'n_features': {'type': 'integer',\n",
       "          'description': 'int, default=20. The total number of features.\\n'},\n",
       "         'n_classes': {'type': 'integer',\n",
       "          'description': 'int, default=5. The number of classes of the classification problem.\\n'},\n",
       "         'n_labels': {'type': 'integer',\n",
       "          'description': 'int, default=2. The average number of labels per instance. More precisely, the number\\nof labels per sample is drawn from a Poisson distribution with\\nn_labels as its expected value, but samples are bounded (using\\nrejection sampling) by n_classes, and must be nonzero if\\nallow_unlabeled is False.\\n'},\n",
       "         'length': {'type': 'integer',\n",
       "          'description': 'int, default=50. The sum of the features (number of words if documents) is drawn from\\na Poisson distribution with this expected value.\\n'},\n",
       "         'allow_unlabeled': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, some instances might not belong to any class.\\n'},\n",
       "         'sparse': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, return a sparse feature matrix.\\n\\nAdded in version 0.17: parameter to allow sparse output.\\n\\n'},\n",
       "         'return_indicator': {'type': 'string',\n",
       "          'enum': ['dense', 'sparse'],\n",
       "          'description': \"{‘dense’, ‘sparse’} or False, default=’dense’. If 'dense' return Y in the dense binary indicator format. If\\n'sparse' return Y in the sparse binary indicator format.\\nFalse returns a list of lists of labels.\\n\"},\n",
       "         'return_distributions': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, return the prior class probability and conditional\\nprobabilities of features given classes, from which the data was\\ndrawn.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['n_samples=100', 'n_features=20']}}},\n",
       "     {'func_name': 'make_s_curve',\n",
       "      'func_desc': 'Generate an S curve dataset.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_s_curve.html#sklearn.datasets.make_s_curve',\n",
       "      'function_definitions': {'function_name': 'make_s_curve',\n",
       "       'full_function': 'sklearn.datasets.make_s_curve(n_samples=100, *, noise=0.0, random_state=None)',\n",
       "       'function_text': 'Generate an S curve dataset. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The number of sample points on the S curve.\\n'},\n",
       "        {'param_name': 'noise',\n",
       "         'param_type': 'float, default=0.0',\n",
       "         'param_desc': 'The standard deviation of the gaussian noise.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_s_curve',\n",
       "       'descriptions': 'Generate an S curve dataset. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=100. The number of sample points on the S curve.\\n'},\n",
       "         'noise': {'type': 'number',\n",
       "          'description': 'float, default=0.0. The standard deviation of the gaussian noise.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['n_samples=100']}}},\n",
       "     {'func_name': 'make_sparse_spd_matrix',\n",
       "      'func_desc': 'Generate a sparse symmetric definite positive matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_sparse_spd_matrix.html#sklearn.datasets.make_sparse_spd_matrix',\n",
       "      'function_definitions': {'function_name': 'make_sparse_spd_matrix',\n",
       "       'full_function': \"sklearn.datasets.make_sparse_spd_matrix(n_dim=None, *, alpha=0.95, norm_diag=False, smallest_coef=0.1, largest_coef=0.9, sparse_format=None, random_state=None, dim='deprecated')\",\n",
       "       'function_text': 'Generate a sparse symmetric definite positive matrix. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'n_dim',\n",
       "         'param_type': 'int, default=1',\n",
       "         'param_desc': 'The size of the random matrix to generate.\\n\\nChanged in version 1.4: Renamed from dim to n_dim.\\n\\n'},\n",
       "        {'param_name': 'alpha',\n",
       "         'param_type': 'float, default=0.95',\n",
       "         'param_desc': 'The probability that a coefficient is zero (see notes). Larger values\\nenforce more sparsity. The value should be in the range 0 and 1.\\n'},\n",
       "        {'param_name': 'norm_diag',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to normalize the output matrix to make the leading diagonal\\nelements all 1.\\n'},\n",
       "        {'param_name': 'smallest_coef',\n",
       "         'param_type': 'float, default=0.1',\n",
       "         'param_desc': 'The value of the smallest coefficient between 0 and 1.\\n'},\n",
       "        {'param_name': 'largest_coef',\n",
       "         'param_type': 'float, default=0.9',\n",
       "         'param_desc': 'The value of the largest coefficient between 0 and 1.\\n'},\n",
       "        {'param_name': 'sparse_format',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'String representing the output sparse format, such as ‘csc’, ‘csr’, etc.\\nIf None, return a dense numpy ndarray.\\n\\nAdded in version 1.4.\\n\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'dim',\n",
       "         'param_type': 'int, default=1',\n",
       "         'param_desc': 'The size of the random matrix to generate.\\n\\nDeprecated since version 1.4: dim is deprecated and will be removed in 1.6.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'make_sparse_spd_matrix',\n",
       "       'descriptions': 'Generate a sparse symmetric definite positive matrix. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_dim': {'type': 'integer',\n",
       "          'description': 'int, default=1. The size of the random matrix to generate.\\n\\nChanged in version 1.4: Renamed from dim to n_dim.\\n\\n'},\n",
       "         'alpha': {'type': 'number',\n",
       "          'description': 'float, default=0.95. The probability that a coefficient is zero (see notes). Larger values\\nenforce more sparsity. The value should be in the range 0 and 1.\\n'},\n",
       "         'norm_diag': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to normalize the output matrix to make the leading diagonal\\nelements all 1.\\n'},\n",
       "         'smallest_coef': {'type': 'number',\n",
       "          'description': 'float, default=0.1. The value of the smallest coefficient between 0 and 1.\\n'},\n",
       "         'largest_coef': {'type': 'number',\n",
       "          'description': 'float, default=0.9. The value of the largest coefficient between 0 and 1.\\n'},\n",
       "         'sparse_format': {'type': 'string',\n",
       "          'description': 'str, default=None. String representing the output sparse format, such as ‘csc’, ‘csr’, etc.\\nIf None, return a dense numpy ndarray.\\n\\nAdded in version 1.4.\\n\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'dim': {'type': 'integer',\n",
       "          'description': 'int, default=1. The size of the random matrix to generate.\\n\\nDeprecated since version 1.4: dim is deprecated and will be removed in 1.6.\\n\\n'}},\n",
       "        'required': ['n_dim=None']}}},\n",
       "     {'func_name': 'make_spd_matrix',\n",
       "      'func_desc': 'Generate a random symmetric, positive-definite matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_spd_matrix.html#sklearn.datasets.make_spd_matrix',\n",
       "      'function_definitions': {'function_name': 'make_spd_matrix',\n",
       "       'full_function': 'sklearn.datasets.make_spd_matrix(n_dim, *, random_state=None)',\n",
       "       'function_text': 'Generate a random symmetric, positive-definite matrix. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'n_dim',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'The matrix dimension.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_spd_matrix',\n",
       "       'descriptions': 'Generate a random symmetric, positive-definite matrix. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_dim': {'type': 'integer',\n",
       "          'description': 'int. The matrix dimension.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['n_dim']}}},\n",
       "     {'func_name': 'make_blobs',\n",
       "      'func_desc': 'Generate isotropic Gaussian blobs for clustering.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs',\n",
       "      'function_definitions': {'function_name': 'make_blobs',\n",
       "       'full_function': 'sklearn.datasets.make_blobs(n_samples=100, n_features=2, *, centers=None, cluster_std=1.0, center_box=(-10.0, 10.0), shuffle=True, random_state=None, return_centers=False)',\n",
       "       'function_text': 'Generate isotropic Gaussian blobs for clustering. For an example of usage, see\\nPlot randomly generated classification dataset. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/datasets/plot_random_dataset.html#sphx-glr-auto-examples-datasets-plot-random-dataset-py',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int or array-like, default=100',\n",
       "         'param_desc': 'If int, it is the total number of points equally divided among\\nclusters.\\nIf array-like, each element of the sequence indicates\\nthe number of samples per cluster.\\n\\nChanged in version v0.20: one can now pass an array-like to the n_samples parameter\\n\\n'},\n",
       "        {'param_name': 'n_features',\n",
       "         'param_type': 'int, default=2',\n",
       "         'param_desc': 'The number of features for each sample.\\n'},\n",
       "        {'param_name': 'centers',\n",
       "         'param_type': 'int or array-like of shape (n_centers, n_features), default=None',\n",
       "         'param_desc': 'The number of centers to generate, or the fixed center locations.\\nIf n_samples is an int and centers is None, 3 centers are generated.\\nIf n_samples is array-like, centers must be\\neither None or an array of length equal to the length of n_samples.\\n'},\n",
       "        {'param_name': 'cluster_std',\n",
       "         'param_type': 'float or array-like of float, default=1.0',\n",
       "         'param_desc': 'The standard deviation of the clusters.\\n'},\n",
       "        {'param_name': 'center_box',\n",
       "         'param_type': 'tuple of float (min, max), default=(-10.0, 10.0)',\n",
       "         'param_desc': 'The bounding box for each cluster center when centers are\\ngenerated at random.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Shuffle the samples.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'return_centers',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, then return the centers of each cluster.\\n\\nAdded in version 0.23.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'make_blobs',\n",
       "       'descriptions': 'Generate isotropic Gaussian blobs for clustering. For an example of usage, see\\nPlot randomly generated classification dataset. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int or array-like, default=100. If int, it is the total number of points equally divided among\\nclusters.\\nIf array-like, each element of the sequence indicates\\nthe number of samples per cluster.\\n\\nChanged in version v0.20: one can now pass an array-like to the n_samples parameter\\n\\n'},\n",
       "         'n_features': {'type': 'integer',\n",
       "          'description': 'int, default=2. The number of features for each sample.\\n'},\n",
       "         'centers': {'type': 'integer',\n",
       "          'description': 'int or array-like of shape (n_centers, n_features), default=None. The number of centers to generate, or the fixed center locations.\\nIf n_samples is an int and centers is None, 3 centers are generated.\\nIf n_samples is array-like, centers must be\\neither None or an array of length equal to the length of n_samples.\\n'},\n",
       "         'cluster_std': {'type': 'number',\n",
       "          'description': 'float or array-like of float, default=1.0. The standard deviation of the clusters.\\n'},\n",
       "         'center_box': {'type': 'number',\n",
       "          'description': 'tuple of float (min, max), default=(-10.0, 10.0). The bounding box for each cluster center when centers are\\ngenerated at random.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Shuffle the samples.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'return_centers': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, then return the centers of each cluster.\\n\\nAdded in version 0.23.\\n\\n'}},\n",
       "        'required': ['n_samples=100', 'n_features=2']}}},\n",
       "     {'func_name': 'make_circles',\n",
       "      'func_desc': 'Make a large circle containing a smaller circle in 2d.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html#sklearn.datasets.make_circles',\n",
       "      'function_definitions': {'function_name': 'make_circles',\n",
       "       'full_function': 'sklearn.datasets.make_circles(n_samples=100, *, shuffle=True, noise=None, random_state=None, factor=0.8)',\n",
       "       'function_text': 'Make a large circle containing a smaller circle in 2d. A simple toy dataset to visualize clustering and classification\\nalgorithms. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int or tuple of shape (2,), dtype=int, default=100',\n",
       "         'param_desc': 'If int, it is the total number of points generated.\\nFor odd numbers, the inner circle will have one point more than the\\nouter circle.\\nIf two-element tuple, number of points in outer circle and inner\\ncircle.\\n\\nChanged in version 0.23: Added two-element tuple.\\n\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to shuffle the samples.\\n'},\n",
       "        {'param_name': 'noise',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': 'Standard deviation of Gaussian noise added to the data.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset shuffling and noise.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'factor',\n",
       "         'param_type': 'float, default=.8',\n",
       "         'param_desc': 'Scale factor between inner and outer circle in the range [0, 1).\\n'}]},\n",
       "      'function_calling': {'name': 'make_circles',\n",
       "       'descriptions': 'Make a large circle containing a smaller circle in 2d. A simple toy dataset to visualize clustering and classification\\nalgorithms. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int or tuple of shape (2,), dtype=int, default=100. If int, it is the total number of points generated.\\nFor odd numbers, the inner circle will have one point more than the\\nouter circle.\\nIf two-element tuple, number of points in outer circle and inner\\ncircle.\\n\\nChanged in version 0.23: Added two-element tuple.\\n\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to shuffle the samples.\\n'},\n",
       "         'noise': {'type': 'number',\n",
       "          'description': 'float, default=None. Standard deviation of Gaussian noise added to the data.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset shuffling and noise.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'factor': {'type': 'number',\n",
       "          'description': 'float, default=.8. Scale factor between inner and outer circle in the range [0, 1).\\n'}},\n",
       "        'required': ['n_samples=100']}}},\n",
       "     {'func_name': 'make_friedman1',\n",
       "      'func_desc': 'Generate the \"Friedman #1\" regression problem.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html#sklearn.datasets.make_friedman1',\n",
       "      'function_definitions': {'function_name': 'make_friedman1',\n",
       "       'full_function': 'sklearn.datasets.make_friedman1(n_samples=100, n_features=10, *, noise=0.0, random_state=None)',\n",
       "       'function_text': 'Generate the “Friedman #1” regression problem. This dataset is described in Friedman [1] and Breiman [2]. Inputs X are independent features uniformly distributed on the interval\\n[0, 1]. The output y is created according to the formula: Out of the n_features features, only 5 are actually used to compute\\ny. The remaining features are independent of y. The number of features has to be >= 5. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The number of samples.\\n'},\n",
       "        {'param_name': 'n_features',\n",
       "         'param_type': 'int, default=10',\n",
       "         'param_desc': 'The number of features. Should be at least 5.\\n'},\n",
       "        {'param_name': 'noise',\n",
       "         'param_type': 'float, default=0.0',\n",
       "         'param_desc': 'The standard deviation of the gaussian noise applied to the output.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset noise. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_friedman1',\n",
       "       'descriptions': 'Generate the “Friedman #1” regression problem. This dataset is described in Friedman [1] and Breiman [2]. Inputs X are independent features uniformly distributed on the interval\\n[0, 1]. The output y is created according to the formula: Out of the n_features features, only 5 are actually used to compute\\ny. The remaining features are independent of y. The number of features has to be >= 5. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=100. The number of samples.\\n'},\n",
       "         'n_features': {'type': 'integer',\n",
       "          'description': 'int, default=10. The number of features. Should be at least 5.\\n'},\n",
       "         'noise': {'type': 'number',\n",
       "          'description': 'float, default=0.0. The standard deviation of the gaussian noise applied to the output.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset noise. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['n_samples=100', 'n_features=10']}}},\n",
       "     {'func_name': 'make_friedman3',\n",
       "      'func_desc': 'Generate the \"Friedman #3\" regression problem.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman3.html#sklearn.datasets.make_friedman3',\n",
       "      'function_definitions': {'function_name': 'make_friedman3',\n",
       "       'full_function': 'sklearn.datasets.make_friedman3(n_samples=100, *, noise=0.0, random_state=None)',\n",
       "       'function_text': 'Generate the “Friedman #3” regression problem. This dataset is described in Friedman [1] and Breiman [2]. Inputs X are 4 independent features uniformly distributed on the\\nintervals: The output y is created according to the formula: Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The number of samples.\\n'},\n",
       "        {'param_name': 'noise',\n",
       "         'param_type': 'float, default=0.0',\n",
       "         'param_desc': 'The standard deviation of the gaussian noise applied to the output.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset noise. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_friedman3',\n",
       "       'descriptions': 'Generate the “Friedman #3” regression problem. This dataset is described in Friedman [1] and Breiman [2]. Inputs X are 4 independent features uniformly distributed on the\\nintervals: The output y is created according to the formula: Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=100. The number of samples.\\n'},\n",
       "         'noise': {'type': 'number',\n",
       "          'description': 'float, default=0.0. The standard deviation of the gaussian noise applied to the output.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset noise. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['n_samples=100']}}},\n",
       "     {'func_name': 'make_hastie_10_2',\n",
       "      'func_desc': 'Generate data for binary classification used in Hastie et al. 2009, Example 10.2.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_hastie_10_2.html#sklearn.datasets.make_hastie_10_2',\n",
       "      'function_definitions': {'function_name': 'make_hastie_10_2',\n",
       "       'full_function': 'sklearn.datasets.make_hastie_10_2(n_samples=12000, *, random_state=None)',\n",
       "       'function_text': 'Generate data for binary classification used in Hastie et al. 2009, Example 10.2. The ten features are standard independent Gaussian and\\nthe target y is defined by: Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=12000',\n",
       "         'param_desc': 'The number of samples.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_hastie_10_2',\n",
       "       'descriptions': 'Generate data for binary classification used in Hastie et al. 2009, Example 10.2. The ten features are standard independent Gaussian and\\nthe target y is defined by: Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=12000. The number of samples.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['n_samples=12000']}}},\n",
       "     {'func_name': 'make_moons',\n",
       "      'func_desc': 'Make two interleaving half circles.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html#sklearn.datasets.make_moons',\n",
       "      'function_definitions': {'function_name': 'make_moons',\n",
       "       'full_function': 'sklearn.datasets.make_moons(n_samples=100, *, shuffle=True, noise=None, random_state=None)',\n",
       "       'function_text': 'Make two interleaving half circles. A simple toy dataset to visualize clustering and classification\\nalgorithms. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int or tuple of shape (2,), dtype=int, default=100',\n",
       "         'param_desc': 'If int, the total number of points generated.\\nIf two-element tuple, number of points in each of two moons.\\n\\nChanged in version 0.23: Added two-element tuple.\\n\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to shuffle the samples.\\n'},\n",
       "        {'param_name': 'noise',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': 'Standard deviation of Gaussian noise added to the data.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset shuffling and noise.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_moons',\n",
       "       'descriptions': 'Make two interleaving half circles. A simple toy dataset to visualize clustering and classification\\nalgorithms. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int or tuple of shape (2,), dtype=int, default=100. If int, the total number of points generated.\\nIf two-element tuple, number of points in each of two moons.\\n\\nChanged in version 0.23: Added two-element tuple.\\n\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to shuffle the samples.\\n'},\n",
       "         'noise': {'type': 'number',\n",
       "          'description': 'float, default=None. Standard deviation of Gaussian noise added to the data.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset shuffling and noise.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['n_samples=100']}}},\n",
       "     {'func_name': 'make_regression',\n",
       "      'func_desc': 'Generate a random regression problem.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression',\n",
       "      'function_definitions': {'function_name': 'make_regression',\n",
       "       'full_function': 'sklearn.datasets.make_regression(n_samples=100, n_features=100, *, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)',\n",
       "       'function_text': 'Generate a random regression problem. The input set can either be well conditioned (by default) or have a low\\nrank-fat tail singular profile. See make_low_rank_matrix for\\nmore details. The output is generated by applying a (potentially biased) random linear\\nregression model with n_informative nonzero regressors to the previously\\ngenerated input and some gaussian centered noise with some adjustable\\nscale. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.datasets.make_low_rank_matrix.html#sklearn.datasets.make_low_rank_matrix',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The number of samples.\\n'},\n",
       "        {'param_name': 'n_features',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The number of features.\\n'},\n",
       "        {'param_name': 'n_informative',\n",
       "         'param_type': 'int, default=10',\n",
       "         'param_desc': 'The number of informative features, i.e., the number of features used\\nto build the linear model used to generate the output.\\n'},\n",
       "        {'param_name': 'n_targets',\n",
       "         'param_type': 'int, default=1',\n",
       "         'param_desc': 'The number of regression targets, i.e., the dimension of the y output\\nvector associated with a sample. By default, the output is a scalar.\\n'},\n",
       "        {'param_name': 'bias',\n",
       "         'param_type': 'float, default=0.0',\n",
       "         'param_desc': 'The bias term in the underlying linear model.\\n'},\n",
       "        {'param_name': 'effective_rank',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': '\\nIf not None:The approximate number of singular vectors required to explain most\\nof the input data by linear combinations. Using this kind of\\nsingular spectrum in the input allows the generator to reproduce\\nthe correlations often observed in practice.\\n\\nIf None:The input set is well conditioned, centered and gaussian with\\nunit variance.\\n\\n\\n'},\n",
       "        {'param_name': 'tail_strength',\n",
       "         'param_type': 'float, default=0.5',\n",
       "         'param_desc': 'The relative importance of the fat noisy tail of the singular values\\nprofile if effective_rank is not None. When a float, it should be\\nbetween 0 and 1.\\n'},\n",
       "        {'param_name': 'noise',\n",
       "         'param_type': 'float, default=0.0',\n",
       "         'param_desc': 'The standard deviation of the gaussian noise applied to the output.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Shuffle the samples and the features.\\n'},\n",
       "        {'param_name': 'coef',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the coefficients of the underlying linear model are returned.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_regression',\n",
       "       'descriptions': 'Generate a random regression problem. The input set can either be well conditioned (by default) or have a low\\nrank-fat tail singular profile. See make_low_rank_matrix for\\nmore details. The output is generated by applying a (potentially biased) random linear\\nregression model with n_informative nonzero regressors to the previously\\ngenerated input and some gaussian centered noise with some adjustable\\nscale. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=100. The number of samples.\\n'},\n",
       "         'n_features': {'type': 'integer',\n",
       "          'description': 'int, default=100. The number of features.\\n'},\n",
       "         'n_informative': {'type': 'integer',\n",
       "          'description': 'int, default=10. The number of informative features, i.e., the number of features used\\nto build the linear model used to generate the output.\\n'},\n",
       "         'n_targets': {'type': 'integer',\n",
       "          'description': 'int, default=1. The number of regression targets, i.e., the dimension of the y output\\nvector associated with a sample. By default, the output is a scalar.\\n'},\n",
       "         'bias': {'type': 'number',\n",
       "          'description': 'float, default=0.0. The bias term in the underlying linear model.\\n'},\n",
       "         'effective_rank': {'type': 'integer',\n",
       "          'description': 'int, default=None. \\nIf not None:The approximate number of singular vectors required to explain most\\nof the input data by linear combinations. Using this kind of\\nsingular spectrum in the input allows the generator to reproduce\\nthe correlations often observed in practice.\\n\\nIf None:The input set is well conditioned, centered and gaussian with\\nunit variance.\\n\\n\\n'},\n",
       "         'tail_strength': {'type': 'number',\n",
       "          'description': 'float, default=0.5. The relative importance of the fat noisy tail of the singular values\\nprofile if effective_rank is not None. When a float, it should be\\nbetween 0 and 1.\\n'},\n",
       "         'noise': {'type': 'number',\n",
       "          'description': 'float, default=0.0. The standard deviation of the gaussian noise applied to the output.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Shuffle the samples and the features.\\n'},\n",
       "         'coef': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the coefficients of the underlying linear model are returned.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['n_samples=100', 'n_features=100']}}},\n",
       "     {'func_name': 'make_sparse_coded_signal',\n",
       "      'func_desc': 'Generate a signal as a sparse combination of dictionary elements.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_sparse_coded_signal.html#sklearn.datasets.make_sparse_coded_signal',\n",
       "      'function_definitions': {'function_name': 'make_sparse_coded_signal',\n",
       "       'full_function': 'sklearn.datasets.make_sparse_coded_signal(n_samples, *, n_components, n_features, n_nonzero_coefs, random_state=None)',\n",
       "       'function_text': 'Generate a signal as a sparse combination of dictionary elements. Returns matrices Y, D and X such that Y = XD where X is of shape\\n(n_samples, n_components), D is of shape (n_components, n_features), and\\neach row of X has exactly n_nonzero_coefs non-zero elements. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Number of samples to generate.\\n'},\n",
       "        {'param_name': 'n_components',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Number of components in the dictionary.\\n'},\n",
       "        {'param_name': 'n_features',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Number of features of the dataset to generate.\\n'},\n",
       "        {'param_name': 'n_nonzero_coefs',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Number of active (non-zero) coefficients in each sample.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_sparse_coded_signal',\n",
       "       'descriptions': 'Generate a signal as a sparse combination of dictionary elements. Returns matrices Y, D and X such that Y = XD where X is of shape\\n(n_samples, n_components), D is of shape (n_components, n_features), and\\neach row of X has exactly n_nonzero_coefs non-zero elements. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int. Number of samples to generate.\\n'},\n",
       "         'n_components': {'type': 'integer',\n",
       "          'description': 'int. Number of components in the dictionary.\\n'},\n",
       "         'n_features': {'type': 'integer',\n",
       "          'description': 'int. Number of features of the dataset to generate.\\n'},\n",
       "         'n_nonzero_coefs': {'type': 'integer',\n",
       "          'description': 'int. Number of active (non-zero) coefficients in each sample.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['n_samples']}}},\n",
       "     {'func_name': 'make_sparse_uncorrelated',\n",
       "      'func_desc': 'Generate a random regression problem with sparse uncorrelated design.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_sparse_uncorrelated.html#sklearn.datasets.make_sparse_uncorrelated',\n",
       "      'function_definitions': {'function_name': 'make_sparse_uncorrelated',\n",
       "       'full_function': 'sklearn.datasets.make_sparse_uncorrelated(n_samples=100, n_features=10, *, random_state=None)',\n",
       "       'function_text': 'Generate a random regression problem with sparse uncorrelated design. This dataset is described in Celeux et al [1]. as: Only the first 4 features are informative. The remaining features are\\nuseless. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The number of samples.\\n'},\n",
       "        {'param_name': 'n_features',\n",
       "         'param_type': 'int, default=10',\n",
       "         'param_desc': 'The number of features.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'make_sparse_uncorrelated',\n",
       "       'descriptions': 'Generate a random regression problem with sparse uncorrelated design. This dataset is described in Celeux et al [1]. as: Only the first 4 features are informative. The remaining features are\\nuseless. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=100. The number of samples.\\n'},\n",
       "         'n_features': {'type': 'integer',\n",
       "          'description': 'int, default=10. The number of features.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['n_samples=100', 'n_features=10']}}},\n",
       "     {'func_name': 'make_swiss_roll',\n",
       "      'func_desc': 'Generate a swiss roll dataset.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_swiss_roll.html#sklearn.datasets.make_swiss_roll',\n",
       "      'function_definitions': {'function_name': 'make_swiss_roll',\n",
       "       'full_function': 'sklearn.datasets.make_swiss_roll(n_samples=100, *, noise=0.0, random_state=None, hole=False)',\n",
       "       'function_text': 'Generate a swiss roll dataset. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/datasets/sample_generators.html#sample-generators',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The number of sample points on the Swiss Roll.\\n'},\n",
       "        {'param_name': 'noise',\n",
       "         'param_type': 'float, default=0.0',\n",
       "         'param_desc': 'The standard deviation of the gaussian noise.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'hole',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True generates the swiss roll with hole dataset.\\n'}]},\n",
       "      'function_calling': {'name': 'make_swiss_roll',\n",
       "       'descriptions': 'Generate a swiss roll dataset. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=100. The number of sample points on the Swiss Roll.\\n'},\n",
       "         'noise': {'type': 'number',\n",
       "          'description': 'float, default=0.0. The standard deviation of the gaussian noise.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for dataset creation. Pass an int\\nfor reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'hole': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True generates the swiss roll with hole dataset.\\n'}},\n",
       "        'required': ['n_samples=100']}}}]}],\n",
       "  'name': 'sklearn.datasets',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.datasets.html'},\n",
       " 'sklearn.decomposition.html': {'functions': [{'defaults': [{'func_name': 'DictionaryLearning',\n",
       "      'func_desc': 'Dictionary learning.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.DictionaryLearning.html#sklearn.decomposition.DictionaryLearning',\n",
       "      'function_definitions': {'function_name': 'DictionaryLearning',\n",
       "       'full_function': \"class sklearn.decomposition.DictionaryLearning(n_components=None, *, alpha=1, max_iter=1000, tol=1e-08, fit_algorithm='lars', transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, n_jobs=None, code_init=None, dict_init=None, callback=None, verbose=False, split_sign=False, random_state=None, positive_code=False, positive_dict=False, transform_max_iter=1000)\",\n",
       "       'function_text': 'Dictionary learning. Finds a dictionary (a set of atoms) that performs well at sparsely\\nencoding the fitted data. Solves the optimization problem: ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\\nthe entry-wise matrix norm which is the sum of the absolute values\\nof all the entries in the matrix. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/decomposition.html#dictionarylearning',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'ndarray of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Test data to be transformed, must have the same number of\\nfeatures as the data used to train the model.\\n'}]},\n",
       "      'function_calling': {'name': 'DictionaryLearning',\n",
       "       'descriptions': 'Dictionary learning. Finds a dictionary (a set of atoms) that performs well at sparsely\\nencoding the fitted data. Solves the optimization problem: ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\\nthe entry-wise matrix norm which is the sum of the absolute values\\nof all the entries in the matrix. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples, n_features). Test data to be transformed, must have the same number of\\nfeatures as the data used to train the model.\\n'}},\n",
       "        'required': ['n_components=None']}}},\n",
       "     {'func_name': 'FastICA',\n",
       "      'func_desc': 'FastICA: a fast algorithm for Independent Component Analysis.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA',\n",
       "      'function_definitions': {'function_name': 'FastICA',\n",
       "       'full_function': \"class sklearn.decomposition.FastICA(n_components=None, *, algorithm='parallel', whiten='unit-variance', fun='logcosh', fun_args=None, max_iter=200, tol=0.0001, w_init=None, whiten_solver='svd', random_state=None)\",\n",
       "       'function_text': 'FastICA: a fast algorithm for Independent Component Analysis. The implementation is based on [1]. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#r44c805292efc-1',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Data to transform, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, data passed to fit can be overwritten. Defaults to True.\\n'}]},\n",
       "      'function_calling': {'name': 'FastICA',\n",
       "       'descriptions': 'FastICA: a fast algorithm for Independent Component Analysis. The implementation is based on [1]. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Data to transform, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, data passed to fit can be overwritten. Defaults to True.\\n'}},\n",
       "        'required': ['n_components=None']}}},\n",
       "     {'func_name': 'KernelPCA',\n",
       "      'func_desc': 'Kernel Principal component analysis (KPCA).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA',\n",
       "      'function_definitions': {'function_name': 'KernelPCA',\n",
       "       'full_function': \"class sklearn.decomposition.KernelPCA(n_components=None, *, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None, alpha=1.0, fit_inverse_transform=False, eigen_solver='auto', tol=0, max_iter=None, iterated_power='auto', remove_zero_eig=False, random_state=None, copy_X=True, n_jobs=None)\",\n",
       "       'function_text': 'Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels [1], see also\\nPairwise metrics, Affinities and Kernels. It uses the scipy.linalg.eigh LAPACK implementation of the full SVD\\nor the scipy.sparse.linalg.eigsh ARPACK implementation of the\\ntruncated SVD, depending on the shape of the input data and the number of\\ncomponents to extract. It can also use a randomized truncated SVD by the\\nmethod proposed in [3], see eigen_solver. For a usage example and comparison between\\nPrincipal Components Analysis (PCA) and its kernelized version (KPCA), see\\nKernel PCA. For a usage example in denoising images using KPCA, see\\nImage denoising using kernel PCA. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#r396fc7d924b8-1',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training vector, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}]},\n",
       "      'function_calling': {'name': 'KernelPCA',\n",
       "       'descriptions': 'Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels [1], see also\\nPairwise metrics, Affinities and Kernels. It uses the scipy.linalg.eigh LAPACK implementation of the full SVD\\nor the scipy.sparse.linalg.eigsh ARPACK implementation of the\\ntruncated SVD, depending on the shape of the input data and the number of\\ncomponents to extract. It can also use a randomized truncated SVD by the\\nmethod proposed in [3], see eigen_solver. For a usage example and comparison between\\nPrincipal Components Analysis (PCA) and its kernelized version (KPCA), see\\nKernel PCA. For a usage example in denoising images using KPCA, see\\nImage denoising using kernel PCA. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Training vector, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}},\n",
       "        'required': ['n_components=None']}}},\n",
       "     {'func_name': 'MiniBatchDictionaryLearning',\n",
       "      'func_desc': 'Mini-batch dictionary learning.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning',\n",
       "      'function_definitions': {'function_name': 'MiniBatchDictionaryLearning',\n",
       "       'full_function': \"class sklearn.decomposition.MiniBatchDictionaryLearning(n_components=None, *, alpha=1, max_iter=1000, fit_algorithm='lars', n_jobs=None, batch_size=256, shuffle=True, dict_init=None, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, verbose=False, split_sign=False, random_state=None, positive_code=False, positive_dict=False, transform_max_iter=1000, callback=None, tol=0.001, max_no_improvement=10)\",\n",
       "       'function_text': 'Mini-batch dictionary learning. Finds a dictionary (a set of atoms) that performs well at sparsely\\nencoding the fitted data. Solves the optimization problem: ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\\nthe entry-wise matrix norm which is the sum of the absolute values\\nof all the entries in the matrix. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/decomposition.html#dictionarylearning',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'ndarray of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Test data to be transformed, must have the same number of\\nfeatures as the data used to train the model.\\n'}]},\n",
       "      'function_calling': {'name': 'MiniBatchDictionaryLearning',\n",
       "       'descriptions': 'Mini-batch dictionary learning. Finds a dictionary (a set of atoms) that performs well at sparsely\\nencoding the fitted data. Solves the optimization problem: ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\\nthe entry-wise matrix norm which is the sum of the absolute values\\nof all the entries in the matrix. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples, n_features). Test data to be transformed, must have the same number of\\nfeatures as the data used to train the model.\\n'}},\n",
       "        'required': ['n_components=None']}}},\n",
       "     {'func_name': 'MiniBatchSparsePCA',\n",
       "      'func_desc': 'Mini-batch Sparse Principal Components Analysis.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchSparsePCA.html#sklearn.decomposition.MiniBatchSparsePCA',\n",
       "      'function_definitions': {'function_name': 'MiniBatchSparsePCA',\n",
       "       'full_function': \"class sklearn.decomposition.MiniBatchSparsePCA(n_components=None, *, alpha=1, ridge_alpha=0.01, max_iter=1000, callback=None, batch_size=3, verbose=False, shuffle=True, n_jobs=None, method='lars', random_state=None, tol=0.001, max_no_improvement=10)\",\n",
       "       'function_text': 'Mini-batch Sparse Principal Components Analysis. Finds the set of sparse components that can optimally reconstruct\\nthe data.  The amount of sparseness is controllable by the coefficient\\nof the L1 penalty, given by the parameter alpha. For an example comparing sparse PCA to PCA, see\\nFaces dataset decompositions Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'ndarray of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Test data to be transformed, must have the same number of\\nfeatures as the data used to train the model.\\n'}]},\n",
       "      'function_calling': {'name': 'MiniBatchSparsePCA',\n",
       "       'descriptions': 'Mini-batch Sparse Principal Components Analysis. Finds the set of sparse components that can optimally reconstruct\\nthe data.  The amount of sparseness is controllable by the coefficient\\nof the L1 penalty, given by the parameter alpha. For an example comparing sparse PCA to PCA, see\\nFaces dataset decompositions Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples, n_features). Test data to be transformed, must have the same number of\\nfeatures as the data used to train the model.\\n'}},\n",
       "        'required': ['n_components=None']}}},\n",
       "     {'func_name': 'PCA',\n",
       "      'func_desc': 'Principal component analysis (PCA).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA',\n",
       "      'function_definitions': {'function_name': 'PCA',\n",
       "       'full_function': \"class sklearn.decomposition.PCA(n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', n_oversamples=10, power_iteration_normalizer='auto', random_state=None)\",\n",
       "       'function_text': 'Principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the\\ndata to project it to a lower dimensional space. The input data is centered\\nbut not scaled for each feature before applying the SVD. It uses the LAPACK implementation of the full SVD or a randomized truncated\\nSVD by the method of Halko et al. 2009, depending on the shape of the input\\ndata and the number of components to extract. With sparse inputs, the ARPACK implementation of the truncated SVD can be\\nused (i.e. through scipy.sparse.linalg.svds). Alternatively, one\\nmay consider TruncatedSVD where the data are not centered. Notice that this class only supports sparse inputs for some solvers such as\\n“arpack” and “covariance_eigh”. See TruncatedSVD for an\\nalternative with sparse data. For a usage example, see\\nPCA example with Iris Data-set Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'New data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}]},\n",
       "      'function_calling': {'name': 'PCA',\n",
       "       'descriptions': 'Principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the\\ndata to project it to a lower dimensional space. The input data is centered\\nbut not scaled for each feature before applying the SVD. It uses the LAPACK implementation of the full SVD or a randomized truncated\\nSVD by the method of Halko et al. 2009, depending on the shape of the input\\ndata and the number of components to extract. With sparse inputs, the ARPACK implementation of the truncated SVD can be\\nused (i.e. through scipy.sparse.linalg.svds). Alternatively, one\\nmay consider TruncatedSVD where the data are not centered. Notice that this class only supports sparse inputs for some solvers such as\\n“arpack” and “covariance_eigh”. See TruncatedSVD for an\\nalternative with sparse data. For a usage example, see\\nPCA example with Iris Data-set Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). New data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}},\n",
       "        'required': ['n_components=None']}}},\n",
       "     {'func_name': 'SparsePCA',\n",
       "      'func_desc': 'Sparse Principal Components Analysis (SparsePCA).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA',\n",
       "      'function_definitions': {'function_name': 'SparsePCA',\n",
       "       'full_function': \"class sklearn.decomposition.SparsePCA(n_components=None, *, alpha=1, ridge_alpha=0.01, max_iter=1000, tol=1e-08, method='lars', n_jobs=None, U_init=None, V_init=None, verbose=False, random_state=None)\",\n",
       "       'function_text': 'Sparse Principal Components Analysis (SparsePCA). Finds the set of sparse components that can optimally reconstruct\\nthe data.  The amount of sparseness is controllable by the coefficient\\nof the L1 penalty, given by the parameter alpha. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/decomposition.html#sparsepca',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'ndarray of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Test data to be transformed, must have the same number of\\nfeatures as the data used to train the model.\\n'}]},\n",
       "      'function_calling': {'name': 'SparsePCA',\n",
       "       'descriptions': 'Sparse Principal Components Analysis (SparsePCA). Finds the set of sparse components that can optimally reconstruct\\nthe data.  The amount of sparseness is controllable by the coefficient\\nof the L1 penalty, given by the parameter alpha. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples, n_features). Test data to be transformed, must have the same number of\\nfeatures as the data used to train the model.\\n'}},\n",
       "        'required': ['n_components=None']}}},\n",
       "     {'func_name': 'dict_learning',\n",
       "      'func_desc': 'Solve a dictionary learning matrix factorization problem.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.dict_learning.html#sklearn.decomposition.dict_learning',\n",
       "      'function_definitions': {'function_name': 'dict_learning',\n",
       "       'full_function': \"sklearn.decomposition.dict_learning(X, n_components, *, alpha, max_iter=100, tol=1e-08, method='lars', n_jobs=None, dict_init=None, code_init=None, callback=None, verbose=False, random_state=None, return_n_iter=False, positive_dict=False, positive_code=False, method_max_iter=1000)\",\n",
       "       'function_text': 'Solve a dictionary learning matrix factorization problem. Finds the best dictionary and the corresponding sparse code for\\napproximating the data matrix X by solving: where V is the dictionary and U is the sparse code. ||.||_Fro stands for\\nthe Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\\nwhich is the sum of the absolute values of all the entries in the matrix. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/decomposition.html#dictionarylearning',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Data matrix.\\n'},\n",
       "        {'param_name': 'n_components',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Number of dictionary atoms to extract.\\n'},\n",
       "        {'param_name': 'alpha',\n",
       "         'param_type': 'int or float',\n",
       "         'param_desc': 'Sparsity controlling parameter.\\n'},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'Maximum number of iterations to perform.\\n'},\n",
       "        {'param_name': 'tol',\n",
       "         'param_type': 'float, default=1e-8',\n",
       "         'param_desc': 'Tolerance for the stopping condition.\\n'},\n",
       "        {'param_name': 'method',\n",
       "         'param_type': '{‘lars’, ‘cd’}, default=’lars’',\n",
       "         'param_desc': \"The method used:\\n\\n\\n'lars': uses the least angle regression method to solve the lassoproblem (linear_model.lars_path);\\n\\n\\n\\n'cd': uses the coordinate descent method to compute the\\nLasso solution (linear_model.Lasso). Lars will be faster if\\nthe estimated components are sparse.\\n\\n\"},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of parallel jobs to run.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': 'dict_init',\n",
       "         'param_type': 'ndarray of shape (n_components, n_features), default=None',\n",
       "         'param_desc': 'Initial value for the dictionary for warm restart scenarios. Only used\\nif code_init and dict_init are not None.\\n'},\n",
       "        {'param_name': 'code_init',\n",
       "         'param_type': 'ndarray of shape (n_samples, n_components), default=None',\n",
       "         'param_desc': 'Initial value for the sparse code for warm restart scenarios. Only used\\nif code_init and dict_init are not None.\\n'},\n",
       "        {'param_name': 'callback',\n",
       "         'param_type': 'callable, default=None',\n",
       "         'param_desc': 'Callable that gets invoked every five iterations.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'To control the verbosity of the procedure.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Used for randomly initializing the dictionary. Pass an int for\\nreproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'return_n_iter',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether or not to return the number of iterations.\\n'},\n",
       "        {'param_name': 'positive_dict',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to enforce positivity when finding the dictionary.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'positive_code',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to enforce positivity when finding the code.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'method_max_iter',\n",
       "         'param_type': 'int, default=1000',\n",
       "         'param_desc': 'Maximum number of iterations to perform.\\n\\nAdded in version 0.22.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'dict_learning',\n",
       "       'descriptions': 'Solve a dictionary learning matrix factorization problem. Finds the best dictionary and the corresponding sparse code for\\napproximating the data matrix X by solving: where V is the dictionary and U is the sparse code. ||.||_Fro stands for\\nthe Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\\nwhich is the sum of the absolute values of all the entries in the matrix. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Data matrix.\\n'},\n",
       "         'n_components': {'type': 'integer',\n",
       "          'description': 'int. Number of dictionary atoms to extract.\\n'},\n",
       "         'alpha': {'type': 'integer',\n",
       "          'description': 'int or float. Sparsity controlling parameter.\\n'},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=100. Maximum number of iterations to perform.\\n'},\n",
       "         'tol': {'type': 'number',\n",
       "          'description': 'float, default=1e-8. Tolerance for the stopping condition.\\n'},\n",
       "         'method': {'type': 'string',\n",
       "          'enum': ['lars', 'cd'],\n",
       "          'description': \"{‘lars’, ‘cd’}, default=’lars’. The method used:\\n\\n\\n'lars': uses the least angle regression method to solve the lassoproblem (linear_model.lars_path);\\n\\n\\n\\n'cd': uses the coordinate descent method to compute the\\nLasso solution (linear_model.Lasso). Lars will be faster if\\nthe estimated components are sparse.\\n\\n\"},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of parallel jobs to run.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "         'dict_init': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_components, n_features), default=None. Initial value for the dictionary for warm restart scenarios. Only used\\nif code_init and dict_init are not None.\\n'},\n",
       "         'code_init': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples, n_components), default=None. Initial value for the sparse code for warm restart scenarios. Only used\\nif code_init and dict_init are not None.\\n'},\n",
       "         'callback': {'type': 'object',\n",
       "          'description': 'callable, default=None. Callable that gets invoked every five iterations.\\n'},\n",
       "         'verbose': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. To control the verbosity of the procedure.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Used for randomly initializing the dictionary. Pass an int for\\nreproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'return_n_iter': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether or not to return the number of iterations.\\n'},\n",
       "         'positive_dict': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to enforce positivity when finding the dictionary.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'positive_code': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to enforce positivity when finding the code.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'method_max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=1000. Maximum number of iterations to perform.\\n\\nAdded in version 0.22.\\n\\n'}},\n",
       "        'required': ['X', 'n_components']}}},\n",
       "     {'func_name': 'fastica',\n",
       "      'func_desc': 'Perform Fast Independent Component Analysis.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/fastica-function.html#sklearn.decomposition.fastica',\n",
       "      'function_definitions': {'function_name': 'fastica',\n",
       "       'full_function': \"sklearn.decomposition.fastica(X, n_components=None, *, algorithm='parallel', whiten='unit-variance', fun='logcosh', fun_args=None, max_iter=200, tol=0.0001, w_init=None, whiten_solver='svd', random_state=None, return_X_mean=False, compute_sources=True, return_n_iter=False)\",\n",
       "       'function_text': 'Perform Fast Independent Component Analysis. The implementation is based on [1]. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#r4ef46ec4ecf2-1',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training vector, where n_samples is the number of samples and\\nn_features is the number of features.\\n'},\n",
       "        {'param_name': 'n_components',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of components to use. If None is passed, all are used.\\n'},\n",
       "        {'param_name': 'algorithm',\n",
       "         'param_type': '{‘parallel’, ‘deflation’}, default=’parallel’',\n",
       "         'param_desc': 'Specify which algorithm to use for FastICA.\\n'},\n",
       "        {'param_name': 'whiten',\n",
       "         'param_type': 'str or bool, default=’unit-variance’',\n",
       "         'param_desc': 'Specify the whitening strategy to use.\\n\\nIf ‘arbitrary-variance’, a whitening with variance\\narbitrary is used.\\nIf ‘unit-variance’, the whitening matrix is rescaled to ensure that\\neach recovered source has unit variance.\\nIf False, the data is already considered to be whitened, and no\\nwhitening is performed.\\n\\n\\nChanged in version 1.3: The default value of whiten changed to ‘unit-variance’ in 1.3.\\n\\n'},\n",
       "        {'param_name': 'fun',\n",
       "         'param_type': '{‘logcosh’, ‘exp’, ‘cube’} or callable, default=’logcosh’',\n",
       "         'param_desc': 'The functional form of the G function used in the\\napproximation to neg-entropy. Could be either ‘logcosh’, ‘exp’,\\nor ‘cube’.\\nYou can also provide your own function. It should return a tuple\\ncontaining the value of the function, and of its derivative, in the\\npoint. The derivative should be averaged along its last dimension.\\nExample:\\ndef my_g(x):\\n    return x ** 3, (3 * x ** 2).mean(axis=-1)\\n\\n\\n'},\n",
       "        {'param_name': 'fun_args',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Arguments to send to the functional form.\\nIf empty or None and if fun=’logcosh’, fun_args will take value\\n{‘alpha’ : 1.0}.\\n'},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=200',\n",
       "         'param_desc': 'Maximum number of iterations to perform.\\n'},\n",
       "        {'param_name': 'tol',\n",
       "         'param_type': 'float, default=1e-4',\n",
       "         'param_desc': 'A positive scalar giving the tolerance at which the\\nun-mixing matrix is considered to have converged.\\n'},\n",
       "        {'param_name': 'w_init',\n",
       "         'param_type': 'ndarray of shape (n_components, n_components), default=None',\n",
       "         'param_desc': 'Initial un-mixing array. If w_init=None, then an array of values\\ndrawn from a normal distribution is used.\\n'},\n",
       "        {'param_name': 'whiten_solver',\n",
       "         'param_type': '{“eigh”, “svd”}, default=”svd”',\n",
       "         'param_desc': 'The solver to use for whitening.\\n\\n“svd” is more stable numerically if the problem is degenerate, and\\noften faster when n_samples <= n_features.\\n“eigh” is generally more memory efficient when\\nn_samples >= n_features, and can be faster when\\nn_samples >= 50 * n_features.\\n\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Used to initialize w_init when not specified, with a\\nnormal distribution. Pass an int, for reproducible results\\nacross multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'return_X_mean',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, X_mean is returned too.\\n'},\n",
       "        {'param_name': 'compute_sources',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, sources are not computed, but only the rotation matrix.\\nThis can save memory when working with big data. Defaults to True.\\n'},\n",
       "        {'param_name': 'return_n_iter',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether or not to return the number of iterations.\\n'}]},\n",
       "      'function_calling': {'name': 'fastica',\n",
       "       'descriptions': 'Perform Fast Independent Component Analysis. The implementation is based on [1]. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training vector, where n_samples is the number of samples and\\nn_features is the number of features.\\n'},\n",
       "         'n_components': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of components to use. If None is passed, all are used.\\n'},\n",
       "         'algorithm': {'type': 'string',\n",
       "          'enum': ['parallel', 'deflation'],\n",
       "          'description': '{‘parallel’, ‘deflation’}, default=’parallel’. Specify which algorithm to use for FastICA.\\n'},\n",
       "         'whiten': {'type': 'string',\n",
       "          'description': 'str or bool, default=’unit-variance’. Specify the whitening strategy to use.\\n\\nIf ‘arbitrary-variance’, a whitening with variance\\narbitrary is used.\\nIf ‘unit-variance’, the whitening matrix is rescaled to ensure that\\neach recovered source has unit variance.\\nIf False, the data is already considered to be whitened, and no\\nwhitening is performed.\\n\\n\\nChanged in version 1.3: The default value of whiten changed to ‘unit-variance’ in 1.3.\\n\\n'},\n",
       "         'fun': {'type': 'object',\n",
       "          'description': '{‘logcosh’, ‘exp’, ‘cube’} or callable, default=’logcosh’. The functional form of the G function used in the\\napproximation to neg-entropy. Could be either ‘logcosh’, ‘exp’,\\nor ‘cube’.\\nYou can also provide your own function. It should return a tuple\\ncontaining the value of the function, and of its derivative, in the\\npoint. The derivative should be averaged along its last dimension.\\nExample:\\ndef my_g(x):\\n    return x ** 3, (3 * x ** 2).mean(axis=-1)\\n\\n\\n'},\n",
       "         'fun_args': {'type': 'array',\n",
       "          'description': 'dict, default=None. Arguments to send to the functional form.\\nIf empty or None and if fun=’logcosh’, fun_args will take value\\n{‘alpha’ : 1.0}.\\n'},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=200. Maximum number of iterations to perform.\\n'},\n",
       "         'tol': {'type': 'number',\n",
       "          'description': 'float, default=1e-4. A positive scalar giving the tolerance at which the\\nun-mixing matrix is considered to have converged.\\n'},\n",
       "         'w_init': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_components, n_components), default=None. Initial un-mixing array. If w_init=None, then an array of values\\ndrawn from a normal distribution is used.\\n'},\n",
       "         'whiten_solver': {'type': 'string',\n",
       "          'enum': ['eigh', 'svd'],\n",
       "          'description': '{“eigh”, “svd”}, default=”svd”. The solver to use for whitening.\\n\\n“svd” is more stable numerically if the problem is degenerate, and\\noften faster when n_samples <= n_features.\\n“eigh” is generally more memory efficient when\\nn_samples >= n_features, and can be faster when\\nn_samples >= 50 * n_features.\\n\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Used to initialize w_init when not specified, with a\\nnormal distribution. Pass an int, for reproducible results\\nacross multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'return_X_mean': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, X_mean is returned too.\\n'},\n",
       "         'compute_sources': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, sources are not computed, but only the rotation matrix.\\nThis can save memory when working with big data. Defaults to True.\\n'},\n",
       "         'return_n_iter': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether or not to return the number of iterations.\\n'}},\n",
       "        'required': ['X', 'n_components=None']}}},\n",
       "     {'func_name': 'sparse_encode',\n",
       "      'func_desc': 'Sparse coding.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.sparse_encode.html#sklearn.decomposition.sparse_encode',\n",
       "      'function_definitions': {'function_name': 'sparse_encode',\n",
       "       'full_function': \"sklearn.decomposition.sparse_encode(X, dictionary, *, gram=None, cov=None, algorithm='lasso_lars', n_nonzero_coefs=None, alpha=None, copy_cov=True, init=None, max_iter=1000, n_jobs=None, check_input=True, verbose=0, positive=False)\",\n",
       "       'function_text': 'Sparse coding. Each row of the result is the solution to a sparse coding problem.\\nThe goal is to find a sparse array code such that: Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/decomposition.html#sparsecoder',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Data matrix.\\n'},\n",
       "        {'param_name': 'dictionary',\n",
       "         'param_type': 'array-like of shape (n_components, n_features)',\n",
       "         'param_desc': 'The dictionary matrix against which to solve the sparse coding of\\nthe data. Some of the algorithms assume normalized rows for meaningful\\noutput.\\n'},\n",
       "        {'param_name': 'gram',\n",
       "         'param_type': 'array-like of shape (n_components, n_components), default=None',\n",
       "         'param_desc': \"Precomputed Gram matrix, dictionary * dictionary'.\\n\"},\n",
       "        {'param_name': 'cov',\n",
       "         'param_type': 'array-like of shape (n_components, n_samples), default=None',\n",
       "         'param_desc': \"Precomputed covariance, dictionary' * X.\\n\"},\n",
       "        {'param_name': 'algorithm',\n",
       "         'param_type': '{‘lasso_lars’, ‘lasso_cd’, ‘lars’, ‘omp’, ‘threshold’},             default=’lasso_lars’',\n",
       "         'param_desc': \"The algorithm used:\\n\\n'lars': uses the least angle regression method\\n(linear_model.lars_path);\\n'lasso_lars': uses Lars to compute the Lasso solution;\\n'lasso_cd': uses the coordinate descent method to compute the\\nLasso solution (linear_model.Lasso). lasso_lars will be faster if\\nthe estimated components are sparse;\\n'omp': uses orthogonal matching pursuit to estimate the sparse\\nsolution;\\n'threshold': squashes to zero all coefficients less than\\nregularization from the projection dictionary * data'.\\n\\n\"},\n",
       "        {'param_name': 'n_nonzero_coefs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': \"Number of nonzero coefficients to target in each column of the\\nsolution. This is only used by algorithm='lars' and algorithm='omp'\\nand is overridden by alpha in the omp case. If None, then\\nn_nonzero_coefs=int(n_features / 10).\\n\"},\n",
       "        {'param_name': 'alpha',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': \"If algorithm='lasso_lars' or algorithm='lasso_cd', alpha is the\\npenalty applied to the L1 norm.\\nIf algorithm='threshold', alpha is the absolute value of the\\nthreshold below which coefficients will be squashed to zero.\\nIf algorithm='omp', alpha is the tolerance parameter: the value of\\nthe reconstruction error targeted. In this case, it overrides\\nn_nonzero_coefs.\\nIf None, default to 1.\\n\"},\n",
       "        {'param_name': 'copy_cov',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to copy the precomputed covariance matrix; if False, it may\\nbe overwritten.\\n'},\n",
       "        {'param_name': 'init',\n",
       "         'param_type': 'ndarray of shape (n_samples, n_components), default=None',\n",
       "         'param_desc': \"Initialization value of the sparse codes. Only used if\\nalgorithm='lasso_cd'.\\n\"},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=1000',\n",
       "         'param_desc': \"Maximum number of iterations to perform if algorithm='lasso_cd' or\\n'lasso_lars'.\\n\"},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of parallel jobs to run.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': 'check_input',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, the input arrays X and dictionary will not be checked.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Controls the verbosity; the higher, the more messages.\\n'},\n",
       "        {'param_name': 'positive',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to enforce positivity when finding the encoding.\\n\\nAdded in version 0.20.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'sparse_encode',\n",
       "       'descriptions': 'Sparse coding. Each row of the result is the solution to a sparse coding problem.\\nThe goal is to find a sparse array code such that: Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Data matrix.\\n'},\n",
       "         'dictionary': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_components, n_features). The dictionary matrix against which to solve the sparse coding of\\nthe data. Some of the algorithms assume normalized rows for meaningful\\noutput.\\n'},\n",
       "         'gram': {'type': 'array',\n",
       "          'description': \"array-like of shape (n_components, n_components), default=None. Precomputed Gram matrix, dictionary * dictionary'.\\n\"},\n",
       "         'cov': {'type': 'array',\n",
       "          'description': \"array-like of shape (n_components, n_samples), default=None. Precomputed covariance, dictionary' * X.\\n\"},\n",
       "         'algorithm': {'type': 'string',\n",
       "          'enum': ['lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'],\n",
       "          'description': \"{‘lasso_lars’, ‘lasso_cd’, ‘lars’, ‘omp’, ‘threshold’},             default=’lasso_lars’. The algorithm used:\\n\\n'lars': uses the least angle regression method\\n(linear_model.lars_path);\\n'lasso_lars': uses Lars to compute the Lasso solution;\\n'lasso_cd': uses the coordinate descent method to compute the\\nLasso solution (linear_model.Lasso). lasso_lars will be faster if\\nthe estimated components are sparse;\\n'omp': uses orthogonal matching pursuit to estimate the sparse\\nsolution;\\n'threshold': squashes to zero all coefficients less than\\nregularization from the projection dictionary * data'.\\n\\n\"},\n",
       "         'n_nonzero_coefs': {'type': 'integer',\n",
       "          'description': \"int, default=None. Number of nonzero coefficients to target in each column of the\\nsolution. This is only used by algorithm='lars' and algorithm='omp'\\nand is overridden by alpha in the omp case. If None, then\\nn_nonzero_coefs=int(n_features / 10).\\n\"},\n",
       "         'alpha': {'type': 'number',\n",
       "          'description': \"float, default=None. If algorithm='lasso_lars' or algorithm='lasso_cd', alpha is the\\npenalty applied to the L1 norm.\\nIf algorithm='threshold', alpha is the absolute value of the\\nthreshold below which coefficients will be squashed to zero.\\nIf algorithm='omp', alpha is the tolerance parameter: the value of\\nthe reconstruction error targeted. In this case, it overrides\\nn_nonzero_coefs.\\nIf None, default to 1.\\n\"},\n",
       "         'copy_cov': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to copy the precomputed covariance matrix; if False, it may\\nbe overwritten.\\n'},\n",
       "         'init': {'type': 'array',\n",
       "          'description': \"ndarray of shape (n_samples, n_components), default=None. Initialization value of the sparse codes. Only used if\\nalgorithm='lasso_cd'.\\n\"},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': \"int, default=1000. Maximum number of iterations to perform if algorithm='lasso_cd' or\\n'lasso_lars'.\\n\"},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of parallel jobs to run.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "         'check_input': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, the input arrays X and dictionary will not be checked.\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'int, default=0. Controls the verbosity; the higher, the more messages.\\n'},\n",
       "         'positive': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to enforce positivity when finding the encoding.\\n\\nAdded in version 0.20.\\n\\n'}},\n",
       "        'required': ['X', 'dictionary']}}},\n",
       "     {'func_name': 'FactorAnalysis',\n",
       "      'func_desc': 'Factor Analysis (FA).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis',\n",
       "      'function_definitions': {'function_name': 'FactorAnalysis',\n",
       "       'full_function': \"class sklearn.decomposition.FactorAnalysis(n_components=None, *, tol=0.01, copy=True, max_iter=1000, noise_variance_init=None, svd_method='randomized', iterated_power=3, rotation=None, random_state=0)\",\n",
       "       'function_text': 'Factor Analysis (FA). A simple linear generative model with Gaussian latent variables. The observations are assumed to be caused by a linear transformation of\\nlower dimensional latent factors and added Gaussian noise.\\nWithout loss of generality the factors are distributed according to a\\nGaussian with zero mean and unit covariance. The noise is also zero mean\\nand has an arbitrary diagonal covariance matrix. If we would restrict the model further, by assuming that the Gaussian\\nnoise is even isotropic (all diagonal entries are the same) we would obtain\\nPCA. FactorAnalysis performs a maximum likelihood estimate of the so-called\\nloading matrix, the transformation of the latent variables to the\\nobserved ones, using SVD based approach. Read more in the User Guide. Added in version 0.13.',\n",
       "       'func_text_user_guide': 'sklearn.decomposition.PCA.html#sklearn.decomposition.PCA',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data.\\n'}]},\n",
       "      'function_calling': {'name': 'FactorAnalysis',\n",
       "       'descriptions': 'Factor Analysis (FA). A simple linear generative model with Gaussian latent variables. The observations are assumed to be caused by a linear transformation of\\nlower dimensional latent factors and added Gaussian noise.\\nWithout loss of generality the factors are distributed according to a\\nGaussian with zero mean and unit covariance. The noise is also zero mean\\nand has an arbitrary diagonal covariance matrix. If we would restrict the model further, by assuming that the Gaussian\\nnoise is even isotropic (all diagonal entries are the same) we would obtain\\nPCA. FactorAnalysis performs a maximum likelihood estimate of the so-called\\nloading matrix, the transformation of the latent variables to the\\nobserved ones, using SVD based approach. Read more in the User Guide. Added in version 0.13.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training data.\\n'}},\n",
       "        'required': ['n_components=None']}}},\n",
       "     {'func_name': 'IncrementalPCA',\n",
       "      'func_desc': 'Incremental principal components analysis (IPCA).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA',\n",
       "      'function_definitions': {'function_name': 'IncrementalPCA',\n",
       "       'full_function': 'class sklearn.decomposition.IncrementalPCA(n_components=None, *, whiten=False, copy=True, batch_size=None)',\n",
       "       'function_text': 'Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of\\nthe data, keeping only the most significant singular vectors to\\nproject the data to a lower dimensional space. The input data is centered\\nbut not scaled for each feature before applying the SVD. Depending on the size of the input data, this algorithm can be much more\\nmemory efficient than a PCA, and allows sparse input. This algorithm has constant memory complexity, on the order\\nof batch_size * n_features, enabling use of np.memmap files without\\nloading the entire file into memory. For sparse matrices, the input\\nis converted to dense in batches (in order to be able to subtract the\\nmean) which avoids storing the entire dense matrix at any one time. The computational overhead of each SVD is\\nO(batch_size * n_features ** 2), but only 2 * batch_size samples\\nremain in memory at a time. There will be n_samples / batch_size SVD\\ncomputations to get the principal components, versus 1 large SVD of\\ncomplexity O(n_samples * n_features ** 2) for PCA. For a usage example, see\\nIncremental PCA. Read more in the User Guide. Added in version 0.16.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/decomposition/plot_incremental_pca.html#sphx-glr-auto-examples-decomposition-plot-incremental-pca-py',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'New data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}]},\n",
       "      'function_calling': {'name': 'IncrementalPCA',\n",
       "       'descriptions': 'Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of\\nthe data, keeping only the most significant singular vectors to\\nproject the data to a lower dimensional space. The input data is centered\\nbut not scaled for each feature before applying the SVD. Depending on the size of the input data, this algorithm can be much more\\nmemory efficient than a PCA, and allows sparse input. This algorithm has constant memory complexity, on the order\\nof batch_size * n_features, enabling use of np.memmap files without\\nloading the entire file into memory. For sparse matrices, the input\\nis converted to dense in batches (in order to be able to subtract the\\nmean) which avoids storing the entire dense matrix at any one time. The computational overhead of each SVD is\\nO(batch_size * n_features ** 2), but only 2 * batch_size samples\\nremain in memory at a time. There will be n_samples / batch_size SVD\\ncomputations to get the principal components, versus 1 large SVD of\\ncomplexity O(n_samples * n_features ** 2) for PCA. For a usage example, see\\nIncremental PCA. Read more in the User Guide. Added in version 0.16.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). New data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}},\n",
       "        'required': ['n_components=None']}}},\n",
       "     {'func_name': 'LatentDirichletAllocation',\n",
       "      'func_desc': 'Latent Dirichlet Allocation with online variational Bayes algorithm.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation',\n",
       "      'function_definitions': {'function_name': 'LatentDirichletAllocation',\n",
       "       'full_function': \"class sklearn.decomposition.LatentDirichletAllocation(n_components=10, *, doc_topic_prior=None, topic_word_prior=None, learning_method='batch', learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=None, verbose=0, random_state=None)\",\n",
       "       'function_text': 'Latent Dirichlet Allocation with online variational Bayes algorithm. The implementation is based on [1] and [2]. Added in version 0.17. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#re25e5648fc37-1',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Document word matrix.\\n'}]},\n",
       "      'function_calling': {'name': 'LatentDirichletAllocation',\n",
       "       'descriptions': 'Latent Dirichlet Allocation with online variational Bayes algorithm. The implementation is based on [1] and [2]. Added in version 0.17. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Document word matrix.\\n'}},\n",
       "        'required': ['n_components=10']}}},\n",
       "     {'func_name': 'MiniBatchNMF',\n",
       "      'func_desc': 'Mini-Batch Non-Negative Matrix Factorization (NMF).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchNMF.html#sklearn.decomposition.MiniBatchNMF',\n",
       "      'function_definitions': {'function_name': 'MiniBatchNMF',\n",
       "       'full_function': \"class sklearn.decomposition.MiniBatchNMF(n_components='warn', *, init=None, batch_size=1024, beta_loss='frobenius', tol=0.0001, max_no_improvement=10, max_iter=200, alpha_W=0.0, alpha_H='same', l1_ratio=0.0, forget_factor=0.7, fresh_restarts=False, fresh_restarts_max_iter=30, transform_max_iter=None, random_state=None, verbose=0)\",\n",
       "       'function_text': 'Mini-Batch Non-Negative Matrix Factorization (NMF). Added in version 1.1. Find two non-negative matrices, i.e. matrices with all non-negative elements,\\n(W, H) whose product approximates the non-negative matrix X. This\\nfactorization can be used for example for dimensionality reduction, source\\nseparation or topic extraction. The objective function is: Where: \\\\(||A||_{Fro}^2 = \\\\sum_{i,j} A_{ij}^2\\\\) (Frobenius norm) \\\\(||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij})\\\\) (Elementwise L1 norm) The generic norm \\\\(||X - WH||_{loss}^2\\\\) may represent\\nthe Frobenius norm or another supported beta-divergence loss.\\nThe choice between options is controlled by the beta_loss parameter. The objective function is minimized with an alternating minimization of W\\nand H. Note that the transformed data is named W and the components matrix is\\nnamed H. In the NMF literature, the naming convention is usually the opposite\\nsince the data matrix X is transposed. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/decomposition.html#minibatchnmf',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Data matrix to be transformed by the model.\\n'}]},\n",
       "      'function_calling': {'name': 'MiniBatchNMF',\n",
       "       'descriptions': 'Mini-Batch Non-Negative Matrix Factorization (NMF). Added in version 1.1. Find two non-negative matrices, i.e. matrices with all non-negative elements,\\n(W, H) whose product approximates the non-negative matrix X. This\\nfactorization can be used for example for dimensionality reduction, source\\nseparation or topic extraction. The objective function is: Where: \\\\(||A||_{Fro}^2 = \\\\sum_{i,j} A_{ij}^2\\\\) (Frobenius norm) \\\\(||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij})\\\\) (Elementwise L1 norm) The generic norm \\\\(||X - WH||_{loss}^2\\\\) may represent\\nthe Frobenius norm or another supported beta-divergence loss.\\nThe choice between options is controlled by the beta_loss parameter. The objective function is minimized with an alternating minimization of W\\nand H. Note that the transformed data is named W and the components matrix is\\nnamed H. In the NMF literature, the naming convention is usually the opposite\\nsince the data matrix X is transposed. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Data matrix to be transformed by the model.\\n'}},\n",
       "        'required': [\"n_components='warn'\"]}}},\n",
       "     {'func_name': 'NMF',\n",
       "      'func_desc': 'Non-Negative Matrix Factorization (NMF).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF',\n",
       "      'function_definitions': {'function_name': 'NMF',\n",
       "       'full_function': \"class sklearn.decomposition.NMF(n_components='warn', *, init=None, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=200, random_state=None, alpha_W=0.0, alpha_H='same', l1_ratio=0.0, verbose=0, shuffle=False)\",\n",
       "       'function_text': 'Non-Negative Matrix Factorization (NMF). Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H)\\nwhose product approximates the non-negative matrix X. This factorization can be used\\nfor example for dimensionality reduction, source separation or topic extraction. The objective function is: Where: \\\\(||A||_{Fro}^2 = \\\\sum_{i,j} A_{ij}^2\\\\) (Frobenius norm) \\\\(||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij})\\\\) (Elementwise L1 norm) The generic norm \\\\(||X - WH||_{loss}\\\\) may represent\\nthe Frobenius norm or another supported beta-divergence loss.\\nThe choice between options is controlled by the beta_loss parameter. The regularization terms are scaled by n_features for W and by n_samples for\\nH to keep their impact balanced with respect to one another and to the data fit\\nterm as independent as possible of the size n_samples of the training set. The objective function is minimized with an alternating minimization of W\\nand H. Note that the transformed data is named W and the components matrix is named H. In\\nthe NMF literature, the naming convention is usually the opposite since the data\\nmatrix X is transposed. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/decomposition.html#nmf',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training vector, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}]},\n",
       "      'function_calling': {'name': 'NMF',\n",
       "       'descriptions': 'Non-Negative Matrix Factorization (NMF). Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H)\\nwhose product approximates the non-negative matrix X. This factorization can be used\\nfor example for dimensionality reduction, source separation or topic extraction. The objective function is: Where: \\\\(||A||_{Fro}^2 = \\\\sum_{i,j} A_{ij}^2\\\\) (Frobenius norm) \\\\(||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij})\\\\) (Elementwise L1 norm) The generic norm \\\\(||X - WH||_{loss}\\\\) may represent\\nthe Frobenius norm or another supported beta-divergence loss.\\nThe choice between options is controlled by the beta_loss parameter. The regularization terms are scaled by n_features for W and by n_samples for\\nH to keep their impact balanced with respect to one another and to the data fit\\nterm as independent as possible of the size n_samples of the training set. The objective function is minimized with an alternating minimization of W\\nand H. Note that the transformed data is named W and the components matrix is named H. In\\nthe NMF literature, the naming convention is usually the opposite since the data\\nmatrix X is transposed. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Training vector, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}},\n",
       "        'required': [\"n_components='warn'\"]}}},\n",
       "     {'func_name': 'SparseCoder',\n",
       "      'func_desc': 'Sparse coding.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparseCoder.html#sklearn.decomposition.SparseCoder',\n",
       "      'function_definitions': {'function_name': 'SparseCoder',\n",
       "       'full_function': \"class sklearn.decomposition.SparseCoder(dictionary, *, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, split_sign=False, n_jobs=None, positive_code=False, transform_max_iter=1000)\",\n",
       "       'function_text': 'Sparse coding. Finds a sparse representation of data against a fixed, precomputed\\ndictionary. Each row of the result is the solution to a sparse coding problem.\\nThe goal is to find a sparse array code such that: Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/decomposition.html#sparsecoder',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'ndarray of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training vector, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'Ignored',\n",
       "         'param_desc': 'Not used, present for API consistency by convention.\\n'}]},\n",
       "      'function_calling': {'name': 'SparseCoder',\n",
       "       'descriptions': 'Sparse coding. Finds a sparse representation of data against a fixed, precomputed\\ndictionary. Each row of the result is the solution to a sparse coding problem.\\nThe goal is to find a sparse array code such that: Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples, n_features). Training vector, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'Ignored. Not used, present for API consistency by convention.\\n'}},\n",
       "        'required': ['dictionary']}}},\n",
       "     {'func_name': 'TruncatedSVD',\n",
       "      'func_desc': 'Dimensionality reduction using truncated SVD (aka LSA).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD',\n",
       "      'function_definitions': {'function_name': 'TruncatedSVD',\n",
       "       'full_function': \"class sklearn.decomposition.TruncatedSVD(n_components=2, *, algorithm='randomized', n_iter=5, n_oversamples=10, power_iteration_normalizer='auto', random_state=None, tol=0.0)\",\n",
       "       'function_text': 'Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of\\ntruncated singular value decomposition (SVD). Contrary to PCA, this\\nestimator does not center the data before computing the singular value\\ndecomposition. This means it can work with sparse matrices\\nefficiently. In particular, truncated SVD works on term count/tf-idf matrices as\\nreturned by the vectorizers in sklearn.feature_extraction.text. In\\nthat context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and\\na “naive” algorithm that uses ARPACK as an eigensolver on X * X.T or\\nX.T * X, whichever is more efficient. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/api/sklearn.feature_extraction.html#module-sklearn.feature_extraction.text',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'New data.\\n'}]},\n",
       "      'function_calling': {'name': 'TruncatedSVD',\n",
       "       'descriptions': 'Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of\\ntruncated singular value decomposition (SVD). Contrary to PCA, this\\nestimator does not center the data before computing the singular value\\ndecomposition. This means it can work with sparse matrices\\nefficiently. In particular, truncated SVD works on term count/tf-idf matrices as\\nreturned by the vectorizers in sklearn.feature_extraction.text. In\\nthat context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and\\na “naive” algorithm that uses ARPACK as an eigensolver on X * X.T or\\nX.T * X, whichever is more efficient. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). New data.\\n'}},\n",
       "        'required': ['n_components=2']}}},\n",
       "     {'func_name': 'dict_learning_online',\n",
       "      'func_desc': 'Solve a dictionary learning matrix factorization problem online.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.dict_learning_online.html#sklearn.decomposition.dict_learning_online',\n",
       "      'function_definitions': {'function_name': 'dict_learning_online',\n",
       "       'full_function': \"sklearn.decomposition.dict_learning_online(X, n_components=2, *, alpha=1, max_iter=100, return_code=True, dict_init=None, callback=None, batch_size=256, verbose=False, shuffle=True, n_jobs=None, method='lars', random_state=None, positive_dict=False, positive_code=False, method_max_iter=1000, tol=0.001, max_no_improvement=10)\",\n",
       "       'function_text': 'Solve a dictionary learning matrix factorization problem online. Finds the best dictionary and the corresponding sparse code for\\napproximating the data matrix X by solving: where V is the dictionary and U is the sparse code. ||.||_Fro stands for\\nthe Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\\nwhich is the sum of the absolute values of all the entries in the matrix.\\nThis is accomplished by repeatedly iterating over mini-batches by slicing\\nthe input data. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/decomposition.html#dictionarylearning',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Data matrix.\\n'},\n",
       "        {'param_name': 'n_components',\n",
       "         'param_type': 'int or None, default=2',\n",
       "         'param_desc': 'Number of dictionary atoms to extract. If None, then n_components\\nis set to n_features.\\n'},\n",
       "        {'param_name': 'alpha',\n",
       "         'param_type': 'float, default=1',\n",
       "         'param_desc': 'Sparsity controlling parameter.\\n'},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'Maximum number of iterations over the complete dataset before\\nstopping independently of any early stopping criterion heuristics.\\n\\nAdded in version 1.1.\\n\\n\\nDeprecated since version 1.4: max_iter=None is deprecated in 1.4 and will be removed in 1.6.\\nUse the default value (i.e. 100) instead.\\n\\n'},\n",
       "        {'param_name': 'return_code',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to also return the code U or just the dictionary V.\\n'},\n",
       "        {'param_name': 'dict_init',\n",
       "         'param_type': 'ndarray of shape (n_components, n_features), default=None',\n",
       "         'param_desc': 'Initial values for the dictionary for warm restart scenarios.\\nIf None, the initial values for the dictionary are created\\nwith an SVD decomposition of the data via\\nrandomized_svd.\\n'},\n",
       "        {'param_name': 'callback',\n",
       "         'param_type': 'callable, default=None',\n",
       "         'param_desc': 'A callable that gets invoked at the end of each iteration.\\n'},\n",
       "        {'param_name': 'batch_size',\n",
       "         'param_type': 'int, default=256',\n",
       "         'param_desc': 'The number of samples to take in each batch.\\n\\nChanged in version 1.3: The default value of batch_size changed from 3 to 256 in version 1.3.\\n\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'To control the verbosity of the procedure.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to shuffle the data before splitting it in batches.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of parallel jobs to run.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': 'method',\n",
       "         'param_type': '{‘lars’, ‘cd’}, default=’lars’',\n",
       "         'param_desc': \"\\n'lars': uses the least angle regression method to solve the lasso\\nproblem (linear_model.lars_path);\\n'cd': uses the coordinate descent method to compute the\\nLasso solution (linear_model.Lasso). Lars will be faster if\\nthe estimated components are sparse.\\n\\n\"},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Used for initializing the dictionary when dict_init is not\\nspecified, randomly shuffling the data when shuffle is set to\\nTrue, and updating the dictionary. Pass an int for reproducible\\nresults across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'positive_dict',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to enforce positivity when finding the dictionary.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'positive_code',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to enforce positivity when finding the code.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'method_max_iter',\n",
       "         'param_type': 'int, default=1000',\n",
       "         'param_desc': 'Maximum number of iterations to perform when solving the lasso problem.\\n\\nAdded in version 0.22.\\n\\n'},\n",
       "        {'param_name': 'tol',\n",
       "         'param_type': 'float, default=1e-3',\n",
       "         'param_desc': 'Control early stopping based on the norm of the differences in the\\ndictionary between 2 steps.\\nTo disable early stopping based on changes in the dictionary, set\\ntol to 0.0.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "        {'param_name': 'max_no_improvement',\n",
       "         'param_type': 'int, default=10',\n",
       "         'param_desc': 'Control early stopping based on the consecutive number of mini batches\\nthat does not yield an improvement on the smoothed cost function.\\nTo disable convergence detection based on cost function, set\\nmax_no_improvement to None.\\n\\nAdded in version 1.1.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'dict_learning_online',\n",
       "       'descriptions': 'Solve a dictionary learning matrix factorization problem online. Finds the best dictionary and the corresponding sparse code for\\napproximating the data matrix X by solving: where V is the dictionary and U is the sparse code. ||.||_Fro stands for\\nthe Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\\nwhich is the sum of the absolute values of all the entries in the matrix.\\nThis is accomplished by repeatedly iterating over mini-batches by slicing\\nthe input data. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Data matrix.\\n'},\n",
       "         'n_components': {'type': 'integer',\n",
       "          'description': 'int or None, default=2. Number of dictionary atoms to extract. If None, then n_components\\nis set to n_features.\\n'},\n",
       "         'alpha': {'type': 'number',\n",
       "          'description': 'float, default=1. Sparsity controlling parameter.\\n'},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=100. Maximum number of iterations over the complete dataset before\\nstopping independently of any early stopping criterion heuristics.\\n\\nAdded in version 1.1.\\n\\n\\nDeprecated since version 1.4: max_iter=None is deprecated in 1.4 and will be removed in 1.6.\\nUse the default value (i.e. 100) instead.\\n\\n'},\n",
       "         'return_code': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to also return the code U or just the dictionary V.\\n'},\n",
       "         'dict_init': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_components, n_features), default=None. Initial values for the dictionary for warm restart scenarios.\\nIf None, the initial values for the dictionary are created\\nwith an SVD decomposition of the data via\\nrandomized_svd.\\n'},\n",
       "         'callback': {'type': 'object',\n",
       "          'description': 'callable, default=None. A callable that gets invoked at the end of each iteration.\\n'},\n",
       "         'batch_size': {'type': 'integer',\n",
       "          'description': 'int, default=256. The number of samples to take in each batch.\\n\\nChanged in version 1.3: The default value of batch_size changed from 3 to 256 in version 1.3.\\n\\n'},\n",
       "         'verbose': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. To control the verbosity of the procedure.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to shuffle the data before splitting it in batches.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of parallel jobs to run.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "         'method': {'type': 'string',\n",
       "          'enum': ['lars', 'cd'],\n",
       "          'description': \"{‘lars’, ‘cd’}, default=’lars’. \\n'lars': uses the least angle regression method to solve the lasso\\nproblem (linear_model.lars_path);\\n'cd': uses the coordinate descent method to compute the\\nLasso solution (linear_model.Lasso). Lars will be faster if\\nthe estimated components are sparse.\\n\\n\"},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Used for initializing the dictionary when dict_init is not\\nspecified, randomly shuffling the data when shuffle is set to\\nTrue, and updating the dictionary. Pass an int for reproducible\\nresults across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'positive_dict': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to enforce positivity when finding the dictionary.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'positive_code': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to enforce positivity when finding the code.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'method_max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=1000. Maximum number of iterations to perform when solving the lasso problem.\\n\\nAdded in version 0.22.\\n\\n'},\n",
       "         'tol': {'type': 'number',\n",
       "          'description': 'float, default=1e-3. Control early stopping based on the norm of the differences in the\\ndictionary between 2 steps.\\nTo disable early stopping based on changes in the dictionary, set\\ntol to 0.0.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "         'max_no_improvement': {'type': 'integer',\n",
       "          'description': 'int, default=10. Control early stopping based on the consecutive number of mini batches\\nthat does not yield an improvement on the smoothed cost function.\\nTo disable convergence detection based on cost function, set\\nmax_no_improvement to None.\\n\\nAdded in version 1.1.\\n\\n'}},\n",
       "        'required': ['X', 'n_components=2']}}},\n",
       "     {'func_name': 'non_negative_factorization',\n",
       "      'func_desc': 'Compute Non-negative Matrix Factorization (NMF).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.non_negative_factorization.html#sklearn.decomposition.non_negative_factorization',\n",
       "      'function_definitions': {'function_name': 'non_negative_factorization',\n",
       "       'full_function': \"sklearn.decomposition.non_negative_factorization(X, W=None, H=None, n_components='warn', *, init=None, update_H=True, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=200, alpha_W=0.0, alpha_H='same', l1_ratio=0.0, random_state=None, verbose=0, shuffle=False)\",\n",
       "       'function_text': 'Compute Non-negative Matrix Factorization (NMF). Find two non-negative matrices (W, H) whose product approximates the non-\\nnegative matrix X. This factorization can be used for example for\\ndimensionality reduction, source separation or topic extraction. The objective function is: Where: \\\\(||A||_{Fro}^2 = \\\\sum_{i,j} A_{ij}^2\\\\) (Frobenius norm) \\\\(||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij})\\\\) (Elementwise L1 norm) The generic norm \\\\(||X - WH||_{loss}^2\\\\) may represent\\nthe Frobenius norm or another supported beta-divergence loss.\\nThe choice between options is controlled by the beta_loss parameter. The regularization terms are scaled by n_features for W and by n_samples for\\nH to keep their impact balanced with respect to one another and to the data fit\\nterm as independent as possible of the size n_samples of the training set. The objective function is minimized with an alternating minimization of W\\nand H. If H is given and update_H=False, it solves for W only. Note that the transformed data is named W and the components matrix is named H. In\\nthe NMF literature, the naming convention is usually the opposite since the data\\nmatrix X is transposed.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-random_state',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Constant matrix.\\n'},\n",
       "        {'param_name': 'W',\n",
       "         'param_type': 'array-like of shape (n_samples, n_components), default=None',\n",
       "         'param_desc': \"If init='custom', it is used as initial guess for the solution.\\nIf update_H=False, it is initialised as an array of zeros, unless\\nsolver='mu', then it is filled with values calculated by\\nnp.sqrt(X.mean() / self._n_components).\\nIf None, uses the initialisation method specified in init.\\n\"},\n",
       "        {'param_name': 'H',\n",
       "         'param_type': 'array-like of shape (n_components, n_features), default=None',\n",
       "         'param_desc': \"If init='custom', it is used as initial guess for the solution.\\nIf update_H=False, it is used as a constant, to solve for W only.\\nIf None, uses the initialisation method specified in init.\\n\"},\n",
       "        {'param_name': 'n_components',\n",
       "         'param_type': 'int or {‘auto’} or None, default=None',\n",
       "         'param_desc': \"Number of components, if n_components is not set all features\\nare kept.\\nIf n_components='auto', the number of components is automatically inferred\\nfrom W or H shapes.\\n\\nChanged in version 1.4: Added 'auto' value.\\n\\n\"},\n",
       "        {'param_name': 'init',\n",
       "         'param_type': '{‘random’, ‘nndsvd’, ‘nndsvda’, ‘nndsvdar’, ‘custom’}, default=None',\n",
       "         'param_desc': 'Method used to initialize the procedure.\\nValid options:\\n\\nNone: ‘nndsvda’ if n_components < n_features, otherwise ‘random’.\\n‘random’: non-negative random matrices, scaled with:\\nsqrt(X.mean() / n_components)\\n‘nndsvd’: Nonnegative Double Singular Value Decomposition (NNDSVD)\\ninitialization (better for sparseness)\\n‘nndsvda’: NNDSVD with zeros filled with the average of X\\n(better when sparsity is not desired)\\n‘nndsvdar’: NNDSVD with zeros filled with small random values\\n(generally faster, less accurate alternative to NNDSVDa\\nfor when sparsity is not desired)\\n‘custom’: If update_H=True, use custom matrices W and H which must both\\nbe provided. If update_H=False, then only custom matrix H is used.\\n\\n\\nChanged in version 0.23: The default value of init changed from ‘random’ to None in 0.23.\\n\\n\\nChanged in version 1.1: When init=None and n_components is less than n_samples and n_features\\ndefaults to nndsvda instead of nndsvd.\\n\\n'},\n",
       "        {'param_name': 'update_H',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Set to True, both W and H will be estimated from initial guesses.\\nSet to False, only W will be estimated.\\n'},\n",
       "        {'param_name': 'solver',\n",
       "         'param_type': '{‘cd’, ‘mu’}, default=’cd’',\n",
       "         'param_desc': 'Numerical solver to use:\\n\\n‘cd’ is a Coordinate Descent solver that uses Fast Hierarchical\\nAlternating Least Squares (Fast HALS).\\n‘mu’ is a Multiplicative Update solver.\\n\\n\\nAdded in version 0.17: Coordinate Descent solver.\\n\\n\\nAdded in version 0.19: Multiplicative Update solver.\\n\\n'},\n",
       "        {'param_name': 'beta_loss',\n",
       "         'param_type': 'float or {‘frobenius’, ‘kullback-leibler’,             ‘itakura-saito’}, default=’frobenius’',\n",
       "         'param_desc': 'Beta divergence to be minimized, measuring the distance between X\\nand the dot product WH. Note that values different from ‘frobenius’\\n(or 2) and ‘kullback-leibler’ (or 1) lead to significantly slower\\nfits. Note that for beta_loss <= 0 (or ‘itakura-saito’), the input\\nmatrix X cannot contain zeros. Used only in ‘mu’ solver.\\n\\nAdded in version 0.19.\\n\\n'},\n",
       "        {'param_name': 'tol',\n",
       "         'param_type': 'float, default=1e-4',\n",
       "         'param_desc': 'Tolerance of the stopping condition.\\n'},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=200',\n",
       "         'param_desc': 'Maximum number of iterations before timing out.\\n'},\n",
       "        {'param_name': 'alpha_W',\n",
       "         'param_type': 'float, default=0.0',\n",
       "         'param_desc': 'Constant that multiplies the regularization terms of W. Set it to zero\\n(default) to have no regularization on W.\\n\\nAdded in version 1.0.\\n\\n'},\n",
       "        {'param_name': 'alpha_H',\n",
       "         'param_type': 'float or “same”, default=”same”',\n",
       "         'param_desc': 'Constant that multiplies the regularization terms of H. Set it to zero to\\nhave no regularization on H. If “same” (default), it takes the same value as\\nalpha_W.\\n\\nAdded in version 1.0.\\n\\n'},\n",
       "        {'param_name': 'l1_ratio',\n",
       "         'param_type': 'float, default=0.0',\n",
       "         'param_desc': 'The regularization mixing parameter, with 0 <= l1_ratio <= 1.\\nFor l1_ratio = 0 the penalty is an elementwise L2 penalty\\n(aka Frobenius Norm).\\nFor l1_ratio = 1 it is an elementwise L1 penalty.\\nFor 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Used for NMF initialisation (when init == ‘nndsvdar’ or\\n‘random’), and in Coordinate Descent. Pass an int for reproducible\\nresults across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'The verbosity level.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If true, randomize the order of coordinates in the CD solver.\\n'}]},\n",
       "      'function_calling': {'name': 'non_negative_factorization',\n",
       "       'descriptions': 'Compute Non-negative Matrix Factorization (NMF). Find two non-negative matrices (W, H) whose product approximates the non-\\nnegative matrix X. This factorization can be used for example for\\ndimensionality reduction, source separation or topic extraction. The objective function is: Where: \\\\(||A||_{Fro}^2 = \\\\sum_{i,j} A_{ij}^2\\\\) (Frobenius norm) \\\\(||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij})\\\\) (Elementwise L1 norm) The generic norm \\\\(||X - WH||_{loss}^2\\\\) may represent\\nthe Frobenius norm or another supported beta-divergence loss.\\nThe choice between options is controlled by the beta_loss parameter. The regularization terms are scaled by n_features for W and by n_samples for\\nH to keep their impact balanced with respect to one another and to the data fit\\nterm as independent as possible of the size n_samples of the training set. The objective function is minimized with an alternating minimization of W\\nand H. If H is given and update_H=False, it solves for W only. Note that the transformed data is named W and the components matrix is named H. In\\nthe NMF literature, the naming convention is usually the opposite since the data\\nmatrix X is transposed.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Constant matrix.\\n'},\n",
       "         'W': {'type': 'array',\n",
       "          'description': \"array-like of shape (n_samples, n_components), default=None. If init='custom', it is used as initial guess for the solution.\\nIf update_H=False, it is initialised as an array of zeros, unless\\nsolver='mu', then it is filled with values calculated by\\nnp.sqrt(X.mean() / self._n_components).\\nIf None, uses the initialisation method specified in init.\\n\"},\n",
       "         'H': {'type': 'array',\n",
       "          'description': \"array-like of shape (n_components, n_features), default=None. If init='custom', it is used as initial guess for the solution.\\nIf update_H=False, it is used as a constant, to solve for W only.\\nIf None, uses the initialisation method specified in init.\\n\"},\n",
       "         'n_components': {'type': 'integer',\n",
       "          'description': \"int or {‘auto’} or None, default=None. Number of components, if n_components is not set all features\\nare kept.\\nIf n_components='auto', the number of components is automatically inferred\\nfrom W or H shapes.\\n\\nChanged in version 1.4: Added 'auto' value.\\n\\n\"},\n",
       "         'init': {'type': 'string',\n",
       "          'enum': ['random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'],\n",
       "          'description': '{‘random’, ‘nndsvd’, ‘nndsvda’, ‘nndsvdar’, ‘custom’}, default=None. Method used to initialize the procedure.\\nValid options:\\n\\nNone: ‘nndsvda’ if n_components < n_features, otherwise ‘random’.\\n‘random’: non-negative random matrices, scaled with:\\nsqrt(X.mean() / n_components)\\n‘nndsvd’: Nonnegative Double Singular Value Decomposition (NNDSVD)\\ninitialization (better for sparseness)\\n‘nndsvda’: NNDSVD with zeros filled with the average of X\\n(better when sparsity is not desired)\\n‘nndsvdar’: NNDSVD with zeros filled with small random values\\n(generally faster, less accurate alternative to NNDSVDa\\nfor when sparsity is not desired)\\n‘custom’: If update_H=True, use custom matrices W and H which must both\\nbe provided. If update_H=False, then only custom matrix H is used.\\n\\n\\nChanged in version 0.23: The default value of init changed from ‘random’ to None in 0.23.\\n\\n\\nChanged in version 1.1: When init=None and n_components is less than n_samples and n_features\\ndefaults to nndsvda instead of nndsvd.\\n\\n'},\n",
       "         'update_H': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Set to True, both W and H will be estimated from initial guesses.\\nSet to False, only W will be estimated.\\n'},\n",
       "         'solver': {'type': 'string',\n",
       "          'enum': ['cd', 'mu'],\n",
       "          'description': '{‘cd’, ‘mu’}, default=’cd’. Numerical solver to use:\\n\\n‘cd’ is a Coordinate Descent solver that uses Fast Hierarchical\\nAlternating Least Squares (Fast HALS).\\n‘mu’ is a Multiplicative Update solver.\\n\\n\\nAdded in version 0.17: Coordinate Descent solver.\\n\\n\\nAdded in version 0.19: Multiplicative Update solver.\\n\\n'},\n",
       "         'beta_loss': {'type': 'number',\n",
       "          'description': 'float or {‘frobenius’, ‘kullback-leibler’,             ‘itakura-saito’}, default=’frobenius’. Beta divergence to be minimized, measuring the distance between X\\nand the dot product WH. Note that values different from ‘frobenius’\\n(or 2) and ‘kullback-leibler’ (or 1) lead to significantly slower\\nfits. Note that for beta_loss <= 0 (or ‘itakura-saito’), the input\\nmatrix X cannot contain zeros. Used only in ‘mu’ solver.\\n\\nAdded in version 0.19.\\n\\n'},\n",
       "         'tol': {'type': 'number',\n",
       "          'description': 'float, default=1e-4. Tolerance of the stopping condition.\\n'},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=200. Maximum number of iterations before timing out.\\n'},\n",
       "         'alpha_W': {'type': 'number',\n",
       "          'description': 'float, default=0.0. Constant that multiplies the regularization terms of W. Set it to zero\\n(default) to have no regularization on W.\\n\\nAdded in version 1.0.\\n\\n'},\n",
       "         'alpha_H': {'type': 'number',\n",
       "          'description': 'float or “same”, default=”same”. Constant that multiplies the regularization terms of H. Set it to zero to\\nhave no regularization on H. If “same” (default), it takes the same value as\\nalpha_W.\\n\\nAdded in version 1.0.\\n\\n'},\n",
       "         'l1_ratio': {'type': 'number',\n",
       "          'description': 'float, default=0.0. The regularization mixing parameter, with 0 <= l1_ratio <= 1.\\nFor l1_ratio = 0 the penalty is an elementwise L2 penalty\\n(aka Frobenius Norm).\\nFor l1_ratio = 1 it is an elementwise L1 penalty.\\nFor 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Used for NMF initialisation (when init == ‘nndsvdar’ or\\n‘random’), and in Coordinate Descent. Pass an int for reproducible\\nresults across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'int, default=0. The verbosity level.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If true, randomize the order of coordinates in the CD solver.\\n'}},\n",
       "        'required': ['X', 'W=None', 'H=None', \"n_components='warn'\"]}}}]}],\n",
       "  'name': 'sklearn.decomposition',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.decomposition.html'},\n",
       " 'sklearn.discriminant_analysis.html': {'functions': [{'defaults': [{'func_name': 'LinearDiscriminantAnalysis',\n",
       "      'func_desc': 'Linear Discriminant Analysis.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis',\n",
       "      'function_definitions': {'function_name': 'LinearDiscriminantAnalysis',\n",
       "       'full_function': \"class sklearn.discriminant_analysis.LinearDiscriminantAnalysis(solver='svd', shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001, covariance_estimator=None)\",\n",
       "       'function_text': 'Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class\\nconditional densities to the data and using Bayes’ rule. The model fits a Gaussian density to each class, assuming that all classes\\nshare the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input\\nby projecting it to the most discriminative directions, using the\\ntransform method. Added in version 0.17. For a comparison between\\nLinearDiscriminantAnalysis\\nand QuadraticDiscriminantAnalysis, see\\nLinear and Quadratic Discriminant Analysis with covariance ellipsoid. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#sklearn.discriminant_analysis.LinearDiscriminantAnalysis',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Input data.\\n'}]},\n",
       "      'function_calling': {'name': 'LinearDiscriminantAnalysis',\n",
       "       'descriptions': 'Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class\\nconditional densities to the data and using Bayes’ rule. The model fits a Gaussian density to each class, assuming that all classes\\nshare the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input\\nby projecting it to the most discriminative directions, using the\\ntransform method. Added in version 0.17. For a comparison between\\nLinearDiscriminantAnalysis\\nand QuadraticDiscriminantAnalysis, see\\nLinear and Quadratic Discriminant Analysis with covariance ellipsoid. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Input data.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'QuadraticDiscriminantAnalysis',\n",
       "      'func_desc': 'Quadratic Discriminant Analysis.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis',\n",
       "      'function_definitions': {'function_name': 'QuadraticDiscriminantAnalysis',\n",
       "       'full_function': 'class sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(*, priors=None, reg_param=0.0, store_covariance=False, tol=0.0001)',\n",
       "       'function_text': 'Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated\\nby fitting class conditional densities to the data\\nand using Bayes’ rule. The model fits a Gaussian density to each class. Added in version 0.17. For a comparison between\\nQuadraticDiscriminantAnalysis\\nand LinearDiscriminantAnalysis, see\\nLinear and Quadratic Discriminant Analysis with covariance ellipsoid. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'QuadraticDiscriminantAnalysis',\n",
       "       'descriptions': 'Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated\\nby fitting class conditional densities to the data\\nand using Bayes’ rule. The model fits a Gaussian density to each class. Added in version 0.17. For a comparison between\\nQuadraticDiscriminantAnalysis\\nand LinearDiscriminantAnalysis, see\\nLinear and Quadratic Discriminant Analysis with covariance ellipsoid. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.discriminant_analysis',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.discriminant_analysis.html'},\n",
       " 'sklearn.dummy.html': {'functions': [{'defaults': [{'func_name': 'DummyClassifier',\n",
       "      'func_desc': 'DummyClassifier makes predictions that ignore the input features.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier',\n",
       "      'function_definitions': {'function_name': 'DummyClassifier',\n",
       "       'full_function': \"class sklearn.dummy.DummyClassifier(*, strategy='prior', random_state=None, constant=None)\",\n",
       "       'function_text': 'DummyClassifier makes predictions that ignore the input features. This classifier serves as a simple baseline to compare against other more\\ncomplex classifiers. The specific behavior of the baseline is selected with the strategy\\nparameter. All strategies make predictions that ignore the input feature values passed\\nas the X argument to fit and predict. The predictions, however,\\ntypically depend on values observed in the y parameter passed to fit. Note that the “stratified” and “uniform” strategies lead to\\nnon-deterministic predictions that can be rendered deterministic by setting\\nthe random_state parameter if needed. The other strategies are naturally\\ndeterministic and, once fit, always return the same constant prediction\\nfor any value of X. Read more in the User Guide. Added in version 0.13.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'DummyClassifier',\n",
       "       'descriptions': 'DummyClassifier makes predictions that ignore the input features. This classifier serves as a simple baseline to compare against other more\\ncomplex classifiers. The specific behavior of the baseline is selected with the strategy\\nparameter. All strategies make predictions that ignore the input feature values passed\\nas the X argument to fit and predict. The predictions, however,\\ntypically depend on values observed in the y parameter passed to fit. Note that the “stratified” and “uniform” strategies lead to\\nnon-deterministic predictions that can be rendered deterministic by setting\\nthe random_state parameter if needed. The other strategies are naturally\\ndeterministic and, once fit, always return the same constant prediction\\nfor any value of X. Read more in the User Guide. Added in version 0.13.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'DummyRegressor',\n",
       "      'func_desc': 'Regressor that makes predictions using simple rules.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html#sklearn.dummy.DummyRegressor',\n",
       "      'function_definitions': {'function_name': 'DummyRegressor',\n",
       "       'full_function': \"class sklearn.dummy.DummyRegressor(*, strategy='mean', constant=None, quantile=None)\",\n",
       "       'function_text': 'Regressor that makes predictions using simple rules. This regressor is useful as a simple baseline to compare with other\\n(real) regressors. Do not use it for real problems. Read more in the User Guide. Added in version 0.13.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'DummyRegressor',\n",
       "       'descriptions': 'Regressor that makes predictions using simple rules. This regressor is useful as a simple baseline to compare with other\\n(real) regressors. Do not use it for real problems. Read more in the User Guide. Added in version 0.13.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.dummy',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.dummy.html'},\n",
       " 'sklearn.ensemble.html': {'functions': [{'defaults': [{'func_name': 'AdaBoostClassifier',\n",
       "      'func_desc': 'An AdaBoost classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier',\n",
       "      'function_definitions': {'function_name': 'AdaBoostClassifier',\n",
       "       'full_function': \"class sklearn.ensemble.AdaBoostClassifier(estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\",\n",
       "       'function_text': 'An AdaBoost classifier. An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\\nclassifier on the original dataset and then fits additional copies of the\\nclassifier on the same dataset but where the weights of incorrectly\\nclassified instances are adjusted such that subsequent classifiers focus\\nmore on difficult cases. This class implements the algorithm based on [2]. Read more in the User Guide. Added in version 0.14.',\n",
       "       'func_text_user_guide': '#r33e4ec8c4ad5-1',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The training input samples. Sparse matrix can be CSC, CSR, COO,\\nDOK, or LIL. COO, DOK, and LIL are converted to CSR.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Labels for X.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'AdaBoostClassifier',\n",
       "       'descriptions': 'An AdaBoost classifier. An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\\nclassifier on the original dataset and then fits additional copies of the\\nclassifier on the same dataset but where the weights of incorrectly\\nclassified instances are adjusted such that subsequent classifiers focus\\nmore on difficult cases. This class implements the algorithm based on [2]. Read more in the User Guide. Added in version 0.14.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The training input samples. Sparse matrix can be CSC, CSR, COO,\\nDOK, or LIL. COO, DOK, and LIL are converted to CSR.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Labels for X.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': ['estimator=None']}}},\n",
       "     {'func_name': 'BaggingClassifier',\n",
       "      'func_desc': 'A Bagging classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier',\n",
       "      'function_definitions': {'function_name': 'BaggingClassifier',\n",
       "       'full_function': 'class sklearn.ensemble.BaggingClassifier(estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)',\n",
       "       'function_text': 'A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base\\nclassifiers each on random subsets of the original dataset and then\\naggregate their individual predictions (either by voting or by averaging)\\nto form a final prediction. Such a meta-estimator can typically be used as\\na way to reduce the variance of a black-box estimator (e.g., a decision\\ntree), by introducing randomization into its construction procedure and\\nthen making an ensemble out of it. This algorithm encompasses several works from the literature. When random\\nsubsets of the dataset are drawn as random subsets of the samples, then\\nthis algorithm is known as Pasting [1]. If samples are drawn with\\nreplacement, then the method is known as Bagging [2]. When random subsets\\nof the dataset are drawn as random subsets of the features, then the method\\nis known as Random Subspaces [3]. Finally, when base estimators are built\\non subsets of both samples and features, then the method is known as\\nRandom Patches [4]. Read more in the User Guide. Added in version 0.15.',\n",
       "       'func_text_user_guide': '#rb1846455d0e5-1',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'BaggingClassifier',\n",
       "       'descriptions': 'A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base\\nclassifiers each on random subsets of the original dataset and then\\naggregate their individual predictions (either by voting or by averaging)\\nto form a final prediction. Such a meta-estimator can typically be used as\\na way to reduce the variance of a black-box estimator (e.g., a decision\\ntree), by introducing randomization into its construction procedure and\\nthen making an ensemble out of it. This algorithm encompasses several works from the literature. When random\\nsubsets of the dataset are drawn as random subsets of the samples, then\\nthis algorithm is known as Pasting [1]. If samples are drawn with\\nreplacement, then the method is known as Bagging [2]. When random subsets\\nof the dataset are drawn as random subsets of the features, then the method\\nis known as Random Subspaces [3]. Finally, when base estimators are built\\non subsets of both samples and features, then the method is known as\\nRandom Patches [4]. Read more in the User Guide. Added in version 0.15.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['estimator=None', 'n_estimators=10']}}},\n",
       "     {'func_name': 'ExtraTreesClassifier',\n",
       "      'func_desc': 'An extra-trees classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier',\n",
       "      'function_definitions': {'function_name': 'ExtraTreesClassifier',\n",
       "       'full_function': \"class sklearn.ensemble.ExtraTreesClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\",\n",
       "       'function_text': 'An extra-trees classifier. This class implements a meta estimator that fits a number of\\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\\nof the dataset and uses averaging to improve the predictive accuracy\\nand control over-fitting. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/ensemble.html#forest',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'ExtraTreesClassifier',\n",
       "       'descriptions': 'An extra-trees classifier. This class implements a meta estimator that fits a number of\\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\\nof the dataset and uses averaging to improve the predictive accuracy\\nand control over-fitting. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['n_estimators=100']}}},\n",
       "     {'func_name': 'GradientBoostingClassifier',\n",
       "      'func_desc': 'Gradient Boosting for classification.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier',\n",
       "      'function_definitions': {'function_name': 'GradientBoostingClassifier',\n",
       "       'full_function': \"class sklearn.ensemble.GradientBoostingClassifier(*, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\",\n",
       "       'function_text': 'Gradient Boosting for classification. This algorithm builds an additive model in a forward stage-wise fashion; it\\nallows for the optimization of arbitrary differentiable loss functions. In\\neach stage n_classes_ regression trees are fit on the negative gradient\\nof the loss function, e.g. binary or multiclass log loss. Binary\\nclassification is a special case where only a single regression tree is\\ninduced. sklearn.ensemble.HistGradientBoostingClassifier is a much faster\\nvariant of this algorithm for intermediate datasets (n_samples >= 10_000). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The input samples. Internally, it will be converted to\\ndtype=np.float32 and if a sparse matrix is provided\\nto a sparse csr_matrix.\\n'}]},\n",
       "      'function_calling': {'name': 'GradientBoostingClassifier',\n",
       "       'descriptions': 'Gradient Boosting for classification. This algorithm builds an additive model in a forward stage-wise fashion; it\\nallows for the optimization of arbitrary differentiable loss functions. In\\neach stage n_classes_ regression trees are fit on the negative gradient\\nof the loss function, e.g. binary or multiclass log loss. Binary\\nclassification is a special case where only a single regression tree is\\ninduced. sklearn.ensemble.HistGradientBoostingClassifier is a much faster\\nvariant of this algorithm for intermediate datasets (n_samples >= 10_000). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The input samples. Internally, it will be converted to\\ndtype=np.float32 and if a sparse matrix is provided\\nto a sparse csr_matrix.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'HistGradientBoostingClassifier',\n",
       "      'func_desc': 'Histogram-based Gradient Boosting Classification Tree.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier',\n",
       "      'function_definitions': {'function_name': 'HistGradientBoostingClassifier',\n",
       "       'full_function': \"class sklearn.ensemble.HistGradientBoostingClassifier(loss='log_loss', *, learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, l2_regularization=0.0, max_features=1.0, max_bins=255, categorical_features='warn', monotonic_cst=None, interaction_cst=None, warm_start=False, early_stopping='auto', scoring='loss', validation_fraction=0.1, n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None, class_weight=None)\",\n",
       "       'function_text': 'Histogram-based Gradient Boosting Classification Tree. This estimator is much faster than\\nGradientBoostingClassifier\\nfor big datasets (n_samples >= 10 000). This estimator has native support for missing values (NaNs). During\\ntraining, the tree grower learns at each split point whether samples\\nwith missing values should go to the left or right child, based on the\\npotential gain. When predicting, samples with missing values are\\nassigned to the left or right child consequently. If no missing values\\nwere encountered for a given feature during training, then samples with\\nmissing values are mapped to whichever child has the most samples. This implementation is inspired by\\nLightGBM. Read more in the User Guide. Added in version 0.21.',\n",
       "       'func_text_user_guide': 'sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'HistGradientBoostingClassifier',\n",
       "       'descriptions': 'Histogram-based Gradient Boosting Classification Tree. This estimator is much faster than\\nGradientBoostingClassifier\\nfor big datasets (n_samples >= 10 000). This estimator has native support for missing values (NaNs). During\\ntraining, the tree grower learns at each split point whether samples\\nwith missing values should go to the left or right child, based on the\\npotential gain. When predicting, samples with missing values are\\nassigned to the left or right child consequently. If no missing values\\nwere encountered for a given feature during training, then samples with\\nmissing values are mapped to whichever child has the most samples. This implementation is inspired by\\nLightGBM. Read more in the User Guide. Added in version 0.21.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). The input samples.\\n'}},\n",
       "        'required': [\"loss='log_loss'\"]}}},\n",
       "     {'func_name': 'IsolationForest',\n",
       "      'func_desc': 'Isolation Forest Algorithm.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest',\n",
       "      'function_definitions': {'function_name': 'IsolationForest',\n",
       "       'full_function': \"class sklearn.ensemble.IsolationForest(*, n_estimators=100, max_samples='auto', contamination='auto', max_features=1.0, bootstrap=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\",\n",
       "       'function_text': 'Isolation Forest Algorithm. Return the anomaly score of each sample using the IsolationForest algorithm The IsolationForest ‘isolates’ observations by randomly selecting a feature\\nand then randomly selecting a split value between the maximum and minimum\\nvalues of the selected feature. Since recursive partitioning can be represented by a tree structure, the\\nnumber of splittings required to isolate a sample is equivalent to the path\\nlength from the root node to the terminating node. This path length, averaged over a forest of such random trees, is a\\nmeasure of normality and our decision function. Random partitioning produces noticeably shorter paths for anomalies.\\nHence, when a forest of random trees collectively produce shorter path\\nlengths for particular samples, they are highly likely to be anomalies. Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'IsolationForest',\n",
       "       'descriptions': 'Isolation Forest Algorithm. Return the anomaly score of each sample using the IsolationForest algorithm The IsolationForest ‘isolates’ observations by randomly selecting a feature\\nand then randomly selecting a split value between the maximum and minimum\\nvalues of the selected feature. Since recursive partitioning can be represented by a tree structure, the\\nnumber of splittings required to isolate a sample is equivalent to the path\\nlength from the root node to the terminating node. This path length, averaged over a forest of such random trees, is a\\nmeasure of normality and our decision function. Random partitioning produces noticeably shorter paths for anomalies.\\nHence, when a forest of random trees collectively produce shorter path\\nlengths for particular samples, they are highly likely to be anomalies. Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object', 'properties': {}, 'required': []}}},\n",
       "     {'func_name': 'RandomForestRegressor',\n",
       "      'func_desc': 'A random forest regressor.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor',\n",
       "      'function_definitions': {'function_name': 'RandomForestRegressor',\n",
       "       'full_function': \"class sklearn.ensemble.RandomForestRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\",\n",
       "       'function_text': 'A random forest regressor. A random forest is a meta estimator that fits a number of decision tree\\nregressors on various sub-samples of the dataset and uses averaging to\\nimprove the predictive accuracy and control over-fitting.\\nTrees in the forest use the best split strategy, i.e. equivalent to passing\\nsplitter=\"best\" to the underlying DecisionTreeRegressor.\\nThe sub-sample size is controlled with the max_samples parameter if\\nbootstrap=True (default), otherwise the whole dataset is used to build\\neach tree. For a comparison between tree-based ensemble models see the example\\nComparing Random Forests and Histogram Gradient Boosting models. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'RandomForestRegressor',\n",
       "       'descriptions': 'A random forest regressor. A random forest is a meta estimator that fits a number of decision tree\\nregressors on various sub-samples of the dataset and uses averaging to\\nimprove the predictive accuracy and control over-fitting.\\nTrees in the forest use the best split strategy, i.e. equivalent to passing\\nsplitter=\"best\" to the underlying DecisionTreeRegressor.\\nThe sub-sample size is controlled with the max_samples parameter if\\nbootstrap=True (default), otherwise the whole dataset is used to build\\neach tree. For a comparison between tree-based ensemble models see the example\\nComparing Random Forests and Histogram Gradient Boosting models. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['n_estimators=100']}}},\n",
       "     {'func_name': 'StackingClassifier',\n",
       "      'func_desc': 'Stack of estimators with a final classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier',\n",
       "      'function_definitions': {'function_name': 'StackingClassifier',\n",
       "       'full_function': \"class sklearn.ensemble.StackingClassifier(estimators, final_estimator=None, *, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)\",\n",
       "       'function_text': 'Stack of estimators with a final classifier. Stacked generalization consists in stacking the output of individual\\nestimator and use a classifier to compute the final prediction. Stacking\\nallows to use the strength of each individual estimator by using their\\noutput as input of a final estimator. Note that estimators_ are fitted on the full X while final_estimator_\\nis trained using cross-validated predictions of the base estimators using\\ncross_val_predict. Read more in the User Guide. Added in version 0.22.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/ensemble.html#stacking',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training vectors, where n_samples is the number of samples and\\nn_features is the number of features.\\n'}]},\n",
       "      'function_calling': {'name': 'StackingClassifier',\n",
       "       'descriptions': 'Stack of estimators with a final classifier. Stacked generalization consists in stacking the output of individual\\nestimator and use a classifier to compute the final prediction. Stacking\\nallows to use the strength of each individual estimator by using their\\noutput as input of a final estimator. Note that estimators_ are fitted on the full X while final_estimator_\\nis trained using cross-validated predictions of the base estimators using\\ncross_val_predict. Read more in the User Guide. Added in version 0.22.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Training vectors, where n_samples is the number of samples and\\nn_features is the number of features.\\n'}},\n",
       "        'required': ['estimators', 'final_estimator=None']}}},\n",
       "     {'func_name': 'VotingClassifier',\n",
       "      'func_desc': 'Soft Voting/Majority Rule classifier for unfitted estimators.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier',\n",
       "      'function_definitions': {'function_name': 'VotingClassifier',\n",
       "       'full_function': \"class sklearn.ensemble.VotingClassifier(estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)\",\n",
       "       'function_text': 'Soft Voting/Majority Rule classifier for unfitted estimators. Read more in the User Guide. Added in version 0.17.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training vectors, where n_samples is the number of samples and\\nn_features is the number of features.\\n'}]},\n",
       "      'function_calling': {'name': 'VotingClassifier',\n",
       "       'descriptions': 'Soft Voting/Majority Rule classifier for unfitted estimators. Read more in the User Guide. Added in version 0.17.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Training vectors, where n_samples is the number of samples and\\nn_features is the number of features.\\n'}},\n",
       "        'required': ['estimators']}}},\n",
       "     {'func_name': 'AdaBoostRegressor',\n",
       "      'func_desc': 'An AdaBoost regressor.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor',\n",
       "      'function_definitions': {'function_name': 'AdaBoostRegressor',\n",
       "       'full_function': \"class sklearn.ensemble.AdaBoostRegressor(estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\",\n",
       "       'function_text': 'An AdaBoost regressor. An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\\nregressor on the original dataset and then fits additional copies of the\\nregressor on the same dataset but where the weights of instances are\\nadjusted according to the error of the current prediction. As such,\\nsubsequent regressors focus more on difficult cases. This class implements the algorithm known as AdaBoost.R2 [2]. Read more in the User Guide. Added in version 0.14.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/ensemble.html#adaboost',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The training input samples. Sparse matrix can be CSC, CSR, COO,\\nDOK, or LIL. COO, DOK, and LIL are converted to CSR.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Labels for X.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'AdaBoostRegressor',\n",
       "       'descriptions': 'An AdaBoost regressor. An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\\nregressor on the original dataset and then fits additional copies of the\\nregressor on the same dataset but where the weights of instances are\\nadjusted according to the error of the current prediction. As such,\\nsubsequent regressors focus more on difficult cases. This class implements the algorithm known as AdaBoost.R2 [2]. Read more in the User Guide. Added in version 0.14.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The training input samples. Sparse matrix can be CSC, CSR, COO,\\nDOK, or LIL. COO, DOK, and LIL are converted to CSR.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Labels for X.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': ['estimator=None']}}},\n",
       "     {'func_name': 'BaggingRegressor',\n",
       "      'func_desc': 'A Bagging regressor.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor',\n",
       "      'function_definitions': {'function_name': 'BaggingRegressor',\n",
       "       'full_function': 'class sklearn.ensemble.BaggingRegressor(estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)',\n",
       "       'function_text': 'A Bagging regressor. A Bagging regressor is an ensemble meta-estimator that fits base\\nregressors each on random subsets of the original dataset and then\\naggregate their individual predictions (either by voting or by averaging)\\nto form a final prediction. Such a meta-estimator can typically be used as\\na way to reduce the variance of a black-box estimator (e.g., a decision\\ntree), by introducing randomization into its construction procedure and\\nthen making an ensemble out of it. This algorithm encompasses several works from the literature. When random\\nsubsets of the dataset are drawn as random subsets of the samples, then\\nthis algorithm is known as Pasting [1]. If samples are drawn with\\nreplacement, then the method is known as Bagging [2]. When random subsets\\nof the dataset are drawn as random subsets of the features, then the method\\nis known as Random Subspaces [3]. Finally, when base estimators are built\\non subsets of both samples and features, then the method is known as\\nRandom Patches [4]. Read more in the User Guide. Added in version 0.15.',\n",
       "       'func_text_user_guide': '#r4d113ba76fc0-1',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'BaggingRegressor',\n",
       "       'descriptions': 'A Bagging regressor. A Bagging regressor is an ensemble meta-estimator that fits base\\nregressors each on random subsets of the original dataset and then\\naggregate their individual predictions (either by voting or by averaging)\\nto form a final prediction. Such a meta-estimator can typically be used as\\na way to reduce the variance of a black-box estimator (e.g., a decision\\ntree), by introducing randomization into its construction procedure and\\nthen making an ensemble out of it. This algorithm encompasses several works from the literature. When random\\nsubsets of the dataset are drawn as random subsets of the samples, then\\nthis algorithm is known as Pasting [1]. If samples are drawn with\\nreplacement, then the method is known as Bagging [2]. When random subsets\\nof the dataset are drawn as random subsets of the features, then the method\\nis known as Random Subspaces [3]. Finally, when base estimators are built\\non subsets of both samples and features, then the method is known as\\nRandom Patches [4]. Read more in the User Guide. Added in version 0.15.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['estimator=None', 'n_estimators=10']}}},\n",
       "     {'func_name': 'ExtraTreesRegressor',\n",
       "      'func_desc': 'An extra-trees regressor.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor',\n",
       "      'function_definitions': {'function_name': 'ExtraTreesRegressor',\n",
       "       'full_function': \"class sklearn.ensemble.ExtraTreesRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\",\n",
       "       'function_text': 'An extra-trees regressor. This class implements a meta estimator that fits a number of\\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\\nof the dataset and uses averaging to improve the predictive accuracy\\nand control over-fitting. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/ensemble.html#forest',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'ExtraTreesRegressor',\n",
       "       'descriptions': 'An extra-trees regressor. This class implements a meta estimator that fits a number of\\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\\nof the dataset and uses averaging to improve the predictive accuracy\\nand control over-fitting. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['n_estimators=100']}}},\n",
       "     {'func_name': 'GradientBoostingRegressor',\n",
       "      'func_desc': 'Gradient Boosting for regression.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor',\n",
       "      'function_definitions': {'function_name': 'GradientBoostingRegressor',\n",
       "       'full_function': \"class sklearn.ensemble.GradientBoostingRegressor(*, loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\",\n",
       "       'function_text': 'Gradient Boosting for regression. This estimator builds an additive model in a forward stage-wise fashion; it\\nallows for the optimization of arbitrary differentiable loss functions. In\\neach stage a regression tree is fit on the negative gradient of the given\\nloss function. sklearn.ensemble.HistGradientBoostingRegressor is a much faster\\nvariant of this algorithm for intermediate datasets (n_samples >= 10_000). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The input samples. Internally, it will be converted to\\ndtype=np.float32 and if a sparse matrix is provided\\nto a sparse csr_matrix.\\n'}]},\n",
       "      'function_calling': {'name': 'GradientBoostingRegressor',\n",
       "       'descriptions': 'Gradient Boosting for regression. This estimator builds an additive model in a forward stage-wise fashion; it\\nallows for the optimization of arbitrary differentiable loss functions. In\\neach stage a regression tree is fit on the negative gradient of the given\\nloss function. sklearn.ensemble.HistGradientBoostingRegressor is a much faster\\nvariant of this algorithm for intermediate datasets (n_samples >= 10_000). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The input samples. Internally, it will be converted to\\ndtype=np.float32 and if a sparse matrix is provided\\nto a sparse csr_matrix.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'HistGradientBoostingRegressor',\n",
       "      'func_desc': 'Histogram-based Gradient Boosting Regression Tree.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor',\n",
       "      'function_definitions': {'function_name': 'HistGradientBoostingRegressor',\n",
       "       'full_function': \"class sklearn.ensemble.HistGradientBoostingRegressor(loss='squared_error', *, quantile=None, learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, l2_regularization=0.0, max_features=1.0, max_bins=255, categorical_features='warn', monotonic_cst=None, interaction_cst=None, warm_start=False, early_stopping='auto', scoring='loss', validation_fraction=0.1, n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None)\",\n",
       "       'function_text': 'Histogram-based Gradient Boosting Regression Tree. This estimator is much faster than\\nGradientBoostingRegressor\\nfor big datasets (n_samples >= 10 000). This estimator has native support for missing values (NaNs). During\\ntraining, the tree grower learns at each split point whether samples\\nwith missing values should go to the left or right child, based on the\\npotential gain. When predicting, samples with missing values are\\nassigned to the left or right child consequently. If no missing values\\nwere encountered for a given feature during training, then samples with\\nmissing values are mapped to whichever child has the most samples.\\nSee Features in Histogram Gradient Boosting Trees for a\\nusecase example of this feature. This implementation is inspired by\\nLightGBM. Read more in the User Guide. Added in version 0.21.',\n",
       "       'func_text_user_guide': 'sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'HistGradientBoostingRegressor',\n",
       "       'descriptions': 'Histogram-based Gradient Boosting Regression Tree. This estimator is much faster than\\nGradientBoostingRegressor\\nfor big datasets (n_samples >= 10 000). This estimator has native support for missing values (NaNs). During\\ntraining, the tree grower learns at each split point whether samples\\nwith missing values should go to the left or right child, based on the\\npotential gain. When predicting, samples with missing values are\\nassigned to the left or right child consequently. If no missing values\\nwere encountered for a given feature during training, then samples with\\nmissing values are mapped to whichever child has the most samples.\\nSee Features in Histogram Gradient Boosting Trees for a\\nusecase example of this feature. This implementation is inspired by\\nLightGBM. Read more in the User Guide. Added in version 0.21.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). The input samples.\\n'}},\n",
       "        'required': [\"loss='squared_error'\"]}}},\n",
       "     {'func_name': 'RandomForestClassifier',\n",
       "      'func_desc': 'A random forest classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier',\n",
       "      'function_definitions': {'function_name': 'RandomForestClassifier',\n",
       "       'full_function': \"class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\",\n",
       "       'function_text': 'A random forest classifier. A random forest is a meta estimator that fits a number of decision tree\\nclassifiers on various sub-samples of the dataset and uses averaging to\\nimprove the predictive accuracy and control over-fitting.\\nTrees in the forest use the best split strategy, i.e. equivalent to passing\\nsplitter=\"best\" to the underlying DecisionTreeRegressor.\\nThe sub-sample size is controlled with the max_samples parameter if\\nbootstrap=True (default), otherwise the whole dataset is used to build\\neach tree. For a comparison between tree-based ensemble models see the example\\nComparing Random Forests and Histogram Gradient Boosting models. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'RandomForestClassifier',\n",
       "       'descriptions': 'A random forest classifier. A random forest is a meta estimator that fits a number of decision tree\\nclassifiers on various sub-samples of the dataset and uses averaging to\\nimprove the predictive accuracy and control over-fitting.\\nTrees in the forest use the best split strategy, i.e. equivalent to passing\\nsplitter=\"best\" to the underlying DecisionTreeRegressor.\\nThe sub-sample size is controlled with the max_samples parameter if\\nbootstrap=True (default), otherwise the whole dataset is used to build\\neach tree. For a comparison between tree-based ensemble models see the example\\nComparing Random Forests and Histogram Gradient Boosting models. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['n_estimators=100']}}},\n",
       "     {'func_name': 'RandomTreesEmbedding',\n",
       "      'func_desc': 'An ensemble of totally random trees.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html#sklearn.ensemble.RandomTreesEmbedding',\n",
       "      'function_definitions': {'function_name': 'RandomTreesEmbedding',\n",
       "       'full_function': 'class sklearn.ensemble.RandomTreesEmbedding(n_estimators=100, *, max_depth=5, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_leaf_nodes=None, min_impurity_decrease=0.0, sparse_output=True, n_jobs=None, random_state=None, verbose=0, warm_start=False)',\n",
       "       'function_text': 'An ensemble of totally random trees. An unsupervised transformation of a dataset to a high-dimensional\\nsparse representation. A datapoint is coded according to which leaf of\\neach tree it is sorted into. Using a one-hot encoding of the leaves,\\nthis leads to a binary coding with as many ones as there are trees in\\nthe forest. The dimensionality of the resulting representation is\\nn_out <= n_estimators * max_leaf_nodes. If max_leaf_nodes == None,\\nthe number of leaf nodes is at most n_estimators * 2 ** max_depth. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/ensemble.html#random-trees-embedding',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Input data to be transformed. Use dtype=np.float32 for maximum\\nefficiency. Sparse matrices are also supported, use sparse\\ncsr_matrix for maximum efficiency.\\n'}]},\n",
       "      'function_calling': {'name': 'RandomTreesEmbedding',\n",
       "       'descriptions': 'An ensemble of totally random trees. An unsupervised transformation of a dataset to a high-dimensional\\nsparse representation. A datapoint is coded according to which leaf of\\neach tree it is sorted into. Using a one-hot encoding of the leaves,\\nthis leads to a binary coding with as many ones as there are trees in\\nthe forest. The dimensionality of the resulting representation is\\nn_out <= n_estimators * max_leaf_nodes. If max_leaf_nodes == None,\\nthe number of leaf nodes is at most n_estimators * 2 ** max_depth. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Input data to be transformed. Use dtype=np.float32 for maximum\\nefficiency. Sparse matrices are also supported, use sparse\\ncsr_matrix for maximum efficiency.\\n'}},\n",
       "        'required': ['n_estimators=100']}}},\n",
       "     {'func_name': 'StackingRegressor',\n",
       "      'func_desc': 'Stack of estimators with a final regressor.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor',\n",
       "      'function_definitions': {'function_name': 'StackingRegressor',\n",
       "       'full_function': 'class sklearn.ensemble.StackingRegressor(estimators, final_estimator=None, *, cv=None, n_jobs=None, passthrough=False, verbose=0)',\n",
       "       'function_text': 'Stack of estimators with a final regressor. Stacked generalization consists in stacking the output of individual\\nestimator and use a regressor to compute the final prediction. Stacking\\nallows to use the strength of each individual estimator by using their\\noutput as input of a final estimator. Note that estimators_ are fitted on the full X while final_estimator_\\nis trained using cross-validated predictions of the base estimators using\\ncross_val_predict. Read more in the User Guide. Added in version 0.22.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/ensemble.html#stacking',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training vectors, where n_samples is the number of samples and\\nn_features is the number of features.\\n'}]},\n",
       "      'function_calling': {'name': 'StackingRegressor',\n",
       "       'descriptions': 'Stack of estimators with a final regressor. Stacked generalization consists in stacking the output of individual\\nestimator and use a regressor to compute the final prediction. Stacking\\nallows to use the strength of each individual estimator by using their\\noutput as input of a final estimator. Note that estimators_ are fitted on the full X while final_estimator_\\nis trained using cross-validated predictions of the base estimators using\\ncross_val_predict. Read more in the User Guide. Added in version 0.22.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Training vectors, where n_samples is the number of samples and\\nn_features is the number of features.\\n'}},\n",
       "        'required': ['estimators', 'final_estimator=None']}}},\n",
       "     {'func_name': 'VotingRegressor',\n",
       "      'func_desc': 'Prediction voting regressor for unfitted estimators.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html#sklearn.ensemble.VotingRegressor',\n",
       "      'function_definitions': {'function_name': 'VotingRegressor',\n",
       "       'full_function': 'class sklearn.ensemble.VotingRegressor(estimators, *, weights=None, n_jobs=None, verbose=False)',\n",
       "       'function_text': 'Prediction voting regressor for unfitted estimators. A voting regressor is an ensemble meta-estimator that fits several base\\nregressors, each on the whole dataset. Then it averages the individual\\npredictions to form a final prediction. Read more in the User Guide. Added in version 0.21.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'VotingRegressor',\n",
       "       'descriptions': 'Prediction voting regressor for unfitted estimators. A voting regressor is an ensemble meta-estimator that fits several base\\nregressors, each on the whole dataset. Then it averages the individual\\npredictions to form a final prediction. Read more in the User Guide. Added in version 0.21.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The input samples.\\n'}},\n",
       "        'required': ['estimators']}}}]}],\n",
       "  'name': 'sklearn.ensemble',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.ensemble.html'},\n",
       " 'sklearn.exceptions.html': {'functions': [{'defaults': [{'func_name': 'ConvergenceWarning',\n",
       "      'func_desc': 'Custom warning to capture convergence problems',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.exceptions.ConvergenceWarning.html#sklearn.exceptions.ConvergenceWarning',\n",
       "      'function_definitions': {'function_name': 'ConvergenceWarning',\n",
       "       'full_function': 'exception sklearn.exceptions.ConvergenceWarning',\n",
       "       'function_text': 'Custom warning to capture convergence problems Changed in version 0.18: Moved from sklearn.utils.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'DataDimensionalityWarning',\n",
       "      'func_desc': 'Custom warning to notify potential issues with data dimensionality.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.exceptions.DataDimensionalityWarning.html#sklearn.exceptions.DataDimensionalityWarning',\n",
       "      'function_definitions': {'function_name': 'DataDimensionalityWarning',\n",
       "       'full_function': 'exception sklearn.exceptions.DataDimensionalityWarning',\n",
       "       'function_text': 'Custom warning to notify potential issues with data dimensionality. For example, in random projection, this warning is raised when the\\nnumber of components, which quantifies the dimensionality of the target\\nprojection space, is higher than the number of features, which quantifies\\nthe dimensionality of the original source space, to imply that the\\ndimensionality of the problem will not be reduced. Changed in version 0.18: Moved from sklearn.utils.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'FitFailedWarning',\n",
       "      'func_desc': 'Warning class used if there is an error while fitting the estimator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.exceptions.FitFailedWarning.html#sklearn.exceptions.FitFailedWarning',\n",
       "      'function_definitions': {'function_name': 'FitFailedWarning',\n",
       "       'full_function': 'exception sklearn.exceptions.FitFailedWarning',\n",
       "       'function_text': 'Warning class used if there is an error while fitting the estimator. This Warning is used in meta estimators GridSearchCV and RandomizedSearchCV\\nand the cross-validation helper function cross_val_score to warn when there\\nis an error while fitting the estimator. Changed in version 0.18: Moved from sklearn.cross_validation.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'NotFittedError',\n",
       "      'func_desc': 'Exception class to raise if estimator is used before fitting.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.exceptions.NotFittedError.html#sklearn.exceptions.NotFittedError',\n",
       "      'function_definitions': {'function_name': 'NotFittedError',\n",
       "       'full_function': 'exception sklearn.exceptions.NotFittedError',\n",
       "       'function_text': 'Exception class to raise if estimator is used before fitting. This class inherits from both ValueError and AttributeError to help with\\nexception handling and backward compatibility. Examples Changed in version 0.18: Moved from sklearn.utils.validation.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'DataConversionWarning',\n",
       "      'func_desc': 'Warning used to notify implicit data conversions happening in the code.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.exceptions.DataConversionWarning.html#sklearn.exceptions.DataConversionWarning',\n",
       "      'function_definitions': {'function_name': 'DataConversionWarning',\n",
       "       'full_function': 'exception sklearn.exceptions.DataConversionWarning',\n",
       "       'function_text': 'Warning used to notify implicit data conversions happening in the code. This warning occurs when some input data needs to be converted or\\ninterpreted in a way that may not match the user’s expectations. passes an integer array to a function which expects float input and\\nwill convert the input requests a non-copying operation, but a copy is required to meet the\\nimplementation’s data-type expectations; passes an input whose shape can be interpreted ambiguously. Changed in version 0.18: Moved from sklearn.utils.validation.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'EfficiencyWarning',\n",
       "      'func_desc': 'Warning used to notify the user of inefficient computation.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.exceptions.EfficiencyWarning.html#sklearn.exceptions.EfficiencyWarning',\n",
       "      'function_definitions': {'function_name': 'EfficiencyWarning',\n",
       "       'full_function': 'exception sklearn.exceptions.EfficiencyWarning',\n",
       "       'function_text': 'Warning used to notify the user of inefficient computation. This warning notifies the user that the efficiency may not be optimal due\\nto some reason which may be included as a part of the warning message.\\nThis may be subclassed into a more specific Warning class. Added in version 0.18.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'InconsistentVersionWarning',\n",
       "      'func_desc': 'Warning raised when an estimator is unpickled with a inconsistent version.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.exceptions.InconsistentVersionWarning.html#sklearn.exceptions.InconsistentVersionWarning',\n",
       "      'function_definitions': {'function_name': 'InconsistentVersionWarning',\n",
       "       'full_function': 'exception sklearn.exceptions.InconsistentVersionWarning(*, estimator_name, current_sklearn_version, original_sklearn_version)',\n",
       "       'function_text': 'Warning raised when an estimator is unpickled with a inconsistent version.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator_name',\n",
       "         'param_type': 'str',\n",
       "         'param_desc': 'Estimator name.\\n'},\n",
       "        {'param_name': 'current_sklearn_version',\n",
       "         'param_type': 'str',\n",
       "         'param_desc': 'Current scikit-learn version.\\n'},\n",
       "        {'param_name': 'original_sklearn_version',\n",
       "         'param_type': 'str',\n",
       "         'param_desc': 'Original scikit-learn version.\\n'}]},\n",
       "      'function_calling': {'name': 'InconsistentVersionWarning',\n",
       "       'descriptions': 'Warning raised when an estimator is unpickled with a inconsistent version.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator_name': {'type': 'string',\n",
       "          'description': 'str. Estimator name.\\n'},\n",
       "         'current_sklearn_version': {'type': 'string',\n",
       "          'description': 'str. Current scikit-learn version.\\n'},\n",
       "         'original_sklearn_version': {'type': 'string',\n",
       "          'description': 'str. Original scikit-learn version.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'UndefinedMetricWarning',\n",
       "      'func_desc': 'Warning used when the metric is invalid',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.exceptions.UndefinedMetricWarning.html#sklearn.exceptions.UndefinedMetricWarning',\n",
       "      'function_definitions': {'function_name': 'UndefinedMetricWarning',\n",
       "       'full_function': 'exception sklearn.exceptions.UndefinedMetricWarning',\n",
       "       'function_text': 'Warning used when the metric is invalid Changed in version 0.18: Moved from sklearn.base.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}}]}],\n",
       "  'name': 'sklearn.exceptions',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.exceptions.html'},\n",
       " 'sklearn.experimental.html': {'functions': [{'defaults': [{'func_name': 'enable_halving_search_cv',\n",
       "      'func_desc': 'Enables Successive Halving search-estimators',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.experimental.enable_halving_search_cv.html#module-sklearn.experimental.enable_halving_search_cv',\n",
       "      'function_definitions': {'function_name': 'UndefinedMetricWarning',\n",
       "       'full_function': 'exception sklearn.exceptions.UndefinedMetricWarning',\n",
       "       'function_text': 'Warning used when the metric is invalid Changed in version 0.18: Moved from sklearn.base.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'enable_iterative_imputer',\n",
       "      'func_desc': 'Enables IterativeImputer',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.experimental.enable_iterative_imputer.html#module-sklearn.experimental.enable_iterative_imputer',\n",
       "      'function_definitions': {'function_name': 'UndefinedMetricWarning',\n",
       "       'full_function': 'exception sklearn.exceptions.UndefinedMetricWarning',\n",
       "       'function_text': 'Warning used when the metric is invalid Changed in version 0.18: Moved from sklearn.base.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}}]}],\n",
       "  'name': 'sklearn.experimental',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.experimental.html'},\n",
       " 'sklearn.feature_extraction.html': {'functions': [{'defaults': [{'func_name': 'DictVectorizer',\n",
       "      'func_desc': 'Transforms lists of feature-value mappings to vectors.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer',\n",
       "      'function_definitions': {'function_name': 'DictVectorizer',\n",
       "       'full_function': \"class sklearn.feature_extraction.DictVectorizer(*, dtype=<class 'numpy.float64'>, separator='=', sparse=True, sort=True)\",\n",
       "       'function_text': 'Transforms lists of feature-value mappings to vectors. This transformer turns lists of mappings (dict-like objects) of feature\\nnames to feature values into Numpy arrays or scipy.sparse matrices for use\\nwith scikit-learn estimators. When feature values are strings, this transformer will do a binary one-hot\\n(aka one-of-K) coding: one boolean-valued feature is constructed for each\\nof the possible string values that the feature can take on. For instance,\\na feature “f” that can take on the values “ham” and “spam” will become two\\nfeatures in the output, one signifying “f=ham”, the other “f=spam”. If a feature value is a sequence or set of strings, this transformer\\nwill iterate over the values and will count the occurrences of each string\\nvalue. However, note that this transformer will only do a binary one-hot encoding\\nwhen feature values are of type string. If categorical features are\\nrepresented as numeric values such as int or iterables of strings, the\\nDictVectorizer can be followed by\\nOneHotEncoder to complete\\nbinary one-hot encoding. Features that do not occur in a sample (mapping) will have a zero value\\nin the resulting array/matrix. For an efficiency comparison of the different feature extractors, see\\nFeatureHasher and DictVectorizer Comparison. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'Mapping or iterable over Mappings of shape (n_samples,)',\n",
       "         'param_desc': 'Dict(s) or Mapping(s) from feature names (arbitrary Python\\nobjects) to feature values (strings or convertible to dtype).\\n'}]},\n",
       "      'function_calling': {'name': 'DictVectorizer',\n",
       "       'descriptions': 'Transforms lists of feature-value mappings to vectors. This transformer turns lists of mappings (dict-like objects) of feature\\nnames to feature values into Numpy arrays or scipy.sparse matrices for use\\nwith scikit-learn estimators. When feature values are strings, this transformer will do a binary one-hot\\n(aka one-of-K) coding: one boolean-valued feature is constructed for each\\nof the possible string values that the feature can take on. For instance,\\na feature “f” that can take on the values “ham” and “spam” will become two\\nfeatures in the output, one signifying “f=ham”, the other “f=spam”. If a feature value is a sequence or set of strings, this transformer\\nwill iterate over the values and will count the occurrences of each string\\nvalue. However, note that this transformer will only do a binary one-hot encoding\\nwhen feature values are of type string. If categorical features are\\nrepresented as numeric values such as int or iterables of strings, the\\nDictVectorizer can be followed by\\nOneHotEncoder to complete\\nbinary one-hot encoding. Features that do not occur in a sample (mapping) will have a zero value\\nin the resulting array/matrix. For an efficiency comparison of the different feature extractors, see\\nFeatureHasher and DictVectorizer Comparison. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'Mapping or iterable over Mappings of shape (n_samples,). Dict(s) or Mapping(s) from feature names (arbitrary Python\\nobjects) to feature values (strings or convertible to dtype).\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'FeatureHasher',\n",
       "      'func_desc': 'Implements feature hashing, aka the hashing trick.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher',\n",
       "      'function_definitions': {'function_name': 'FeatureHasher',\n",
       "       'full_function': \"class sklearn.feature_extraction.FeatureHasher(n_features=1048576, *, input_type='dict', dtype=<class 'numpy.float64'>, alternate_sign=True)\",\n",
       "       'function_text': 'Implements feature hashing, aka the hashing trick. This class turns sequences of symbolic feature names (strings) into\\nscipy.sparse matrices, using a hash function to compute the matrix column\\ncorresponding to a name. The hash function employed is the signed 32-bit\\nversion of Murmurhash3. Feature names of type byte string are used as-is. Unicode strings are\\nconverted to UTF-8 first, but no Unicode normalization is done.\\nFeature values must be (finite) numbers. This class is a low-memory alternative to DictVectorizer and\\nCountVectorizer, intended for large-scale (online) learning and situations\\nwhere memory is tight, e.g. when running prediction code on embedded\\ndevices. For an efficiency comparison of the different feature extractors, see\\nFeatureHasher and DictVectorizer Comparison. Read more in the User Guide. Added in version 0.13.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/text/plot_hashing_vs_dict_vectorizer.html#sphx-glr-auto-examples-text-plot-hashing-vs-dict-vectorizer-py',\n",
       "       'parameter_names_desc': [{'param_name': 'raw_X',\n",
       "         'param_type': 'iterable over iterable over raw features, length = n_samples',\n",
       "         'param_desc': 'Samples. Each sample must be iterable an (e.g., a list or tuple)\\ncontaining/generating feature names (and optionally values, see\\nthe input_type constructor argument) which will be hashed.\\nraw_X need not support the len function, so it can be the result\\nof a generator; n_samples is determined on the fly.\\n'}]},\n",
       "      'function_calling': {'name': 'FeatureHasher',\n",
       "       'descriptions': 'Implements feature hashing, aka the hashing trick. This class turns sequences of symbolic feature names (strings) into\\nscipy.sparse matrices, using a hash function to compute the matrix column\\ncorresponding to a name. The hash function employed is the signed 32-bit\\nversion of Murmurhash3. Feature names of type byte string are used as-is. Unicode strings are\\nconverted to UTF-8 first, but no Unicode normalization is done.\\nFeature values must be (finite) numbers. This class is a low-memory alternative to DictVectorizer and\\nCountVectorizer, intended for large-scale (online) learning and situations\\nwhere memory is tight, e.g. when running prediction code on embedded\\ndevices. For an efficiency comparison of the different feature extractors, see\\nFeatureHasher and DictVectorizer Comparison. Read more in the User Guide. Added in version 0.13.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'raw_X': {'type': 'array',\n",
       "          'description': 'iterable over iterable over raw features, length = n_samples. Samples. Each sample must be iterable an (e.g., a list or tuple)\\ncontaining/generating feature names (and optionally values, see\\nthe input_type constructor argument) which will be hashed.\\nraw_X need not support the len function, so it can be the result\\nof a generator; n_samples is determined on the fly.\\n'}},\n",
       "        'required': ['n_features=1048576']}}}]},\n",
       "   {'From images': [{'func_name': 'image.PatchExtractor',\n",
       "      'func_desc': 'Extracts patches from a collection of images.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.PatchExtractor.html#sklearn.feature_extraction.image.PatchExtractor',\n",
       "      'function_definitions': {'function_name': 'PatchExtractor',\n",
       "       'full_function': 'class sklearn.feature_extraction.image.PatchExtractor(*, patch_size=None, max_patches=None, random_state=None)',\n",
       "       'function_text': 'Extracts patches from a collection of images. Read more in the User Guide. Added in version 0.9.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_extraction.html#image-feature-extraction',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'ndarray of shape (n_samples, image_height, image_width) or                 (n_samples, image_height, image_width, n_channels)',\n",
       "         'param_desc': 'Array of images from which to extract patches. For color images,\\nthe last dimension specifies the channel: a RGB image would have\\nn_channels=3.\\n'}]},\n",
       "      'function_calling': {'name': 'PatchExtractor',\n",
       "       'descriptions': 'Extracts patches from a collection of images. Read more in the User Guide. Added in version 0.9.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples, image_height, image_width) or                 (n_samples, image_height, image_width, n_channels). Array of images from which to extract patches. For color images,\\nthe last dimension specifies the channel: a RGB image would have\\nn_channels=3.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'image.grid_to_graph',\n",
       "      'func_desc': 'Graph of the pixel-to-pixel connections.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.grid_to_graph.html#sklearn.feature_extraction.image.grid_to_graph',\n",
       "      'function_definitions': {'function_name': 'grid_to_graph',\n",
       "       'full_function': \"sklearn.feature_extraction.image.grid_to_graph(n_x, n_y, n_z=1, *, mask=None, return_as=<class 'scipy.sparse._coo.coo_matrix'>, dtype=<class 'int'>)\",\n",
       "       'function_text': 'Graph of the pixel-to-pixel connections. Edges exist if 2 voxels are connected.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'n_x',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Dimension in x axis.\\n'},\n",
       "        {'param_name': 'n_y',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Dimension in y axis.\\n'},\n",
       "        {'param_name': 'n_z',\n",
       "         'param_type': 'int, default=1',\n",
       "         'param_desc': 'Dimension in z axis.\\n'},\n",
       "        {'param_name': 'mask',\n",
       "         'param_type': 'ndarray of shape (n_x, n_y, n_z), dtype=bool, default=None',\n",
       "         'param_desc': 'An optional mask of the image, to consider only part of the\\npixels.\\n'},\n",
       "        {'param_name': 'return_as',\n",
       "         'param_type': 'np.ndarray or a sparse matrix class,             default=sparse.coo_matrix',\n",
       "         'param_desc': 'The class to use to build the returned adjacency matrix.\\n'},\n",
       "        {'param_name': 'dtype',\n",
       "         'param_type': 'dtype, default=int',\n",
       "         'param_desc': 'The data of the returned sparse matrix. By default it is int.\\n'}]},\n",
       "      'function_calling': {'name': 'grid_to_graph',\n",
       "       'descriptions': 'Graph of the pixel-to-pixel connections. Edges exist if 2 voxels are connected.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_x': {'type': 'integer',\n",
       "          'description': 'int. Dimension in x axis.\\n'},\n",
       "         'n_y': {'type': 'integer',\n",
       "          'description': 'int. Dimension in y axis.\\n'},\n",
       "         'n_z': {'type': 'integer',\n",
       "          'description': 'int, default=1. Dimension in z axis.\\n'},\n",
       "         'mask': {'type': 'boolean',\n",
       "          'description': 'ndarray of shape (n_x, n_y, n_z), dtype=bool, default=None. An optional mask of the image, to consider only part of the\\npixels.\\n'},\n",
       "         'return_as': {'type': 'array',\n",
       "          'description': 'np.ndarray or a sparse matrix class,             default=sparse.coo_matrix. The class to use to build the returned adjacency matrix.\\n'},\n",
       "         'dtype': {'type': 'integer',\n",
       "          'description': 'dtype, default=int. The data of the returned sparse matrix. By default it is int.\\n'}},\n",
       "        'required': ['n_x', 'n_y', 'n_z=1']}}},\n",
       "     {'func_name': 'image.reconstruct_from_patches_2d',\n",
       "      'func_desc': 'Reconstruct the image from all of its patches.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.reconstruct_from_patches_2d.html#sklearn.feature_extraction.image.reconstruct_from_patches_2d',\n",
       "      'function_definitions': {'function_name': 'reconstruct_from_patches_2d',\n",
       "       'full_function': 'sklearn.feature_extraction.image.reconstruct_from_patches_2d(patches, image_size)',\n",
       "       'function_text': 'Reconstruct the image from all of its patches. Patches are assumed to overlap and the image is constructed by filling in\\nthe patches from left to right, top to bottom, averaging the overlapping\\nregions. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_extraction.html#image-feature-extraction',\n",
       "       'parameter_names_desc': [{'param_name': 'patches',\n",
       "         'param_type': 'ndarray of shape (n_patches, patch_height, patch_width) or         (n_patches, patch_height, patch_width, n_channels)',\n",
       "         'param_desc': 'The complete set of patches. If the patches contain colour information,\\nchannels are indexed along the last dimension: RGB patches would\\nhave n_channels=3.\\n'},\n",
       "        {'param_name': 'image_size',\n",
       "         'param_type': 'tuple of int (image_height, image_width) or         (image_height, image_width, n_channels)',\n",
       "         'param_desc': 'The size of the image that will be reconstructed.\\n'}]},\n",
       "      'function_calling': {'name': 'reconstruct_from_patches_2d',\n",
       "       'descriptions': 'Reconstruct the image from all of its patches. Patches are assumed to overlap and the image is constructed by filling in\\nthe patches from left to right, top to bottom, averaging the overlapping\\nregions. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'patches': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_patches, patch_height, patch_width) or         (n_patches, patch_height, patch_width, n_channels). The complete set of patches. If the patches contain colour information,\\nchannels are indexed along the last dimension: RGB patches would\\nhave n_channels=3.\\n'},\n",
       "         'image_size': {'type': 'integer',\n",
       "          'description': 'tuple of int (image_height, image_width) or         (image_height, image_width, n_channels). The size of the image that will be reconstructed.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'image.extract_patches_2d',\n",
       "      'func_desc': 'Reshape a 2D image into a collection of patches.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.extract_patches_2d.html#sklearn.feature_extraction.image.extract_patches_2d',\n",
       "      'function_definitions': {'function_name': 'extract_patches_2d',\n",
       "       'full_function': 'sklearn.feature_extraction.image.extract_patches_2d(image, patch_size, *, max_patches=None, random_state=None)',\n",
       "       'function_text': 'Reshape a 2D image into a collection of patches. The resulting patches are allocated in a dedicated array. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_extraction.html#image-feature-extraction',\n",
       "       'parameter_names_desc': [{'param_name': 'image',\n",
       "         'param_type': 'ndarray of shape (image_height, image_width) or         (image_height, image_width, n_channels)',\n",
       "         'param_desc': 'The original image data. For color images, the last dimension specifies\\nthe channel: a RGB image would have n_channels=3.\\n'},\n",
       "        {'param_name': 'patch_size',\n",
       "         'param_type': 'tuple of int (patch_height, patch_width)',\n",
       "         'param_desc': 'The dimensions of one patch.\\n'},\n",
       "        {'param_name': 'max_patches',\n",
       "         'param_type': 'int or float, default=None',\n",
       "         'param_desc': 'The maximum number of patches to extract. If max_patches is a float\\nbetween 0 and 1, it is taken to be a proportion of the total number\\nof patches. If max_patches is None it corresponds to the total number\\nof patches that can be extracted.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance, default=None',\n",
       "         'param_desc': 'Determines the random number generator used for random sampling when\\nmax_patches is not None. Use an int to make the randomness\\ndeterministic.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'extract_patches_2d',\n",
       "       'descriptions': 'Reshape a 2D image into a collection of patches. The resulting patches are allocated in a dedicated array. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'image': {'type': 'array',\n",
       "          'description': 'ndarray of shape (image_height, image_width) or         (image_height, image_width, n_channels). The original image data. For color images, the last dimension specifies\\nthe channel: a RGB image would have n_channels=3.\\n'},\n",
       "         'patch_size': {'type': 'integer',\n",
       "          'description': 'tuple of int (patch_height, patch_width). The dimensions of one patch.\\n'},\n",
       "         'max_patches': {'type': 'integer',\n",
       "          'description': 'int or float, default=None. The maximum number of patches to extract. If max_patches is a float\\nbetween 0 and 1, it is taken to be a proportion of the total number\\nof patches. If max_patches is None it corresponds to the total number\\nof patches that can be extracted.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance, default=None. Determines the random number generator used for random sampling when\\nmax_patches is not None. Use an int to make the randomness\\ndeterministic.\\nSee Glossary.\\n'}},\n",
       "        'required': ['image', 'patch_size']}}},\n",
       "     {'func_name': 'image.img_to_graph',\n",
       "      'func_desc': 'Graph of the pixel-to-pixel gradient connections.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.img_to_graph.html#sklearn.feature_extraction.image.img_to_graph',\n",
       "      'function_definitions': {'function_name': 'img_to_graph',\n",
       "       'full_function': \"sklearn.feature_extraction.image.img_to_graph(img, *, mask=None, return_as=<class 'scipy.sparse._coo.coo_matrix'>, dtype=None)\",\n",
       "       'function_text': 'Graph of the pixel-to-pixel gradient connections. Edges are weighted with the gradient values. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_extraction.html#image-feature-extraction',\n",
       "       'parameter_names_desc': [{'param_name': 'img',\n",
       "         'param_type': 'array-like of shape (height, width) or (height, width, channel)',\n",
       "         'param_desc': '2D or 3D image.\\n'},\n",
       "        {'param_name': 'mask',\n",
       "         'param_type': 'ndarray of shape (height, width) or             (height, width, channel), dtype=bool, default=None',\n",
       "         'param_desc': 'An optional mask of the image, to consider only part of the\\npixels.\\n'},\n",
       "        {'param_name': 'return_as',\n",
       "         'param_type': 'np.ndarray or a sparse matrix class,             default=sparse.coo_matrix',\n",
       "         'param_desc': 'The class to use to build the returned adjacency matrix.\\n'},\n",
       "        {'param_name': 'dtype',\n",
       "         'param_type': 'dtype, default=None',\n",
       "         'param_desc': 'The data of the returned sparse matrix. By default it is the\\ndtype of img.\\n'}]},\n",
       "      'function_calling': {'name': 'img_to_graph',\n",
       "       'descriptions': 'Graph of the pixel-to-pixel gradient connections. Edges are weighted with the gradient values. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'img': {'type': 'array',\n",
       "          'description': 'array-like of shape (height, width) or (height, width, channel). 2D or 3D image.\\n'},\n",
       "         'mask': {'type': 'boolean',\n",
       "          'description': 'ndarray of shape (height, width) or             (height, width, channel), dtype=bool, default=None. An optional mask of the image, to consider only part of the\\npixels.\\n'},\n",
       "         'return_as': {'type': 'array',\n",
       "          'description': 'np.ndarray or a sparse matrix class,             default=sparse.coo_matrix. The class to use to build the returned adjacency matrix.\\n'},\n",
       "         'dtype': {'type': 'object',\n",
       "          'description': 'dtype, default=None. The data of the returned sparse matrix. By default it is the\\ndtype of img.\\n'}},\n",
       "        'required': ['img']}}}]},\n",
       "   {'From text': [{'func_name': 'text.CountVectorizer',\n",
       "      'func_desc': 'Convert a collection of text documents to a matrix of token counts.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer',\n",
       "      'function_definitions': {'function_name': 'CountVectorizer',\n",
       "       'full_function': \"class sklearn.feature_extraction.text.CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\\\\\b\\\\\\\\w\\\\\\\\w+\\\\\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\",\n",
       "       'function_text': 'Convert a collection of text documents to a matrix of token counts. This implementation produces a sparse representation of the counts using\\nscipy.sparse.csr_matrix. If you do not provide an a-priori dictionary and you do not use an analyzer\\nthat does some kind of feature selection then the number of features will\\nbe equal to the vocabulary size found by analyzing the data. For an efficiency comparison of the different feature extractors, see\\nFeatureHasher and DictVectorizer Comparison. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/text/plot_hashing_vs_dict_vectorizer.html#sphx-glr-auto-examples-text-plot-hashing-vs-dict-vectorizer-py',\n",
       "       'parameter_names_desc': [{'param_name': 'raw_documents',\n",
       "         'param_type': 'iterable',\n",
       "         'param_desc': 'An iterable which generates either str, unicode or file objects.\\n'}]},\n",
       "      'function_calling': {'name': 'CountVectorizer',\n",
       "       'descriptions': 'Convert a collection of text documents to a matrix of token counts. This implementation produces a sparse representation of the counts using\\nscipy.sparse.csr_matrix. If you do not provide an a-priori dictionary and you do not use an analyzer\\nthat does some kind of feature selection then the number of features will\\nbe equal to the vocabulary size found by analyzing the data. For an efficiency comparison of the different feature extractors, see\\nFeatureHasher and DictVectorizer Comparison. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'raw_documents': {'type': 'array',\n",
       "          'description': 'iterable. An iterable which generates either str, unicode or file objects.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'text.TfidfTransformer',\n",
       "      'func_desc': 'Transform a count matrix to a normalized tf or tf-idf representation.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer',\n",
       "      'function_definitions': {'function_name': 'TfidfTransformer',\n",
       "       'full_function': \"class sklearn.feature_extraction.text.TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\",\n",
       "       'function_text': \"Transform a count matrix to a normalized tf or tf-idf representation. Tf means term-frequency while tf-idf means term-frequency times inverse\\ndocument-frequency. This is a common term weighting scheme in information\\nretrieval, that has also found good use in document classification. The goal of using tf-idf instead of the raw frequencies of occurrence of a\\ntoken in a given document is to scale down the impact of tokens that occur\\nvery frequently in a given corpus and that are hence empirically less\\ninformative than features that occur in a small fraction of the training\\ncorpus. The formula that is used to compute the tf-idf for a term t of a document d\\nin a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\\ncomputed as idf(t) = log [ n / df(t) ] + 1 (if smooth_idf=False), where\\nn is the total number of documents in the document set and df(t) is the\\ndocument frequency of t; the document frequency is the number of documents\\nin the document set that contain the term t. The effect of adding “1” to\\nthe idf in the equation above is that terms with zero idf, i.e., terms\\nthat occur in all documents in a training set, will not be entirely\\nignored.\\n(Note that the idf formula above differs from the standard textbook\\nnotation that defines the idf as\\nidf(t) = log [ n / (df(t) + 1) ]). If smooth_idf=True (the default), the constant “1” is added to the\\nnumerator and denominator of the idf as if an extra document was seen\\ncontaining every term in the collection exactly once, which prevents\\nzero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1. Furthermore, the formulas used to compute tf and idf depend\\non parameter settings that correspond to the SMART notation used in IR\\nas follows: Tf is “n” (natural) by default, “l” (logarithmic) when\\nsublinear_tf=True.\\nIdf is “t” when use_idf is given, “n” (none) otherwise.\\nNormalization is “c” (cosine) when norm='l2', “n” (none)\\nwhen norm=None. Read more in the User Guide.\",\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'sparse matrix of (n_samples, n_features)',\n",
       "         'param_desc': 'A matrix of term/token counts.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to copy X and operate on the copy or perform in-place\\noperations. copy=False will only be effective with CSR sparse matrix.\\n'}]},\n",
       "      'function_calling': {'name': 'TfidfTransformer',\n",
       "       'descriptions': \"Transform a count matrix to a normalized tf or tf-idf representation. Tf means term-frequency while tf-idf means term-frequency times inverse\\ndocument-frequency. This is a common term weighting scheme in information\\nretrieval, that has also found good use in document classification. The goal of using tf-idf instead of the raw frequencies of occurrence of a\\ntoken in a given document is to scale down the impact of tokens that occur\\nvery frequently in a given corpus and that are hence empirically less\\ninformative than features that occur in a small fraction of the training\\ncorpus. The formula that is used to compute the tf-idf for a term t of a document d\\nin a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\\ncomputed as idf(t) = log [ n / df(t) ] + 1 (if smooth_idf=False), where\\nn is the total number of documents in the document set and df(t) is the\\ndocument frequency of t; the document frequency is the number of documents\\nin the document set that contain the term t. The effect of adding “1” to\\nthe idf in the equation above is that terms with zero idf, i.e., terms\\nthat occur in all documents in a training set, will not be entirely\\nignored.\\n(Note that the idf formula above differs from the standard textbook\\nnotation that defines the idf as\\nidf(t) = log [ n / (df(t) + 1) ]). If smooth_idf=True (the default), the constant “1” is added to the\\nnumerator and denominator of the idf as if an extra document was seen\\ncontaining every term in the collection exactly once, which prevents\\nzero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1. Furthermore, the formulas used to compute tf and idf depend\\non parameter settings that correspond to the SMART notation used in IR\\nas follows: Tf is “n” (natural) by default, “l” (logarithmic) when\\nsublinear_tf=True.\\nIdf is “t” when use_idf is given, “n” (none) otherwise.\\nNormalization is “c” (cosine) when norm='l2', “n” (none)\\nwhen norm=None. Read more in the User Guide.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'sparse matrix of (n_samples, n_features). A matrix of term/token counts.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to copy X and operate on the copy or perform in-place\\noperations. copy=False will only be effective with CSR sparse matrix.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'text.HashingVectorizer',\n",
       "      'func_desc': 'Convert a collection of text documents to a matrix of token occurrences.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer',\n",
       "      'function_definitions': {'function_name': 'HashingVectorizer',\n",
       "       'full_function': \"class sklearn.feature_extraction.text.HashingVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\\\\\b\\\\\\\\w\\\\\\\\w+\\\\\\\\b', ngram_range=(1, 1), analyzer='word', n_features=1048576, binary=False, norm='l2', alternate_sign=True, dtype=<class 'numpy.float64'>)\",\n",
       "       'function_text': 'Convert a collection of text documents to a matrix of token occurrences. It turns a collection of text documents into a scipy.sparse matrix holding\\ntoken occurrence counts (or binary occurrence information), possibly\\nnormalized as token frequencies if norm=’l1’ or projected on the euclidean\\nunit sphere if norm=’l2’. This text vectorizer implementation uses the hashing trick to find the\\ntoken string name to feature integer index mapping. This strategy has several advantages: it is very low memory scalable to large datasets as there is no need to\\nstore a vocabulary dictionary in memory. it is fast to pickle and un-pickle as it holds no state besides the\\nconstructor parameters. it can be used in a streaming (partial fit) or parallel pipeline as there\\nis no state computed during fit. There are also a couple of cons (vs using a CountVectorizer with an\\nin-memory vocabulary): there is no way to compute the inverse transform (from feature indices to\\nstring feature names) which can be a problem when trying to introspect\\nwhich features are most important to a model. there can be collisions: distinct tokens can be mapped to the same\\nfeature index. However in practice this is rarely an issue if n_features\\nis large enough (e.g. 2 ** 18 for text classification problems). no IDF weighting as this would render the transformer stateful. The hash function employed is the signed 32-bit version of Murmurhash3. For an efficiency comparison of the different feature extractors, see\\nFeatureHasher and DictVectorizer Comparison. For an example of document clustering and comparison with\\nTfidfVectorizer, see\\nClustering text documents using k-means. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/text/plot_hashing_vs_dict_vectorizer.html#sphx-glr-auto-examples-text-plot-hashing-vs-dict-vectorizer-py',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'iterable over raw text documents, length = n_samples',\n",
       "         'param_desc': 'Samples. Each sample must be a text document (either bytes or\\nunicode strings, file name or file object depending on the\\nconstructor argument) which will be tokenized and hashed.\\n'}]},\n",
       "      'function_calling': {'name': 'HashingVectorizer',\n",
       "       'descriptions': 'Convert a collection of text documents to a matrix of token occurrences. It turns a collection of text documents into a scipy.sparse matrix holding\\ntoken occurrence counts (or binary occurrence information), possibly\\nnormalized as token frequencies if norm=’l1’ or projected on the euclidean\\nunit sphere if norm=’l2’. This text vectorizer implementation uses the hashing trick to find the\\ntoken string name to feature integer index mapping. This strategy has several advantages: it is very low memory scalable to large datasets as there is no need to\\nstore a vocabulary dictionary in memory. it is fast to pickle and un-pickle as it holds no state besides the\\nconstructor parameters. it can be used in a streaming (partial fit) or parallel pipeline as there\\nis no state computed during fit. There are also a couple of cons (vs using a CountVectorizer with an\\nin-memory vocabulary): there is no way to compute the inverse transform (from feature indices to\\nstring feature names) which can be a problem when trying to introspect\\nwhich features are most important to a model. there can be collisions: distinct tokens can be mapped to the same\\nfeature index. However in practice this is rarely an issue if n_features\\nis large enough (e.g. 2 ** 18 for text classification problems). no IDF weighting as this would render the transformer stateful. The hash function employed is the signed 32-bit version of Murmurhash3. For an efficiency comparison of the different feature extractors, see\\nFeatureHasher and DictVectorizer Comparison. For an example of document clustering and comparison with\\nTfidfVectorizer, see\\nClustering text documents using k-means. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'iterable over raw text documents, length = n_samples. Samples. Each sample must be a text document (either bytes or\\nunicode strings, file name or file object depending on the\\nconstructor argument) which will be tokenized and hashed.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'text.TfidfVectorizer',\n",
       "      'func_desc': 'Convert a collection of raw documents to a matrix of TF-IDF features.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer',\n",
       "      'function_definitions': {'function_name': 'TfidfVectorizer',\n",
       "       'full_function': \"class sklearn.feature_extraction.text.TfidfVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\\\\\\\b\\\\\\\\w\\\\\\\\w+\\\\\\\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\",\n",
       "       'function_text': 'Convert a collection of raw documents to a matrix of TF-IDF features. Equivalent to CountVectorizer followed by\\nTfidfTransformer. For an example of usage, see\\nClassification of text documents using sparse features. For an efficiency comparison of the different feature extractors, see\\nFeatureHasher and DictVectorizer Comparison. For an example of document clustering and comparison with\\nHashingVectorizer, see\\nClustering text documents using k-means. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer',\n",
       "       'parameter_names_desc': [{'param_name': 'raw_documents',\n",
       "         'param_type': 'iterable',\n",
       "         'param_desc': 'An iterable which generates either str, unicode or file objects.\\n'}]},\n",
       "      'function_calling': {'name': 'TfidfVectorizer',\n",
       "       'descriptions': 'Convert a collection of raw documents to a matrix of TF-IDF features. Equivalent to CountVectorizer followed by\\nTfidfTransformer. For an example of usage, see\\nClassification of text documents using sparse features. For an efficiency comparison of the different feature extractors, see\\nFeatureHasher and DictVectorizer Comparison. For an example of document clustering and comparison with\\nHashingVectorizer, see\\nClustering text documents using k-means. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'raw_documents': {'type': 'array',\n",
       "          'description': 'iterable. An iterable which generates either str, unicode or file objects.\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.feature_extraction',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.feature_extraction.html'},\n",
       " 'sklearn.feature_selection.html': {'functions': [{'defaults': [{'func_name': 'GenericUnivariateSelect',\n",
       "      'func_desc': 'Univariate feature selector with configurable strategy.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.GenericUnivariateSelect.html#sklearn.feature_selection.GenericUnivariateSelect',\n",
       "      'function_definitions': {'function_name': 'GenericUnivariateSelect',\n",
       "       'full_function': \"class sklearn.feature_selection.GenericUnivariateSelect(score_func=<function f_classif>, *, mode='percentile', param=1e-05)\",\n",
       "       'function_text': 'Univariate feature selector with configurable strategy. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array of shape [n_samples, n_features]',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'GenericUnivariateSelect',\n",
       "       'descriptions': 'Univariate feature selector with configurable strategy. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array of shape [n_samples, n_features]. The input samples.\\n'}},\n",
       "        'required': ['score_func=<function f_classif>']}}},\n",
       "     {'func_name': 'RFECV',\n",
       "      'func_desc': 'Recursive feature elimination with cross-validation to select features.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV',\n",
       "      'function_definitions': {'function_name': 'RFECV',\n",
       "       'full_function': \"class sklearn.feature_selection.RFECV(estimator, *, step=1, min_features_to_select=1, cv=None, scoring=None, verbose=0, n_jobs=None, importance_getter='auto')\",\n",
       "       'function_text': 'Recursive feature elimination with cross-validation to select features. The number of features selected is tuned automatically by fitting an RFE\\nselector on the different cross-validation splits (provided by the cv parameter).\\nThe performance of the RFE selector are evaluated using scorer for\\ndifferent number of selected features and aggregated together. Finally, the scores\\nare averaged across folds and the number of features selected is set to the number\\nof features that maximize the cross-validation score.\\nSee glossary entry for cross-validation estimator. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array of shape [n_samples, n_features]',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'RFECV',\n",
       "       'descriptions': 'Recursive feature elimination with cross-validation to select features. The number of features selected is tuned automatically by fitting an RFE\\nselector on the different cross-validation splits (provided by the cv parameter).\\nThe performance of the RFE selector are evaluated using scorer for\\ndifferent number of selected features and aggregated together. Finally, the scores\\nare averaged across folds and the number of features selected is set to the number\\nof features that maximize the cross-validation score.\\nSee glossary entry for cross-validation estimator. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array of shape [n_samples, n_features]. The input samples.\\n'}},\n",
       "        'required': ['estimator']}}},\n",
       "     {'func_name': 'SelectFpr',\n",
       "      'func_desc': 'Filter: Select the pvalues below alpha based on a FPR test.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFpr.html#sklearn.feature_selection.SelectFpr',\n",
       "      'function_definitions': {'function_name': 'SelectFpr',\n",
       "       'full_function': 'class sklearn.feature_selection.SelectFpr(score_func=<function f_classif>, *, alpha=0.05)',\n",
       "       'function_text': 'Filter: Select the pvalues below alpha based on a FPR test. FPR test stands for False Positive Rate test. It controls the total\\namount of false detections. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array of shape [n_samples, n_features]',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'SelectFpr',\n",
       "       'descriptions': 'Filter: Select the pvalues below alpha based on a FPR test. FPR test stands for False Positive Rate test. It controls the total\\namount of false detections. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array of shape [n_samples, n_features]. The input samples.\\n'}},\n",
       "        'required': ['score_func=<function f_classif>']}}},\n",
       "     {'func_name': 'SelectFwe',\n",
       "      'func_desc': 'Filter: Select the p-values corresponding to Family-wise error rate.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFwe.html#sklearn.feature_selection.SelectFwe',\n",
       "      'function_definitions': {'function_name': 'SelectFwe',\n",
       "       'full_function': 'class sklearn.feature_selection.SelectFwe(score_func=<function f_classif>, *, alpha=0.05)',\n",
       "       'function_text': 'Filter: Select the p-values corresponding to Family-wise error rate. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array of shape [n_samples, n_features]',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'SelectFwe',\n",
       "       'descriptions': 'Filter: Select the p-values corresponding to Family-wise error rate. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array of shape [n_samples, n_features]. The input samples.\\n'}},\n",
       "        'required': ['score_func=<function f_classif>']}}},\n",
       "     {'func_name': 'SelectPercentile',\n",
       "      'func_desc': 'Select features according to a percentile of the highest scores.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile',\n",
       "      'function_definitions': {'function_name': 'SelectPercentile',\n",
       "       'full_function': 'class sklearn.feature_selection.SelectPercentile(score_func=<function f_classif>, *, percentile=10)',\n",
       "       'function_text': 'Select features according to a percentile of the highest scores. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array of shape [n_samples, n_features]',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'SelectPercentile',\n",
       "       'descriptions': 'Select features according to a percentile of the highest scores. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array of shape [n_samples, n_features]. The input samples.\\n'}},\n",
       "        'required': ['score_func=<function f_classif>']}}},\n",
       "     {'func_name': 'SequentialFeatureSelector',\n",
       "      'func_desc': 'Transformer that performs Sequential Feature Selection.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector',\n",
       "      'function_definitions': {'function_name': 'SequentialFeatureSelector',\n",
       "       'full_function': \"class sklearn.feature_selection.SequentialFeatureSelector(estimator, *, n_features_to_select='auto', tol=None, direction='forward', scoring=None, cv=5, n_jobs=None)\",\n",
       "       'function_text': 'Transformer that performs Sequential Feature Selection. This Sequential Feature Selector adds (forward selection) or\\nremoves (backward selection) features to form a feature subset in a\\ngreedy fashion. At each stage, this estimator chooses the best feature to\\nadd or remove based on the cross-validation score of an estimator. In\\nthe case of unsupervised learning, this Sequential Feature Selector\\nlooks only at the features (X), not the desired outputs (y). Read more in the User Guide. Added in version 0.24.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_selection.html#sequential-feature-selection',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array of shape [n_samples, n_features]',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'SequentialFeatureSelector',\n",
       "       'descriptions': 'Transformer that performs Sequential Feature Selection. This Sequential Feature Selector adds (forward selection) or\\nremoves (backward selection) features to form a feature subset in a\\ngreedy fashion. At each stage, this estimator chooses the best feature to\\nadd or remove based on the cross-validation score of an estimator. In\\nthe case of unsupervised learning, this Sequential Feature Selector\\nlooks only at the features (X), not the desired outputs (y). Read more in the User Guide. Added in version 0.24.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array of shape [n_samples, n_features]. The input samples.\\n'}},\n",
       "        'required': ['estimator']}}},\n",
       "     {'func_name': 'chi2',\n",
       "      'func_desc': 'Compute chi-squared stats between each non-negative feature and class.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2',\n",
       "      'function_definitions': {'function_name': 'chi2',\n",
       "       'full_function': 'sklearn.feature_selection.chi2(X, y)',\n",
       "       'function_text': 'Compute chi-squared stats between each non-negative feature and class. This score can be used to select the n_features features with the\\nhighest values for the test chi-squared statistic from X, which must\\ncontain only non-negative features such as booleans or frequencies\\n(e.g., term counts in document classification), relative to the classes. Recall that the chi-square test measures dependence between stochastic\\nvariables, so using this function “weeds out” the features that are the\\nmost likely to be independent of class and therefore irrelevant for\\nclassification. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Sample vectors.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Target vector (class labels).\\n'}]},\n",
       "      'function_calling': {'name': 'chi2',\n",
       "       'descriptions': 'Compute chi-squared stats between each non-negative feature and class. This score can be used to select the n_features features with the\\nhighest values for the test chi-squared statistic from X, which must\\ncontain only non-negative features such as booleans or frequencies\\n(e.g., term counts in document classification), relative to the classes. Recall that the chi-square test measures dependence between stochastic\\nvariables, so using this function “weeds out” the features that are the\\nmost likely to be independent of class and therefore irrelevant for\\nclassification. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Sample vectors.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Target vector (class labels).\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'f_regression',\n",
       "      'func_desc': 'Univariate linear regression tests returning F-statistic and p-values.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression',\n",
       "      'function_definitions': {'function_name': 'f_regression',\n",
       "       'full_function': 'sklearn.feature_selection.f_regression(X, y, *, center=True, force_finite=True)',\n",
       "       'function_text': 'Univariate linear regression tests returning F-statistic and p-values. Quick linear model for testing the effect of a single regressor,\\nsequentially for many regressors. This is done in 2 steps: The cross correlation between each regressor and the target is computed\\nusing r_regression as: It is converted to an F score and then to a p-value. f_regression is derived from r_regression and will rank\\nfeatures in the same order if all the features are positively correlated\\nwith the target. Note however that contrary to f_regression, r_regression\\nvalues lie in [-1, 1] and can thus be negative. f_regression is\\ntherefore recommended as a feature selection criterion to identify\\npotentially predictive feature for a downstream classifier, irrespective of\\nthe sign of the association with the target variable. Furthermore f_regression returns p-values while\\nr_regression does not. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.feature_selection.r_regression.html#sklearn.feature_selection.r_regression',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data matrix.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'The target vector.\\n'},\n",
       "        {'param_name': 'center',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether or not to center the data matrix X and the target vector y.\\nBy default, X and y will be centered.\\n'},\n",
       "        {'param_name': 'force_finite',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether or not to force the F-statistics and associated p-values to\\nbe finite. There are two cases where the F-statistic is expected to not\\nbe finite:\\n\\nwhen the target y or some features in X are constant. In this\\ncase, the Pearson’s R correlation is not defined leading to obtain\\nnp.nan values in the F-statistic and p-value. When\\nforce_finite=True, the F-statistic is set to 0.0 and the\\nassociated p-value is set to 1.0.\\nwhen a feature in X is perfectly correlated (or\\nanti-correlated) with the target y. In this case, the F-statistic\\nis expected to be np.inf. When force_finite=True, the F-statistic\\nis set to np.finfo(dtype).max and the associated p-value is set to\\n0.0.\\n\\n\\nAdded in version 1.1.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'f_regression',\n",
       "       'descriptions': 'Univariate linear regression tests returning F-statistic and p-values. Quick linear model for testing the effect of a single regressor,\\nsequentially for many regressors. This is done in 2 steps: The cross correlation between each regressor and the target is computed\\nusing r_regression as: It is converted to an F score and then to a p-value. f_regression is derived from r_regression and will rank\\nfeatures in the same order if all the features are positively correlated\\nwith the target. Note however that contrary to f_regression, r_regression\\nvalues lie in [-1, 1] and can thus be negative. f_regression is\\ntherefore recommended as a feature selection criterion to identify\\npotentially predictive feature for a downstream classifier, irrespective of\\nthe sign of the association with the target variable. Furthermore f_regression returns p-values while\\nr_regression does not. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data matrix.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). The target vector.\\n'},\n",
       "         'center': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether or not to center the data matrix X and the target vector y.\\nBy default, X and y will be centered.\\n'},\n",
       "         'force_finite': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether or not to force the F-statistics and associated p-values to\\nbe finite. There are two cases where the F-statistic is expected to not\\nbe finite:\\n\\nwhen the target y or some features in X are constant. In this\\ncase, the Pearson’s R correlation is not defined leading to obtain\\nnp.nan values in the F-statistic and p-value. When\\nforce_finite=True, the F-statistic is set to 0.0 and the\\nassociated p-value is set to 1.0.\\nwhen a feature in X is perfectly correlated (or\\nanti-correlated) with the target y. In this case, the F-statistic\\nis expected to be np.inf. When force_finite=True, the F-statistic\\nis set to np.finfo(dtype).max and the associated p-value is set to\\n0.0.\\n\\n\\nAdded in version 1.1.\\n\\n'}},\n",
       "        'required': ['X', 'y']}}},\n",
       "     {'func_name': 'mutual_info_regression',\n",
       "      'func_desc': 'Estimate mutual information for a continuous target variable.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression',\n",
       "      'function_definitions': {'function_name': 'mutual_info_regression',\n",
       "       'full_function': \"sklearn.feature_selection.mutual_info_regression(X, y, *, discrete_features='auto', n_neighbors=3, copy=True, random_state=None, n_jobs=None)\",\n",
       "       'function_text': 'Estimate mutual information for a continuous target variable. Mutual information (MI) [1] between two random variables is a non-negative\\nvalue, which measures the dependency between the variables. It is equal\\nto zero if and only if two random variables are independent, and higher\\nvalues mean higher dependency. The function relies on nonparametric methods based on entropy estimation\\nfrom k-nearest neighbors distances as described in [2] and [3]. Both\\nmethods are based on the idea originally proposed in [4]. It can be used for univariate features selection, read more in the\\nUser Guide.',\n",
       "       'func_text_user_guide': '#r37d39d7589e2-1',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like or sparse matrix, shape (n_samples, n_features)',\n",
       "         'param_desc': 'Feature matrix.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Target vector.\\n'},\n",
       "        {'param_name': 'discrete_features',\n",
       "         'param_type': '{‘auto’, bool, array-like}, default=’auto’',\n",
       "         'param_desc': 'If bool, then determines whether to consider all features discrete\\nor continuous. If array, then it should be either a boolean mask\\nwith shape (n_features,) or array with indices of discrete features.\\nIf ‘auto’, it is assigned to False for dense X and to True for\\nsparse X.\\n'},\n",
       "        {'param_name': 'n_neighbors',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of neighbors to use for MI estimation for continuous variables,\\nsee [2] and [3]. Higher values reduce variance of the estimation, but\\ncould introduce a bias.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to make a copy of the given data. If set to False, the initial\\ndata will be overwritten.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for adding small noise to\\ncontinuous variables in order to remove repeated values.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of jobs to use for computing the mutual information.\\nThe parallelization is done on the columns of X.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n\\nAdded in version 1.5.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'mutual_info_regression',\n",
       "       'descriptions': 'Estimate mutual information for a continuous target variable. Mutual information (MI) [1] between two random variables is a non-negative\\nvalue, which measures the dependency between the variables. It is equal\\nto zero if and only if two random variables are independent, and higher\\nvalues mean higher dependency. The function relies on nonparametric methods based on entropy estimation\\nfrom k-nearest neighbors distances as described in [2] and [3]. Both\\nmethods are based on the idea originally proposed in [4]. It can be used for univariate features selection, read more in the\\nUser Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like or sparse matrix, shape (n_samples, n_features). Feature matrix.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Target vector.\\n'},\n",
       "         'discrete_features': {'type': 'boolean',\n",
       "          'description': '{‘auto’, bool, array-like}, default=’auto’. If bool, then determines whether to consider all features discrete\\nor continuous. If array, then it should be either a boolean mask\\nwith shape (n_features,) or array with indices of discrete features.\\nIf ‘auto’, it is assigned to False for dense X and to True for\\nsparse X.\\n'},\n",
       "         'n_neighbors': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of neighbors to use for MI estimation for continuous variables,\\nsee [2] and [3]. Higher values reduce variance of the estimation, but\\ncould introduce a bias.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to make a copy of the given data. If set to False, the initial\\ndata will be overwritten.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for adding small noise to\\ncontinuous variables in order to remove repeated values.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of jobs to use for computing the mutual information.\\nThe parallelization is done on the columns of X.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n\\nAdded in version 1.5.\\n\\n'}},\n",
       "        'required': ['X', 'y']}}},\n",
       "     {'func_name': 'RFE',\n",
       "      'func_desc': 'Feature ranking with recursive feature elimination.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE',\n",
       "      'function_definitions': {'function_name': 'RFE',\n",
       "       'full_function': \"class sklearn.feature_selection.RFE(estimator, *, n_features_to_select=None, step=1, verbose=0, importance_getter='auto')\",\n",
       "       'function_text': 'Feature ranking with recursive feature elimination. Given an external estimator that assigns weights to features (e.g., the\\ncoefficients of a linear model), the goal of recursive feature elimination\\n(RFE) is to select features by recursively considering smaller and smaller\\nsets of features. First, the estimator is trained on the initial set of\\nfeatures and the importance of each feature is obtained either through\\nany specific attribute or callable.\\nThen, the least important features are pruned from current set of features.\\nThat procedure is recursively repeated on the pruned set until the desired\\nnumber of features to select is eventually reached. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_selection.html#rfe',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array of shape [n_samples, n_features]',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'RFE',\n",
       "       'descriptions': 'Feature ranking with recursive feature elimination. Given an external estimator that assigns weights to features (e.g., the\\ncoefficients of a linear model), the goal of recursive feature elimination\\n(RFE) is to select features by recursively considering smaller and smaller\\nsets of features. First, the estimator is trained on the initial set of\\nfeatures and the importance of each feature is obtained either through\\nany specific attribute or callable.\\nThen, the least important features are pruned from current set of features.\\nThat procedure is recursively repeated on the pruned set until the desired\\nnumber of features to select is eventually reached. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array of shape [n_samples, n_features]. The input samples.\\n'}},\n",
       "        'required': ['estimator']}}},\n",
       "     {'func_name': 'SelectFdr',\n",
       "      'func_desc': 'Filter: Select the p-values for an estimated false discovery rate.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFdr.html#sklearn.feature_selection.SelectFdr',\n",
       "      'function_definitions': {'function_name': 'SelectFdr',\n",
       "       'full_function': 'class sklearn.feature_selection.SelectFdr(score_func=<function f_classif>, *, alpha=0.05)',\n",
       "       'function_text': 'Filter: Select the p-values for an estimated false discovery rate. This uses the Benjamini-Hochberg procedure. alpha is an upper bound\\non the expected false discovery rate. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array of shape [n_samples, n_features]',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'SelectFdr',\n",
       "       'descriptions': 'Filter: Select the p-values for an estimated false discovery rate. This uses the Benjamini-Hochberg procedure. alpha is an upper bound\\non the expected false discovery rate. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array of shape [n_samples, n_features]. The input samples.\\n'}},\n",
       "        'required': ['score_func=<function f_classif>']}}},\n",
       "     {'func_name': 'SelectFromModel',\n",
       "      'func_desc': 'Meta-transformer for selecting features based on importance weights.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel',\n",
       "      'function_definitions': {'function_name': 'SelectFromModel',\n",
       "       'full_function': \"class sklearn.feature_selection.SelectFromModel(estimator, *, threshold=None, prefit=False, norm_order=1, max_features=None, importance_getter='auto')\",\n",
       "       'function_text': 'Meta-transformer for selecting features based on importance weights. Added in version 0.17. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_selection.html#select-from-model',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array of shape [n_samples, n_features]',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'SelectFromModel',\n",
       "       'descriptions': 'Meta-transformer for selecting features based on importance weights. Added in version 0.17. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array of shape [n_samples, n_features]. The input samples.\\n'}},\n",
       "        'required': ['estimator']}}},\n",
       "     {'func_name': 'SelectKBest',\n",
       "      'func_desc': 'Select features according to the k highest scores.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest',\n",
       "      'function_definitions': {'function_name': 'SelectKBest',\n",
       "       'full_function': 'class sklearn.feature_selection.SelectKBest(score_func=<function f_classif>, *, k=10)',\n",
       "       'function_text': 'Select features according to the k highest scores. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array of shape [n_samples, n_features]',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'SelectKBest',\n",
       "       'descriptions': 'Select features according to the k highest scores. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array of shape [n_samples, n_features]. The input samples.\\n'}},\n",
       "        'required': ['score_func=<function f_classif>']}}},\n",
       "     {'func_name': 'SelectorMixin',\n",
       "      'func_desc': 'Transformer mixin that performs feature selection given a support mask',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectorMixin.html#sklearn.feature_selection.SelectorMixin',\n",
       "      'function_definitions': {'function_name': 'SelectorMixin',\n",
       "       'full_function': 'class sklearn.feature_selection.SelectorMixin',\n",
       "       'function_text': 'Transformer mixin that performs feature selection given a support mask This mixin provides a feature selector implementation with transform and\\ninverse_transform functionality given an implementation of\\n_get_support_mask. Examples Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params\\nand returns a transformed version of X.',\n",
       "       'func_text_user_guide': '#sklearn.feature_selection.SelectorMixin.transform',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array of shape [n_samples, n_features]',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'SelectorMixin',\n",
       "       'descriptions': 'Transformer mixin that performs feature selection given a support mask This mixin provides a feature selector implementation with transform and\\ninverse_transform functionality given an implementation of\\n_get_support_mask. Examples Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params\\nand returns a transformed version of X.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array of shape [n_samples, n_features]. The input samples.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'VarianceThreshold',\n",
       "      'func_desc': 'Feature selector that removes all low-variance features.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold',\n",
       "      'function_definitions': {'function_name': 'VarianceThreshold',\n",
       "       'full_function': 'class sklearn.feature_selection.VarianceThreshold(threshold=0.0)',\n",
       "       'function_text': 'Feature selector that removes all low-variance features. This feature selection algorithm looks only at the features (X), not the\\ndesired outputs (y), and can thus be used for unsupervised learning. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_selection.html#variance-threshold',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array of shape [n_samples, n_features]',\n",
       "         'param_desc': 'The input samples.\\n'}]},\n",
       "      'function_calling': {'name': 'VarianceThreshold',\n",
       "       'descriptions': 'Feature selector that removes all low-variance features. This feature selection algorithm looks only at the features (X), not the\\ndesired outputs (y), and can thus be used for unsupervised learning. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array of shape [n_samples, n_features]. The input samples.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'f_classif',\n",
       "      'func_desc': 'Compute the ANOVA F-value for the provided sample.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif',\n",
       "      'function_definitions': {'function_name': 'f_classif',\n",
       "       'full_function': 'sklearn.feature_selection.f_classif(X, y)',\n",
       "       'function_text': 'Compute the ANOVA F-value for the provided sample. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The set of regressors that will be tested sequentially.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'The target vector.\\n'}]},\n",
       "      'function_calling': {'name': 'f_classif',\n",
       "       'descriptions': 'Compute the ANOVA F-value for the provided sample. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The set of regressors that will be tested sequentially.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). The target vector.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'mutual_info_classif',\n",
       "      'func_desc': 'Estimate mutual information for a discrete target variable.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif',\n",
       "      'function_definitions': {'function_name': 'mutual_info_classif',\n",
       "       'full_function': \"sklearn.feature_selection.mutual_info_classif(X, y, *, discrete_features='auto', n_neighbors=3, copy=True, random_state=None, n_jobs=None)\",\n",
       "       'function_text': 'Estimate mutual information for a discrete target variable. Mutual information (MI) [1] between two random variables is a non-negative\\nvalue, which measures the dependency between the variables. It is equal\\nto zero if and only if two random variables are independent, and higher\\nvalues mean higher dependency. The function relies on nonparametric methods based on entropy estimation\\nfrom k-nearest neighbors distances as described in [2] and [3]. Both\\nmethods are based on the idea originally proposed in [4]. It can be used for univariate features selection, read more in the\\nUser Guide.',\n",
       "       'func_text_user_guide': '#r50b872b699c4-1',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Feature matrix.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Target vector.\\n'},\n",
       "        {'param_name': 'discrete_features',\n",
       "         'param_type': '‘auto’, bool or array-like, default=’auto’',\n",
       "         'param_desc': 'If bool, then determines whether to consider all features discrete\\nor continuous. If array, then it should be either a boolean mask\\nwith shape (n_features,) or array with indices of discrete features.\\nIf ‘auto’, it is assigned to False for dense X and to True for\\nsparse X.\\n'},\n",
       "        {'param_name': 'n_neighbors',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of neighbors to use for MI estimation for continuous variables,\\nsee [2] and [3]. Higher values reduce variance of the estimation, but\\ncould introduce a bias.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to make a copy of the given data. If set to False, the initial\\ndata will be overwritten.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for adding small noise to\\ncontinuous variables in order to remove repeated values.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of jobs to use for computing the mutual information.\\nThe parallelization is done on the columns of X.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n\\nAdded in version 1.5.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'mutual_info_classif',\n",
       "       'descriptions': 'Estimate mutual information for a discrete target variable. Mutual information (MI) [1] between two random variables is a non-negative\\nvalue, which measures the dependency between the variables. It is equal\\nto zero if and only if two random variables are independent, and higher\\nvalues mean higher dependency. The function relies on nonparametric methods based on entropy estimation\\nfrom k-nearest neighbors distances as described in [2] and [3]. Both\\nmethods are based on the idea originally proposed in [4]. It can be used for univariate features selection, read more in the\\nUser Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Feature matrix.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Target vector.\\n'},\n",
       "         'discrete_features': {'type': 'boolean',\n",
       "          'description': '‘auto’, bool or array-like, default=’auto’. If bool, then determines whether to consider all features discrete\\nor continuous. If array, then it should be either a boolean mask\\nwith shape (n_features,) or array with indices of discrete features.\\nIf ‘auto’, it is assigned to False for dense X and to True for\\nsparse X.\\n'},\n",
       "         'n_neighbors': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of neighbors to use for MI estimation for continuous variables,\\nsee [2] and [3]. Higher values reduce variance of the estimation, but\\ncould introduce a bias.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to make a copy of the given data. If set to False, the initial\\ndata will be overwritten.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for adding small noise to\\ncontinuous variables in order to remove repeated values.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of jobs to use for computing the mutual information.\\nThe parallelization is done on the columns of X.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n\\nAdded in version 1.5.\\n\\n'}},\n",
       "        'required': ['X', 'y']}}},\n",
       "     {'func_name': 'r_regression',\n",
       "      'func_desc': \"Compute Pearson's r for each features and the target.\",\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.r_regression.html#sklearn.feature_selection.r_regression',\n",
       "      'function_definitions': {'function_name': 'r_regression',\n",
       "       'full_function': 'sklearn.feature_selection.r_regression(X, y, *, center=True, force_finite=True)',\n",
       "       'function_text': 'Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors.\\nThis is a scoring function to be used in a feature selection procedure, not\\na free standing feature selection procedure. The cross correlation between each regressor and the target is computed\\nas: For more on usage see the User Guide. Added in version 1.0.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data matrix.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'The target vector.\\n'},\n",
       "        {'param_name': 'center',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether or not to center the data matrix X and the target vector y.\\nBy default, X and y will be centered.\\n'},\n",
       "        {'param_name': 'force_finite',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether or not to force the Pearson’s R correlation to be finite.\\nIn the particular case where some features in X or the target y\\nare constant, the Pearson’s R correlation is not defined. When\\nforce_finite=False, a correlation of np.nan is returned to\\nacknowledge this case. When force_finite=True, this value will be\\nforced to a minimal correlation of 0.0.\\n\\nAdded in version 1.1.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'r_regression',\n",
       "       'descriptions': 'Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors.\\nThis is a scoring function to be used in a feature selection procedure, not\\na free standing feature selection procedure. The cross correlation between each regressor and the target is computed\\nas: For more on usage see the User Guide. Added in version 1.0.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data matrix.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). The target vector.\\n'},\n",
       "         'center': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether or not to center the data matrix X and the target vector y.\\nBy default, X and y will be centered.\\n'},\n",
       "         'force_finite': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether or not to force the Pearson’s R correlation to be finite.\\nIn the particular case where some features in X or the target y\\nare constant, the Pearson’s R correlation is not defined. When\\nforce_finite=False, a correlation of np.nan is returned to\\nacknowledge this case. When force_finite=True, this value will be\\nforced to a minimal correlation of 0.0.\\n\\nAdded in version 1.1.\\n\\n'}},\n",
       "        'required': ['X', 'y']}}}]}],\n",
       "  'name': 'sklearn.feature_selection',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.feature_selection.html'},\n",
       " 'sklearn.gaussian_process.html': {'functions': [{'defaults': [{'func_name': 'GaussianProcessClassifier',\n",
       "      'func_desc': 'Gaussian process classification (GPC) based on Laplace approximation.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier',\n",
       "      'function_definitions': {'function_name': 'GaussianProcessClassifier',\n",
       "       'full_function': \"class sklearn.gaussian_process.GaussianProcessClassifier(kernel=None, *, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=0, max_iter_predict=100, warm_start=False, copy_X_train=True, random_state=None, multi_class='one_vs_rest', n_jobs=None)\",\n",
       "       'function_text': 'Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 from [RW2006]. Internally, the Laplace approximation is used for approximating the\\nnon-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link\\nfunction. For multi-class classification, several binary one-versus rest\\nclassifiers are fitted. Note that this class thus does not implement\\na true multi-class Laplace approximation. Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': '#r2da648a61a73-rw2006',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'GaussianProcessClassifier',\n",
       "       'descriptions': 'Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 from [RW2006]. Internally, the Laplace approximation is used for approximating the\\nnon-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link\\nfunction. For multi-class classification, several binary one-versus rest\\nclassifiers are fitted. Note that this class thus does not implement\\na true multi-class Laplace approximation. Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['kernel=None']}}},\n",
       "     {'func_name': 'GaussianProcessRegressor',\n",
       "      'func_desc': 'Gaussian process regression (GPR).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor',\n",
       "      'function_definitions': {'function_name': 'GaussianProcessRegressor',\n",
       "       'full_function': \"class sklearn.gaussian_process.GaussianProcessRegressor(kernel=None, *, alpha=1e-10, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=0, normalize_y=False, copy_X_train=True, n_targets=None, random_state=None)\",\n",
       "       'function_text': 'Gaussian process regression (GPR). The implementation is based on Algorithm 2.1 of [RW2006]. In addition to standard scikit-learn estimator API,\\nGaussianProcessRegressor: allows prediction without prior fitting (based on the GP prior) provides an additional method sample_y(X), which evaluates samples\\ndrawn from the GPR (prior or posterior) at given inputs exposes a method log_marginal_likelihood(theta), which can be used\\nexternally for other ways of selecting hyperparameters, e.g., via\\nMarkov chain Monte Carlo. To learn the difference between a point-estimate approach vs. a more\\nBayesian modelling approach, refer to the example entitled\\nComparison of kernel ridge and Gaussian process regression. Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': '#rf75674b0f418-rw2006',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'GaussianProcessRegressor',\n",
       "       'descriptions': 'Gaussian process regression (GPR). The implementation is based on Algorithm 2.1 of [RW2006]. In addition to standard scikit-learn estimator API,\\nGaussianProcessRegressor: allows prediction without prior fitting (based on the GP prior) provides an additional method sample_y(X), which evaluates samples\\ndrawn from the GPR (prior or posterior) at given inputs exposes a method log_marginal_likelihood(theta), which can be used\\nexternally for other ways of selecting hyperparameters, e.g., via\\nMarkov chain Monte Carlo. To learn the difference between a point-estimate approach vs. a more\\nBayesian modelling approach, refer to the example entitled\\nComparison of kernel ridge and Gaussian process regression. Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['kernel=None']}}}]},\n",
       "   {'Kernels': [{'func_name': 'kernels.CompoundKernel',\n",
       "      'func_desc': 'Kernel which is composed of a set of other kernels.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.CompoundKernel.html#sklearn.gaussian_process.kernels.CompoundKernel',\n",
       "      'function_definitions': {'function_name': 'CompoundKernel',\n",
       "       'full_function': 'class sklearn.gaussian_process.kernels.CompoundKernel(kernels)',\n",
       "       'function_text': 'Kernel which is composed of a set of other kernels. Added in version 0.18.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'theta',\n",
       "         'param_type': 'ndarray of shape (n_dims,)',\n",
       "         'param_desc': 'The non-fixed, log-transformed hyperparameters of the kernel\\n'}]},\n",
       "      'function_calling': {'name': 'CompoundKernel',\n",
       "       'descriptions': 'Kernel which is composed of a set of other kernels. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'theta': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_dims,). The non-fixed, log-transformed hyperparameters of the kernel\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'kernels.DotProduct',\n",
       "      'func_desc': 'Dot-Product kernel.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.DotProduct.html#sklearn.gaussian_process.kernels.DotProduct',\n",
       "      'function_definitions': {'function_name': 'DotProduct',\n",
       "       'full_function': 'class sklearn.gaussian_process.kernels.DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-05, 100000.0))',\n",
       "       'function_text': 'Dot-Product kernel. The DotProduct kernel is non-stationary and can be obtained from linear\\nregression by putting \\\\(N(0, 1)\\\\) priors on the coefficients\\nof \\\\(x_d (d = 1, . . . , D)\\\\) and a prior of \\\\(N(0, \\\\sigma_0^2)\\\\)\\non the bias. The DotProduct kernel is invariant to a rotation of\\nthe coordinates about the origin, but not translations.\\nIt is parameterized by a parameter sigma_0 \\\\(\\\\sigma\\\\)\\nwhich controls the inhomogenity of the kernel. For \\\\(\\\\sigma_0^2 =0\\\\),\\nthe kernel is called the homogeneous linear kernel, otherwise\\nit is inhomogeneous. The kernel is given by The DotProduct kernel is commonly combined with exponentiation. See [1], Chapter 4, Section 4.2, for further details regarding the\\nDotProduct kernel. Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': '#r95f74c4622c1-1',\n",
       "       'parameter_names_desc': [{'param_name': 'theta',\n",
       "         'param_type': 'ndarray of shape (n_dims,)',\n",
       "         'param_desc': 'The non-fixed, log-transformed hyperparameters of the kernel\\n'}]},\n",
       "      'function_calling': {'name': 'DotProduct',\n",
       "       'descriptions': 'Dot-Product kernel. The DotProduct kernel is non-stationary and can be obtained from linear\\nregression by putting \\\\(N(0, 1)\\\\) priors on the coefficients\\nof \\\\(x_d (d = 1, . . . , D)\\\\) and a prior of \\\\(N(0, \\\\sigma_0^2)\\\\)\\non the bias. The DotProduct kernel is invariant to a rotation of\\nthe coordinates about the origin, but not translations.\\nIt is parameterized by a parameter sigma_0 \\\\(\\\\sigma\\\\)\\nwhich controls the inhomogenity of the kernel. For \\\\(\\\\sigma_0^2 =0\\\\),\\nthe kernel is called the homogeneous linear kernel, otherwise\\nit is inhomogeneous. The kernel is given by The DotProduct kernel is commonly combined with exponentiation. See [1], Chapter 4, Section 4.2, for further details regarding the\\nDotProduct kernel. Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'theta': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_dims,). The non-fixed, log-transformed hyperparameters of the kernel\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'kernels.Exponentiation',\n",
       "      'func_desc': 'The Exponentiation kernel takes one base kernel and a scalar parameter \\\\(p\\\\) and combines them via',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Exponentiation.html#sklearn.gaussian_process.kernels.Exponentiation',\n",
       "      'function_definitions': {'function_name': 'Exponentiation',\n",
       "       'full_function': 'class sklearn.gaussian_process.kernels.Exponentiation(kernel, exponent)',\n",
       "       'function_text': 'The Exponentiation kernel takes one base kernel and a scalar parameter\\n\\\\(p\\\\) and combines them via Note that the __pow__ magic method is overridden, so\\nExponentiation(RBF(), 2) is equivalent to using the ** operator\\nwith RBF() ** 2. Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/gaussian_process.html#gp-kernels',\n",
       "       'parameter_names_desc': [{'param_name': 'theta',\n",
       "         'param_type': 'ndarray of shape (n_dims,)',\n",
       "         'param_desc': 'The non-fixed, log-transformed hyperparameters of the kernel\\n'}]},\n",
       "      'function_calling': {'name': 'Exponentiation',\n",
       "       'descriptions': 'The Exponentiation kernel takes one base kernel and a scalar parameter\\n\\\\(p\\\\) and combines them via Note that the __pow__ magic method is overridden, so\\nExponentiation(RBF(), 2) is equivalent to using the ** operator\\nwith RBF() ** 2. Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'theta': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_dims,). The non-fixed, log-transformed hyperparameters of the kernel\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'kernels.Kernel',\n",
       "      'func_desc': 'Base class for all kernels.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Kernel.html#sklearn.gaussian_process.kernels.Kernel',\n",
       "      'function_definitions': {'function_name': 'Kernel',\n",
       "       'full_function': 'class sklearn.gaussian_process.kernels.Kernel',\n",
       "       'function_text': 'Base class for all kernels. Added in version 0.18. Examples Evaluate the kernel. Returns the log-transformed bounds on the theta.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'kernels.PairwiseKernel',\n",
       "      'func_desc': 'Wrapper for kernels in sklearn.metrics.pairwise.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.PairwiseKernel.html#sklearn.gaussian_process.kernels.PairwiseKernel',\n",
       "      'function_definitions': {'function_name': 'PairwiseKernel',\n",
       "       'full_function': \"class sklearn.gaussian_process.kernels.PairwiseKernel(gamma=1.0, gamma_bounds=(1e-05, 100000.0), metric='linear', pairwise_kernels_kwargs=None)\",\n",
       "       'function_text': 'Wrapper for kernels in sklearn.metrics.pairwise. A thin wrapper around the functionality of the kernels in\\nsklearn.metrics.pairwise. kernels support only isotropic distances. The parameter gamma is\\nconsidered to be a hyperparameter and may be optimized. The other\\nkernel parameters are set directly at initialization and are kept\\nfixed. Added in version 0.18.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'theta',\n",
       "         'param_type': 'ndarray of shape (n_dims,)',\n",
       "         'param_desc': 'The non-fixed, log-transformed hyperparameters of the kernel\\n'}]},\n",
       "      'function_calling': {'name': 'PairwiseKernel',\n",
       "       'descriptions': 'Wrapper for kernels in sklearn.metrics.pairwise. A thin wrapper around the functionality of the kernels in\\nsklearn.metrics.pairwise. kernels support only isotropic distances. The parameter gamma is\\nconsidered to be a hyperparameter and may be optimized. The other\\nkernel parameters are set directly at initialization and are kept\\nfixed. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'theta': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_dims,). The non-fixed, log-transformed hyperparameters of the kernel\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'kernels.RBF',\n",
       "      'func_desc': 'Radial basis function kernel (aka squared-exponential kernel).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF',\n",
       "      'function_definitions': {'function_name': 'RBF',\n",
       "       'full_function': 'class sklearn.gaussian_process.kernels.RBF(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0))',\n",
       "       'function_text': 'Radial basis function kernel (aka squared-exponential kernel). The RBF kernel is a stationary kernel. It is also known as the\\n“squared exponential” kernel. It is parameterized by a length scale\\nparameter \\\\(l>0\\\\), which can either be a scalar (isotropic variant\\nof the kernel) or a vector with the same number of dimensions as the inputs\\nX (anisotropic variant of the kernel). The kernel is given by: where \\\\(l\\\\) is the length scale of the kernel and\\n\\\\(d(\\\\cdot,\\\\cdot)\\\\) is the Euclidean distance.\\nFor advice on how to set the length scale parameter, see e.g. [1]. This kernel is infinitely differentiable, which implies that GPs with this\\nkernel as covariance function have mean square derivatives of all orders,\\nand are thus very smooth.\\nSee [2], Chapter 4, Section 4.2, for further details of the RBF kernel. Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': '#redc669bcbe98-1',\n",
       "       'parameter_names_desc': [{'param_name': 'theta',\n",
       "         'param_type': 'ndarray of shape (n_dims,)',\n",
       "         'param_desc': 'The non-fixed, log-transformed hyperparameters of the kernel\\n'}]},\n",
       "      'function_calling': {'name': 'RBF',\n",
       "       'descriptions': 'Radial basis function kernel (aka squared-exponential kernel). The RBF kernel is a stationary kernel. It is also known as the\\n“squared exponential” kernel. It is parameterized by a length scale\\nparameter \\\\(l>0\\\\), which can either be a scalar (isotropic variant\\nof the kernel) or a vector with the same number of dimensions as the inputs\\nX (anisotropic variant of the kernel). The kernel is given by: where \\\\(l\\\\) is the length scale of the kernel and\\n\\\\(d(\\\\cdot,\\\\cdot)\\\\) is the Euclidean distance.\\nFor advice on how to set the length scale parameter, see e.g. [1]. This kernel is infinitely differentiable, which implies that GPs with this\\nkernel as covariance function have mean square derivatives of all orders,\\nand are thus very smooth.\\nSee [2], Chapter 4, Section 4.2, for further details of the RBF kernel. Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'theta': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_dims,). The non-fixed, log-transformed hyperparameters of the kernel\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'kernels.Sum',\n",
       "      'func_desc': 'The Sum kernel takes two kernels \\\\(k_1\\\\) and \\\\(k_2\\\\) and combines them via',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Sum.html#sklearn.gaussian_process.kernels.Sum',\n",
       "      'function_definitions': {'function_name': 'Sum',\n",
       "       'full_function': 'class sklearn.gaussian_process.kernels.Sum(k1, k2)',\n",
       "       'function_text': 'The Sum kernel takes two kernels \\\\(k_1\\\\) and \\\\(k_2\\\\)\\nand combines them via Note that the __add__ magic method is overridden, so\\nSum(RBF(), RBF()) is equivalent to using the + operator\\nwith RBF() + RBF(). Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/gaussian_process.html#gp-kernels',\n",
       "       'parameter_names_desc': [{'param_name': 'theta',\n",
       "         'param_type': 'ndarray of shape (n_dims,)',\n",
       "         'param_desc': 'The non-fixed, log-transformed hyperparameters of the kernel\\n'}]},\n",
       "      'function_calling': {'name': 'Sum',\n",
       "       'descriptions': 'The Sum kernel takes two kernels \\\\(k_1\\\\) and \\\\(k_2\\\\)\\nand combines them via Note that the __add__ magic method is overridden, so\\nSum(RBF(), RBF()) is equivalent to using the + operator\\nwith RBF() + RBF(). Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'theta': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_dims,). The non-fixed, log-transformed hyperparameters of the kernel\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'kernels.ConstantKernel',\n",
       "      'func_desc': 'Constant kernel.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ConstantKernel.html#sklearn.gaussian_process.kernels.ConstantKernel',\n",
       "      'function_definitions': {'function_name': 'ConstantKernel',\n",
       "       'full_function': 'class sklearn.gaussian_process.kernels.ConstantKernel(constant_value=1.0, constant_value_bounds=(1e-05, 100000.0))',\n",
       "       'function_text': 'Constant kernel. Can be used as part of a product-kernel where it scales the magnitude of\\nthe other factor (kernel) or as part of a sum-kernel, where it modifies\\nthe mean of the Gaussian process. Adding a constant kernel is equivalent to adding a constant: is the same as: Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/gaussian_process.html#gp-kernels',\n",
       "       'parameter_names_desc': [{'param_name': 'theta',\n",
       "         'param_type': 'ndarray of shape (n_dims,)',\n",
       "         'param_desc': 'The non-fixed, log-transformed hyperparameters of the kernel\\n'}]},\n",
       "      'function_calling': {'name': 'ConstantKernel',\n",
       "       'descriptions': 'Constant kernel. Can be used as part of a product-kernel where it scales the magnitude of\\nthe other factor (kernel) or as part of a sum-kernel, where it modifies\\nthe mean of the Gaussian process. Adding a constant kernel is equivalent to adding a constant: is the same as: Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'theta': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_dims,). The non-fixed, log-transformed hyperparameters of the kernel\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'kernels.ExpSineSquared',\n",
       "      'func_desc': 'Exp-Sine-Squared kernel (aka periodic kernel).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ExpSineSquared.html#sklearn.gaussian_process.kernels.ExpSineSquared',\n",
       "      'function_definitions': {'function_name': 'ExpSineSquared',\n",
       "       'full_function': 'class sklearn.gaussian_process.kernels.ExpSineSquared(length_scale=1.0, periodicity=1.0, length_scale_bounds=(1e-05, 100000.0), periodicity_bounds=(1e-05, 100000.0))',\n",
       "       'function_text': 'Exp-Sine-Squared kernel (aka periodic kernel). The ExpSineSquared kernel allows one to model functions which repeat\\nthemselves exactly. It is parameterized by a length scale\\nparameter \\\\(l>0\\\\) and a periodicity parameter \\\\(p>0\\\\).\\nOnly the isotropic variant where \\\\(l\\\\) is a scalar is\\nsupported at the moment. The kernel is given by: where \\\\(l\\\\) is the length scale of the kernel, \\\\(p\\\\) the\\nperiodicity of the kernel and \\\\(d(\\\\cdot,\\\\cdot)\\\\) is the\\nEuclidean distance. Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/gaussian_process.html#gp-kernels',\n",
       "       'parameter_names_desc': [{'param_name': 'theta',\n",
       "         'param_type': 'ndarray of shape (n_dims,)',\n",
       "         'param_desc': 'The non-fixed, log-transformed hyperparameters of the kernel\\n'}]},\n",
       "      'function_calling': {'name': 'ExpSineSquared',\n",
       "       'descriptions': 'Exp-Sine-Squared kernel (aka periodic kernel). The ExpSineSquared kernel allows one to model functions which repeat\\nthemselves exactly. It is parameterized by a length scale\\nparameter \\\\(l>0\\\\) and a periodicity parameter \\\\(p>0\\\\).\\nOnly the isotropic variant where \\\\(l\\\\) is a scalar is\\nsupported at the moment. The kernel is given by: where \\\\(l\\\\) is the length scale of the kernel, \\\\(p\\\\) the\\nperiodicity of the kernel and \\\\(d(\\\\cdot,\\\\cdot)\\\\) is the\\nEuclidean distance. Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'theta': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_dims,). The non-fixed, log-transformed hyperparameters of the kernel\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'kernels.Hyperparameter',\n",
       "      'func_desc': \"A kernel hyperparameter's specification in form of a namedtuple.\",\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Hyperparameter.html#sklearn.gaussian_process.kernels.Hyperparameter',\n",
       "      'function_definitions': {'function_name': 'Hyperparameter',\n",
       "       'full_function': 'class sklearn.gaussian_process.kernels.Hyperparameter(name, value_type, bounds, n_elements=1, fixed=None)',\n",
       "       'function_text': 'A kernel hyperparameter’s specification in form of a namedtuple. Added in version 0.18.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'kernels.Matern',\n",
       "      'func_desc': 'Matern kernel.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html#sklearn.gaussian_process.kernels.Matern',\n",
       "      'function_definitions': {'function_name': 'Matern',\n",
       "       'full_function': 'class sklearn.gaussian_process.kernels.Matern(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0), nu=1.5)',\n",
       "       'function_text': 'Matern kernel. The class of Matern kernels is a generalization of the RBF.\\nIt has an additional parameter \\\\(\\\\nu\\\\) which controls the\\nsmoothness of the resulting function. The smaller \\\\(\\\\nu\\\\),\\nthe less smooth the approximated function is.\\nAs \\\\(\\\\nu\\\\rightarrow\\\\infty\\\\), the kernel becomes equivalent to\\nthe RBF kernel. When \\\\(\\\\nu = 1/2\\\\), the Matérn kernel\\nbecomes identical to the absolute exponential kernel.\\nImportant intermediate values are\\n\\\\(\\\\nu=1.5\\\\) (once differentiable functions)\\nand \\\\(\\\\nu=2.5\\\\) (twice differentiable functions). The kernel is given by: where \\\\(d(\\\\cdot,\\\\cdot)\\\\) is the Euclidean distance,\\n\\\\(K_{\\\\nu}(\\\\cdot)\\\\) is a modified Bessel function and\\n\\\\(\\\\Gamma(\\\\cdot)\\\\) is the gamma function.\\nSee [1], Chapter 4, Section 4.2, for details regarding the different\\nvariants of the Matern kernel. Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': 'sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF',\n",
       "       'parameter_names_desc': [{'param_name': 'theta',\n",
       "         'param_type': 'ndarray of shape (n_dims,)',\n",
       "         'param_desc': 'The non-fixed, log-transformed hyperparameters of the kernel\\n'}]},\n",
       "      'function_calling': {'name': 'Matern',\n",
       "       'descriptions': 'Matern kernel. The class of Matern kernels is a generalization of the RBF.\\nIt has an additional parameter \\\\(\\\\nu\\\\) which controls the\\nsmoothness of the resulting function. The smaller \\\\(\\\\nu\\\\),\\nthe less smooth the approximated function is.\\nAs \\\\(\\\\nu\\\\rightarrow\\\\infty\\\\), the kernel becomes equivalent to\\nthe RBF kernel. When \\\\(\\\\nu = 1/2\\\\), the Matérn kernel\\nbecomes identical to the absolute exponential kernel.\\nImportant intermediate values are\\n\\\\(\\\\nu=1.5\\\\) (once differentiable functions)\\nand \\\\(\\\\nu=2.5\\\\) (twice differentiable functions). The kernel is given by: where \\\\(d(\\\\cdot,\\\\cdot)\\\\) is the Euclidean distance,\\n\\\\(K_{\\\\nu}(\\\\cdot)\\\\) is a modified Bessel function and\\n\\\\(\\\\Gamma(\\\\cdot)\\\\) is the gamma function.\\nSee [1], Chapter 4, Section 4.2, for details regarding the different\\nvariants of the Matern kernel. Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'theta': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_dims,). The non-fixed, log-transformed hyperparameters of the kernel\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'kernels.Product',\n",
       "      'func_desc': 'The Product kernel takes two kernels \\\\(k_1\\\\) and \\\\(k_2\\\\) and combines them via',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Product.html#sklearn.gaussian_process.kernels.Product',\n",
       "      'function_definitions': {'function_name': 'Product',\n",
       "       'full_function': 'class sklearn.gaussian_process.kernels.Product(k1, k2)',\n",
       "       'function_text': 'The Product kernel takes two kernels \\\\(k_1\\\\) and \\\\(k_2\\\\)\\nand combines them via Note that the __mul__ magic method is overridden, so\\nProduct(RBF(), RBF()) is equivalent to using the * operator\\nwith RBF() * RBF(). Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/gaussian_process.html#gp-kernels',\n",
       "       'parameter_names_desc': [{'param_name': 'theta',\n",
       "         'param_type': 'ndarray of shape (n_dims,)',\n",
       "         'param_desc': 'The non-fixed, log-transformed hyperparameters of the kernel\\n'}]},\n",
       "      'function_calling': {'name': 'Product',\n",
       "       'descriptions': 'The Product kernel takes two kernels \\\\(k_1\\\\) and \\\\(k_2\\\\)\\nand combines them via Note that the __mul__ magic method is overridden, so\\nProduct(RBF(), RBF()) is equivalent to using the * operator\\nwith RBF() * RBF(). Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'theta': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_dims,). The non-fixed, log-transformed hyperparameters of the kernel\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'kernels.RationalQuadratic',\n",
       "      'func_desc': 'Rational Quadratic kernel.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RationalQuadratic.html#sklearn.gaussian_process.kernels.RationalQuadratic',\n",
       "      'function_definitions': {'function_name': 'RationalQuadratic',\n",
       "       'full_function': 'class sklearn.gaussian_process.kernels.RationalQuadratic(length_scale=1.0, alpha=1.0, length_scale_bounds=(1e-05, 100000.0), alpha_bounds=(1e-05, 100000.0))',\n",
       "       'function_text': 'Rational Quadratic kernel. The RationalQuadratic kernel can be seen as a scale mixture (an infinite\\nsum) of RBF kernels with different characteristic length scales. It is\\nparameterized by a length scale parameter \\\\(l>0\\\\) and a scale\\nmixture parameter \\\\(\\\\alpha>0\\\\). Only the isotropic variant\\nwhere length_scale \\\\(l\\\\) is a scalar is supported at the moment.\\nThe kernel is given by: where \\\\(\\\\alpha\\\\) is the scale mixture parameter, \\\\(l\\\\) is\\nthe length scale of the kernel and \\\\(d(\\\\cdot,\\\\cdot)\\\\) is the\\nEuclidean distance.\\nFor advice on how to set the parameters, see e.g. [1]. Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': '#rc7764613bdcf-1',\n",
       "       'parameter_names_desc': [{'param_name': 'theta',\n",
       "         'param_type': 'ndarray of shape (n_dims,)',\n",
       "         'param_desc': 'The non-fixed, log-transformed hyperparameters of the kernel\\n'}]},\n",
       "      'function_calling': {'name': 'RationalQuadratic',\n",
       "       'descriptions': 'Rational Quadratic kernel. The RationalQuadratic kernel can be seen as a scale mixture (an infinite\\nsum) of RBF kernels with different characteristic length scales. It is\\nparameterized by a length scale parameter \\\\(l>0\\\\) and a scale\\nmixture parameter \\\\(\\\\alpha>0\\\\). Only the isotropic variant\\nwhere length_scale \\\\(l\\\\) is a scalar is supported at the moment.\\nThe kernel is given by: where \\\\(\\\\alpha\\\\) is the scale mixture parameter, \\\\(l\\\\) is\\nthe length scale of the kernel and \\\\(d(\\\\cdot,\\\\cdot)\\\\) is the\\nEuclidean distance.\\nFor advice on how to set the parameters, see e.g. [1]. Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'theta': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_dims,). The non-fixed, log-transformed hyperparameters of the kernel\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'kernels.WhiteKernel',\n",
       "      'func_desc': 'White kernel.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.WhiteKernel.html#sklearn.gaussian_process.kernels.WhiteKernel',\n",
       "      'function_definitions': {'function_name': 'WhiteKernel',\n",
       "       'full_function': 'class sklearn.gaussian_process.kernels.WhiteKernel(noise_level=1.0, noise_level_bounds=(1e-05, 100000.0))',\n",
       "       'function_text': 'White kernel. The main use-case of this kernel is as part of a sum-kernel where it\\nexplains the noise of the signal as independently and identically\\nnormally-distributed. The parameter noise_level equals the variance of this\\nnoise. Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/gaussian_process.html#gp-kernels',\n",
       "       'parameter_names_desc': [{'param_name': 'theta',\n",
       "         'param_type': 'ndarray of shape (n_dims,)',\n",
       "         'param_desc': 'The non-fixed, log-transformed hyperparameters of the kernel\\n'}]},\n",
       "      'function_calling': {'name': 'WhiteKernel',\n",
       "       'descriptions': 'White kernel. The main use-case of this kernel is as part of a sum-kernel where it\\nexplains the noise of the signal as independently and identically\\nnormally-distributed. The parameter noise_level equals the variance of this\\nnoise. Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'theta': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_dims,). The non-fixed, log-transformed hyperparameters of the kernel\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.gaussian_process',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.gaussian_process.html'},\n",
       " 'sklearn.impute.html': {'functions': [{'defaults': [{'func_name': 'IterativeImputer',\n",
       "      'func_desc': 'Multivariate imputer that estimates each feature from all the others.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer',\n",
       "      'function_definitions': {'function_name': 'IterativeImputer',\n",
       "       'full_function': \"class sklearn.impute.IterativeImputer(estimator=None, *, missing_values=nan, sample_posterior=False, max_iter=10, tol=0.001, n_nearest_features=None, initial_strategy='mean', fill_value=None, imputation_order='ascending', skip_complete=False, min_value=-inf, max_value=inf, verbose=0, random_state=None, add_indicator=False, keep_empty_features=False)\",\n",
       "       'function_text': 'Multivariate imputer that estimates each feature from all the others. A strategy for imputing missing values by modeling each feature with\\nmissing values as a function of other features in a round-robin fashion. Read more in the User Guide. Added in version 0.21. Note This estimator is still experimental for now: the predictions\\nand the API might change without any deprecation cycle. To use it,\\nyou need to explicitly import enable_iterative_imputer:',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/impute.html#iterative-imputer',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The input data to complete.\\n'}]},\n",
       "      'function_calling': {'name': 'IterativeImputer',\n",
       "       'descriptions': 'Multivariate imputer that estimates each feature from all the others. A strategy for imputing missing values by modeling each feature with\\nmissing values as a function of other features in a round-robin fashion. Read more in the User Guide. Added in version 0.21. Note This estimator is still experimental for now: the predictions\\nand the API might change without any deprecation cycle. To use it,\\nyou need to explicitly import enable_iterative_imputer:',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). The input data to complete.\\n'}},\n",
       "        'required': ['estimator=None']}}},\n",
       "     {'func_name': 'MissingIndicator',\n",
       "      'func_desc': 'Binary indicators for missing values.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator',\n",
       "      'function_definitions': {'function_name': 'MissingIndicator',\n",
       "       'full_function': \"class sklearn.impute.MissingIndicator(*, missing_values=nan, features='missing-only', sparse='auto', error_on_new=True)\",\n",
       "       'function_text': 'Binary indicators for missing values. Note that this component typically should not be used in a vanilla\\nPipeline consisting of transformers and a\\nclassifier, but rather could be added using a\\nFeatureUnion or\\nColumnTransformer. Read more in the User Guide. Added in version 0.20.',\n",
       "       'func_text_user_guide': 'sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The input data to complete.\\n'}]},\n",
       "      'function_calling': {'name': 'MissingIndicator',\n",
       "       'descriptions': 'Binary indicators for missing values. Note that this component typically should not be used in a vanilla\\nPipeline consisting of transformers and a\\nclassifier, but rather could be added using a\\nFeatureUnion or\\nColumnTransformer. Read more in the User Guide. Added in version 0.20.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The input data to complete.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'KNNImputer',\n",
       "      'func_desc': 'Imputation for completing missing values using k-Nearest Neighbors.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer',\n",
       "      'function_definitions': {'function_name': 'KNNImputer',\n",
       "       'full_function': \"class sklearn.impute.KNNImputer(*, missing_values=nan, n_neighbors=5, weights='uniform', metric='nan_euclidean', copy=True, add_indicator=False, keep_empty_features=False)\",\n",
       "       'function_text': 'Imputation for completing missing values using k-Nearest Neighbors. Each sample’s missing values are imputed using the mean value from\\nn_neighbors nearest neighbors found in the training set. Two samples are\\nclose if the features that neither is missing are close. Read more in the User Guide. Added in version 0.22.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/impute.html#knnimpute',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The input data to complete.\\n'}]},\n",
       "      'function_calling': {'name': 'KNNImputer',\n",
       "       'descriptions': 'Imputation for completing missing values using k-Nearest Neighbors. Each sample’s missing values are imputed using the mean value from\\nn_neighbors nearest neighbors found in the training set. Two samples are\\nclose if the features that neither is missing are close. Read more in the User Guide. Added in version 0.22.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). The input data to complete.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'SimpleImputer',\n",
       "      'func_desc': 'Univariate imputer for completing missing values with simple strategies.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer',\n",
       "      'function_definitions': {'function_name': 'SimpleImputer',\n",
       "       'full_function': \"class sklearn.impute.SimpleImputer(*, missing_values=nan, strategy='mean', fill_value=None, copy=True, add_indicator=False, keep_empty_features=False)\",\n",
       "       'function_text': 'Univariate imputer for completing missing values with simple strategies. Replace missing values using a descriptive statistic (e.g. mean, median, or\\nmost frequent) along each column, or using a constant value. Read more in the User Guide. Added in version 0.20: SimpleImputer replaces the previous sklearn.preprocessing.Imputer\\nestimator which is now removed.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/impute.html#impute',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix}, shape (n_samples, n_features)',\n",
       "         'param_desc': 'The input data to complete.\\n'}]},\n",
       "      'function_calling': {'name': 'SimpleImputer',\n",
       "       'descriptions': 'Univariate imputer for completing missing values with simple strategies. Replace missing values using a descriptive statistic (e.g. mean, median, or\\nmost frequent) along each column, or using a constant value. Read more in the User Guide. Added in version 0.20: SimpleImputer replaces the previous sklearn.preprocessing.Imputer\\nestimator which is now removed.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix}, shape (n_samples, n_features). The input data to complete.\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.impute',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.impute.html'},\n",
       " 'sklearn.inspection.html': {'functions': [{'defaults': [{'func_name': 'partial_dependence',\n",
       "      'func_desc': 'Partial dependence of features.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.inspection.partial_dependence.html#sklearn.inspection.partial_dependence',\n",
       "      'function_definitions': {'function_name': 'partial_dependence',\n",
       "       'full_function': \"sklearn.inspection.partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average')\",\n",
       "       'function_text': \"Partial dependence of features. Partial dependence of a feature (or a set of features) corresponds to\\nthe average response of an estimator for each possible value of the\\nfeature. Read more in the User Guide. Warning For GradientBoostingClassifier and\\nGradientBoostingRegressor, the\\n'recursion' method (used by default) will not account for the init\\npredictor of the boosting process. In practice, this will produce\\nthe same values as 'brute' up to a constant offset in the target\\nresponse, provided that init is a constant estimator (which is the\\ndefault). However, if init is not a constant estimator, the\\npartial dependence values are incorrect for 'recursion' because the\\noffset will be sample-dependent. It is preferable to use the 'brute'\\nmethod. Note that this only applies to\\nGradientBoostingClassifier and\\nGradientBoostingRegressor, not to\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor.\",\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/partial_dependence.html#partial-dependence',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'BaseEstimator',\n",
       "         'param_desc': 'A fitted estimator object implementing predict,\\npredict_proba, or decision_function.\\nMultioutput-multiclass classifiers are not supported.\\n'},\n",
       "        {'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix or dataframe} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'X is used to generate a grid of values for the target\\nfeatures (where the partial dependence will be evaluated), and\\nalso to generate values for the complement features when the\\nmethod is ‘brute’.\\n'},\n",
       "        {'param_name': 'features',\n",
       "         'param_type': 'array-like of {int, str, bool} or int or str',\n",
       "         'param_desc': 'The feature (e.g. [0]) or pair of interacting features\\n(e.g. [(0, 1)]) for which the partial dependency should be computed.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': \"Sample weights are used to calculate weighted means when averaging the\\nmodel output. If None, then samples are equally weighted. If\\nsample_weight is not None, then method will be set to 'brute'.\\nNote that sample_weight is ignored for kind='individual'.\\n\\nAdded in version 1.3.\\n\\n\"},\n",
       "        {'param_name': 'categorical_features',\n",
       "         'param_type': 'array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None',\n",
       "         'param_desc': 'Indicates the categorical features.\\n\\nNone: no feature will be considered categorical;\\n\\nboolean array-like: boolean mask of shape (n_features,)indicating which features are categorical. Thus, this array has\\nthe same shape has X.shape[1];\\n\\n\\n\\n\\ninteger or string array-like: integer indices or stringsindicating categorical features.\\n\\n\\n\\n\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "        {'param_name': 'feature_names',\n",
       "         'param_type': 'array-like of shape (n_features,), dtype=str, default=None',\n",
       "         'param_desc': 'Name of each feature; feature_names[i] holds the name of the feature\\nwith index i.\\nBy default, the name of the feature corresponds to their numerical\\nindex for NumPy array and their column name for pandas dataframe.\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "        {'param_name': 'response_method',\n",
       "         'param_type': '{‘auto’, ‘predict_proba’, ‘decision_function’},             default=’auto’',\n",
       "         'param_desc': 'Specifies whether to use predict_proba or\\ndecision_function as the target response. For regressors\\nthis parameter is ignored and the response is always the output of\\npredict. By default, predict_proba is tried first\\nand we revert to decision_function if it doesn’t exist. If\\nmethod is ‘recursion’, the response is always the output of\\ndecision_function.\\n'},\n",
       "        {'param_name': 'percentiles',\n",
       "         'param_type': 'tuple of float, default=(0.05, 0.95)',\n",
       "         'param_desc': 'The lower and upper percentile used to create the extreme values\\nfor the grid. Must be in [0, 1].\\n'},\n",
       "        {'param_name': 'grid_resolution',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'The number of equally spaced points on the grid, for each target\\nfeature.\\n'},\n",
       "        {'param_name': 'method',\n",
       "         'param_type': '{‘auto’, ‘recursion’, ‘brute’}, default=’auto’',\n",
       "         'param_desc': \"The method used to calculate the averaged predictions:\\n\\n'recursion' is only supported for some tree-based estimators\\n(namely\\nGradientBoostingClassifier,\\nGradientBoostingRegressor,\\nHistGradientBoostingClassifier,\\nHistGradientBoostingRegressor,\\nDecisionTreeRegressor,\\nRandomForestRegressor,\\n) when kind='average'.\\nThis is more efficient in terms of speed.\\nWith this method, the target response of a\\nclassifier is always the decision function, not the predicted\\nprobabilities. Since the 'recursion' method implicitly computes\\nthe average of the Individual Conditional Expectation (ICE) by\\ndesign, it is not compatible with ICE and thus kind must be\\n'average'.\\n'brute' is supported for any estimator, but is more\\ncomputationally intensive.\\n'auto': the 'recursion' is used for estimators that support it,\\nand 'brute' is used otherwise. If sample_weight is not None,\\nthen 'brute' is used regardless of the estimator.\\n\\nPlease see this note for\\ndifferences between the 'brute' and 'recursion' method.\\n\"},\n",
       "        {'param_name': 'kind',\n",
       "         'param_type': '{‘average’, ‘individual’, ‘both’}, default=’average’',\n",
       "         'param_desc': \"Whether to return the partial dependence averaged across all the\\nsamples in the dataset or one value per sample or both.\\nSee Returns below.\\nNote that the fast method='recursion' option is only available for\\nkind='average' and sample_weights=None. Computing individual\\ndependencies and doing weighted averages requires using the slower\\nmethod='brute'.\\n\\nAdded in version 0.24.\\n\\n\"}]},\n",
       "      'function_calling': {'name': 'partial_dependence',\n",
       "       'descriptions': \"Partial dependence of features. Partial dependence of a feature (or a set of features) corresponds to\\nthe average response of an estimator for each possible value of the\\nfeature. Read more in the User Guide. Warning For GradientBoostingClassifier and\\nGradientBoostingRegressor, the\\n'recursion' method (used by default) will not account for the init\\npredictor of the boosting process. In practice, this will produce\\nthe same values as 'brute' up to a constant offset in the target\\nresponse, provided that init is a constant estimator (which is the\\ndefault). However, if init is not a constant estimator, the\\npartial dependence values are incorrect for 'recursion' because the\\noffset will be sample-dependent. It is preferable to use the 'brute'\\nmethod. Note that this only applies to\\nGradientBoostingClassifier and\\nGradientBoostingRegressor, not to\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'BaseEstimator. A fitted estimator object implementing predict,\\npredict_proba, or decision_function.\\nMultioutput-multiclass classifiers are not supported.\\n'},\n",
       "         'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix or dataframe} of shape (n_samples, n_features). X is used to generate a grid of values for the target\\nfeatures (where the partial dependence will be evaluated), and\\nalso to generate values for the complement features when the\\nmethod is ‘brute’.\\n'},\n",
       "         'features': {'type': 'integer',\n",
       "          'description': 'array-like of {int, str, bool} or int or str. The feature (e.g. [0]) or pair of interacting features\\n(e.g. [(0, 1)]) for which the partial dependency should be computed.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': \"array-like of shape (n_samples,), default=None. Sample weights are used to calculate weighted means when averaging the\\nmodel output. If None, then samples are equally weighted. If\\nsample_weight is not None, then method will be set to 'brute'.\\nNote that sample_weight is ignored for kind='individual'.\\n\\nAdded in version 1.3.\\n\\n\"},\n",
       "         'categorical_features': {'type': 'integer',\n",
       "          'description': 'array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None. Indicates the categorical features.\\n\\nNone: no feature will be considered categorical;\\n\\nboolean array-like: boolean mask of shape (n_features,)indicating which features are categorical. Thus, this array has\\nthe same shape has X.shape[1];\\n\\n\\n\\n\\ninteger or string array-like: integer indices or stringsindicating categorical features.\\n\\n\\n\\n\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "         'feature_names': {'type': 'string',\n",
       "          'description': 'array-like of shape (n_features,), dtype=str, default=None. Name of each feature; feature_names[i] holds the name of the feature\\nwith index i.\\nBy default, the name of the feature corresponds to their numerical\\nindex for NumPy array and their column name for pandas dataframe.\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "         'response_method': {'type': 'array',\n",
       "          'description': '{‘auto’, ‘predict_proba’, ‘decision_function’},             default=’auto’. Specifies whether to use predict_proba or\\ndecision_function as the target response. For regressors\\nthis parameter is ignored and the response is always the output of\\npredict. By default, predict_proba is tried first\\nand we revert to decision_function if it doesn’t exist. If\\nmethod is ‘recursion’, the response is always the output of\\ndecision_function.\\n'},\n",
       "         'percentiles': {'type': 'number',\n",
       "          'description': 'tuple of float, default=(0.05, 0.95). The lower and upper percentile used to create the extreme values\\nfor the grid. Must be in [0, 1].\\n'},\n",
       "         'grid_resolution': {'type': 'integer',\n",
       "          'description': 'int, default=100. The number of equally spaced points on the grid, for each target\\nfeature.\\n'},\n",
       "         'method': {'type': 'string',\n",
       "          'enum': ['auto', 'recursion', 'brute'],\n",
       "          'description': \"{‘auto’, ‘recursion’, ‘brute’}, default=’auto’. The method used to calculate the averaged predictions:\\n\\n'recursion' is only supported for some tree-based estimators\\n(namely\\nGradientBoostingClassifier,\\nGradientBoostingRegressor,\\nHistGradientBoostingClassifier,\\nHistGradientBoostingRegressor,\\nDecisionTreeRegressor,\\nRandomForestRegressor,\\n) when kind='average'.\\nThis is more efficient in terms of speed.\\nWith this method, the target response of a\\nclassifier is always the decision function, not the predicted\\nprobabilities. Since the 'recursion' method implicitly computes\\nthe average of the Individual Conditional Expectation (ICE) by\\ndesign, it is not compatible with ICE and thus kind must be\\n'average'.\\n'brute' is supported for any estimator, but is more\\ncomputationally intensive.\\n'auto': the 'recursion' is used for estimators that support it,\\nand 'brute' is used otherwise. If sample_weight is not None,\\nthen 'brute' is used regardless of the estimator.\\n\\nPlease see this note for\\ndifferences between the 'brute' and 'recursion' method.\\n\"},\n",
       "         'kind': {'type': 'string',\n",
       "          'enum': ['average', 'individual', 'both'],\n",
       "          'description': \"{‘average’, ‘individual’, ‘both’}, default=’average’. Whether to return the partial dependence averaged across all the\\nsamples in the dataset or one value per sample or both.\\nSee Returns below.\\nNote that the fast method='recursion' option is only available for\\nkind='average' and sample_weights=None. Computing individual\\ndependencies and doing weighted averages requires using the slower\\nmethod='brute'.\\n\\nAdded in version 0.24.\\n\\n\"}},\n",
       "        'required': ['estimator', 'X', 'features']}}},\n",
       "     {'func_name': 'permutation_importance',\n",
       "      'func_desc': 'Permutation importance for feature evaluation [Rd9e56ef97513-BRE].',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance',\n",
       "      'function_definitions': {'function_name': 'permutation_importance',\n",
       "       'full_function': 'sklearn.inspection.permutation_importance(estimator, X, y, *, scoring=None, n_repeats=5, n_jobs=None, random_state=None, sample_weight=None, max_samples=1.0)',\n",
       "       'function_text': 'Permutation importance for feature evaluation [BRE]. The estimator is required to be a fitted estimator. X can be the\\ndata set used to train the estimator or a hold-out set. The permutation\\nimportance of a feature is calculated as follows. First, a baseline metric,\\ndefined by scoring, is evaluated on a (potentially different)\\ndataset defined by the X. Next, a feature column from the validation set\\nis permuted and the metric is evaluated again. The permutation importance\\nis defined to be the difference between the baseline metric and metric from\\npermutating the feature column. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#rd9e56ef97513-bre',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'An estimator that has already been fitted and is compatible\\nwith scorer.\\n'},\n",
       "        {'param_name': 'X',\n",
       "         'param_type': 'ndarray or DataFrame, shape (n_samples, n_features)',\n",
       "         'param_desc': 'Data on which permutation importance will be computed.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like or None, shape (n_samples, ) or (n_samples, n_classes)',\n",
       "         'param_desc': 'Targets for supervised or None for unsupervised.\\n'},\n",
       "        {'param_name': 'scoring',\n",
       "         'param_type': 'str, callable, list, tuple, or dict, default=None',\n",
       "         'param_desc': 'Scorer to use.\\nIf scoring represents a single score, one can use:\\n\\na single string (see The scoring parameter: defining model evaluation rules);\\na callable (see Defining your scoring strategy from metric functions) that returns a single value.\\n\\nIf scoring represents multiple scores, one can use:\\n\\na list or tuple of unique strings;\\na callable returning a dictionary where the keys are the metric\\nnames and the values are the metric scores;\\na dictionary with metric names as keys and callables a values.\\n\\nPassing multiple scores to scoring is more efficient than calling\\npermutation_importance for each of the scores as it reuses\\npredictions to avoid redundant computation.\\nIf None, the estimator’s default scorer is used.\\n'},\n",
       "        {'param_name': 'n_repeats',\n",
       "         'param_type': 'int, default=5',\n",
       "         'param_desc': 'Number of times to permute a feature.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int or None, default=None',\n",
       "         'param_desc': 'Number of jobs to run in parallel. The computation is done by computing\\npermutation score for each columns and parallelized over the columns.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance, default=None',\n",
       "         'param_desc': 'Pseudo-random number generator to control the permutations of each\\nfeature.\\nPass an int to get reproducible results across function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights used in scoring.\\n\\nAdded in version 0.24.\\n\\n'},\n",
       "        {'param_name': 'max_samples',\n",
       "         'param_type': 'int or float, default=1.0',\n",
       "         'param_desc': 'The number of samples to draw from X to compute feature importance\\nin each repeat (without replacement).\\n\\nIf int, then draw max_samples samples.\\nIf float, then draw max_samples * X.shape[0] samples.\\nIf max_samples is equal to 1.0 or X.shape[0], all samples\\nwill be used.\\n\\nWhile using this option may provide less accurate importance estimates,\\nit keeps the method tractable when evaluating feature importance on\\nlarge datasets. In combination with n_repeats, this allows to control\\nthe computational speed vs statistical accuracy trade-off of this method.\\n\\nAdded in version 1.0.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'permutation_importance',\n",
       "       'descriptions': 'Permutation importance for feature evaluation [BRE]. The estimator is required to be a fitted estimator. X can be the\\ndata set used to train the estimator or a hold-out set. The permutation\\nimportance of a feature is calculated as follows. First, a baseline metric,\\ndefined by scoring, is evaluated on a (potentially different)\\ndataset defined by the X. Next, a feature column from the validation set\\nis permuted and the metric is evaluated again. The permutation importance\\nis defined to be the difference between the baseline metric and metric from\\npermutating the feature column. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'object. An estimator that has already been fitted and is compatible\\nwith scorer.\\n'},\n",
       "         'X': {'type': 'object',\n",
       "          'description': 'ndarray or DataFrame, shape (n_samples, n_features). Data on which permutation importance will be computed.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like or None, shape (n_samples, ) or (n_samples, n_classes). Targets for supervised or None for unsupervised.\\n'},\n",
       "         'scoring': {'type': 'string',\n",
       "          'description': 'str, callable, list, tuple, or dict, default=None. Scorer to use.\\nIf scoring represents a single score, one can use:\\n\\na single string (see The scoring parameter: defining model evaluation rules);\\na callable (see Defining your scoring strategy from metric functions) that returns a single value.\\n\\nIf scoring represents multiple scores, one can use:\\n\\na list or tuple of unique strings;\\na callable returning a dictionary where the keys are the metric\\nnames and the values are the metric scores;\\na dictionary with metric names as keys and callables a values.\\n\\nPassing multiple scores to scoring is more efficient than calling\\npermutation_importance for each of the scores as it reuses\\npredictions to avoid redundant computation.\\nIf None, the estimator’s default scorer is used.\\n'},\n",
       "         'n_repeats': {'type': 'integer',\n",
       "          'description': 'int, default=5. Number of times to permute a feature.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int or None, default=None. Number of jobs to run in parallel. The computation is done by computing\\npermutation score for each columns and parallelized over the columns.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance, default=None. Pseudo-random number generator to control the permutations of each\\nfeature.\\nPass an int to get reproducible results across function calls.\\nSee Glossary.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights used in scoring.\\n\\nAdded in version 0.24.\\n\\n'},\n",
       "         'max_samples': {'type': 'integer',\n",
       "          'description': 'int or float, default=1.0. The number of samples to draw from X to compute feature importance\\nin each repeat (without replacement).\\n\\nIf int, then draw max_samples samples.\\nIf float, then draw max_samples * X.shape[0] samples.\\nIf max_samples is equal to 1.0 or X.shape[0], all samples\\nwill be used.\\n\\nWhile using this option may provide less accurate importance estimates,\\nit keeps the method tractable when evaluating feature importance on\\nlarge datasets. In combination with n_repeats, this allows to control\\nthe computational speed vs statistical accuracy trade-off of this method.\\n\\nAdded in version 1.0.\\n\\n'}},\n",
       "        'required': ['estimator', 'X', 'y']}}}]},\n",
       "   {'Plotting': [{'func_name': 'DecisionBoundaryDisplay',\n",
       "      'func_desc': 'Decisions boundary visualization.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html#sklearn.inspection.DecisionBoundaryDisplay',\n",
       "      'function_definitions': {'function_name': 'DecisionBoundaryDisplay',\n",
       "       'full_function': 'class sklearn.inspection.DecisionBoundaryDisplay(*, xx0, xx1, response, xlabel=None, ylabel=None)',\n",
       "       'function_text': 'Decisions boundary visualization. It is recommended to use\\nfrom_estimator\\nto create a DecisionBoundaryDisplay. All parameters are stored as\\nattributes. Read more in the User Guide. Added in version 1.1.',\n",
       "       'func_text_user_guide': '#sklearn.inspection.DecisionBoundaryDisplay.from_estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'plot_method',\n",
       "         'param_type': '{‘contourf’, ‘contour’, ‘pcolormesh’}, default=’contourf’',\n",
       "         'param_desc': 'Plotting method to call when plotting the response. Please refer\\nto the following matplotlib documentation for details:\\ncontourf,\\ncontour,\\npcolormesh.\\n'},\n",
       "        {'param_name': 'ax',\n",
       "         'param_type': 'Matplotlib axes, default=None',\n",
       "         'param_desc': 'Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "        {'param_name': 'xlabel',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'Overwrite the x-axis label.\\n'},\n",
       "        {'param_name': 'ylabel',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'Overwrite the y-axis label.\\n'}]},\n",
       "      'function_calling': {'name': 'DecisionBoundaryDisplay',\n",
       "       'descriptions': 'Decisions boundary visualization. It is recommended to use\\nfrom_estimator\\nto create a DecisionBoundaryDisplay. All parameters are stored as\\nattributes. Read more in the User Guide. Added in version 1.1.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'plot_method': {'type': 'string',\n",
       "          'enum': ['contourf', 'contour', 'pcolormesh'],\n",
       "          'description': '{‘contourf’, ‘contour’, ‘pcolormesh’}, default=’contourf’. Plotting method to call when plotting the response. Please refer\\nto the following matplotlib documentation for details:\\ncontourf,\\ncontour,\\npcolormesh.\\n'},\n",
       "         'ax': {'type': 'object',\n",
       "          'description': 'Matplotlib axes, default=None. Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "         'xlabel': {'type': 'string',\n",
       "          'description': 'str, default=None. Overwrite the x-axis label.\\n'},\n",
       "         'ylabel': {'type': 'string',\n",
       "          'description': 'str, default=None. Overwrite the y-axis label.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'PartialDependenceDisplay',\n",
       "      'func_desc': 'Partial Dependence Plot (PDP).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.inspection.PartialDependenceDisplay.html#sklearn.inspection.PartialDependenceDisplay',\n",
       "      'function_definitions': {'function_name': 'PartialDependenceDisplay',\n",
       "       'full_function': \"class sklearn.inspection.PartialDependenceDisplay(pd_results, *, features, feature_names, target_idx, deciles, kind='average', subsample=1000, random_state=None, is_categorical=None)\",\n",
       "       'function_text': 'Partial Dependence Plot (PDP). This can also display individual partial dependencies which are often\\nreferred to as: Individual Condition Expectation (ICE). It is recommended to use\\nfrom_estimator to create a\\nPartialDependenceDisplay. All parameters are\\nstored as attributes. Read more in\\nAdvanced Plotting With Partial Dependence\\nand the User Guide. Added in version 0.22.',\n",
       "       'func_text_user_guide': '#sklearn.inspection.PartialDependenceDisplay.from_estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'ax',\n",
       "         'param_type': 'Matplotlib axes or array-like of Matplotlib axes, default=None',\n",
       "         'param_desc': '\\n\\nIf a single axis is passed in, it is treated as a bounding axesand a grid of partial dependence plots will be drawn within\\nthese bounds. The n_cols parameter controls the number of\\ncolumns in the grid.\\n\\n\\n\\n\\nIf an array-like of axes are passed in, the partial dependenceplots will be drawn directly into these axes.\\n\\n\\n\\n\\nIf None, a figure and a bounding axes is created and treatedas the single axes case.\\n\\n\\n\\n\\n'},\n",
       "        {'param_name': 'n_cols',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'The maximum number of columns in the grid plot. Only active when\\nax is a single axes or None.\\n'},\n",
       "        {'param_name': 'line_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Dict with keywords passed to the matplotlib.pyplot.plot call.\\nFor one-way partial dependence plots.\\n'},\n",
       "        {'param_name': 'ice_lines_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Dictionary with keywords passed to the matplotlib.pyplot.plot call.\\nFor ICE lines in the one-way partial dependence plots.\\nThe key value pairs defined in ice_lines_kw takes priority over\\nline_kw.\\n\\nAdded in version 1.0.\\n\\n'},\n",
       "        {'param_name': 'pd_line_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Dictionary with keywords passed to the matplotlib.pyplot.plot call.\\nFor partial dependence in one-way partial dependence plots.\\nThe key value pairs defined in pd_line_kw takes priority over\\nline_kw.\\n\\nAdded in version 1.0.\\n\\n'},\n",
       "        {'param_name': 'contour_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Dict with keywords passed to the matplotlib.pyplot.contourf\\ncall for two-way partial dependence plots.\\n'},\n",
       "        {'param_name': 'bar_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Dict with keywords passed to the matplotlib.pyplot.bar\\ncall for one-way categorical partial dependence plots.\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "        {'param_name': 'heatmap_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Dict with keywords passed to the matplotlib.pyplot.imshow\\ncall for two-way categorical partial dependence plots.\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "        {'param_name': 'pdp_lim',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Global min and max average predictions, such that all plots will have the\\nsame scale and y limits. pdp_lim[1] is the global min and max for single\\npartial dependence curves. pdp_lim[2] is the global min and max for\\ntwo-way partial dependence curves. If None (default), the limit will be\\ninferred from the global minimum and maximum of all predictions.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "        {'param_name': 'centered',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the ICE and PD lines will start at the origin of the\\ny-axis. By default, no centering is done.\\n\\nAdded in version 1.1.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'PartialDependenceDisplay',\n",
       "       'descriptions': 'Partial Dependence Plot (PDP). This can also display individual partial dependencies which are often\\nreferred to as: Individual Condition Expectation (ICE). It is recommended to use\\nfrom_estimator to create a\\nPartialDependenceDisplay. All parameters are\\nstored as attributes. Read more in\\nAdvanced Plotting With Partial Dependence\\nand the User Guide. Added in version 0.22.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'ax': {'type': 'array',\n",
       "          'description': 'Matplotlib axes or array-like of Matplotlib axes, default=None. \\n\\nIf a single axis is passed in, it is treated as a bounding axesand a grid of partial dependence plots will be drawn within\\nthese bounds. The n_cols parameter controls the number of\\ncolumns in the grid.\\n\\n\\n\\n\\nIf an array-like of axes are passed in, the partial dependenceplots will be drawn directly into these axes.\\n\\n\\n\\n\\nIf None, a figure and a bounding axes is created and treatedas the single axes case.\\n\\n\\n\\n\\n'},\n",
       "         'n_cols': {'type': 'integer',\n",
       "          'description': 'int, default=3. The maximum number of columns in the grid plot. Only active when\\nax is a single axes or None.\\n'},\n",
       "         'line_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Dict with keywords passed to the matplotlib.pyplot.plot call.\\nFor one-way partial dependence plots.\\n'},\n",
       "         'ice_lines_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Dictionary with keywords passed to the matplotlib.pyplot.plot call.\\nFor ICE lines in the one-way partial dependence plots.\\nThe key value pairs defined in ice_lines_kw takes priority over\\nline_kw.\\n\\nAdded in version 1.0.\\n\\n'},\n",
       "         'pd_line_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Dictionary with keywords passed to the matplotlib.pyplot.plot call.\\nFor partial dependence in one-way partial dependence plots.\\nThe key value pairs defined in pd_line_kw takes priority over\\nline_kw.\\n\\nAdded in version 1.0.\\n\\n'},\n",
       "         'contour_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Dict with keywords passed to the matplotlib.pyplot.contourf\\ncall for two-way partial dependence plots.\\n'},\n",
       "         'bar_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Dict with keywords passed to the matplotlib.pyplot.bar\\ncall for one-way categorical partial dependence plots.\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "         'heatmap_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Dict with keywords passed to the matplotlib.pyplot.imshow\\ncall for two-way categorical partial dependence plots.\\n\\nAdded in version 1.2.\\n\\n'},\n",
       "         'pdp_lim': {'type': 'array',\n",
       "          'description': 'dict, default=None. Global min and max average predictions, such that all plots will have the\\nsame scale and y limits. pdp_lim[1] is the global min and max for single\\npartial dependence curves. pdp_lim[2] is the global min and max for\\ntwo-way partial dependence curves. If None (default), the limit will be\\ninferred from the global minimum and maximum of all predictions.\\n\\nAdded in version 1.1.\\n\\n'},\n",
       "         'centered': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the ICE and PD lines will start at the origin of the\\ny-axis. By default, no centering is done.\\n\\nAdded in version 1.1.\\n\\n'}},\n",
       "        'required': ['pd_results']}}}]}],\n",
       "  'name': 'sklearn.inspection',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.inspection.html'},\n",
       " 'sklearn.isotonic.html': {'functions': [{'defaults': [{'func_name': 'IsotonicRegression',\n",
       "      'func_desc': 'Isotonic regression model.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.isotonic.IsotonicRegression.html#sklearn.isotonic.IsotonicRegression',\n",
       "      'function_definitions': {'function_name': 'IsotonicRegression',\n",
       "       'full_function': \"class sklearn.isotonic.IsotonicRegression(*, y_min=None, y_max=None, increasing=True, out_of_bounds='nan')\",\n",
       "       'function_text': 'Isotonic regression model. Read more in the User Guide. Added in version 0.13.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/isotonic.html#isotonic',\n",
       "       'parameter_names_desc': [{'param_name': 'T',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, 1)',\n",
       "         'param_desc': 'Data to transform.\\n\\nChanged in version 0.24: Also accepts 2d array with 1 feature.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'IsotonicRegression',\n",
       "       'descriptions': 'Isotonic regression model. Read more in the User Guide. Added in version 0.13.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'T': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, 1). Data to transform.\\n\\nChanged in version 0.24: Also accepts 2d array with 1 feature.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'isotonic_regression',\n",
       "      'func_desc': 'Solve the isotonic regression model.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.isotonic.isotonic_regression.html#sklearn.isotonic.isotonic_regression',\n",
       "      'function_definitions': {'function_name': 'isotonic_regression',\n",
       "       'full_function': 'sklearn.isotonic.isotonic_regression(y, *, sample_weight=None, y_min=None, y_max=None, increasing=True)',\n",
       "       'function_text': 'Solve the isotonic regression model. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/isotonic.html#isotonic',\n",
       "       'parameter_names_desc': [{'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'The data.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Weights on each point of the regression.\\nIf None, weight is set to 1 (equal weights).\\n'},\n",
       "        {'param_name': 'y_min',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': 'Lower bound on the lowest predicted value (the minimum value may\\nstill be higher). If not set, defaults to -inf.\\n'},\n",
       "        {'param_name': 'y_max',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': 'Upper bound on the highest predicted value (the maximum may still be\\nlower). If not set, defaults to +inf.\\n'},\n",
       "        {'param_name': 'increasing',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to compute y_ is increasing (if set to True) or decreasing\\n(if set to False).\\n'}]},\n",
       "      'function_calling': {'name': 'isotonic_regression',\n",
       "       'descriptions': 'Solve the isotonic regression model. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). The data.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Weights on each point of the regression.\\nIf None, weight is set to 1 (equal weights).\\n'},\n",
       "         'y_min': {'type': 'number',\n",
       "          'description': 'float, default=None. Lower bound on the lowest predicted value (the minimum value may\\nstill be higher). If not set, defaults to -inf.\\n'},\n",
       "         'y_max': {'type': 'number',\n",
       "          'description': 'float, default=None. Upper bound on the highest predicted value (the maximum may still be\\nlower). If not set, defaults to +inf.\\n'},\n",
       "         'increasing': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to compute y_ is increasing (if set to True) or decreasing\\n(if set to False).\\n'}},\n",
       "        'required': ['y']}}},\n",
       "     {'func_name': 'check_increasing',\n",
       "      'func_desc': 'Determine whether y is monotonically correlated with x.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.isotonic.check_increasing.html#sklearn.isotonic.check_increasing',\n",
       "      'function_definitions': {'function_name': 'check_increasing',\n",
       "       'full_function': 'sklearn.isotonic.check_increasing(x, y)',\n",
       "       'function_text': 'Determine whether y is monotonically correlated with x. y is found increasing or decreasing with respect to x based on a Spearman\\ncorrelation test.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'x',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Training data.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Training target.\\n'}]},\n",
       "      'function_calling': {'name': 'check_increasing',\n",
       "       'descriptions': 'Determine whether y is monotonically correlated with x. y is found increasing or decreasing with respect to x based on a Spearman\\ncorrelation test.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'x': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Training data.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Training target.\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.isotonic',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.isotonic.html'},\n",
       " 'sklearn.kernel_approximation.html': {'functions': [{'defaults': [{'func_name': 'AdditiveChi2Sampler',\n",
       "      'func_desc': 'Approximate feature map for additive chi2 kernel.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.AdditiveChi2Sampler.html#sklearn.kernel_approximation.AdditiveChi2Sampler',\n",
       "      'function_definitions': {'function_name': 'AdditiveChi2Sampler',\n",
       "       'full_function': 'class sklearn.kernel_approximation.AdditiveChi2Sampler(*, sample_steps=2, sample_interval=None)',\n",
       "       'function_text': 'Approximate feature map for additive chi2 kernel. Uses sampling the fourier transform of the kernel characteristic\\nat regular intervals. Since the kernel that is to be approximated is additive, the components of\\nthe input vectors can be treated separately.  Each entry in the original\\nspace is transformed into 2*sample_steps-1 features, where sample_steps is\\na parameter of the method. Typical values of sample_steps include 1, 2 and\\n3. Optimal choices for the sampling interval for certain data ranges can be\\ncomputed (see the reference). The default values should be reasonable. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/kernel_approximation.html#additive-chi-kernel-approx',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix}, shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}]},\n",
       "      'function_calling': {'name': 'AdditiveChi2Sampler',\n",
       "       'descriptions': 'Approximate feature map for additive chi2 kernel. Uses sampling the fourier transform of the kernel characteristic\\nat regular intervals. Since the kernel that is to be approximated is additive, the components of\\nthe input vectors can be treated separately.  Each entry in the original\\nspace is transformed into 2*sample_steps-1 features, where sample_steps is\\na parameter of the method. Typical values of sample_steps include 1, 2 and\\n3. Optimal choices for the sampling interval for certain data ranges can be\\ncomputed (see the reference). The default values should be reasonable. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix}, shape (n_samples, n_features). Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'PolynomialCountSketch',\n",
       "      'func_desc': 'Polynomial kernel approximation via Tensor Sketch.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.PolynomialCountSketch.html#sklearn.kernel_approximation.PolynomialCountSketch',\n",
       "      'function_definitions': {'function_name': 'PolynomialCountSketch',\n",
       "       'full_function': 'class sklearn.kernel_approximation.PolynomialCountSketch(*, gamma=1.0, degree=2, coef0=0, n_components=100, random_state=None)',\n",
       "       'function_text': 'Polynomial kernel approximation via Tensor Sketch. Implements Tensor Sketch, which approximates the feature map\\nof the polynomial kernel: by efficiently computing a Count Sketch of the outer product of a\\nvector with itself using Fast Fourier Transforms (FFT). Read more in the\\nUser Guide. Added in version 0.24.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/kernel_approximation.html#polynomial-kernel-approx',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like}, shape (n_samples, n_features)',\n",
       "         'param_desc': 'New data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}]},\n",
       "      'function_calling': {'name': 'PolynomialCountSketch',\n",
       "       'descriptions': 'Polynomial kernel approximation via Tensor Sketch. Implements Tensor Sketch, which approximates the feature map\\nof the polynomial kernel: by efficiently computing a Count Sketch of the outer product of a\\nvector with itself using Fast Fourier Transforms (FFT). Read more in the\\nUser Guide. Added in version 0.24.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like}, shape (n_samples, n_features). New data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'SkewedChi2Sampler',\n",
       "      'func_desc': 'Approximate feature map for \"skewed chi-squared\" kernel.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.SkewedChi2Sampler.html#sklearn.kernel_approximation.SkewedChi2Sampler',\n",
       "      'function_definitions': {'function_name': 'SkewedChi2Sampler',\n",
       "       'full_function': 'class sklearn.kernel_approximation.SkewedChi2Sampler(*, skewedness=1.0, n_components=100, random_state=None)',\n",
       "       'function_text': 'Approximate feature map for “skewed chi-squared” kernel. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/kernel_approximation.html#skewed-chi-kernel-approx',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like, shape (n_samples, n_features)',\n",
       "         'param_desc': 'New data, where n_samples is the number of samples\\nand n_features is the number of features. All values of X must be\\nstrictly greater than “-skewedness”.\\n'}]},\n",
       "      'function_calling': {'name': 'SkewedChi2Sampler',\n",
       "       'descriptions': 'Approximate feature map for “skewed chi-squared” kernel. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like, shape (n_samples, n_features). New data, where n_samples is the number of samples\\nand n_features is the number of features. All values of X must be\\nstrictly greater than “-skewedness”.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'Nystroem',\n",
       "      'func_desc': 'Approximate a kernel map using a subset of the training data.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html#sklearn.kernel_approximation.Nystroem',\n",
       "      'function_definitions': {'function_name': 'Nystroem',\n",
       "       'full_function': \"class sklearn.kernel_approximation.Nystroem(kernel='rbf', *, gamma=None, coef0=None, degree=None, kernel_params=None, n_components=100, random_state=None, n_jobs=None)\",\n",
       "       'function_text': 'Approximate a kernel map using a subset of the training data. Constructs an approximate feature map for an arbitrary kernel\\nusing a subset of the data as basis. Read more in the User Guide. Added in version 0.13.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/kernel_approximation.html#nystroem-kernel-approx',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Data to transform.\\n'}]},\n",
       "      'function_calling': {'name': 'Nystroem',\n",
       "       'descriptions': 'Approximate a kernel map using a subset of the training data. Constructs an approximate feature map for an arbitrary kernel\\nusing a subset of the data as basis. Read more in the User Guide. Added in version 0.13.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Data to transform.\\n'}},\n",
       "        'required': [\"kernel='rbf'\"]}}},\n",
       "     {'func_name': 'RBFSampler',\n",
       "      'func_desc': 'Approximate a RBF kernel feature map using random Fourier features.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html#sklearn.kernel_approximation.RBFSampler',\n",
       "      'function_definitions': {'function_name': 'RBFSampler',\n",
       "       'full_function': 'class sklearn.kernel_approximation.RBFSampler(*, gamma=1.0, n_components=100, random_state=None)',\n",
       "       'function_text': 'Approximate a RBF kernel feature map using random Fourier features. It implements a variant of Random Kitchen Sinks.[1] Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/kernel_approximation.html#rbf-kernel-approx',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix}, shape (n_samples, n_features)',\n",
       "         'param_desc': 'New data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}]},\n",
       "      'function_calling': {'name': 'RBFSampler',\n",
       "       'descriptions': 'Approximate a RBF kernel feature map using random Fourier features. It implements a variant of Random Kitchen Sinks.[1] Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix}, shape (n_samples, n_features). New data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.kernel_approximation',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.kernel_approximation.html'},\n",
       " 'sklearn.kernel_ridge.html': {'functions': [{'defaults': [{'func_name': 'KernelRidge',\n",
       "      'func_desc': 'Kernel ridge regression.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge',\n",
       "      'function_definitions': {'function_name': 'KernelRidge',\n",
       "       'full_function': \"class sklearn.kernel_ridge.KernelRidge(alpha=1, *, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None)\",\n",
       "       'function_text': 'Kernel ridge regression. Kernel ridge regression (KRR) combines ridge regression (linear least\\nsquares with l2-norm regularization) with the kernel trick. It thus\\nlearns a linear function in the space induced by the respective kernel and\\nthe data. For non-linear kernels, this corresponds to a non-linear\\nfunction in the original space. The form of the model learned by KRR is identical to support vector\\nregression (SVR). However, different loss functions are used: KRR uses\\nsquared error loss while support vector regression uses epsilon-insensitive\\nloss, both combined with l2 regularization. In contrast to SVR, fitting a\\nKRR model can be done in closed-form and is typically faster for\\nmedium-sized datasets. On the other hand, the learned model is non-sparse\\nand thus slower than SVR, which learns a sparse model for epsilon > 0, at\\nprediction-time. This estimator has built-in support for multi-variate regression\\n(i.e., when y is a 2d-array of shape [n_samples, n_targets]). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/kernel_ridge.html#kernel-ridge',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'KernelRidge',\n",
       "       'descriptions': 'Kernel ridge regression. Kernel ridge regression (KRR) combines ridge regression (linear least\\nsquares with l2-norm regularization) with the kernel trick. It thus\\nlearns a linear function in the space induced by the respective kernel and\\nthe data. For non-linear kernels, this corresponds to a non-linear\\nfunction in the original space. The form of the model learned by KRR is identical to support vector\\nregression (SVR). However, different loss functions are used: KRR uses\\nsquared error loss while support vector regression uses epsilon-insensitive\\nloss, both combined with l2 regularization. In contrast to SVR, fitting a\\nKRR model can be done in closed-form and is typically faster for\\nmedium-sized datasets. On the other hand, the learned model is non-sparse\\nand thus slower than SVR, which learns a sparse model for epsilon > 0, at\\nprediction-time. This estimator has built-in support for multi-variate regression\\n(i.e., when y is a 2d-array of shape [n_samples, n_targets]). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['alpha=1']}}}]}],\n",
       "  'name': 'sklearn.kernel_ridge',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.kernel_ridge.html'},\n",
       " 'sklearn.linear_model.html': {'functions': [{'Linear classifiers': [{'func_name': 'LogisticRegression',\n",
       "      'func_desc': 'Logistic Regression (aka logit, MaxEnt) classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression',\n",
       "      'function_definitions': {'function_name': 'LogisticRegression',\n",
       "       'full_function': \"class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='deprecated', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\",\n",
       "       'function_text': 'Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\\nscheme if the ‘multi_class’ option is set to ‘ovr’, and uses the\\ncross-entropy loss if the ‘multi_class’ option is set to ‘multinomial’.\\n(Currently the ‘multinomial’ option is supported only by the ‘lbfgs’,\\n‘sag’, ‘saga’ and ‘newton-cg’ solvers.) This class implements regularized logistic regression using the\\n‘liblinear’ library, ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers. Note\\nthat regularization is applied by default. It can handle both dense\\nand sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\\nfloats for optimal performance; any other input format will be converted\\n(and copied). The ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support only L2 regularization\\nwith primal formulation, or no regularization. The ‘liblinear’ solver\\nsupports both L1 and L2 regularization, with a dual formulation only for\\nthe L2 penalty. The Elastic-Net regularization is only supported by the\\n‘saga’ solver. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'PassiveAggressiveClassifier',\n",
       "      'func_desc': 'Passive Aggressive Classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html#sklearn.linear_model.PassiveAggressiveClassifier',\n",
       "      'function_definitions': {'function_name': 'PassiveAggressiveClassifier',\n",
       "       'full_function': \"class sklearn.linear_model.PassiveAggressiveClassifier(*, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='hinge', n_jobs=None, random_state=None, warm_start=False, class_weight=None, average=False)\",\n",
       "       'function_text': 'Passive Aggressive Classifier. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'RidgeClassifier',\n",
       "      'func_desc': 'Classifier using Ridge regression.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier',\n",
       "      'function_definitions': {'function_name': 'RidgeClassifier',\n",
       "       'full_function': \"class sklearn.linear_model.RidgeClassifier(alpha=1.0, *, fit_intercept=True, copy_X=True, max_iter=None, tol=0.0001, class_weight=None, solver='auto', positive=False, random_state=None)\",\n",
       "       'function_text': 'Classifier using Ridge regression. This classifier first converts the target values into {-1, 1} and\\nthen treats the problem as a regression task (multi-output regression in\\nthe multiclass case). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'RidgeClassifier',\n",
       "       'descriptions': 'Classifier using Ridge regression. This classifier first converts the target values into {-1, 1} and\\nthen treats the problem as a regression task (multi-output regression in\\nthe multiclass case). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['alpha=1.0']}}},\n",
       "     {'func_name': 'SGDClassifier',\n",
       "      'func_desc': 'Linear classifiers (SVM, logistic regression, etc.) with SGD training.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier',\n",
       "      'function_definitions': {'function_name': 'SGDClassifier',\n",
       "       'full_function': \"class sklearn.linear_model.SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\",\n",
       "       'function_text': 'Linear classifiers (SVM, logistic regression, etc.) with SGD training. This estimator implements regularized linear models with stochastic\\ngradient descent (SGD) learning: the gradient of the loss is estimated\\neach sample at a time and the model is updated along the way with a\\ndecreasing strength schedule (aka learning rate). SGD allows minibatch\\n(online/out-of-core) learning via the partial_fit method.\\nFor best results using the default learning rate schedule, the data should\\nhave zero mean and unit variance. This implementation works with data represented as dense or sparse arrays\\nof floating point values for the features. The model it fits can be\\ncontrolled with the loss parameter; by default, it fits a linear support\\nvector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model\\nparameters towards the zero vector using either the squared euclidean norm\\nL2 or the absolute norm L1 or a combination of both (Elastic Net). If the\\nparameter update crosses the 0.0 value because of the regularizer, the\\nupdate is truncated to 0.0 to allow for learning sparse models and achieve\\nonline feature selection. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/sgd.html#sgd',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'LogisticRegressionCV',\n",
       "      'func_desc': 'Logistic Regression CV (aka logit, MaxEnt) classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV',\n",
       "      'function_definitions': {'function_name': 'LogisticRegressionCV',\n",
       "       'full_function': \"class sklearn.linear_model.LogisticRegressionCV(*, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='deprecated', random_state=None, l1_ratios=None)\",\n",
       "       'function_text': 'Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag\\nor lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\\nregularization with primal formulation. The liblinear solver supports both\\nL1 and L2 regularization, with a dual formulation only for the L2 penalty.\\nElastic-Net penalty is only supported by the saga solver. For the grid of Cs values and l1_ratios values, the best hyperparameter\\nis selected by the cross-validator\\nStratifiedKFold, but it can be changed\\nusing the cv parameter. The ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’\\nsolvers can warm-start the coefficients (see Glossary). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-cross-validation-estimator',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'Perceptron',\n",
       "      'func_desc': 'Linear perceptron classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron',\n",
       "      'function_definitions': {'function_name': 'Perceptron',\n",
       "       'full_function': 'class sklearn.linear_model.Perceptron(*, penalty=None, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False)',\n",
       "       'function_text': 'Linear perceptron classifier. The implementation is a wrapper around SGDClassifier\\nby fixing the loss and learning_rate parameters as: Other available parameters are described below and are forwarded to\\nSGDClassifier. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'RidgeClassifierCV',\n",
       "      'func_desc': 'Ridge classifier with built-in cross-validation.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifierCV.html#sklearn.linear_model.RidgeClassifierCV',\n",
       "      'function_definitions': {'function_name': 'RidgeClassifierCV',\n",
       "       'full_function': \"class sklearn.linear_model.RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, scoring=None, cv=None, class_weight=None, store_cv_results=None, store_cv_values='deprecated')\",\n",
       "       'function_text': 'Ridge classifier with built-in cross-validation. See glossary entry for cross-validation estimator. By default, it performs Leave-One-Out Cross-Validation. Currently,\\nonly the n_features > n_samples case is handled efficiently. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-cross-validation-estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'RidgeClassifierCV',\n",
       "       'descriptions': 'Ridge classifier with built-in cross-validation. See glossary entry for cross-validation estimator. By default, it performs Leave-One-Out Cross-Validation. Currently,\\nonly the n_features > n_samples case is handled efficiently. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['alphas=(0.1', '1.0', '10.0)']}}},\n",
       "     {'func_name': 'SGDOneClassSVM',\n",
       "      'func_desc': 'Solves linear One-Class SVM using Stochastic Gradient Descent.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDOneClassSVM.html#sklearn.linear_model.SGDOneClassSVM',\n",
       "      'function_definitions': {'function_name': 'SGDOneClassSVM',\n",
       "       'full_function': \"class sklearn.linear_model.SGDOneClassSVM(nu=0.5, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, warm_start=False, average=False)\",\n",
       "       'function_text': 'Solves linear One-Class SVM using Stochastic Gradient Descent. This implementation is meant to be used with a kernel approximation\\ntechnique (e.g. sklearn.kernel_approximation.Nystroem) to obtain results\\nsimilar to sklearn.svm.OneClassSVM which uses a Gaussian kernel by\\ndefault. Read more in the User Guide. Added in version 1.0.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/sgd.html#sgd-online-one-class-svm',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}}]},\n",
       "   {'Classical linear regressors': [{'func_name': 'LinearRegression',\n",
       "      'func_desc': 'Ordinary least squares Linear Regression.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression',\n",
       "      'function_definitions': {'function_name': 'LinearRegression',\n",
       "       'full_function': 'class sklearn.linear_model.LinearRegression(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)',\n",
       "       'function_text': 'Ordinary least squares Linear Regression. LinearRegression fits a linear model with coefficients w = (w1, …, wp)\\nto minimize the residual sum of squares between the observed targets in\\nthe dataset, and the targets predicted by the linear approximation.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-n_jobs',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'LinearRegression',\n",
       "       'descriptions': 'Ordinary least squares Linear Regression. LinearRegression fits a linear model with coefficients w = (w1, …, wp)\\nto minimize the residual sum of squares between the observed targets in\\nthe dataset, and the targets predicted by the linear approximation.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'RidgeCV',\n",
       "      'func_desc': 'Ridge regression with built-in cross-validation.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV',\n",
       "      'function_definitions': {'function_name': 'RidgeCV',\n",
       "       'full_function': \"class sklearn.linear_model.RidgeCV(alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, scoring=None, cv=None, gcv_mode=None, store_cv_results=None, alpha_per_target=False, store_cv_values='deprecated')\",\n",
       "       'function_text': 'Ridge regression with built-in cross-validation. See glossary entry for cross-validation estimator. By default, it performs efficient Leave-One-Out Cross-Validation. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-cross-validation-estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'RidgeCV',\n",
       "       'descriptions': 'Ridge regression with built-in cross-validation. See glossary entry for cross-validation estimator. By default, it performs efficient Leave-One-Out Cross-Validation. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['alphas=(0.1', '1.0', '10.0)']}}},\n",
       "     {'func_name': 'Ridge',\n",
       "      'func_desc': 'Linear least squares with l2 regularization.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge',\n",
       "      'function_definitions': {'function_name': 'Ridge',\n",
       "       'full_function': \"class sklearn.linear_model.Ridge(alpha=1.0, *, fit_intercept=True, copy_X=True, max_iter=None, tol=0.0001, solver='auto', positive=False, random_state=None)\",\n",
       "       'function_text': 'Linear least squares with l2 regularization. Minimizes the objective function: This model solves a regression model where the loss function is\\nthe linear least squares function and regularization is given by\\nthe l2-norm. Also known as Ridge Regression or Tikhonov regularization.\\nThis estimator has built-in support for multi-variate regression\\n(i.e., when y is a 2d-array of shape (n_samples, n_targets)). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'Ridge',\n",
       "       'descriptions': 'Linear least squares with l2 regularization. Minimizes the objective function: This model solves a regression model where the loss function is\\nthe linear least squares function and regularization is given by\\nthe l2-norm. Also known as Ridge Regression or Tikhonov regularization.\\nThis estimator has built-in support for multi-variate regression\\n(i.e., when y is a 2d-array of shape (n_samples, n_targets)). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['alpha=1.0']}}},\n",
       "     {'func_name': 'SGDRegressor',\n",
       "      'func_desc': 'Linear model fitted by minimizing a regularized empirical loss with SGD.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor',\n",
       "      'function_definitions': {'function_name': 'SGDRegressor',\n",
       "       'full_function': \"class sklearn.linear_model.SGDRegressor(loss='squared_error', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\",\n",
       "       'function_text': 'Linear model fitted by minimizing a regularized empirical loss with SGD. SGD stands for Stochastic Gradient Descent: the gradient of the loss is\\nestimated each sample at a time and the model is updated along the way with\\na decreasing strength schedule (aka learning rate). The regularizer is a penalty added to the loss function that shrinks model\\nparameters towards the zero vector using either the squared euclidean norm\\nL2 or the absolute norm L1 or a combination of both (Elastic Net). If the\\nparameter update crosses the 0.0 value because of the regularizer, the\\nupdate is truncated to 0.0 to allow for learning sparse models and achieve\\nonline feature selection. This implementation works with data represented as dense numpy arrays of\\nfloating point values for the features. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/sgd.html#sgd',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}}]},\n",
       "   {'Regressors with variable selection': [{'func_name': 'ElasticNet',\n",
       "      'func_desc': 'Linear regression with combined L1 and L2 priors as regularizer.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet',\n",
       "      'function_definitions': {'function_name': 'ElasticNet',\n",
       "       'full_function': \"class sklearn.linear_model.ElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\",\n",
       "       'function_text': 'Linear regression with combined L1 and L2 priors as regularizer. Minimizes the objective function: If you are interested in controlling the L1 and L2 penalty\\nseparately, keep in mind that this is equivalent to: where: The parameter l1_ratio corresponds to alpha in the glmnet R package while\\nalpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\\n= 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\\nunless you supply your own sequence of alpha. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#elastic-net',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'ElasticNet',\n",
       "       'descriptions': 'Linear regression with combined L1 and L2 priors as regularizer. Minimizes the objective function: If you are interested in controlling the L1 and L2 penalty\\nseparately, keep in mind that this is equivalent to: where: The parameter l1_ratio corresponds to alpha in the glmnet R package while\\nalpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\\n= 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\\nunless you supply your own sequence of alpha. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['alpha=1.0']}}},\n",
       "     {'func_name': 'Lars',\n",
       "      'func_desc': 'Least Angle Regression model a.k.a.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lars.html#sklearn.linear_model.Lars',\n",
       "      'function_definitions': {'function_name': 'Lars',\n",
       "       'full_function': \"class sklearn.linear_model.Lars(*, fit_intercept=True, verbose=False, precompute='auto', n_nonzero_coefs=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, jitter=None, random_state=None)\",\n",
       "       'function_text': 'Least Angle Regression model a.k.a. LAR. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'Lars',\n",
       "       'descriptions': 'Least Angle Regression model a.k.a. LAR. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'Lasso',\n",
       "      'func_desc': 'Linear Model trained with L1 prior as regularizer (aka the Lasso).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso',\n",
       "      'function_definitions': {'function_name': 'Lasso',\n",
       "       'full_function': \"class sklearn.linear_model.Lasso(alpha=1.0, *, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\",\n",
       "       'function_text': 'Linear Model trained with L1 prior as regularizer (aka the Lasso). The optimization objective for Lasso is: Technically the Lasso model is optimizing the same objective function as\\nthe Elastic Net with l1_ratio=1.0 (no L2 penalty). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#lasso',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'Lasso',\n",
       "       'descriptions': 'Linear Model trained with L1 prior as regularizer (aka the Lasso). The optimization objective for Lasso is: Technically the Lasso model is optimizing the same objective function as\\nthe Elastic Net with l1_ratio=1.0 (no L2 penalty). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['alpha=1.0']}}},\n",
       "     {'func_name': 'LassoLars',\n",
       "      'func_desc': 'Lasso model fit with Least Angle Regression a.k.a.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars',\n",
       "      'function_definitions': {'function_name': 'LassoLars',\n",
       "       'full_function': \"class sklearn.linear_model.LassoLars(alpha=1.0, *, fit_intercept=True, verbose=False, precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, positive=False, jitter=None, random_state=None)\",\n",
       "       'function_text': 'Lasso model fit with Least Angle Regression a.k.a. Lars. It is a Linear Model trained with an L1 prior as regularizer. The optimization objective for Lasso is: Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'LassoLars',\n",
       "       'descriptions': 'Lasso model fit with Least Angle Regression a.k.a. Lars. It is a Linear Model trained with an L1 prior as regularizer. The optimization objective for Lasso is: Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['alpha=1.0']}}},\n",
       "     {'func_name': 'LassoLarsIC',\n",
       "      'func_desc': 'Lasso model fit with Lars using BIC or AIC for model selection.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html#sklearn.linear_model.LassoLarsIC',\n",
       "      'function_definitions': {'function_name': 'LassoLarsIC',\n",
       "       'full_function': \"class sklearn.linear_model.LassoLarsIC(criterion='aic', *, fit_intercept=True, verbose=False, precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, positive=False, noise_variance=None)\",\n",
       "       'function_text': 'Lasso model fit with Lars using BIC or AIC for model selection. The optimization objective for Lasso is: AIC is the Akaike information criterion [2] and BIC is the Bayes\\nInformation criterion [3]. Such criteria are useful to select the value\\nof the regularization parameter by making a trade-off between the\\ngoodness of fit and the complexity of the model. A good model should\\nexplain well the data while being simple. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#rde9cc43d0d41-2',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'LassoLarsIC',\n",
       "       'descriptions': 'Lasso model fit with Lars using BIC or AIC for model selection. The optimization objective for Lasso is: AIC is the Akaike information criterion [2] and BIC is the Bayes\\nInformation criterion [3]. Such criteria are useful to select the value\\nof the regularization parameter by making a trade-off between the\\ngoodness of fit and the complexity of the model. A good model should\\nexplain well the data while being simple. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': [\"criterion='aic'\"]}}},\n",
       "     {'func_name': 'OrthogonalMatchingPursuitCV',\n",
       "      'func_desc': 'Cross-validated Orthogonal Matching Pursuit model (OMP).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuitCV.html#sklearn.linear_model.OrthogonalMatchingPursuitCV',\n",
       "      'function_definitions': {'function_name': 'OrthogonalMatchingPursuitCV',\n",
       "       'full_function': 'class sklearn.linear_model.OrthogonalMatchingPursuitCV(*, copy=True, fit_intercept=True, max_iter=None, cv=None, n_jobs=None, verbose=False)',\n",
       "       'function_text': 'Cross-validated Orthogonal Matching Pursuit model (OMP). See glossary entry for cross-validation estimator. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-cross-validation-estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'OrthogonalMatchingPursuitCV',\n",
       "       'descriptions': 'Cross-validated Orthogonal Matching Pursuit model (OMP). See glossary entry for cross-validation estimator. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'ElasticNetCV',\n",
       "      'func_desc': 'Elastic Net model with iterative fitting along a regularization path.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html#sklearn.linear_model.ElasticNetCV',\n",
       "      'function_definitions': {'function_name': 'ElasticNetCV',\n",
       "       'full_function': \"class sklearn.linear_model.ElasticNetCV(*, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, positive=False, random_state=None, selection='cyclic')\",\n",
       "       'function_text': 'Elastic Net model with iterative fitting along a regularization path. See glossary entry for cross-validation estimator. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-cross-validation-estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'ElasticNetCV',\n",
       "       'descriptions': 'Elastic Net model with iterative fitting along a regularization path. See glossary entry for cross-validation estimator. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'LarsCV',\n",
       "      'func_desc': 'Cross-validated Least Angle Regression model.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LarsCV.html#sklearn.linear_model.LarsCV',\n",
       "      'function_definitions': {'function_name': 'LarsCV',\n",
       "       'full_function': \"class sklearn.linear_model.LarsCV(*, fit_intercept=True, verbose=False, max_iter=500, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True)\",\n",
       "       'function_text': 'Cross-validated Least Angle Regression model. See glossary entry for cross-validation estimator. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-cross-validation-estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'LarsCV',\n",
       "       'descriptions': 'Cross-validated Least Angle Regression model. See glossary entry for cross-validation estimator. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'LassoCV',\n",
       "      'func_desc': 'Lasso linear model with iterative fitting along a regularization path.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV',\n",
       "      'function_definitions': {'function_name': 'LassoCV',\n",
       "       'full_function': \"class sklearn.linear_model.LassoCV(*, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, positive=False, random_state=None, selection='cyclic')\",\n",
       "       'function_text': 'Lasso linear model with iterative fitting along a regularization path. See glossary entry for cross-validation estimator. The best model is selected by cross-validation. The optimization objective for Lasso is: Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-cross-validation-estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'LassoCV',\n",
       "       'descriptions': 'Lasso linear model with iterative fitting along a regularization path. See glossary entry for cross-validation estimator. The best model is selected by cross-validation. The optimization objective for Lasso is: Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'LassoLarsCV',\n",
       "      'func_desc': 'Cross-validated Lasso, using the LARS algorithm.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV',\n",
       "      'function_definitions': {'function_name': 'LassoLarsCV',\n",
       "       'full_function': \"class sklearn.linear_model.LassoLarsCV(*, fit_intercept=True, verbose=False, max_iter=500, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True, positive=False)\",\n",
       "       'function_text': 'Cross-validated Lasso, using the LARS algorithm. See glossary entry for cross-validation estimator. The optimization objective for Lasso is: Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-cross-validation-estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'LassoLarsCV',\n",
       "       'descriptions': 'Cross-validated Lasso, using the LARS algorithm. See glossary entry for cross-validation estimator. The optimization objective for Lasso is: Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'OrthogonalMatchingPursuit',\n",
       "      'func_desc': 'Orthogonal Matching Pursuit model (OMP).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuit.html#sklearn.linear_model.OrthogonalMatchingPursuit',\n",
       "      'function_definitions': {'function_name': 'OrthogonalMatchingPursuit',\n",
       "       'full_function': \"class sklearn.linear_model.OrthogonalMatchingPursuit(*, n_nonzero_coefs=None, tol=None, fit_intercept=True, precompute='auto')\",\n",
       "       'function_text': 'Orthogonal Matching Pursuit model (OMP). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#omp',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'OrthogonalMatchingPursuit',\n",
       "       'descriptions': 'Orthogonal Matching Pursuit model (OMP). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Bayesian regressors': [{'func_name': 'ARDRegression',\n",
       "      'func_desc': 'Bayesian ARD regression.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html#sklearn.linear_model.ARDRegression',\n",
       "      'function_definitions': {'function_name': 'ARDRegression',\n",
       "       'full_function': 'class sklearn.linear_model.ARDRegression(*, max_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, copy_X=True, verbose=False)',\n",
       "       'function_text': 'Bayesian ARD regression. Fit the weights of a regression model, using an ARD prior. The weights of\\nthe regression model are assumed to be in Gaussian distributions.\\nAlso estimate the parameters lambda (precisions of the distributions of the\\nweights) and alpha (precision of the distribution of the noise).\\nThe estimation is done by an iterative procedures (Evidence Maximization) Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'ARDRegression',\n",
       "       'descriptions': 'Bayesian ARD regression. Fit the weights of a regression model, using an ARD prior. The weights of\\nthe regression model are assumed to be in Gaussian distributions.\\nAlso estimate the parameters lambda (precisions of the distributions of the\\nweights) and alpha (precision of the distribution of the noise).\\nThe estimation is done by an iterative procedures (Evidence Maximization) Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'BayesianRidge',\n",
       "      'func_desc': 'Bayesian ridge regression.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html#sklearn.linear_model.BayesianRidge',\n",
       "      'function_definitions': {'function_name': 'BayesianRidge',\n",
       "       'full_function': 'class sklearn.linear_model.BayesianRidge(*, max_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, copy_X=True, verbose=False)',\n",
       "       'function_text': 'Bayesian ridge regression. Fit a Bayesian ridge model. See the Notes section for details on this\\nimplementation and the optimization of the regularization parameters\\nlambda (precision of the weights) and alpha (precision of the noise). Read more in the User Guide.\\nFor an intuitive visualization of how the sinusoid is approximated by\\na polynomial using different pairs of initial values, see\\nCurve Fitting with Bayesian Ridge Regression.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'BayesianRidge',\n",
       "       'descriptions': 'Bayesian ridge regression. Fit a Bayesian ridge model. See the Notes section for details on this\\nimplementation and the optimization of the regularization parameters\\nlambda (precision of the weights) and alpha (precision of the noise). Read more in the User Guide.\\nFor an intuitive visualization of how the sinusoid is approximated by\\na polynomial using different pairs of initial values, see\\nCurve Fitting with Bayesian Ridge Regression.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Multi-task linear regressors with variable selection': [{'func_name': 'MultiTaskElasticNet',\n",
       "      'func_desc': 'Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNet.html#sklearn.linear_model.MultiTaskElasticNet',\n",
       "      'function_definitions': {'function_name': 'MultiTaskElasticNet',\n",
       "       'full_function': \"class sklearn.linear_model.MultiTaskElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\",\n",
       "       'function_text': 'Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer. The optimization objective for MultiTaskElasticNet is: Where: i.e. the sum of norms of each row. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#multi-task-elastic-net',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'MultiTaskElasticNet',\n",
       "       'descriptions': 'Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer. The optimization objective for MultiTaskElasticNet is: Where: i.e. the sum of norms of each row. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['alpha=1.0']}}},\n",
       "     {'func_name': 'MultiTaskLasso',\n",
       "      'func_desc': 'Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso',\n",
       "      'function_definitions': {'function_name': 'MultiTaskLasso',\n",
       "       'full_function': \"class sklearn.linear_model.MultiTaskLasso(alpha=1.0, *, fit_intercept=True, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\",\n",
       "       'function_text': 'Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer. The optimization objective for Lasso is: Where: i.e. the sum of norm of each row. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'MultiTaskLasso',\n",
       "       'descriptions': 'Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer. The optimization objective for Lasso is: Where: i.e. the sum of norm of each row. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['alpha=1.0']}}},\n",
       "     {'func_name': 'MultiTaskElasticNetCV',\n",
       "      'func_desc': 'Multi-task L1/L2 ElasticNet with built-in cross-validation.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNetCV.html#sklearn.linear_model.MultiTaskElasticNetCV',\n",
       "      'function_definitions': {'function_name': 'MultiTaskElasticNetCV',\n",
       "       'full_function': \"class sklearn.linear_model.MultiTaskElasticNetCV(*, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, random_state=None, selection='cyclic')\",\n",
       "       'function_text': 'Multi-task L1/L2 ElasticNet with built-in cross-validation. See glossary entry for cross-validation estimator. The optimization objective for MultiTaskElasticNet is: Where: i.e. the sum of norm of each row. Read more in the User Guide. Added in version 0.15.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-cross-validation-estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'MultiTaskElasticNetCV',\n",
       "       'descriptions': 'Multi-task L1/L2 ElasticNet with built-in cross-validation. See glossary entry for cross-validation estimator. The optimization objective for MultiTaskElasticNet is: Where: i.e. the sum of norm of each row. Read more in the User Guide. Added in version 0.15.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'MultiTaskLassoCV',\n",
       "      'func_desc': 'Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLassoCV.html#sklearn.linear_model.MultiTaskLassoCV',\n",
       "      'function_definitions': {'function_name': 'MultiTaskLassoCV',\n",
       "       'full_function': \"class sklearn.linear_model.MultiTaskLassoCV(*, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, random_state=None, selection='cyclic')\",\n",
       "       'function_text': 'Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer. See glossary entry for cross-validation estimator. The optimization objective for MultiTaskLasso is: Where: i.e. the sum of norm of each row. Read more in the User Guide. Added in version 0.15.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-cross-validation-estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'MultiTaskLassoCV',\n",
       "       'descriptions': 'Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer. See glossary entry for cross-validation estimator. The optimization objective for MultiTaskLasso is: Where: i.e. the sum of norm of each row. Read more in the User Guide. Added in version 0.15.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Outlier-robust regressors': [{'func_name': 'HuberRegressor',\n",
       "      'func_desc': 'L2-regularized linear regression model that is robust to outliers.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor',\n",
       "      'function_definitions': {'function_name': 'HuberRegressor',\n",
       "       'full_function': 'class sklearn.linear_model.HuberRegressor(*, epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)',\n",
       "       'function_text': 'L2-regularized linear regression model that is robust to outliers. The Huber Regressor optimizes the squared loss for the samples where\\n|(y - Xw - c) / sigma| < epsilon and the absolute loss for the samples\\nwhere |(y - Xw - c) / sigma| > epsilon, where the model coefficients\\nw, the intercept c and the scale sigma are parameters\\nto be optimized. The parameter sigma makes sure that if y is scaled up\\nor down by a certain factor, one does not need to rescale epsilon to\\nachieve the same robustness. Note that this does not take into account\\nthe fact that the different features of X may be of different scales. The Huber loss function has the advantage of not being heavily influenced\\nby the outliers while not completely ignoring their effect. Read more in the User Guide Added in version 0.18.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#huber-regression',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'HuberRegressor',\n",
       "       'descriptions': 'L2-regularized linear regression model that is robust to outliers. The Huber Regressor optimizes the squared loss for the samples where\\n|(y - Xw - c) / sigma| < epsilon and the absolute loss for the samples\\nwhere |(y - Xw - c) / sigma| > epsilon, where the model coefficients\\nw, the intercept c and the scale sigma are parameters\\nto be optimized. The parameter sigma makes sure that if y is scaled up\\nor down by a certain factor, one does not need to rescale epsilon to\\nachieve the same robustness. Note that this does not take into account\\nthe fact that the different features of X may be of different scales. The Huber loss function has the advantage of not being heavily influenced\\nby the outliers while not completely ignoring their effect. Read more in the User Guide Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'RANSACRegressor',\n",
       "      'func_desc': 'RANSAC (RANdom SAmple Consensus) algorithm.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html#sklearn.linear_model.RANSACRegressor',\n",
       "      'function_definitions': {'function_name': 'RANSACRegressor',\n",
       "       'full_function': \"class sklearn.linear_model.RANSACRegressor(estimator=None, *, min_samples=None, residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, max_skips=inf, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, loss='absolute_error', random_state=None)\",\n",
       "       'function_text': 'RANSAC (RANdom SAmple Consensus) algorithm. RANSAC is an iterative algorithm for the robust estimation of parameters\\nfrom a subset of inliers from the complete data set. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#ransac-regression',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'RANSACRegressor',\n",
       "       'descriptions': 'RANSAC (RANdom SAmple Consensus) algorithm. RANSAC is an iterative algorithm for the robust estimation of parameters\\nfrom a subset of inliers from the complete data set. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {},\n",
       "        'required': ['estimator=None']}}},\n",
       "     {'func_name': 'QuantileRegressor',\n",
       "      'func_desc': 'Linear regression model that predicts conditional quantiles.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.QuantileRegressor.html#sklearn.linear_model.QuantileRegressor',\n",
       "      'function_definitions': {'function_name': 'QuantileRegressor',\n",
       "       'full_function': \"class sklearn.linear_model.QuantileRegressor(*, quantile=0.5, alpha=1.0, fit_intercept=True, solver='highs', solver_options=None)\",\n",
       "       'function_text': 'Linear regression model that predicts conditional quantiles. The linear QuantileRegressor optimizes the pinball loss for a\\ndesired quantile and is robust to outliers. This model uses an L1 regularization like\\nLasso. Read more in the User Guide. Added in version 1.0.',\n",
       "       'func_text_user_guide': '#sklearn.linear_model.QuantileRegressor',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'QuantileRegressor',\n",
       "       'descriptions': 'Linear regression model that predicts conditional quantiles. The linear QuantileRegressor optimizes the pinball loss for a\\ndesired quantile and is robust to outliers. This model uses an L1 regularization like\\nLasso. Read more in the User Guide. Added in version 1.0.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'TheilSenRegressor',\n",
       "      'func_desc': 'Theil-Sen Estimator: robust multivariate regression model.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor',\n",
       "      'function_definitions': {'function_name': 'TheilSenRegressor',\n",
       "       'full_function': 'class sklearn.linear_model.TheilSenRegressor(*, fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=None, verbose=False)',\n",
       "       'function_text': 'Theil-Sen Estimator: robust multivariate regression model. The algorithm calculates least square solutions on subsets with size\\nn_subsamples of the samples in X. Any value of n_subsamples between the\\nnumber of features and samples leads to an estimator with a compromise\\nbetween robustness and efficiency. Since the number of least square\\nsolutions is “n_samples choose n_subsamples”, it can be extremely large\\nand can therefore be limited with max_subpopulation. If this limit is\\nreached, the subsets are chosen randomly. In a final step, the spatial\\nmedian (or L1 median) is calculated of all least square solutions. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#theil-sen-regression',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'TheilSenRegressor',\n",
       "       'descriptions': 'Theil-Sen Estimator: robust multivariate regression model. The algorithm calculates least square solutions on subsets with size\\nn_subsamples of the samples in X. Any value of n_subsamples between the\\nnumber of features and samples leads to an estimator with a compromise\\nbetween robustness and efficiency. Since the number of least square\\nsolutions is “n_samples choose n_subsamples”, it can be extremely large\\nand can therefore be limited with max_subpopulation. If this limit is\\nreached, the subsets are chosen randomly. In a final step, the spatial\\nmedian (or L1 median) is calculated of all least square solutions. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Generalized linear models (GLM) for regression': [{'func_name': 'GammaRegressor',\n",
       "      'func_desc': 'Generalized Linear Model with a Gamma distribution.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.GammaRegressor.html#sklearn.linear_model.GammaRegressor',\n",
       "      'function_definitions': {'function_name': 'GammaRegressor',\n",
       "       'full_function': \"class sklearn.linear_model.GammaRegressor(*, alpha=1.0, fit_intercept=True, solver='lbfgs', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\",\n",
       "       'function_text': 'Generalized Linear Model with a Gamma distribution. This regressor uses the ‘log’ link function. Read more in the User Guide. Added in version 0.23.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'GammaRegressor',\n",
       "       'descriptions': 'Generalized Linear Model with a Gamma distribution. This regressor uses the ‘log’ link function. Read more in the User Guide. Added in version 0.23.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'TweedieRegressor',\n",
       "      'func_desc': 'Generalized Linear Model with a Tweedie distribution.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TweedieRegressor.html#sklearn.linear_model.TweedieRegressor',\n",
       "      'function_definitions': {'function_name': 'TweedieRegressor',\n",
       "       'full_function': \"class sklearn.linear_model.TweedieRegressor(*, power=0.0, alpha=1.0, fit_intercept=True, link='auto', solver='lbfgs', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\",\n",
       "       'function_text': 'Generalized Linear Model with a Tweedie distribution. This estimator can be used to model different GLMs depending on the\\npower parameter, which determines the underlying distribution. Read more in the User Guide. Added in version 0.23.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'TweedieRegressor',\n",
       "       'descriptions': 'Generalized Linear Model with a Tweedie distribution. This estimator can be used to model different GLMs depending on the\\npower parameter, which determines the underlying distribution. Read more in the User Guide. Added in version 0.23.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'PoissonRegressor',\n",
       "      'func_desc': 'Generalized Linear Model with a Poisson distribution.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html#sklearn.linear_model.PoissonRegressor',\n",
       "      'function_definitions': {'function_name': 'PoissonRegressor',\n",
       "       'full_function': \"class sklearn.linear_model.PoissonRegressor(*, alpha=1.0, fit_intercept=True, solver='lbfgs', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\",\n",
       "       'function_text': 'Generalized Linear Model with a Poisson distribution. This regressor uses the ‘log’ link function. Read more in the User Guide. Added in version 0.23.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'PoissonRegressor',\n",
       "       'descriptions': 'Generalized Linear Model with a Poisson distribution. This regressor uses the ‘log’ link function. Read more in the User Guide. Added in version 0.23.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Miscellaneous': [{'func_name': 'PassiveAggressiveRegressor',\n",
       "      'func_desc': 'Passive Aggressive Regressor.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveRegressor.html#sklearn.linear_model.PassiveAggressiveRegressor',\n",
       "      'function_definitions': {'function_name': 'PassiveAggressiveRegressor',\n",
       "       'full_function': \"class sklearn.linear_model.PassiveAggressiveRegressor(*, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='epsilon_insensitive', epsilon=0.1, random_state=None, warm_start=False, average=False)\",\n",
       "       'function_text': 'Passive Aggressive Regressor. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'lars_path',\n",
       "      'func_desc': 'Compute Least Angle Regression or Lasso path using the LARS algorithm.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lars_path.html#sklearn.linear_model.lars_path',\n",
       "      'function_definitions': {'function_name': 'lars_path',\n",
       "       'full_function': \"sklearn.linear_model.lars_path(X, y, Xy=None, *, Gram=None, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.220446049250313e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)\",\n",
       "       'function_text': 'Compute Least Angle Regression or Lasso path using the LARS algorithm. The optimization objective for the case method=’lasso’ is: in the case of method=’lar’, the objective function is only known in\\nthe form of an implicit equation (see discussion in [1]). Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#r2391cff0bbde-1',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'None or ndarray of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Input data. Note that if X is None then the Gram matrix must be\\nspecified, i.e., cannot be None or False.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'None or ndarray of shape (n_samples,)',\n",
       "         'param_desc': 'Input targets.\\n'},\n",
       "        {'param_name': 'Xy',\n",
       "         'param_type': 'array-like of shape (n_features,), default=None',\n",
       "         'param_desc': 'Xy = X.T @ y that can be precomputed. It is useful\\nonly when the Gram matrix is precomputed.\\n'},\n",
       "        {'param_name': 'Gram',\n",
       "         'param_type': 'None, ‘auto’, bool, ndarray of shape (n_features, n_features),             default=None',\n",
       "         'param_desc': \"Precomputed Gram matrix X.T @ X, if 'auto', the Gram\\nmatrix is precomputed from the given X, if there are more samples\\nthan features.\\n\"},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=500',\n",
       "         'param_desc': 'Maximum number of iterations to perform, set to infinity for no limit.\\n'},\n",
       "        {'param_name': 'alpha_min',\n",
       "         'param_type': 'float, default=0',\n",
       "         'param_desc': 'Minimum correlation along the path. It corresponds to the\\nregularization parameter alpha in the Lasso.\\n'},\n",
       "        {'param_name': 'method',\n",
       "         'param_type': '{‘lar’, ‘lasso’}, default=’lar’',\n",
       "         'param_desc': \"Specifies the returned model. Select 'lar' for Least Angle\\nRegression, 'lasso' for the Lasso.\\n\"},\n",
       "        {'param_name': 'copy_X',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, X is overwritten.\\n'},\n",
       "        {'param_name': 'eps',\n",
       "         'param_type': 'float, default=np.finfo(float).eps',\n",
       "         'param_desc': 'The machine-precision regularization in the computation of the\\nCholesky diagonal factors. Increase this for very ill-conditioned\\nsystems. Unlike the tol parameter in some iterative\\noptimization-based algorithms, this parameter does not control\\nthe tolerance of the optimization.\\n'},\n",
       "        {'param_name': 'copy_Gram',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, Gram is overwritten.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Controls output verbosity.\\n'},\n",
       "        {'param_name': 'return_path',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, returns the entire path, else returns only the\\nlast point of the path.\\n'},\n",
       "        {'param_name': 'return_n_iter',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to return the number of iterations.\\n'},\n",
       "        {'param_name': 'positive',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Restrict coefficients to be >= 0.\\nThis option is only allowed with method ‘lasso’. Note that the model\\ncoefficients will not converge to the ordinary-least-squares solution\\nfor small values of alpha. Only coefficients up to the smallest alpha\\nvalue (alphas_[alphas_ > 0.].min() when fit_path=True) reached by\\nthe stepwise Lars-Lasso algorithm are typically in congruence with the\\nsolution of the coordinate descent lasso_path function.\\n'}]},\n",
       "      'function_calling': {'name': 'lars_path',\n",
       "       'descriptions': 'Compute Least Angle Regression or Lasso path using the LARS algorithm. The optimization objective for the case method=’lasso’ is: in the case of method=’lar’, the objective function is only known in\\nthe form of an implicit equation (see discussion in [1]). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'None or ndarray of shape (n_samples, n_features). Input data. Note that if X is None then the Gram matrix must be\\nspecified, i.e., cannot be None or False.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'None or ndarray of shape (n_samples,). Input targets.\\n'},\n",
       "         'Xy': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_features,), default=None. Xy = X.T @ y that can be precomputed. It is useful\\nonly when the Gram matrix is precomputed.\\n'},\n",
       "         'Gram': {'type': 'boolean',\n",
       "          'description': \"None, ‘auto’, bool, ndarray of shape (n_features, n_features),             default=None. Precomputed Gram matrix X.T @ X, if 'auto', the Gram\\nmatrix is precomputed from the given X, if there are more samples\\nthan features.\\n\"},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=500. Maximum number of iterations to perform, set to infinity for no limit.\\n'},\n",
       "         'alpha_min': {'type': 'number',\n",
       "          'description': 'float, default=0. Minimum correlation along the path. It corresponds to the\\nregularization parameter alpha in the Lasso.\\n'},\n",
       "         'method': {'type': 'string',\n",
       "          'enum': ['lar', 'lasso'],\n",
       "          'description': \"{‘lar’, ‘lasso’}, default=’lar’. Specifies the returned model. Select 'lar' for Least Angle\\nRegression, 'lasso' for the Lasso.\\n\"},\n",
       "         'copy_X': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, X is overwritten.\\n'},\n",
       "         'eps': {'type': 'number',\n",
       "          'description': 'float, default=np.finfo(float).eps. The machine-precision regularization in the computation of the\\nCholesky diagonal factors. Increase this for very ill-conditioned\\nsystems. Unlike the tol parameter in some iterative\\noptimization-based algorithms, this parameter does not control\\nthe tolerance of the optimization.\\n'},\n",
       "         'copy_Gram': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, Gram is overwritten.\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'int, default=0. Controls output verbosity.\\n'},\n",
       "         'return_path': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, returns the entire path, else returns only the\\nlast point of the path.\\n'},\n",
       "         'return_n_iter': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to return the number of iterations.\\n'},\n",
       "         'positive': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Restrict coefficients to be >= 0.\\nThis option is only allowed with method ‘lasso’. Note that the model\\ncoefficients will not converge to the ordinary-least-squares solution\\nfor small values of alpha. Only coefficients up to the smallest alpha\\nvalue (alphas_[alphas_ > 0.].min() when fit_path=True) reached by\\nthe stepwise Lars-Lasso algorithm are typically in congruence with the\\nsolution of the coordinate descent lasso_path function.\\n'}},\n",
       "        'required': ['X', 'y', 'Xy=None']}}},\n",
       "     {'func_name': 'lasso_path',\n",
       "      'func_desc': 'Compute Lasso path with coordinate descent.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lasso_path.html#sklearn.linear_model.lasso_path',\n",
       "      'function_definitions': {'function_name': 'lasso_path',\n",
       "       'full_function': \"sklearn.linear_model.lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\",\n",
       "       'function_text': 'Compute Lasso path with coordinate descent. The Lasso optimization function varies for mono and multi-outputs. For mono-output tasks it is: For multi-output tasks it is: Where: i.e. the sum of norm of each row. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#lasso',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data. Pass directly as Fortran-contiguous data to avoid\\nunnecessary memory duplication. If y is mono-output then X\\ncan be sparse.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)',\n",
       "         'param_desc': 'Target values.\\n'},\n",
       "        {'param_name': 'eps',\n",
       "         'param_type': 'float, default=1e-3',\n",
       "         'param_desc': 'Length of the path. eps=1e-3 means that\\nalpha_min / alpha_max = 1e-3.\\n'},\n",
       "        {'param_name': 'n_alphas',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'Number of alphas along the regularization path.\\n'},\n",
       "        {'param_name': 'alphas',\n",
       "         'param_type': 'array-like, default=None',\n",
       "         'param_desc': 'List of alphas where to compute the models.\\nIf None alphas are set automatically.\\n'},\n",
       "        {'param_name': 'precompute',\n",
       "         'param_type': '‘auto’, bool or array-like of shape             (n_features, n_features), default=’auto’',\n",
       "         'param_desc': \"Whether to use a precomputed Gram matrix to speed up\\ncalculations. If set to 'auto' let us decide. The Gram\\nmatrix can also be passed as argument.\\n\"},\n",
       "        {'param_name': 'Xy',\n",
       "         'param_type': 'array-like of shape (n_features,) or (n_features, n_targets),         default=None',\n",
       "         'param_desc': 'Xy = np.dot(X.T, y) that can be precomputed. It is useful\\nonly when the Gram matrix is precomputed.\\n'},\n",
       "        {'param_name': 'copy_X',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, X will be copied; else, it may be overwritten.\\n'},\n",
       "        {'param_name': 'coef_init',\n",
       "         'param_type': 'array-like of shape (n_features, ), default=None',\n",
       "         'param_desc': 'The initial values of the coefficients.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'bool or int, default=False',\n",
       "         'param_desc': 'Amount of verbosity.\\n'},\n",
       "        {'param_name': 'return_n_iter',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to return the number of iterations or not.\\n'},\n",
       "        {'param_name': 'positive',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If set to True, forces coefficients to be positive.\\n(Only allowed when y.ndim == 1).\\n'},\n",
       "        {'param_name': '**params',\n",
       "         'param_type': 'kwargs',\n",
       "         'param_desc': 'Keyword arguments passed to the coordinate descent solver.\\n'}]},\n",
       "      'function_calling': {'name': 'lasso_path',\n",
       "       'descriptions': 'Compute Lasso path with coordinate descent. The Lasso optimization function varies for mono and multi-outputs. For mono-output tasks it is: For multi-output tasks it is: Where: i.e. the sum of norm of each row. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Training data. Pass directly as Fortran-contiguous data to avoid\\nunnecessary memory duplication. If y is mono-output then X\\ncan be sparse.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets). Target values.\\n'},\n",
       "         'eps': {'type': 'number',\n",
       "          'description': 'float, default=1e-3. Length of the path. eps=1e-3 means that\\nalpha_min / alpha_max = 1e-3.\\n'},\n",
       "         'n_alphas': {'type': 'integer',\n",
       "          'description': 'int, default=100. Number of alphas along the regularization path.\\n'},\n",
       "         'alphas': {'type': 'array',\n",
       "          'description': 'array-like, default=None. List of alphas where to compute the models.\\nIf None alphas are set automatically.\\n'},\n",
       "         'precompute': {'type': 'boolean',\n",
       "          'description': \"‘auto’, bool or array-like of shape             (n_features, n_features), default=’auto’. Whether to use a precomputed Gram matrix to speed up\\ncalculations. If set to 'auto' let us decide. The Gram\\nmatrix can also be passed as argument.\\n\"},\n",
       "         'Xy': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_features,) or (n_features, n_targets),         default=None. Xy = np.dot(X.T, y) that can be precomputed. It is useful\\nonly when the Gram matrix is precomputed.\\n'},\n",
       "         'copy_X': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, X will be copied; else, it may be overwritten.\\n'},\n",
       "         'coef_init': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_features, ), default=None. The initial values of the coefficients.\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'bool or int, default=False. Amount of verbosity.\\n'},\n",
       "         'return_n_iter': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to return the number of iterations or not.\\n'},\n",
       "         'positive': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If set to True, forces coefficients to be positive.\\n(Only allowed when y.ndim == 1).\\n'}},\n",
       "        'required': ['X', 'y']}}},\n",
       "     {'func_name': 'orthogonal_mp_gram',\n",
       "      'func_desc': 'Gram Orthogonal Matching Pursuit (OMP).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.orthogonal_mp_gram.html#sklearn.linear_model.orthogonal_mp_gram',\n",
       "      'function_definitions': {'function_name': 'orthogonal_mp_gram',\n",
       "       'full_function': 'sklearn.linear_model.orthogonal_mp_gram(Gram, Xy, *, n_nonzero_coefs=None, tol=None, norms_squared=None, copy_Gram=True, copy_Xy=True, return_path=False, return_n_iter=False)',\n",
       "       'function_text': 'Gram Orthogonal Matching Pursuit (OMP). Solves n_targets Orthogonal Matching Pursuit problems using only\\nthe Gram matrix X.T * X and the product X.T * y. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#omp',\n",
       "       'parameter_names_desc': [{'param_name': 'Gram',\n",
       "         'param_type': 'array-like of shape (n_features, n_features)',\n",
       "         'param_desc': 'Gram matrix of the input data: X.T * X.\\n'},\n",
       "        {'param_name': 'Xy',\n",
       "         'param_type': 'array-like of shape (n_features,) or (n_features, n_targets)',\n",
       "         'param_desc': 'Input targets multiplied by X: X.T * y.\\n'},\n",
       "        {'param_name': 'n_nonzero_coefs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Desired number of non-zero entries in the solution. If None (by\\ndefault) this value is set to 10% of n_features.\\n'},\n",
       "        {'param_name': 'tol',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': 'Maximum squared norm of the residual. If not None,\\noverrides n_nonzero_coefs.\\n'},\n",
       "        {'param_name': 'norms_squared',\n",
       "         'param_type': 'array-like of shape (n_targets,), default=None',\n",
       "         'param_desc': 'Squared L2 norms of the lines of y. Required if tol is not None.\\n'},\n",
       "        {'param_name': 'copy_Gram',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether the gram matrix must be copied by the algorithm. A False\\nvalue is only helpful if it is already Fortran-ordered, otherwise a\\ncopy is made anyway.\\n'},\n",
       "        {'param_name': 'copy_Xy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether the covariance vector Xy must be copied by the algorithm.\\nIf False, it may be overwritten.\\n'},\n",
       "        {'param_name': 'return_path',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to return every value of the nonzero coefficients along the\\nforward path. Useful for cross-validation.\\n'},\n",
       "        {'param_name': 'return_n_iter',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether or not to return the number of iterations.\\n'}]},\n",
       "      'function_calling': {'name': 'orthogonal_mp_gram',\n",
       "       'descriptions': 'Gram Orthogonal Matching Pursuit (OMP). Solves n_targets Orthogonal Matching Pursuit problems using only\\nthe Gram matrix X.T * X and the product X.T * y. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'Gram': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_features, n_features). Gram matrix of the input data: X.T * X.\\n'},\n",
       "         'Xy': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_features,) or (n_features, n_targets). Input targets multiplied by X: X.T * y.\\n'},\n",
       "         'n_nonzero_coefs': {'type': 'integer',\n",
       "          'description': 'int, default=None. Desired number of non-zero entries in the solution. If None (by\\ndefault) this value is set to 10% of n_features.\\n'},\n",
       "         'tol': {'type': 'number',\n",
       "          'description': 'float, default=None. Maximum squared norm of the residual. If not None,\\noverrides n_nonzero_coefs.\\n'},\n",
       "         'norms_squared': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_targets,), default=None. Squared L2 norms of the lines of y. Required if tol is not None.\\n'},\n",
       "         'copy_Gram': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether the gram matrix must be copied by the algorithm. A False\\nvalue is only helpful if it is already Fortran-ordered, otherwise a\\ncopy is made anyway.\\n'},\n",
       "         'copy_Xy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether the covariance vector Xy must be copied by the algorithm.\\nIf False, it may be overwritten.\\n'},\n",
       "         'return_path': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to return every value of the nonzero coefficients along the\\nforward path. Useful for cross-validation.\\n'},\n",
       "         'return_n_iter': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether or not to return the number of iterations.\\n'}},\n",
       "        'required': ['Gram', 'Xy']}}},\n",
       "     {'func_name': 'enet_path',\n",
       "      'func_desc': 'Compute elastic net path with coordinate descent.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.enet_path.html#sklearn.linear_model.enet_path',\n",
       "      'function_definitions': {'function_name': 'enet_path',\n",
       "       'full_function': \"sklearn.linear_model.enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\",\n",
       "       'function_text': 'Compute elastic net path with coordinate descent. The elastic net optimization function varies for mono and multi-outputs. For mono-output tasks it is: For multi-output tasks it is: Where: i.e. the sum of norm of each row. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#elastic-net',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data. Pass directly as Fortran-contiguous data to avoid\\nunnecessary memory duplication. If y is mono-output then X\\ncan be sparse.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)',\n",
       "         'param_desc': 'Target values.\\n'},\n",
       "        {'param_name': 'l1_ratio',\n",
       "         'param_type': 'float, default=0.5',\n",
       "         'param_desc': 'Number between 0 and 1 passed to elastic net (scaling between\\nl1 and l2 penalties). l1_ratio=1 corresponds to the Lasso.\\n'},\n",
       "        {'param_name': 'eps',\n",
       "         'param_type': 'float, default=1e-3',\n",
       "         'param_desc': 'Length of the path. eps=1e-3 means that\\nalpha_min / alpha_max = 1e-3.\\n'},\n",
       "        {'param_name': 'n_alphas',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'Number of alphas along the regularization path.\\n'},\n",
       "        {'param_name': 'alphas',\n",
       "         'param_type': 'array-like, default=None',\n",
       "         'param_desc': 'List of alphas where to compute the models.\\nIf None alphas are set automatically.\\n'},\n",
       "        {'param_name': 'precompute',\n",
       "         'param_type': '‘auto’, bool or array-like of shape             (n_features, n_features), default=’auto’',\n",
       "         'param_desc': \"Whether to use a precomputed Gram matrix to speed up\\ncalculations. If set to 'auto' let us decide. The Gram\\nmatrix can also be passed as argument.\\n\"},\n",
       "        {'param_name': 'Xy',\n",
       "         'param_type': 'array-like of shape (n_features,) or (n_features, n_targets),         default=None',\n",
       "         'param_desc': 'Xy = np.dot(X.T, y) that can be precomputed. It is useful\\nonly when the Gram matrix is precomputed.\\n'},\n",
       "        {'param_name': 'copy_X',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, X will be copied; else, it may be overwritten.\\n'},\n",
       "        {'param_name': 'coef_init',\n",
       "         'param_type': 'array-like of shape (n_features, ), default=None',\n",
       "         'param_desc': 'The initial values of the coefficients.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'bool or int, default=False',\n",
       "         'param_desc': 'Amount of verbosity.\\n'},\n",
       "        {'param_name': 'return_n_iter',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to return the number of iterations or not.\\n'},\n",
       "        {'param_name': 'positive',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If set to True, forces coefficients to be positive.\\n(Only allowed when y.ndim == 1).\\n'},\n",
       "        {'param_name': 'check_input',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If set to False, the input validation checks are skipped (including the\\nGram matrix when provided). It is assumed that they are handled\\nby the caller.\\n'},\n",
       "        {'param_name': '**params',\n",
       "         'param_type': 'kwargs',\n",
       "         'param_desc': 'Keyword arguments passed to the coordinate descent solver.\\n'}]},\n",
       "      'function_calling': {'name': 'enet_path',\n",
       "       'descriptions': 'Compute elastic net path with coordinate descent. The elastic net optimization function varies for mono and multi-outputs. For mono-output tasks it is: For multi-output tasks it is: Where: i.e. the sum of norm of each row. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Training data. Pass directly as Fortran-contiguous data to avoid\\nunnecessary memory duplication. If y is mono-output then X\\ncan be sparse.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets). Target values.\\n'},\n",
       "         'l1_ratio': {'type': 'number',\n",
       "          'description': 'float, default=0.5. Number between 0 and 1 passed to elastic net (scaling between\\nl1 and l2 penalties). l1_ratio=1 corresponds to the Lasso.\\n'},\n",
       "         'eps': {'type': 'number',\n",
       "          'description': 'float, default=1e-3. Length of the path. eps=1e-3 means that\\nalpha_min / alpha_max = 1e-3.\\n'},\n",
       "         'n_alphas': {'type': 'integer',\n",
       "          'description': 'int, default=100. Number of alphas along the regularization path.\\n'},\n",
       "         'alphas': {'type': 'array',\n",
       "          'description': 'array-like, default=None. List of alphas where to compute the models.\\nIf None alphas are set automatically.\\n'},\n",
       "         'precompute': {'type': 'boolean',\n",
       "          'description': \"‘auto’, bool or array-like of shape             (n_features, n_features), default=’auto’. Whether to use a precomputed Gram matrix to speed up\\ncalculations. If set to 'auto' let us decide. The Gram\\nmatrix can also be passed as argument.\\n\"},\n",
       "         'Xy': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_features,) or (n_features, n_targets),         default=None. Xy = np.dot(X.T, y) that can be precomputed. It is useful\\nonly when the Gram matrix is precomputed.\\n'},\n",
       "         'copy_X': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, X will be copied; else, it may be overwritten.\\n'},\n",
       "         'coef_init': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_features, ), default=None. The initial values of the coefficients.\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'bool or int, default=False. Amount of verbosity.\\n'},\n",
       "         'return_n_iter': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to return the number of iterations or not.\\n'},\n",
       "         'positive': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If set to True, forces coefficients to be positive.\\n(Only allowed when y.ndim == 1).\\n'},\n",
       "         'check_input': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If set to False, the input validation checks are skipped (including the\\nGram matrix when provided). It is assumed that they are handled\\nby the caller.\\n'}},\n",
       "        'required': ['X', 'y']}}},\n",
       "     {'func_name': 'lars_path_gram',\n",
       "      'func_desc': 'The lars_path in the sufficient stats mode.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lars_path_gram.html#sklearn.linear_model.lars_path_gram',\n",
       "      'function_definitions': {'function_name': 'lars_path_gram',\n",
       "       'full_function': \"sklearn.linear_model.lars_path_gram(Xy, Gram, *, n_samples, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.220446049250313e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)\",\n",
       "       'function_text': 'The lars_path in the sufficient stats mode. The optimization objective for the case method=’lasso’ is: in the case of method=’lar’, the objective function is only known in\\nthe form of an implicit equation (see discussion in [1]). Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#r34229eeff553-1',\n",
       "       'parameter_names_desc': [{'param_name': 'Xy',\n",
       "         'param_type': 'ndarray of shape (n_features,)',\n",
       "         'param_desc': 'Xy = X.T @ y.\\n'},\n",
       "        {'param_name': 'Gram',\n",
       "         'param_type': 'ndarray of shape (n_features, n_features)',\n",
       "         'param_desc': 'Gram = X.T @ X.\\n'},\n",
       "        {'param_name': 'n_samples',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Equivalent size of sample.\\n'},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=500',\n",
       "         'param_desc': 'Maximum number of iterations to perform, set to infinity for no limit.\\n'},\n",
       "        {'param_name': 'alpha_min',\n",
       "         'param_type': 'float, default=0',\n",
       "         'param_desc': 'Minimum correlation along the path. It corresponds to the\\nregularization parameter alpha parameter in the Lasso.\\n'},\n",
       "        {'param_name': 'method',\n",
       "         'param_type': '{‘lar’, ‘lasso’}, default=’lar’',\n",
       "         'param_desc': \"Specifies the returned model. Select 'lar' for Least Angle\\nRegression, 'lasso' for the Lasso.\\n\"},\n",
       "        {'param_name': 'copy_X',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, X is overwritten.\\n'},\n",
       "        {'param_name': 'eps',\n",
       "         'param_type': 'float, default=np.finfo(float).eps',\n",
       "         'param_desc': 'The machine-precision regularization in the computation of the\\nCholesky diagonal factors. Increase this for very ill-conditioned\\nsystems. Unlike the tol parameter in some iterative\\noptimization-based algorithms, this parameter does not control\\nthe tolerance of the optimization.\\n'},\n",
       "        {'param_name': 'copy_Gram',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, Gram is overwritten.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Controls output verbosity.\\n'},\n",
       "        {'param_name': 'return_path',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If return_path==True returns the entire path, else returns only the\\nlast point of the path.\\n'},\n",
       "        {'param_name': 'return_n_iter',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to return the number of iterations.\\n'},\n",
       "        {'param_name': 'positive',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Restrict coefficients to be >= 0.\\nThis option is only allowed with method ‘lasso’. Note that the model\\ncoefficients will not converge to the ordinary-least-squares solution\\nfor small values of alpha. Only coefficients up to the smallest alpha\\nvalue (alphas_[alphas_ > 0.].min() when fit_path=True) reached by\\nthe stepwise Lars-Lasso algorithm are typically in congruence with the\\nsolution of the coordinate descent lasso_path function.\\n'}]},\n",
       "      'function_calling': {'name': 'lars_path_gram',\n",
       "       'descriptions': 'The lars_path in the sufficient stats mode. The optimization objective for the case method=’lasso’ is: in the case of method=’lar’, the objective function is only known in\\nthe form of an implicit equation (see discussion in [1]). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'Xy': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_features,). Xy = X.T @ y.\\n'},\n",
       "         'Gram': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_features, n_features). Gram = X.T @ X.\\n'},\n",
       "         'n_samples': {'type': 'integer',\n",
       "          'description': 'int. Equivalent size of sample.\\n'},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=500. Maximum number of iterations to perform, set to infinity for no limit.\\n'},\n",
       "         'alpha_min': {'type': 'number',\n",
       "          'description': 'float, default=0. Minimum correlation along the path. It corresponds to the\\nregularization parameter alpha parameter in the Lasso.\\n'},\n",
       "         'method': {'type': 'string',\n",
       "          'enum': ['lar', 'lasso'],\n",
       "          'description': \"{‘lar’, ‘lasso’}, default=’lar’. Specifies the returned model. Select 'lar' for Least Angle\\nRegression, 'lasso' for the Lasso.\\n\"},\n",
       "         'copy_X': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, X is overwritten.\\n'},\n",
       "         'eps': {'type': 'number',\n",
       "          'description': 'float, default=np.finfo(float).eps. The machine-precision regularization in the computation of the\\nCholesky diagonal factors. Increase this for very ill-conditioned\\nsystems. Unlike the tol parameter in some iterative\\noptimization-based algorithms, this parameter does not control\\nthe tolerance of the optimization.\\n'},\n",
       "         'copy_Gram': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, Gram is overwritten.\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'int, default=0. Controls output verbosity.\\n'},\n",
       "         'return_path': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If return_path==True returns the entire path, else returns only the\\nlast point of the path.\\n'},\n",
       "         'return_n_iter': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to return the number of iterations.\\n'},\n",
       "         'positive': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Restrict coefficients to be >= 0.\\nThis option is only allowed with method ‘lasso’. Note that the model\\ncoefficients will not converge to the ordinary-least-squares solution\\nfor small values of alpha. Only coefficients up to the smallest alpha\\nvalue (alphas_[alphas_ > 0.].min() when fit_path=True) reached by\\nthe stepwise Lars-Lasso algorithm are typically in congruence with the\\nsolution of the coordinate descent lasso_path function.\\n'}},\n",
       "        'required': ['Xy', 'Gram']}}},\n",
       "     {'func_name': 'orthogonal_mp',\n",
       "      'func_desc': 'Orthogonal Matching Pursuit (OMP).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.orthogonal_mp.html#sklearn.linear_model.orthogonal_mp',\n",
       "      'function_definitions': {'function_name': 'orthogonal_mp',\n",
       "       'full_function': 'sklearn.linear_model.orthogonal_mp(X, y, *, n_nonzero_coefs=None, tol=None, precompute=False, copy_X=True, return_path=False, return_n_iter=False)',\n",
       "       'function_text': 'Orthogonal Matching Pursuit (OMP). Solves n_targets Orthogonal Matching Pursuit problems.\\nAn instance of the problem has the form: When parametrized by the number of non-zero coefficients using\\nn_nonzero_coefs:\\nargmin ||y - Xgamma||^2 subject to ||gamma||_0 <= n_{nonzero coefs} When parametrized by error using the parameter tol:\\nargmin ||gamma||_0 subject to ||y - Xgamma||^2 <= tol Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#omp',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Input data. Columns are assumed to have unit norm.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'ndarray of shape (n_samples,) or (n_samples, n_targets)',\n",
       "         'param_desc': 'Input targets.\\n'},\n",
       "        {'param_name': 'n_nonzero_coefs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Desired number of non-zero entries in the solution. If None (by\\ndefault) this value is set to 10% of n_features.\\n'},\n",
       "        {'param_name': 'tol',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': 'Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs.\\n'},\n",
       "        {'param_name': 'precompute',\n",
       "         'param_type': '‘auto’ or bool, default=False',\n",
       "         'param_desc': 'Whether to perform precomputations. Improves performance when n_targets\\nor n_samples is very large.\\n'},\n",
       "        {'param_name': 'copy_X',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether the design matrix X must be copied by the algorithm. A false\\nvalue is only helpful if X is already Fortran-ordered, otherwise a\\ncopy is made anyway.\\n'},\n",
       "        {'param_name': 'return_path',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to return every value of the nonzero coefficients along the\\nforward path. Useful for cross-validation.\\n'},\n",
       "        {'param_name': 'return_n_iter',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether or not to return the number of iterations.\\n'}]},\n",
       "      'function_calling': {'name': 'orthogonal_mp',\n",
       "       'descriptions': 'Orthogonal Matching Pursuit (OMP). Solves n_targets Orthogonal Matching Pursuit problems.\\nAn instance of the problem has the form: When parametrized by the number of non-zero coefficients using\\nn_nonzero_coefs:\\nargmin ||y - Xgamma||^2 subject to ||gamma||_0 <= n_{nonzero coefs} When parametrized by error using the parameter tol:\\nargmin ||gamma||_0 subject to ||y - Xgamma||^2 <= tol Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Input data. Columns are assumed to have unit norm.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples,) or (n_samples, n_targets). Input targets.\\n'},\n",
       "         'n_nonzero_coefs': {'type': 'integer',\n",
       "          'description': 'int, default=None. Desired number of non-zero entries in the solution. If None (by\\ndefault) this value is set to 10% of n_features.\\n'},\n",
       "         'tol': {'type': 'number',\n",
       "          'description': 'float, default=None. Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs.\\n'},\n",
       "         'precompute': {'type': 'boolean',\n",
       "          'description': '‘auto’ or bool, default=False. Whether to perform precomputations. Improves performance when n_targets\\nor n_samples is very large.\\n'},\n",
       "         'copy_X': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether the design matrix X must be copied by the algorithm. A false\\nvalue is only helpful if X is already Fortran-ordered, otherwise a\\ncopy is made anyway.\\n'},\n",
       "         'return_path': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to return every value of the nonzero coefficients along the\\nforward path. Useful for cross-validation.\\n'},\n",
       "         'return_n_iter': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether or not to return the number of iterations.\\n'}},\n",
       "        'required': ['X', 'y']}}},\n",
       "     {'func_name': 'ridge_regression',\n",
       "      'func_desc': 'Solve the ridge equation by the method of normal equations.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ridge_regression.html#sklearn.linear_model.ridge_regression',\n",
       "      'function_definitions': {'function_name': 'ridge_regression',\n",
       "       'full_function': \"sklearn.linear_model.ridge_regression(X, y, alpha, *, sample_weight=None, solver='auto', max_iter=None, tol=0.0001, verbose=0, positive=False, random_state=None, return_n_iter=False, return_intercept=False, check_input=True)\",\n",
       "       'function_text': 'Solve the ridge equation by the method of normal equations. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix, LinearOperator} of shape         (n_samples, n_features)',\n",
       "         'param_desc': 'Training data.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_targets)',\n",
       "         'param_desc': 'Target values.\\n'},\n",
       "        {'param_name': 'alpha',\n",
       "         'param_type': 'float or array-like of shape (n_targets,)',\n",
       "         'param_desc': 'Constant that multiplies the L2 term, controlling regularization\\nstrength. alpha must be a non-negative float i.e. in [0, inf).\\nWhen alpha = 0, the objective is equivalent to ordinary least\\nsquares, solved by the LinearRegression object. For numerical\\nreasons, using alpha = 0 with the Ridge object is not advised.\\nInstead, you should use the LinearRegression object.\\nIf an array is passed, penalties are assumed to be specific to the\\ntargets. Hence they must correspond in number.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'float or array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Individual weights for each sample. If given a float, every sample\\nwill have the same weight. If sample_weight is not None and\\nsolver=’auto’, the solver will be set to ‘cholesky’.\\n\\nAdded in version 0.17.\\n\\n'},\n",
       "        {'param_name': 'solver',\n",
       "         'param_type': '{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’,             ‘sag’, ‘saga’, ‘lbfgs’}, default=’auto’',\n",
       "         'param_desc': 'Solver to use in the computational routines:\\n\\n‘auto’ chooses the solver automatically based on the type of data.\\n‘svd’ uses a Singular Value Decomposition of X to compute the Ridge\\ncoefficients. It is the most stable solver, in particular more stable\\nfor singular matrices than ‘cholesky’ at the cost of being slower.\\n‘cholesky’ uses the standard scipy.linalg.solve function to\\nobtain a closed-form solution via a Cholesky decomposition of\\ndot(X.T, X)\\n‘sparse_cg’ uses the conjugate gradient solver as found in\\nscipy.sparse.linalg.cg. As an iterative algorithm, this solver is\\nmore appropriate than ‘cholesky’ for large-scale data\\n(possibility to set tol and max_iter).\\n‘lsqr’ uses the dedicated regularized least-squares routine\\nscipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\\nprocedure.\\n‘sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses\\nits improved, unbiased version named SAGA. Both methods also use an\\niterative procedure, and are often faster than other solvers when\\nboth n_samples and n_features are large. Note that ‘sag’ and\\n‘saga’ fast convergence is only guaranteed on features with\\napproximately the same scale. You can preprocess the data with a\\nscaler from sklearn.preprocessing.\\n‘lbfgs’ uses L-BFGS-B algorithm implemented in\\nscipy.optimize.minimize. It can be used only when positive\\nis True.\\n\\nAll solvers except ‘svd’ support both dense and sparse data. However, only\\n‘lsqr’, ‘sag’, ‘sparse_cg’, and ‘lbfgs’ support sparse input when\\nfit_intercept is True.\\n\\nAdded in version 0.17: Stochastic Average Gradient descent solver.\\n\\n\\nAdded in version 0.19: SAGA solver.\\n\\n'},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Maximum number of iterations for conjugate gradient solver.\\nFor the ‘sparse_cg’ and ‘lsqr’ solvers, the default value is determined\\nby scipy.sparse.linalg. For ‘sag’ and saga solver, the default value is\\n1000. For ‘lbfgs’ solver, the default value is 15000.\\n'},\n",
       "        {'param_name': 'tol',\n",
       "         'param_type': 'float, default=1e-4',\n",
       "         'param_desc': 'Precision of the solution. Note that tol has no effect for solvers ‘svd’ and\\n‘cholesky’.\\n\\nChanged in version 1.2: Default value changed from 1e-3 to 1e-4 for consistency with other linear\\nmodels.\\n\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Verbosity level. Setting verbose > 0 will display additional\\ninformation depending on the solver used.\\n'},\n",
       "        {'param_name': 'positive',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When set to True, forces the coefficients to be positive.\\nOnly ‘lbfgs’ solver is supported in this case.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance, default=None',\n",
       "         'param_desc': 'Used when solver == ‘sag’ or ‘saga’ to shuffle the data.\\nSee Glossary for details.\\n'},\n",
       "        {'param_name': 'return_n_iter',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the method also returns n_iter, the actual number of\\niteration performed by the solver.\\n\\nAdded in version 0.17.\\n\\n'},\n",
       "        {'param_name': 'return_intercept',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True and if X is sparse, the method also returns the intercept,\\nand the solver is automatically changed to ‘sag’. This is only a\\ntemporary fix for fitting the intercept with sparse data. For dense\\ndata, use sklearn.linear_model._preprocess_data before your regression.\\n\\nAdded in version 0.17.\\n\\n'},\n",
       "        {'param_name': 'check_input',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, the input arrays X and y will not be checked.\\n\\nAdded in version 0.21.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'ridge_regression',\n",
       "       'descriptions': 'Solve the ridge equation by the method of normal equations. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix, LinearOperator} of shape         (n_samples, n_features). Training data.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_targets). Target values.\\n'},\n",
       "         'alpha': {'type': 'number',\n",
       "          'description': 'float or array-like of shape (n_targets,). Constant that multiplies the L2 term, controlling regularization\\nstrength. alpha must be a non-negative float i.e. in [0, inf).\\nWhen alpha = 0, the objective is equivalent to ordinary least\\nsquares, solved by the LinearRegression object. For numerical\\nreasons, using alpha = 0 with the Ridge object is not advised.\\nInstead, you should use the LinearRegression object.\\nIf an array is passed, penalties are assumed to be specific to the\\ntargets. Hence they must correspond in number.\\n'},\n",
       "         'sample_weight': {'type': 'number',\n",
       "          'description': 'float or array-like of shape (n_samples,), default=None. Individual weights for each sample. If given a float, every sample\\nwill have the same weight. If sample_weight is not None and\\nsolver=’auto’, the solver will be set to ‘cholesky’.\\n\\nAdded in version 0.17.\\n\\n'},\n",
       "         'solver': {'type': 'string',\n",
       "          'enum': ['auto',\n",
       "           'svd',\n",
       "           'cholesky',\n",
       "           'lsqr',\n",
       "           'sparse_cg',\n",
       "           'sag',\n",
       "           'saga',\n",
       "           'lbfgs'],\n",
       "          'description': '{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’,             ‘sag’, ‘saga’, ‘lbfgs’}, default=’auto’. Solver to use in the computational routines:\\n\\n‘auto’ chooses the solver automatically based on the type of data.\\n‘svd’ uses a Singular Value Decomposition of X to compute the Ridge\\ncoefficients. It is the most stable solver, in particular more stable\\nfor singular matrices than ‘cholesky’ at the cost of being slower.\\n‘cholesky’ uses the standard scipy.linalg.solve function to\\nobtain a closed-form solution via a Cholesky decomposition of\\ndot(X.T, X)\\n‘sparse_cg’ uses the conjugate gradient solver as found in\\nscipy.sparse.linalg.cg. As an iterative algorithm, this solver is\\nmore appropriate than ‘cholesky’ for large-scale data\\n(possibility to set tol and max_iter).\\n‘lsqr’ uses the dedicated regularized least-squares routine\\nscipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\\nprocedure.\\n‘sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses\\nits improved, unbiased version named SAGA. Both methods also use an\\niterative procedure, and are often faster than other solvers when\\nboth n_samples and n_features are large. Note that ‘sag’ and\\n‘saga’ fast convergence is only guaranteed on features with\\napproximately the same scale. You can preprocess the data with a\\nscaler from sklearn.preprocessing.\\n‘lbfgs’ uses L-BFGS-B algorithm implemented in\\nscipy.optimize.minimize. It can be used only when positive\\nis True.\\n\\nAll solvers except ‘svd’ support both dense and sparse data. However, only\\n‘lsqr’, ‘sag’, ‘sparse_cg’, and ‘lbfgs’ support sparse input when\\nfit_intercept is True.\\n\\nAdded in version 0.17: Stochastic Average Gradient descent solver.\\n\\n\\nAdded in version 0.19: SAGA solver.\\n\\n'},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=None. Maximum number of iterations for conjugate gradient solver.\\nFor the ‘sparse_cg’ and ‘lsqr’ solvers, the default value is determined\\nby scipy.sparse.linalg. For ‘sag’ and saga solver, the default value is\\n1000. For ‘lbfgs’ solver, the default value is 15000.\\n'},\n",
       "         'tol': {'type': 'number',\n",
       "          'description': 'float, default=1e-4. Precision of the solution. Note that tol has no effect for solvers ‘svd’ and\\n‘cholesky’.\\n\\nChanged in version 1.2: Default value changed from 1e-3 to 1e-4 for consistency with other linear\\nmodels.\\n\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'int, default=0. Verbosity level. Setting verbose > 0 will display additional\\ninformation depending on the solver used.\\n'},\n",
       "         'positive': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When set to True, forces the coefficients to be positive.\\nOnly ‘lbfgs’ solver is supported in this case.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance, default=None. Used when solver == ‘sag’ or ‘saga’ to shuffle the data.\\nSee Glossary for details.\\n'},\n",
       "         'return_n_iter': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the method also returns n_iter, the actual number of\\niteration performed by the solver.\\n\\nAdded in version 0.17.\\n\\n'},\n",
       "         'return_intercept': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True and if X is sparse, the method also returns the intercept,\\nand the solver is automatically changed to ‘sag’. This is only a\\ntemporary fix for fitting the intercept with sparse data. For dense\\ndata, use sklearn.linear_model._preprocess_data before your regression.\\n\\nAdded in version 0.17.\\n\\n'},\n",
       "         'check_input': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, the input arrays X and y will not be checked.\\n\\nAdded in version 0.21.\\n\\n'}},\n",
       "        'required': ['X', 'y', 'alpha']}}}]}],\n",
       "  'name': 'sklearn.linear_model',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.linear_model.html'},\n",
       " 'sklearn.manifold.html': {'functions': [{'defaults': [{'func_name': 'Isomap',\n",
       "      'func_desc': 'Isomap Embedding.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap',\n",
       "      'function_definitions': {'function_name': 'Isomap',\n",
       "       'full_function': \"class sklearn.manifold.Isomap(*, n_neighbors=5, radius=None, n_components=2, eigen_solver='auto', tol=0, max_iter=None, path_method='auto', neighbors_algorithm='auto', n_jobs=None, metric='minkowski', p=2, metric_params=None)\",\n",
       "       'function_text': 'Isomap Embedding. Non-linear dimensionality reduction through Isometric Mapping Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/manifold.html#isomap',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix}, shape (n_queries, n_features)',\n",
       "         'param_desc': 'If neighbors_algorithm=’precomputed’, X is assumed to be a\\ndistance matrix or a sparse graph of shape\\n(n_queries, n_samples_fit).\\n'}]},\n",
       "      'function_calling': {'name': 'Isomap',\n",
       "       'descriptions': 'Isomap Embedding. Non-linear dimensionality reduction through Isometric Mapping Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix}, shape (n_queries, n_features). If neighbors_algorithm=’precomputed’, X is assumed to be a\\ndistance matrix or a sparse graph of shape\\n(n_queries, n_samples_fit).\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'MDS',\n",
       "      'func_desc': 'Multidimensional scaling.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS',\n",
       "      'function_definitions': {'function_name': 'MDS',\n",
       "       'full_function': \"class sklearn.manifold.MDS(n_components=2, *, metric=True, n_init=4, max_iter=300, verbose=0, eps=0.001, n_jobs=None, random_state=None, dissimilarity='euclidean', normalized_stress='auto')\",\n",
       "       'function_text': 'Multidimensional scaling. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/manifold.html#multidimensional-scaling',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'MDS',\n",
       "       'descriptions': 'Multidimensional scaling. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {},\n",
       "        'required': ['n_components=2']}}},\n",
       "     {'func_name': 'TSNE',\n",
       "      'func_desc': 'T-distributed Stochastic Neighbor Embedding.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE',\n",
       "      'function_definitions': {'function_name': 'TSNE',\n",
       "       'full_function': \"class sklearn.manifold.TSNE(n_components=2, *, perplexity=30.0, early_exaggeration=12.0, learning_rate='auto', max_iter=None, n_iter_without_progress=300, min_grad_norm=1e-07, metric='euclidean', metric_params=None, init='pca', verbose=0, random_state=None, method='barnes_hut', angle=0.5, n_jobs=None, n_iter='deprecated')\",\n",
       "       'function_text': 'T-distributed Stochastic Neighbor Embedding. t-SNE [1] is a tool to visualize high-dimensional data. It converts\\nsimilarities between data points to joint probabilities and tries\\nto minimize the Kullback-Leibler divergence between the joint\\nprobabilities of the low-dimensional embedding and the\\nhigh-dimensional data. t-SNE has a cost function that is not convex,\\ni.e. with different initializations we can get different results. It is highly recommended to use another dimensionality reduction\\nmethod (e.g. PCA for dense data or TruncatedSVD for sparse data)\\nto reduce the number of dimensions to a reasonable amount (e.g. 50)\\nif the number of features is very high. This will suppress some\\nnoise and speed up the computation of pairwise distances between\\nsamples. For more tips see Laurens van der Maaten’s FAQ [2]. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/manifold.html#t-sne',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'TSNE',\n",
       "       'descriptions': 'T-distributed Stochastic Neighbor Embedding. t-SNE [1] is a tool to visualize high-dimensional data. It converts\\nsimilarities between data points to joint probabilities and tries\\nto minimize the Kullback-Leibler divergence between the joint\\nprobabilities of the low-dimensional embedding and the\\nhigh-dimensional data. t-SNE has a cost function that is not convex,\\ni.e. with different initializations we can get different results. It is highly recommended to use another dimensionality reduction\\nmethod (e.g. PCA for dense data or TruncatedSVD for sparse data)\\nto reduce the number of dimensions to a reasonable amount (e.g. 50)\\nif the number of features is very high. This will suppress some\\nnoise and speed up the computation of pairwise distances between\\nsamples. For more tips see Laurens van der Maaten’s FAQ [2]. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {},\n",
       "        'required': ['n_components=2']}}},\n",
       "     {'func_name': 'smacof',\n",
       "      'func_desc': 'Compute multidimensional scaling using the SMACOF algorithm.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.manifold.smacof.html#sklearn.manifold.smacof',\n",
       "      'function_definitions': {'function_name': 'smacof',\n",
       "       'full_function': \"sklearn.manifold.smacof(dissimilarities, *, metric=True, n_components=2, init=None, n_init=8, n_jobs=None, max_iter=300, verbose=0, eps=0.001, random_state=None, return_n_iter=False, normalized_stress='auto')\",\n",
       "       'function_text': 'Compute multidimensional scaling using the SMACOF algorithm. The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a\\nmultidimensional scaling algorithm which minimizes an objective function\\n(the stress) using a majorization technique. Stress majorization, also\\nknown as the Guttman Transform, guarantees a monotone convergence of\\nstress, and is more powerful than traditional techniques such as gradient\\ndescent. The SMACOF algorithm for metric MDS can be summarized by the following\\nsteps: Set an initial start configuration, randomly or not. Compute the stress Compute the Guttman Transform Iterate 2 and 3 until convergence. The nonmetric algorithm adds a monotonic regression step before computing\\nthe stress.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-n_jobs',\n",
       "       'parameter_names_desc': [{'param_name': 'dissimilarities',\n",
       "         'param_type': 'array-like of shape (n_samples, n_samples)',\n",
       "         'param_desc': 'Pairwise dissimilarities between the points. Must be symmetric.\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Compute metric or nonmetric SMACOF algorithm.\\nWhen False (i.e. non-metric MDS), dissimilarities with 0 are considered as\\nmissing values.\\n'},\n",
       "        {'param_name': 'n_components',\n",
       "         'param_type': 'int, default=2',\n",
       "         'param_desc': 'Number of dimensions in which to immerse the dissimilarities. If an\\ninit array is provided, this option is overridden and the shape of\\ninit is used to determine the dimensionality of the embedding\\nspace.\\n'},\n",
       "        {'param_name': 'init',\n",
       "         'param_type': 'array-like of shape (n_samples, n_components), default=None',\n",
       "         'param_desc': 'Starting configuration of the embedding to initialize the algorithm. By\\ndefault, the algorithm is initialized with a randomly chosen array.\\n'},\n",
       "        {'param_name': 'n_init',\n",
       "         'param_type': 'int, default=8',\n",
       "         'param_desc': 'Number of times the SMACOF algorithm will be run with different\\ninitializations. The final results will be the best output of the runs,\\ndetermined by the run with the smallest final stress. If init is\\nprovided, this option is overridden and a single run is performed.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of jobs to use for the computation. If multiple\\ninitializations are used (n_init), each run of the algorithm is\\ncomputed in parallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=300',\n",
       "         'param_desc': 'Maximum number of iterations of the SMACOF algorithm for a single run.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Level of verbosity.\\n'},\n",
       "        {'param_name': 'eps',\n",
       "         'param_type': 'float, default=1e-3',\n",
       "         'param_desc': 'Relative tolerance with respect to stress at which to declare\\nconvergence. The value of eps should be tuned separately depending\\non whether or not normalized_stress is being used.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines the random number generator used to initialize the centers.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'return_n_iter',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether or not to return the number of iterations.\\n'},\n",
       "        {'param_name': 'normalized_stress',\n",
       "         'param_type': 'bool or “auto” default=”auto”',\n",
       "         'param_desc': 'Whether use and return normed stress value (Stress-1) instead of raw\\nstress calculated by default. Only supported in non-metric MDS.\\n\\nAdded in version 1.2.\\n\\n\\nChanged in version 1.4: The default value changed from False to \"auto\" in version 1.4.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'smacof',\n",
       "       'descriptions': 'Compute multidimensional scaling using the SMACOF algorithm. The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a\\nmultidimensional scaling algorithm which minimizes an objective function\\n(the stress) using a majorization technique. Stress majorization, also\\nknown as the Guttman Transform, guarantees a monotone convergence of\\nstress, and is more powerful than traditional techniques such as gradient\\ndescent. The SMACOF algorithm for metric MDS can be summarized by the following\\nsteps: Set an initial start configuration, randomly or not. Compute the stress Compute the Guttman Transform Iterate 2 and 3 until convergence. The nonmetric algorithm adds a monotonic regression step before computing\\nthe stress.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'dissimilarities': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_samples). Pairwise dissimilarities between the points. Must be symmetric.\\n'},\n",
       "         'metric': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Compute metric or nonmetric SMACOF algorithm.\\nWhen False (i.e. non-metric MDS), dissimilarities with 0 are considered as\\nmissing values.\\n'},\n",
       "         'n_components': {'type': 'integer',\n",
       "          'description': 'int, default=2. Number of dimensions in which to immerse the dissimilarities. If an\\ninit array is provided, this option is overridden and the shape of\\ninit is used to determine the dimensionality of the embedding\\nspace.\\n'},\n",
       "         'init': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_components), default=None. Starting configuration of the embedding to initialize the algorithm. By\\ndefault, the algorithm is initialized with a randomly chosen array.\\n'},\n",
       "         'n_init': {'type': 'integer',\n",
       "          'description': 'int, default=8. Number of times the SMACOF algorithm will be run with different\\ninitializations. The final results will be the best output of the runs,\\ndetermined by the run with the smallest final stress. If init is\\nprovided, this option is overridden and a single run is performed.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of jobs to use for the computation. If multiple\\ninitializations are used (n_init), each run of the algorithm is\\ncomputed in parallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=300. Maximum number of iterations of the SMACOF algorithm for a single run.\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'int, default=0. Level of verbosity.\\n'},\n",
       "         'eps': {'type': 'number',\n",
       "          'description': 'float, default=1e-3. Relative tolerance with respect to stress at which to declare\\nconvergence. The value of eps should be tuned separately depending\\non whether or not normalized_stress is being used.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines the random number generator used to initialize the centers.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'return_n_iter': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether or not to return the number of iterations.\\n'},\n",
       "         'normalized_stress': {'type': 'boolean',\n",
       "          'description': 'bool or “auto” default=”auto”. Whether use and return normed stress value (Stress-1) instead of raw\\nstress calculated by default. Only supported in non-metric MDS.\\n\\nAdded in version 1.2.\\n\\n\\nChanged in version 1.4: The default value changed from False to \"auto\" in version 1.4.\\n\\n'}},\n",
       "        'required': ['dissimilarities']}}},\n",
       "     {'func_name': 'trustworthiness',\n",
       "      'func_desc': 'Indicate to what extent the local structure is retained.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.manifold.trustworthiness.html#sklearn.manifold.trustworthiness',\n",
       "      'function_definitions': {'function_name': 'trustworthiness',\n",
       "       'full_function': \"sklearn.manifold.trustworthiness(X, X_embedded, *, n_neighbors=5, metric='euclidean')\",\n",
       "       'function_text': 'Indicate to what extent the local structure is retained. The trustworthiness is within [0, 1]. It is defined as where for each sample i, \\\\(\\\\mathcal{N}_{i}^{k}\\\\) are its k nearest\\nneighbors in the output space, and every sample j is its \\\\(r(i, j)\\\\)-th\\nnearest neighbor in the input space. In other words, any unexpected nearest\\nneighbors in the output space are penalised in proportion to their rank in\\nthe input space.',\n",
       "       'func_text_user_guide': '#r5831441d8a57-1',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features) or ',\n",
       "         'param_desc': '(n_samples, n_samples)\\nIf the metric is ‘precomputed’ X must be a square distance\\nmatrix. Otherwise it contains a sample per row.\\n'},\n",
       "        {'param_name': 'X_embedded',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_components)',\n",
       "         'param_desc': 'Embedding of the training data in low-dimensional space.\\n'},\n",
       "        {'param_name': 'n_neighbors',\n",
       "         'param_type': 'int, default=5',\n",
       "         'param_desc': 'The number of neighbors that will be considered. Should be fewer than\\nn_samples / 2 to ensure the trustworthiness to lies within [0, 1], as\\nmentioned in [1]. An error will be raised otherwise.\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'str or callable, default=’euclidean’',\n",
       "         'param_desc': 'Which metric to use for computing pairwise distances between samples\\nfrom the original input space. If metric is ‘precomputed’, X must be a\\nmatrix of pairwise distances or squared distances. Otherwise, for a list\\nof available metrics, see the documentation of argument metric in\\nsklearn.pairwise.pairwise_distances and metrics listed in\\nsklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS. Note that the\\n“cosine” metric uses cosine_distances.\\n\\nAdded in version 0.20.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'trustworthiness',\n",
       "       'descriptions': 'Indicate to what extent the local structure is retained. The trustworthiness is within [0, 1]. It is defined as where for each sample i, \\\\(\\\\mathcal{N}_{i}^{k}\\\\) are its k nearest\\nneighbors in the output space, and every sample j is its \\\\(r(i, j)\\\\)-th\\nnearest neighbor in the input space. In other words, any unexpected nearest\\nneighbors in the output space are penalised in proportion to their rank in\\nthe input space.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features) or . (n_samples, n_samples)\\nIf the metric is ‘precomputed’ X must be a square distance\\nmatrix. Otherwise it contains a sample per row.\\n'},\n",
       "         'X_embedded': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_components). Embedding of the training data in low-dimensional space.\\n'},\n",
       "         'n_neighbors': {'type': 'integer',\n",
       "          'description': 'int, default=5. The number of neighbors that will be considered. Should be fewer than\\nn_samples / 2 to ensure the trustworthiness to lies within [0, 1], as\\nmentioned in [1]. An error will be raised otherwise.\\n'},\n",
       "         'metric': {'type': 'string',\n",
       "          'description': 'str or callable, default=’euclidean’. Which metric to use for computing pairwise distances between samples\\nfrom the original input space. If metric is ‘precomputed’, X must be a\\nmatrix of pairwise distances or squared distances. Otherwise, for a list\\nof available metrics, see the documentation of argument metric in\\nsklearn.pairwise.pairwise_distances and metrics listed in\\nsklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS. Note that the\\n“cosine” metric uses cosine_distances.\\n\\nAdded in version 0.20.\\n\\n'}},\n",
       "        'required': ['X', 'X_embedded']}}},\n",
       "     {'func_name': 'LocallyLinearEmbedding',\n",
       "      'func_desc': 'Locally Linear Embedding.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding',\n",
       "      'function_definitions': {'function_name': 'LocallyLinearEmbedding',\n",
       "       'full_function': \"class sklearn.manifold.LocallyLinearEmbedding(*, n_neighbors=5, n_components=2, reg=0.001, eigen_solver='auto', tol=1e-06, max_iter=100, method='standard', hessian_tol=0.0001, modified_tol=1e-12, neighbors_algorithm='auto', random_state=None, n_jobs=None)\",\n",
       "       'function_text': 'Locally Linear Embedding. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/manifold.html#locally-linear-embedding',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training set.\\n'}]},\n",
       "      'function_calling': {'name': 'LocallyLinearEmbedding',\n",
       "       'descriptions': 'Locally Linear Embedding. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training set.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'SpectralEmbedding',\n",
       "      'func_desc': 'Spectral embedding for non-linear dimensionality reduction.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html#sklearn.manifold.SpectralEmbedding',\n",
       "      'function_definitions': {'function_name': 'SpectralEmbedding',\n",
       "       'full_function': \"class sklearn.manifold.SpectralEmbedding(n_components=2, *, affinity='nearest_neighbors', gamma=None, random_state=None, eigen_solver=None, eigen_tol='auto', n_neighbors=None, n_jobs=None)\",\n",
       "       'function_text': 'Spectral embedding for non-linear dimensionality reduction. Forms an affinity matrix given by the specified function and\\napplies spectral decomposition to the corresponding graph laplacian.\\nThe resulting transformation is given by the value of the\\neigenvectors for each data point. Note : Laplacian Eigenmaps is the actual algorithm implemented here. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/manifold.html#spectral-embedding',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'SpectralEmbedding',\n",
       "       'descriptions': 'Spectral embedding for non-linear dimensionality reduction. Forms an affinity matrix given by the specified function and\\napplies spectral decomposition to the corresponding graph laplacian.\\nThe resulting transformation is given by the value of the\\neigenvectors for each data point. Note : Laplacian Eigenmaps is the actual algorithm implemented here. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {},\n",
       "        'required': ['n_components=2']}}},\n",
       "     {'func_name': 'locally_linear_embedding',\n",
       "      'func_desc': 'Perform a Locally Linear Embedding analysis on the data.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding',\n",
       "      'function_definitions': {'function_name': 'locally_linear_embedding',\n",
       "       'full_function': \"sklearn.manifold.locally_linear_embedding(X, *, n_neighbors, n_components, reg=0.001, eigen_solver='auto', tol=1e-06, max_iter=100, method='standard', hessian_tol=0.0001, modified_tol=1e-12, random_state=None, n_jobs=None)\",\n",
       "       'function_text': 'Perform a Locally Linear Embedding analysis on the data. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/manifold.html#locally-linear-embedding',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, NearestNeighbors}',\n",
       "         'param_desc': 'Sample data, shape = (n_samples, n_features), in the form of a\\nnumpy array or a NearestNeighbors object.\\n'},\n",
       "        {'param_name': 'n_neighbors',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Number of neighbors to consider for each point.\\n'},\n",
       "        {'param_name': 'n_components',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Number of coordinates for the manifold.\\n'},\n",
       "        {'param_name': 'reg',\n",
       "         'param_type': 'float, default=1e-3',\n",
       "         'param_desc': 'Regularization constant, multiplies the trace of the local covariance\\nmatrix of the distances.\\n'},\n",
       "        {'param_name': 'eigen_solver',\n",
       "         'param_type': '{‘auto’, ‘arpack’, ‘dense’}, default=’auto’',\n",
       "         'param_desc': 'auto : algorithm will attempt to choose the best method for input data\\n\\narpackuse arnoldi iteration in shift-invert mode.For this method, M may be a dense matrix, sparse matrix,\\nor general linear operator.\\nWarning: ARPACK can be unstable for some problems.  It is\\nbest to try several random seeds in order to check results.\\n\\ndenseuse standard dense matrix operations for the eigenvaluedecomposition.  For this method, M must be an array\\nor matrix type.  This method should be avoided for\\nlarge problems.\\n\\n\\n'},\n",
       "        {'param_name': 'tol',\n",
       "         'param_type': 'float, default=1e-6',\n",
       "         'param_desc': 'Tolerance for ‘arpack’ method\\nNot used if eigen_solver==’dense’.\\n'},\n",
       "        {'param_name': 'max_iter',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'Maximum number of iterations for the arpack solver.\\n'},\n",
       "        {'param_name': 'method',\n",
       "         'param_type': '{‘standard’, ‘hessian’, ‘modified’, ‘ltsa’}, default=’standard’',\n",
       "         'param_desc': '\\nstandarduse the standard locally linear embedding algorithm.see reference [1]\\n\\nhessianuse the Hessian eigenmap method.  This method requiresn_neighbors > n_components * (1 + (n_components + 1) / 2.\\nsee reference [2]\\n\\nmodifieduse the modified locally linear embedding algorithm.see reference [3]\\n\\nltsause local tangent space alignment algorithmsee reference [4]\\n\\n\\n'},\n",
       "        {'param_name': 'hessian_tol',\n",
       "         'param_type': 'float, default=1e-4',\n",
       "         'param_desc': 'Tolerance for Hessian eigenmapping method.\\nOnly used if method == ‘hessian’.\\n'},\n",
       "        {'param_name': 'modified_tol',\n",
       "         'param_type': 'float, default=1e-12',\n",
       "         'param_desc': 'Tolerance for modified LLE method.\\nOnly used if method == ‘modified’.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance, default=None',\n",
       "         'param_desc': 'Determines the random number generator when solver == ‘arpack’.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int or None, default=None',\n",
       "         'param_desc': 'The number of parallel jobs to run for neighbors search.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'}]},\n",
       "      'function_calling': {'name': 'locally_linear_embedding',\n",
       "       'descriptions': 'Perform a Locally Linear Embedding analysis on the data. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, NearestNeighbors}. Sample data, shape = (n_samples, n_features), in the form of a\\nnumpy array or a NearestNeighbors object.\\n'},\n",
       "         'n_neighbors': {'type': 'integer',\n",
       "          'description': 'int. Number of neighbors to consider for each point.\\n'},\n",
       "         'n_components': {'type': 'integer',\n",
       "          'description': 'int. Number of coordinates for the manifold.\\n'},\n",
       "         'reg': {'type': 'number',\n",
       "          'description': 'float, default=1e-3. Regularization constant, multiplies the trace of the local covariance\\nmatrix of the distances.\\n'},\n",
       "         'eigen_solver': {'type': 'string',\n",
       "          'enum': ['auto', 'arpack', 'dense'],\n",
       "          'description': '{‘auto’, ‘arpack’, ‘dense’}, default=’auto’. auto : algorithm will attempt to choose the best method for input data\\n\\narpackuse arnoldi iteration in shift-invert mode.For this method, M may be a dense matrix, sparse matrix,\\nor general linear operator.\\nWarning: ARPACK can be unstable for some problems.  It is\\nbest to try several random seeds in order to check results.\\n\\ndenseuse standard dense matrix operations for the eigenvaluedecomposition.  For this method, M must be an array\\nor matrix type.  This method should be avoided for\\nlarge problems.\\n\\n\\n'},\n",
       "         'tol': {'type': 'number',\n",
       "          'description': 'float, default=1e-6. Tolerance for ‘arpack’ method\\nNot used if eigen_solver==’dense’.\\n'},\n",
       "         'max_iter': {'type': 'integer',\n",
       "          'description': 'int, default=100. Maximum number of iterations for the arpack solver.\\n'},\n",
       "         'method': {'type': 'string',\n",
       "          'enum': ['standard', 'hessian', 'modified', 'ltsa'],\n",
       "          'description': '{‘standard’, ‘hessian’, ‘modified’, ‘ltsa’}, default=’standard’. \\nstandarduse the standard locally linear embedding algorithm.see reference [1]\\n\\nhessianuse the Hessian eigenmap method.  This method requiresn_neighbors > n_components * (1 + (n_components + 1) / 2.\\nsee reference [2]\\n\\nmodifieduse the modified locally linear embedding algorithm.see reference [3]\\n\\nltsause local tangent space alignment algorithmsee reference [4]\\n\\n\\n'},\n",
       "         'hessian_tol': {'type': 'number',\n",
       "          'description': 'float, default=1e-4. Tolerance for Hessian eigenmapping method.\\nOnly used if method == ‘hessian’.\\n'},\n",
       "         'modified_tol': {'type': 'number',\n",
       "          'description': 'float, default=1e-12. Tolerance for modified LLE method.\\nOnly used if method == ‘modified’.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance, default=None. Determines the random number generator when solver == ‘arpack’.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int or None, default=None. The number of parallel jobs to run for neighbors search.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'spectral_embedding',\n",
       "      'func_desc': 'Project the sample on the first eigenvectors of the graph Laplacian.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.manifold.spectral_embedding.html#sklearn.manifold.spectral_embedding',\n",
       "      'function_definitions': {'function_name': 'spectral_embedding',\n",
       "       'full_function': \"sklearn.manifold.spectral_embedding(adjacency, *, n_components=8, eigen_solver=None, random_state=None, eigen_tol='auto', norm_laplacian=True, drop_first=True)\",\n",
       "       'function_text': 'Project the sample on the first eigenvectors of the graph Laplacian. The adjacency matrix is used to compute a normalized graph Laplacian\\nwhose spectrum (especially the eigenvectors associated to the\\nsmallest eigenvalues) has an interpretation in terms of minimal\\nnumber of cuts necessary to split the graph into comparably sized\\ncomponents. This embedding can also ‘work’ even if the adjacency variable is\\nnot strictly the adjacency matrix of a graph but more generally\\nan affinity or similarity matrix between samples (for instance the\\nheat kernel of a euclidean distance matrix or a k-NN matrix). However care must taken to always make the affinity matrix symmetric\\nso that the eigenvector decomposition works as expected. Note : Laplacian Eigenmaps is the actual algorithm implemented here. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/manifold.html#spectral-embedding',\n",
       "       'parameter_names_desc': [{'param_name': 'adjacency',\n",
       "         'param_type': '{array-like, sparse graph} of shape (n_samples, n_samples)',\n",
       "         'param_desc': 'The adjacency matrix of the graph to embed.\\n'},\n",
       "        {'param_name': 'n_components',\n",
       "         'param_type': 'int, default=8',\n",
       "         'param_desc': 'The dimension of the projection subspace.\\n'},\n",
       "        {'param_name': 'eigen_solver',\n",
       "         'param_type': '{‘arpack’, ‘lobpcg’, ‘amg’}, default=None',\n",
       "         'param_desc': \"The eigenvalue decomposition strategy to use. AMG requires pyamg\\nto be installed. It can be faster on very large, sparse problems,\\nbut may also lead to instabilities. If None, then 'arpack' is\\nused.\\n\"},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': \"A pseudo random number generator used for the initialization\\nof the lobpcg eigen vectors decomposition when eigen_solver ==\\n'amg', and for the K-Means initialization. Use an int to make\\nthe results deterministic across calls (See\\nGlossary).\\n\\nNote\\nWhen using eigen_solver == 'amg',\\nit is necessary to also fix the global numpy seed with\\nnp.random.seed(int) to get deterministic results. See\\npyamg/pyamg#139 for further\\ninformation.\\n\\n\"},\n",
       "        {'param_name': 'eigen_tol',\n",
       "         'param_type': 'float, default=”auto”',\n",
       "         'param_desc': 'Stopping criterion for eigendecomposition of the Laplacian matrix.\\nIf eigen_tol=\"auto\" then the passed tolerance will depend on the\\neigen_solver:\\n\\nIf eigen_solver=\"arpack\", then eigen_tol=0.0;\\nIf eigen_solver=\"lobpcg\" or eigen_solver=\"amg\", then\\neigen_tol=None which configures the underlying lobpcg solver to\\nautomatically resolve the value according to their heuristics. See,\\nscipy.sparse.linalg.lobpcg for details.\\n\\nNote that when using eigen_solver=\"amg\" values of tol<1e-5 may lead\\nto convergence issues and should be avoided.\\n\\nAdded in version 1.2: Added ‘auto’ option.\\n\\n'},\n",
       "        {'param_name': 'norm_laplacian',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, then compute symmetric normalized Laplacian.\\n'},\n",
       "        {'param_name': 'drop_first',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to drop the first eigenvector. For spectral embedding, this\\nshould be True as the first eigenvector should be constant vector for\\nconnected graph, but for spectral clustering, this should be kept as\\nFalse to retain the first eigenvector.\\n'}]},\n",
       "      'function_calling': {'name': 'spectral_embedding',\n",
       "       'descriptions': 'Project the sample on the first eigenvectors of the graph Laplacian. The adjacency matrix is used to compute a normalized graph Laplacian\\nwhose spectrum (especially the eigenvectors associated to the\\nsmallest eigenvalues) has an interpretation in terms of minimal\\nnumber of cuts necessary to split the graph into comparably sized\\ncomponents. This embedding can also ‘work’ even if the adjacency variable is\\nnot strictly the adjacency matrix of a graph but more generally\\nan affinity or similarity matrix between samples (for instance the\\nheat kernel of a euclidean distance matrix or a k-NN matrix). However care must taken to always make the affinity matrix symmetric\\nso that the eigenvector decomposition works as expected. Note : Laplacian Eigenmaps is the actual algorithm implemented here. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'adjacency': {'type': 'array',\n",
       "          'description': '{array-like, sparse graph} of shape (n_samples, n_samples). The adjacency matrix of the graph to embed.\\n'},\n",
       "         'n_components': {'type': 'integer',\n",
       "          'description': 'int, default=8. The dimension of the projection subspace.\\n'},\n",
       "         'eigen_solver': {'type': 'string',\n",
       "          'enum': ['arpack', 'lobpcg', 'amg'],\n",
       "          'description': \"{‘arpack’, ‘lobpcg’, ‘amg’}, default=None. The eigenvalue decomposition strategy to use. AMG requires pyamg\\nto be installed. It can be faster on very large, sparse problems,\\nbut may also lead to instabilities. If None, then 'arpack' is\\nused.\\n\"},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': \"int, RandomState instance or None, default=None. A pseudo random number generator used for the initialization\\nof the lobpcg eigen vectors decomposition when eigen_solver ==\\n'amg', and for the K-Means initialization. Use an int to make\\nthe results deterministic across calls (See\\nGlossary).\\n\\nNote\\nWhen using eigen_solver == 'amg',\\nit is necessary to also fix the global numpy seed with\\nnp.random.seed(int) to get deterministic results. See\\npyamg/pyamg#139 for further\\ninformation.\\n\\n\"},\n",
       "         'eigen_tol': {'type': 'number',\n",
       "          'description': 'float, default=”auto”. Stopping criterion for eigendecomposition of the Laplacian matrix.\\nIf eigen_tol=\"auto\" then the passed tolerance will depend on the\\neigen_solver:\\n\\nIf eigen_solver=\"arpack\", then eigen_tol=0.0;\\nIf eigen_solver=\"lobpcg\" or eigen_solver=\"amg\", then\\neigen_tol=None which configures the underlying lobpcg solver to\\nautomatically resolve the value according to their heuristics. See,\\nscipy.sparse.linalg.lobpcg for details.\\n\\nNote that when using eigen_solver=\"amg\" values of tol<1e-5 may lead\\nto convergence issues and should be avoided.\\n\\nAdded in version 1.2: Added ‘auto’ option.\\n\\n'},\n",
       "         'norm_laplacian': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, then compute symmetric normalized Laplacian.\\n'},\n",
       "         'drop_first': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to drop the first eigenvector. For spectral embedding, this\\nshould be True as the first eigenvector should be constant vector for\\nconnected graph, but for spectral clustering, this should be kept as\\nFalse to retain the first eigenvector.\\n'}},\n",
       "        'required': ['adjacency']}}}]}],\n",
       "  'name': 'sklearn.manifold',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.manifold.html'},\n",
       " 'sklearn.metrics.html': {'functions': [{'Model selection interface': [{'func_name': 'check_scoring',\n",
       "      'func_desc': 'Determine scorer from user options.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.check_scoring.html#sklearn.metrics.check_scoring',\n",
       "      'function_definitions': {'function_name': 'check_scoring',\n",
       "       'full_function': 'sklearn.metrics.check_scoring(estimator=None, scoring=None, *, allow_none=False)',\n",
       "       'function_text': 'Determine scorer from user options. A TypeError will be thrown if the estimator cannot be scored.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'estimator object implementing ‘fit’ or None, default=None',\n",
       "         'param_desc': 'The object to use to fit the data. If None, then this function may error\\ndepending on allow_none.\\n'},\n",
       "        {'param_name': 'scoring',\n",
       "         'param_type': 'str, callable, list, tuple, or dict, default=None',\n",
       "         'param_desc': 'Scorer to use. If scoring represents a single score, one can use:\\n\\na single string (see The scoring parameter: defining model evaluation rules);\\na callable (see Defining your scoring strategy from metric functions) that returns a single value.\\n\\nIf scoring represents multiple scores, one can use:\\n\\na list or tuple of unique strings;\\na callable returning a dictionary where the keys are the metric\\nnames and the values are the metric scorers;\\na dictionary with metric names as keys and callables a values.\\n\\nIf None, the provided estimator object’s score method is used.\\n'},\n",
       "        {'param_name': 'allow_none',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If no scoring is specified and the estimator has no score function, we\\ncan either return None or raise an exception.\\n'}]},\n",
       "      'function_calling': {'name': 'check_scoring',\n",
       "       'descriptions': 'Determine scorer from user options. A TypeError will be thrown if the estimator cannot be scored.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'estimator object implementing ‘fit’ or None, default=None. The object to use to fit the data. If None, then this function may error\\ndepending on allow_none.\\n'},\n",
       "         'scoring': {'type': 'string',\n",
       "          'description': 'str, callable, list, tuple, or dict, default=None. Scorer to use. If scoring represents a single score, one can use:\\n\\na single string (see The scoring parameter: defining model evaluation rules);\\na callable (see Defining your scoring strategy from metric functions) that returns a single value.\\n\\nIf scoring represents multiple scores, one can use:\\n\\na list or tuple of unique strings;\\na callable returning a dictionary where the keys are the metric\\nnames and the values are the metric scorers;\\na dictionary with metric names as keys and callables a values.\\n\\nIf None, the provided estimator object’s score method is used.\\n'},\n",
       "         'allow_none': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If no scoring is specified and the estimator has no score function, we\\ncan either return None or raise an exception.\\n'}},\n",
       "        'required': ['estimator=None', 'scoring=None']}}},\n",
       "     {'func_name': 'get_scorer_names',\n",
       "      'func_desc': 'Get the names of all available scorers.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.get_scorer_names.html#sklearn.metrics.get_scorer_names',\n",
       "      'function_definitions': {'function_name': 'get_scorer_names',\n",
       "       'full_function': 'sklearn.metrics.get_scorer_names()',\n",
       "       'function_text': 'Get the names of all available scorers. These names can be passed to get_scorer to\\nretrieve the scorer object.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.get_scorer.html#sklearn.metrics.get_scorer',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'get_scorer',\n",
       "      'func_desc': 'Get a scorer from string.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.get_scorer.html#sklearn.metrics.get_scorer',\n",
       "      'function_definitions': {'function_name': 'get_scorer',\n",
       "       'full_function': 'sklearn.metrics.get_scorer(scoring)',\n",
       "       'function_text': 'Get a scorer from string. Read more in the User Guide.\\nget_scorer_names can be used to retrieve the names\\nof all available scorers.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter',\n",
       "       'parameter_names_desc': [{'param_name': 'scoring',\n",
       "         'param_type': 'str, callable or None',\n",
       "         'param_desc': 'Scoring method as string. If callable it is returned as is.\\nIf None, returns None.\\n'}]},\n",
       "      'function_calling': {'name': 'get_scorer',\n",
       "       'descriptions': 'Get a scorer from string. Read more in the User Guide.\\nget_scorer_names can be used to retrieve the names\\nof all available scorers.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'scoring': {'type': 'string',\n",
       "          'description': 'str, callable or None. Scoring method as string. If callable it is returned as is.\\nIf None, returns None.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'make_scorer',\n",
       "      'func_desc': 'Make a scorer from a performance metric or loss function.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer',\n",
       "      'function_definitions': {'function_name': 'make_scorer',\n",
       "       'full_function': \"sklearn.metrics.make_scorer(score_func, *, response_method=None, greater_is_better=True, needs_proba='deprecated', needs_threshold='deprecated', **kwargs)\",\n",
       "       'function_text': 'Make a scorer from a performance metric or loss function. A scorer is a wrapper around an arbitrary metric or loss function that is called\\nwith the signature scorer(estimator, X, y_true, **kwargs). It is accepted in all scikit-learn estimators or functions allowing a scoring\\nparameter. The parameter response_method allows to specify which method of the estimator\\nshould be used to feed the scoring/loss function. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#scoring',\n",
       "       'parameter_names_desc': [{'param_name': 'score_func',\n",
       "         'param_type': 'callable',\n",
       "         'param_desc': 'Score function (or loss function) with signature\\nscore_func(y, y_pred, **kwargs).\\n'},\n",
       "        {'param_name': 'response_method',\n",
       "         'param_type': '{“predict_proba”, “decision_function”, “predict”} or             list/tuple of such str, default=None',\n",
       "         'param_desc': 'Specifies the response method to use get prediction from an estimator\\n(i.e. predict_proba, decision_function or\\npredict). Possible choices are:\\n\\nif str, it corresponds to the name to the method to return;\\nif a list or tuple of str, it provides the method names in order of\\npreference. The method returned corresponds to the first method in\\nthe list and which is implemented by estimator.\\nif None, it is equivalent to \"predict\".\\n\\n\\nAdded in version 1.4.\\n\\n'},\n",
       "        {'param_name': 'greater_is_better',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether score_func is a score function (default), meaning high is\\ngood, or a loss function, meaning low is good. In the latter case, the\\nscorer object will sign-flip the outcome of the score_func.\\n'},\n",
       "        {'param_name': 'needs_proba',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether score_func requires predict_proba to get probability\\nestimates out of a classifier.\\nIf True, for binary y_true, the score function is supposed to accept\\na 1D y_pred (i.e., probability of the positive class, shape\\n(n_samples,)).\\n\\nDeprecated since version 1.4: needs_proba is deprecated in version 1.4 and will be removed in\\n1.6. Use response_method=\"predict_proba\" instead.\\n\\n'},\n",
       "        {'param_name': 'needs_threshold',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether score_func takes a continuous decision certainty.\\nThis only works for binary classification using estimators that\\nhave either a decision_function or predict_proba method.\\nIf True, for binary y_true, the score function is supposed to accept\\na 1D y_pred (i.e., probability of the positive class or the decision\\nfunction, shape (n_samples,)).\\nFor example average_precision or the area under the roc curve\\ncan not be computed using discrete predictions alone.\\n\\nDeprecated since version 1.4: needs_threshold is deprecated in version 1.4 and will be removed\\nin 1.6. Use response_method=(\"decision_function\", \"predict_proba\")\\ninstead to preserve the same behaviour.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'make_scorer',\n",
       "       'descriptions': 'Make a scorer from a performance metric or loss function. A scorer is a wrapper around an arbitrary metric or loss function that is called\\nwith the signature scorer(estimator, X, y_true, **kwargs). It is accepted in all scikit-learn estimators or functions allowing a scoring\\nparameter. The parameter response_method allows to specify which method of the estimator\\nshould be used to feed the scoring/loss function. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'score_func': {'type': 'object',\n",
       "          'description': 'callable. Score function (or loss function) with signature\\nscore_func(y, y_pred, **kwargs).\\n'},\n",
       "         'response_method': {'type': 'string',\n",
       "          'description': '{“predict_proba”, “decision_function”, “predict”} or             list/tuple of such str, default=None. Specifies the response method to use get prediction from an estimator\\n(i.e. predict_proba, decision_function or\\npredict). Possible choices are:\\n\\nif str, it corresponds to the name to the method to return;\\nif a list or tuple of str, it provides the method names in order of\\npreference. The method returned corresponds to the first method in\\nthe list and which is implemented by estimator.\\nif None, it is equivalent to \"predict\".\\n\\n\\nAdded in version 1.4.\\n\\n'},\n",
       "         'greater_is_better': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether score_func is a score function (default), meaning high is\\ngood, or a loss function, meaning low is good. In the latter case, the\\nscorer object will sign-flip the outcome of the score_func.\\n'},\n",
       "         'needs_proba': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether score_func requires predict_proba to get probability\\nestimates out of a classifier.\\nIf True, for binary y_true, the score function is supposed to accept\\na 1D y_pred (i.e., probability of the positive class, shape\\n(n_samples,)).\\n\\nDeprecated since version 1.4: needs_proba is deprecated in version 1.4 and will be removed in\\n1.6. Use response_method=\"predict_proba\" instead.\\n\\n'},\n",
       "         'needs_threshold': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether score_func takes a continuous decision certainty.\\nThis only works for binary classification using estimators that\\nhave either a decision_function or predict_proba method.\\nIf True, for binary y_true, the score function is supposed to accept\\na 1D y_pred (i.e., probability of the positive class or the decision\\nfunction, shape (n_samples,)).\\nFor example average_precision or the area under the roc curve\\ncan not be computed using discrete predictions alone.\\n\\nDeprecated since version 1.4: needs_threshold is deprecated in version 1.4 and will be removed\\nin 1.6. Use response_method=(\"decision_function\", \"predict_proba\")\\ninstead to preserve the same behaviour.\\n\\n'}},\n",
       "        'required': ['score_func']}}}]},\n",
       "   {'Classification metrics': [{'func_name': 'accuracy_score',\n",
       "      'func_desc': 'Accuracy classification score.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score',\n",
       "      'function_definitions': {'function_name': 'accuracy_score',\n",
       "       'full_function': 'sklearn.metrics.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)',\n",
       "       'function_text': 'Accuracy classification score. In multilabel classification, this function computes subset accuracy:\\nthe set of labels predicted for a sample must exactly match the\\ncorresponding set of labels in y_true. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Ground truth (correct) labels.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Predicted labels, as returned by a classifier.\\n'},\n",
       "        {'param_name': 'normalize',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, return the number of correctly classified samples.\\nOtherwise, return the fraction of correctly classified samples.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'accuracy_score',\n",
       "       'descriptions': 'Accuracy classification score. In multilabel classification, this function computes subset accuracy:\\nthe set of labels predicted for a sample must exactly match the\\ncorresponding set of labels in y_true. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Ground truth (correct) labels.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Predicted labels, as returned by a classifier.\\n'},\n",
       "         'normalize': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, return the number of correctly classified samples.\\nOtherwise, return the fraction of correctly classified samples.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'average_precision_score',\n",
       "      'func_desc': 'Compute average precision (AP) from prediction scores.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score',\n",
       "      'function_definitions': {'function_name': 'average_precision_score',\n",
       "       'full_function': \"sklearn.metrics.average_precision_score(y_true, y_score, *, average='macro', pos_label=1, sample_weight=None)\",\n",
       "       'function_text': 'Compute average precision (AP) from prediction scores. AP summarizes a precision-recall curve as the weighted mean of precisions\\nachieved at each threshold, with the increase in recall from the previous\\nthreshold used as the weight: where \\\\(P_n\\\\) and \\\\(R_n\\\\) are the precision and recall at the nth\\nthreshold [1]. This implementation is not interpolated and is different\\nfrom computing the area under the precision-recall curve with the\\ntrapezoidal rule, which uses linear interpolation and can be too\\noptimistic. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#rcdf8f32d7f9d-1',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_classes)',\n",
       "         'param_desc': 'True binary labels or binary label indicators.\\n'},\n",
       "        {'param_name': 'y_score',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_classes)',\n",
       "         'param_desc': 'Target scores, can either be probability estimates of the positive\\nclass, confidence values, or non-thresholded measure of decisions\\n(as returned by decision_function on some classifiers).\\n'},\n",
       "        {'param_name': 'average',\n",
       "         'param_type': '{‘micro’, ‘samples’, ‘weighted’, ‘macro’} or None,             default=’macro’',\n",
       "         'param_desc': \"If None, the scores for each class are returned. Otherwise,\\nthis determines the type of averaging performed on the data:\\n\\n'micro':Calculate metrics globally by considering each element of the label\\nindicator matrix as a label.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average, weighted\\nby support (the number of true instances for each label).\\n\\n'samples':Calculate metrics for each instance, and find their average.\\n\\n\\nWill be ignored when y_true is binary.\\n\"},\n",
       "        {'param_name': 'pos_label',\n",
       "         'param_type': 'int, float, bool or str, default=1',\n",
       "         'param_desc': 'The label of the positive class. Only applied to binary y_true.\\nFor multilabel-indicator y_true, pos_label is fixed to 1.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'average_precision_score',\n",
       "       'descriptions': 'Compute average precision (AP) from prediction scores. AP summarizes a precision-recall curve as the weighted mean of precisions\\nachieved at each threshold, with the increase in recall from the previous\\nthreshold used as the weight: where \\\\(P_n\\\\) and \\\\(R_n\\\\) are the precision and recall at the nth\\nthreshold [1]. This implementation is not interpolated and is different\\nfrom computing the area under the precision-recall curve with the\\ntrapezoidal rule, which uses linear interpolation and can be too\\noptimistic. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_classes). True binary labels or binary label indicators.\\n'},\n",
       "         'y_score': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_classes). Target scores, can either be probability estimates of the positive\\nclass, confidence values, or non-thresholded measure of decisions\\n(as returned by decision_function on some classifiers).\\n'},\n",
       "         'average': {'type': 'string',\n",
       "          'enum': ['micro', 'samples', 'weighted', 'macro'],\n",
       "          'description': \"{‘micro’, ‘samples’, ‘weighted’, ‘macro’} or None,             default=’macro’. If None, the scores for each class are returned. Otherwise,\\nthis determines the type of averaging performed on the data:\\n\\n'micro':Calculate metrics globally by considering each element of the label\\nindicator matrix as a label.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average, weighted\\nby support (the number of true instances for each label).\\n\\n'samples':Calculate metrics for each instance, and find their average.\\n\\n\\nWill be ignored when y_true is binary.\\n\"},\n",
       "         'pos_label': {'type': 'integer',\n",
       "          'description': 'int, float, bool or str, default=1. The label of the positive class. Only applied to binary y_true.\\nFor multilabel-indicator y_true, pos_label is fixed to 1.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': ['y_true', 'y_score']}}},\n",
       "     {'func_name': 'brier_score_loss',\n",
       "      'func_desc': 'Compute the Brier score loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss',\n",
       "      'function_definitions': {'function_name': 'brier_score_loss',\n",
       "       'full_function': \"sklearn.metrics.brier_score_loss(y_true, y_proba=None, *, sample_weight=None, pos_label=None, y_prob='deprecated')\",\n",
       "       'function_text': 'Compute the Brier score loss. The smaller the Brier score loss, the better, hence the naming with “loss”.\\nThe Brier score measures the mean squared difference between the predicted\\nprobability and the actual outcome. The Brier score always\\ntakes on a value between zero and one, since this is the largest\\npossible difference between a predicted probability (which must be\\nbetween zero and one) and the actual outcome (which can take on values\\nof only 0 and 1). It can be decomposed as the sum of refinement loss and\\ncalibration loss. The Brier score is appropriate for binary and categorical outcomes that\\ncan be structured as true or false, but is inappropriate for ordinal\\nvariables which can take on three or more values (this is because the\\nBrier score assumes that all possible outcomes are equivalently\\n“distant” from one another). Which label is considered to be the positive\\nlabel is controlled via the parameter pos_label, which defaults to\\nthe greater label unless y_true is all 0 or all -1, in which case\\npos_label defaults to 1. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#brier-score-loss',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'True targets.\\n'},\n",
       "        {'param_name': 'y_proba',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Probabilities of the positive class.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'pos_label',\n",
       "         'param_type': 'int, float, bool or str, default=None',\n",
       "         'param_desc': 'Label of the positive class. pos_label will be inferred in the\\nfollowing manner:\\n\\nif y_true in {-1, 1} or {0, 1}, pos_label defaults to 1;\\nelse if y_true contains string, an error will be raised and\\npos_label should be explicitly specified;\\notherwise, pos_label defaults to the greater label,\\ni.e. np.unique(y_true)[-1].\\n\\n'},\n",
       "        {'param_name': 'y_prob',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Probabilities of the positive class.\\n\\nDeprecated since version 1.5: y_prob is deprecated and will be removed in 1.7. Use\\ny_proba instead.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'brier_score_loss',\n",
       "       'descriptions': 'Compute the Brier score loss. The smaller the Brier score loss, the better, hence the naming with “loss”.\\nThe Brier score measures the mean squared difference between the predicted\\nprobability and the actual outcome. The Brier score always\\ntakes on a value between zero and one, since this is the largest\\npossible difference between a predicted probability (which must be\\nbetween zero and one) and the actual outcome (which can take on values\\nof only 0 and 1). It can be decomposed as the sum of refinement loss and\\ncalibration loss. The Brier score is appropriate for binary and categorical outcomes that\\ncan be structured as true or false, but is inappropriate for ordinal\\nvariables which can take on three or more values (this is because the\\nBrier score assumes that all possible outcomes are equivalently\\n“distant” from one another). Which label is considered to be the positive\\nlabel is controlled via the parameter pos_label, which defaults to\\nthe greater label unless y_true is all 0 or all -1, in which case\\npos_label defaults to 1. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). True targets.\\n'},\n",
       "         'y_proba': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Probabilities of the positive class.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'pos_label': {'type': 'integer',\n",
       "          'description': 'int, float, bool or str, default=None. Label of the positive class. pos_label will be inferred in the\\nfollowing manner:\\n\\nif y_true in {-1, 1} or {0, 1}, pos_label defaults to 1;\\nelse if y_true contains string, an error will be raised and\\npos_label should be explicitly specified;\\notherwise, pos_label defaults to the greater label,\\ni.e. np.unique(y_true)[-1].\\n\\n'},\n",
       "         'y_prob': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Probabilities of the positive class.\\n\\nDeprecated since version 1.5: y_prob is deprecated and will be removed in 1.7. Use\\ny_proba instead.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_proba=None']}}},\n",
       "     {'func_name': 'classification_report',\n",
       "      'func_desc': 'Build a text report showing the main classification metrics.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report',\n",
       "      'function_definitions': {'function_name': 'classification_report',\n",
       "       'full_function': \"sklearn.metrics.classification_report(y_true, y_pred, *, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')\",\n",
       "       'function_text': 'Build a text report showing the main classification metrics. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Estimated targets as returned by a classifier.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like of shape (n_labels,), default=None',\n",
       "         'param_desc': 'Optional list of label indices to include in the report.\\n'},\n",
       "        {'param_name': 'target_names',\n",
       "         'param_type': 'array-like of shape (n_labels,), default=None',\n",
       "         'param_desc': 'Optional display names matching the labels (same order).\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'digits',\n",
       "         'param_type': 'int, default=2',\n",
       "         'param_desc': 'Number of digits for formatting output floating point values.\\nWhen output_dict is True, this will be ignored and the\\nreturned values will not be rounded.\\n'},\n",
       "        {'param_name': 'output_dict',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, return output as dict.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'zero_division',\n",
       "         'param_type': '{“warn”, 0.0, 1.0, np.nan}, default=”warn”',\n",
       "         'param_desc': 'Sets the value to return when there is a zero division. If set to\\n“warn”, this acts as 0, but warnings are also raised.\\n\\nAdded in version 1.3: np.nan option was added.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'classification_report',\n",
       "       'descriptions': 'Build a text report showing the main classification metrics. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Estimated targets as returned by a classifier.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_labels,), default=None. Optional list of label indices to include in the report.\\n'},\n",
       "         'target_names': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_labels,), default=None. Optional display names matching the labels (same order).\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'digits': {'type': 'integer',\n",
       "          'description': 'int, default=2. Number of digits for formatting output floating point values.\\nWhen output_dict is True, this will be ignored and the\\nreturned values will not be rounded.\\n'},\n",
       "         'output_dict': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, return output as dict.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'zero_division': {'type': 'string',\n",
       "          'enum': ['warn', '0.0', '1.0', 'np.nan'],\n",
       "          'description': '{“warn”, 0.0, 1.0, np.nan}, default=”warn”. Sets the value to return when there is a zero division. If set to\\n“warn”, this acts as 0, but warnings are also raised.\\n\\nAdded in version 1.3: np.nan option was added.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'confusion_matrix',\n",
       "      'func_desc': 'Compute confusion matrix to evaluate the accuracy of a classification.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix',\n",
       "      'function_definitions': {'function_name': 'confusion_matrix',\n",
       "       'full_function': 'sklearn.metrics.confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)',\n",
       "       'function_text': 'Compute confusion matrix to evaluate the accuracy of a classification. By definition a confusion matrix \\\\(C\\\\) is such that \\\\(C_{i, j}\\\\)\\nis equal to the number of observations known to be in group \\\\(i\\\\) and\\npredicted to be in group \\\\(j\\\\). Thus in binary classification, the count of true negatives is\\n\\\\(C_{0,0}\\\\), false negatives is \\\\(C_{1,0}\\\\), true positives is\\n\\\\(C_{1,1}\\\\) and false positives is \\\\(C_{0,1}\\\\). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Estimated targets as returned by a classifier.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like of shape (n_classes), default=None',\n",
       "         'param_desc': 'List of labels to index the matrix. This may be used to reorder\\nor select a subset of labels.\\nIf None is given, those that appear at least once\\nin y_true or y_pred are used in sorted order.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "        {'param_name': 'normalize',\n",
       "         'param_type': '{‘true’, ‘pred’, ‘all’}, default=None',\n",
       "         'param_desc': 'Normalizes confusion matrix over the true (rows), predicted (columns)\\nconditions or all the population. If None, confusion matrix will not be\\nnormalized.\\n'}]},\n",
       "      'function_calling': {'name': 'confusion_matrix',\n",
       "       'descriptions': 'Compute confusion matrix to evaluate the accuracy of a classification. By definition a confusion matrix \\\\(C\\\\) is such that \\\\(C_{i, j}\\\\)\\nis equal to the number of observations known to be in group \\\\(i\\\\) and\\npredicted to be in group \\\\(j\\\\). Thus in binary classification, the count of true negatives is\\n\\\\(C_{0,0}\\\\), false negatives is \\\\(C_{1,0}\\\\), true positives is\\n\\\\(C_{1,1}\\\\) and false positives is \\\\(C_{0,1}\\\\). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Estimated targets as returned by a classifier.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_classes), default=None. List of labels to index the matrix. This may be used to reorder\\nor select a subset of labels.\\nIf None is given, those that appear at least once\\nin y_true or y_pred are used in sorted order.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "         'normalize': {'type': 'string',\n",
       "          'enum': ['true', 'pred', 'all'],\n",
       "          'description': '{‘true’, ‘pred’, ‘all’}, default=None. Normalizes confusion matrix over the true (rows), predicted (columns)\\nconditions or all the population. If None, confusion matrix will not be\\nnormalized.\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'dcg_score',\n",
       "      'func_desc': 'Compute Discounted Cumulative Gain.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.dcg_score.html#sklearn.metrics.dcg_score',\n",
       "      'function_definitions': {'function_name': 'dcg_score',\n",
       "       'full_function': 'sklearn.metrics.dcg_score(y_true, y_score, *, k=None, log_base=2, sample_weight=None, ignore_ties=False)',\n",
       "       'function_text': 'Compute Discounted Cumulative Gain. Sum the true scores ranked in the order induced by the predicted scores,\\nafter applying a logarithmic discount. This ranking metric yields a high value if true labels are ranked high by\\ny_score. Usually the Normalized Discounted Cumulative Gain (NDCG, computed by\\nndcg_score) is preferred.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.ndcg_score.html#sklearn.metrics.ndcg_score',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples, n_labels)',\n",
       "         'param_desc': 'True targets of multilabel classification, or true scores of entities\\nto be ranked.\\n'},\n",
       "        {'param_name': 'y_score',\n",
       "         'param_type': 'array-like of shape (n_samples, n_labels)',\n",
       "         'param_desc': 'Target scores, can either be probability estimates, confidence values,\\nor non-thresholded measure of decisions (as returned by\\n“decision_function” on some classifiers).\\n'},\n",
       "        {'param_name': 'k',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Only consider the highest k scores in the ranking. If None, use all\\noutputs.\\n'},\n",
       "        {'param_name': 'log_base',\n",
       "         'param_type': 'float, default=2',\n",
       "         'param_desc': 'Base of the logarithm used for the discount. A low value means a\\nsharper discount (top results are more important).\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights. If None, all samples are given the same weight.\\n'},\n",
       "        {'param_name': 'ignore_ties',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Assume that there are no ties in y_score (which is likely to be the\\ncase if y_score is continuous) for efficiency gains.\\n'}]},\n",
       "      'function_calling': {'name': 'dcg_score',\n",
       "       'descriptions': 'Compute Discounted Cumulative Gain. Sum the true scores ranked in the order induced by the predicted scores,\\nafter applying a logarithmic discount. This ranking metric yields a high value if true labels are ranked high by\\ny_score. Usually the Normalized Discounted Cumulative Gain (NDCG, computed by\\nndcg_score) is preferred.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_labels). True targets of multilabel classification, or true scores of entities\\nto be ranked.\\n'},\n",
       "         'y_score': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_labels). Target scores, can either be probability estimates, confidence values,\\nor non-thresholded measure of decisions (as returned by\\n“decision_function” on some classifiers).\\n'},\n",
       "         'k': {'type': 'integer',\n",
       "          'description': 'int, default=None. Only consider the highest k scores in the ranking. If None, use all\\noutputs.\\n'},\n",
       "         'log_base': {'type': 'number',\n",
       "          'description': 'float, default=2. Base of the logarithm used for the discount. A low value means a\\nsharper discount (top results are more important).\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights. If None, all samples are given the same weight.\\n'},\n",
       "         'ignore_ties': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Assume that there are no ties in y_score (which is likely to be the\\ncase if y_score is continuous) for efficiency gains.\\n'}},\n",
       "        'required': ['y_true', 'y_score']}}},\n",
       "     {'func_name': 'f1_score',\n",
       "      'func_desc': 'Compute the F1 score, also known as balanced F-score or F-measure.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score',\n",
       "      'function_definitions': {'function_name': 'f1_score',\n",
       "       'full_function': \"sklearn.metrics.f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\",\n",
       "       'function_text': \"Compute the F1 score, also known as balanced F-score or F-measure. The F1 score can be interpreted as a harmonic mean of the precision and\\nrecall, where an F1 score reaches its best value at 1 and worst score at 0.\\nThe relative contribution of precision and recall to the F1 score are\\nequal. The formula for the F1 score is: Where \\\\(\\\\text{TP}\\\\) is the number of true positives, \\\\(\\\\text{FN}\\\\) is the\\nnumber of false negatives, and \\\\(\\\\text{FP}\\\\) is the number of false positives.\\nF1 is by default\\ncalculated as 0.0 when there are no true positives, false negatives, or\\nfalse positives. Support beyond binary targets is achieved by treating multiclass\\nand multilabel data as a collection of binary problems, one for each\\nlabel. For the binary case, setting average='binary' will return\\nF1 score for pos_label. If average is not 'binary', pos_label is ignored\\nand F1 score for both classes are computed, then averaged or both returned (when\\naverage=None). Similarly, for multiclass and multilabel targets,\\nF1 score for all labels are either returned or averaged depending on the\\naverage parameter. Use labels specify the set of labels to calculate F1 score\\nfor. Read more in the User Guide.\",\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-binary',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Estimated targets as returned by a classifier.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like, default=None',\n",
       "         'param_desc': \"The set of labels to include when average != 'binary', and their\\norder if average is None. Labels present in the data can be\\nexcluded, for example in multiclass classification to exclude a “negative\\nclass”. Labels not present in the data can be included and will be\\n“assigned” 0 samples. For multilabel targets, labels are column indices.\\nBy default, all labels in y_true and y_pred are used in sorted order.\\n\\nChanged in version 0.17: Parameter labels improved for multiclass problem.\\n\\n\"},\n",
       "        {'param_name': 'pos_label',\n",
       "         'param_type': 'int, float, bool or str, default=1',\n",
       "         'param_desc': \"The class to report if average='binary' and the data is binary,\\notherwise this parameter is ignored.\\nFor multiclass or multilabel targets, set labels=[pos_label] and\\naverage != 'binary' to report metrics for one label only.\\n\"},\n",
       "        {'param_name': 'average',\n",
       "         'param_type': '{‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None,             default=’binary’',\n",
       "         'param_desc': \"This parameter is required for multiclass/multilabel targets.\\nIf None, the scores for each class are returned. Otherwise, this\\ndetermines the type of averaging performed on the data:\\n\\n'binary':Only report results for the class specified by pos_label.\\nThis is applicable only if targets (y_{true,pred}) are binary.\\n\\n'micro':Calculate metrics globally by counting the total true positives,\\nfalse negatives and false positives.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average weighted\\nby support (the number of true instances for each label). This\\nalters ‘macro’ to account for label imbalance; it can result in an\\nF-score that is not between precision and recall.\\n\\n'samples':Calculate metrics for each instance, and find their average (only\\nmeaningful for multilabel classification where this differs from\\naccuracy_score).\\n\\n\\n\"},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'zero_division',\n",
       "         'param_type': '{“warn”, 0.0, 1.0, np.nan}, default=”warn”',\n",
       "         'param_desc': 'Sets the value to return when there is a zero division, i.e. when all\\npredictions and labels are negative.\\nNotes:\\n- If set to “warn”, this acts like 0, but a warning is also raised.\\n- If set to np.nan, such values will be excluded from the average.\\n\\nAdded in version 1.3: np.nan option was added.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'f1_score',\n",
       "       'descriptions': \"Compute the F1 score, also known as balanced F-score or F-measure. The F1 score can be interpreted as a harmonic mean of the precision and\\nrecall, where an F1 score reaches its best value at 1 and worst score at 0.\\nThe relative contribution of precision and recall to the F1 score are\\nequal. The formula for the F1 score is: Where \\\\(\\\\text{TP}\\\\) is the number of true positives, \\\\(\\\\text{FN}\\\\) is the\\nnumber of false negatives, and \\\\(\\\\text{FP}\\\\) is the number of false positives.\\nF1 is by default\\ncalculated as 0.0 when there are no true positives, false negatives, or\\nfalse positives. Support beyond binary targets is achieved by treating multiclass\\nand multilabel data as a collection of binary problems, one for each\\nlabel. For the binary case, setting average='binary' will return\\nF1 score for pos_label. If average is not 'binary', pos_label is ignored\\nand F1 score for both classes are computed, then averaged or both returned (when\\naverage=None). Similarly, for multiclass and multilabel targets,\\nF1 score for all labels are either returned or averaged depending on the\\naverage parameter. Use labels specify the set of labels to calculate F1 score\\nfor. Read more in the User Guide.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Estimated targets as returned by a classifier.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': \"array-like, default=None. The set of labels to include when average != 'binary', and their\\norder if average is None. Labels present in the data can be\\nexcluded, for example in multiclass classification to exclude a “negative\\nclass”. Labels not present in the data can be included and will be\\n“assigned” 0 samples. For multilabel targets, labels are column indices.\\nBy default, all labels in y_true and y_pred are used in sorted order.\\n\\nChanged in version 0.17: Parameter labels improved for multiclass problem.\\n\\n\"},\n",
       "         'pos_label': {'type': 'integer',\n",
       "          'description': \"int, float, bool or str, default=1. The class to report if average='binary' and the data is binary,\\notherwise this parameter is ignored.\\nFor multiclass or multilabel targets, set labels=[pos_label] and\\naverage != 'binary' to report metrics for one label only.\\n\"},\n",
       "         'average': {'type': 'string',\n",
       "          'enum': ['micro', 'macro', 'samples', 'weighted', 'binary'],\n",
       "          'description': \"{‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None,             default=’binary’. This parameter is required for multiclass/multilabel targets.\\nIf None, the scores for each class are returned. Otherwise, this\\ndetermines the type of averaging performed on the data:\\n\\n'binary':Only report results for the class specified by pos_label.\\nThis is applicable only if targets (y_{true,pred}) are binary.\\n\\n'micro':Calculate metrics globally by counting the total true positives,\\nfalse negatives and false positives.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average weighted\\nby support (the number of true instances for each label). This\\nalters ‘macro’ to account for label imbalance; it can result in an\\nF-score that is not between precision and recall.\\n\\n'samples':Calculate metrics for each instance, and find their average (only\\nmeaningful for multilabel classification where this differs from\\naccuracy_score).\\n\\n\\n\"},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'zero_division': {'type': 'string',\n",
       "          'enum': ['warn', '0.0', '1.0', 'np.nan'],\n",
       "          'description': '{“warn”, 0.0, 1.0, np.nan}, default=”warn”. Sets the value to return when there is a zero division, i.e. when all\\npredictions and labels are negative.\\nNotes:\\n- If set to “warn”, this acts like 0, but a warning is also raised.\\n- If set to np.nan, such values will be excluded from the average.\\n\\nAdded in version 1.3: np.nan option was added.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'hamming_loss',\n",
       "      'func_desc': 'Compute the average Hamming loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss',\n",
       "      'function_definitions': {'function_name': 'hamming_loss',\n",
       "       'full_function': 'sklearn.metrics.hamming_loss(y_true, y_pred, *, sample_weight=None)',\n",
       "       'function_text': 'Compute the average Hamming loss. The Hamming loss is the fraction of labels that are incorrectly predicted. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#hamming-loss',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Ground truth (correct) labels.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Predicted labels, as returned by a classifier.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n\\nAdded in version 0.18.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'hamming_loss',\n",
       "       'descriptions': 'Compute the average Hamming loss. The Hamming loss is the fraction of labels that are incorrectly predicted. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Ground truth (correct) labels.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Predicted labels, as returned by a classifier.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n\\nAdded in version 0.18.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'jaccard_score',\n",
       "      'func_desc': 'Jaccard similarity coefficient score.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html#sklearn.metrics.jaccard_score',\n",
       "      'function_definitions': {'function_name': 'jaccard_score',\n",
       "       'full_function': \"sklearn.metrics.jaccard_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\",\n",
       "       'function_text': \"Jaccard similarity coefficient score. The Jaccard index [1], or Jaccard similarity coefficient, defined as\\nthe size of the intersection divided by the size of the union of two label\\nsets, is used to compare set of predicted labels for a sample to the\\ncorresponding set of labels in y_true. Support beyond term:binary targets is achieved by treating multiclass\\nand multilabel data as a collection of binary problems, one for each\\nlabel. For the binary case, setting average='binary' will return the\\nJaccard similarity coefficient for pos_label. If average is not 'binary',\\npos_label is ignored and scores for both classes are computed, then averaged or\\nboth returned (when average=None). Similarly, for multiclass and\\nmultilabel targets, scores for all labels are either returned or\\naveraged depending on the average parameter. Use labels specify the set of\\nlabels to calculate the score for. Read more in the User Guide.\",\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-multiclass',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Ground truth (correct) labels.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Predicted labels, as returned by a classifier.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like of shape (n_classes,), default=None',\n",
       "         'param_desc': \"The set of labels to include when average != 'binary', and their\\norder if average is None. Labels present in the data can be\\nexcluded, for example in multiclass classification to exclude a “negative\\nclass”. Labels not present in the data can be included and will be\\n“assigned” 0 samples. For multilabel targets, labels are column indices.\\nBy default, all labels in y_true and y_pred are used in sorted order.\\n\"},\n",
       "        {'param_name': 'pos_label',\n",
       "         'param_type': 'int, float, bool or str, default=1',\n",
       "         'param_desc': \"The class to report if average='binary' and the data is binary,\\notherwise this parameter is ignored.\\nFor multiclass or multilabel targets, set labels=[pos_label] and\\naverage != 'binary' to report metrics for one label only.\\n\"},\n",
       "        {'param_name': 'average',\n",
       "         'param_type': '{‘micro’, ‘macro’, ‘samples’, ‘weighted’,             ‘binary’} or None, default=’binary’',\n",
       "         'param_desc': \"If None, the scores for each class are returned. Otherwise, this\\ndetermines the type of averaging performed on the data:\\n\\n'binary':Only report results for the class specified by pos_label.\\nThis is applicable only if targets (y_{true,pred}) are binary.\\n\\n'micro':Calculate metrics globally by counting the total true positives,\\nfalse negatives and false positives.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average, weighted\\nby support (the number of true instances for each label). This\\nalters ‘macro’ to account for label imbalance.\\n\\n'samples':Calculate metrics for each instance, and find their average (only\\nmeaningful for multilabel classification).\\n\\n\\n\"},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'zero_division',\n",
       "         'param_type': '“warn”, {0.0, 1.0}, default=”warn”',\n",
       "         'param_desc': 'Sets the value to return when there is a zero division, i.e. when there\\nthere are no negative values in predictions and labels. If set to\\n“warn”, this acts like 0, but a warning is also raised.\\n'}]},\n",
       "      'function_calling': {'name': 'jaccard_score',\n",
       "       'descriptions': \"Jaccard similarity coefficient score. The Jaccard index [1], or Jaccard similarity coefficient, defined as\\nthe size of the intersection divided by the size of the union of two label\\nsets, is used to compare set of predicted labels for a sample to the\\ncorresponding set of labels in y_true. Support beyond term:binary targets is achieved by treating multiclass\\nand multilabel data as a collection of binary problems, one for each\\nlabel. For the binary case, setting average='binary' will return the\\nJaccard similarity coefficient for pos_label. If average is not 'binary',\\npos_label is ignored and scores for both classes are computed, then averaged or\\nboth returned (when average=None). Similarly, for multiclass and\\nmultilabel targets, scores for all labels are either returned or\\naveraged depending on the average parameter. Use labels specify the set of\\nlabels to calculate the score for. Read more in the User Guide.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Ground truth (correct) labels.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Predicted labels, as returned by a classifier.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': \"array-like of shape (n_classes,), default=None. The set of labels to include when average != 'binary', and their\\norder if average is None. Labels present in the data can be\\nexcluded, for example in multiclass classification to exclude a “negative\\nclass”. Labels not present in the data can be included and will be\\n“assigned” 0 samples. For multilabel targets, labels are column indices.\\nBy default, all labels in y_true and y_pred are used in sorted order.\\n\"},\n",
       "         'pos_label': {'type': 'integer',\n",
       "          'description': \"int, float, bool or str, default=1. The class to report if average='binary' and the data is binary,\\notherwise this parameter is ignored.\\nFor multiclass or multilabel targets, set labels=[pos_label] and\\naverage != 'binary' to report metrics for one label only.\\n\"},\n",
       "         'average': {'type': 'string',\n",
       "          'enum': ['micro', 'macro', 'samples', 'weighted', 'binary'],\n",
       "          'description': \"{‘micro’, ‘macro’, ‘samples’, ‘weighted’,             ‘binary’} or None, default=’binary’. If None, the scores for each class are returned. Otherwise, this\\ndetermines the type of averaging performed on the data:\\n\\n'binary':Only report results for the class specified by pos_label.\\nThis is applicable only if targets (y_{true,pred}) are binary.\\n\\n'micro':Calculate metrics globally by counting the total true positives,\\nfalse negatives and false positives.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average, weighted\\nby support (the number of true instances for each label). This\\nalters ‘macro’ to account for label imbalance.\\n\\n'samples':Calculate metrics for each instance, and find their average (only\\nmeaningful for multilabel classification).\\n\\n\\n\"},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'zero_division': {'type': 'string',\n",
       "          'enum': ['0.0', '1.0'],\n",
       "          'description': '“warn”, {0.0, 1.0}, default=”warn”. Sets the value to return when there is a zero division, i.e. when there\\nthere are no negative values in predictions and labels. If set to\\n“warn”, this acts like 0, but a warning is also raised.\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'matthews_corrcoef',\n",
       "      'func_desc': 'Compute the Matthews correlation coefficient (MCC).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef',\n",
       "      'function_definitions': {'function_name': 'matthews_corrcoef',\n",
       "       'full_function': 'sklearn.metrics.matthews_corrcoef(y_true, y_pred, *, sample_weight=None)',\n",
       "       'function_text': 'Compute the Matthews correlation coefficient (MCC). The Matthews correlation coefficient is used in machine learning as a\\nmeasure of the quality of binary and multiclass classifications. It takes\\ninto account true and false positives and negatives and is generally\\nregarded as a balanced measure which can be used even if the classes are of\\nvery different sizes. The MCC is in essence a correlation coefficient value\\nbetween -1 and +1. A coefficient of +1 represents a perfect prediction, 0\\nan average random prediction and -1 an inverse prediction.  The statistic\\nis also known as the phi coefficient. [source: Wikipedia] Binary and multiclass labels are supported.  Only in the binary case does\\nthis relate to information about true and false positives and negatives.\\nSee references below. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#matthews-corrcoef',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Estimated targets as returned by a classifier.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n\\nAdded in version 0.18.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'matthews_corrcoef',\n",
       "       'descriptions': 'Compute the Matthews correlation coefficient (MCC). The Matthews correlation coefficient is used in machine learning as a\\nmeasure of the quality of binary and multiclass classifications. It takes\\ninto account true and false positives and negatives and is generally\\nregarded as a balanced measure which can be used even if the classes are of\\nvery different sizes. The MCC is in essence a correlation coefficient value\\nbetween -1 and +1. A coefficient of +1 represents a perfect prediction, 0\\nan average random prediction and -1 an inverse prediction.  The statistic\\nis also known as the phi coefficient. [source: Wikipedia] Binary and multiclass labels are supported.  Only in the binary case does\\nthis relate to information about true and false positives and negatives.\\nSee references below. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Estimated targets as returned by a classifier.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n\\nAdded in version 0.18.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'ndcg_score',\n",
       "      'func_desc': 'Compute Normalized Discounted Cumulative Gain.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ndcg_score.html#sklearn.metrics.ndcg_score',\n",
       "      'function_definitions': {'function_name': 'ndcg_score',\n",
       "       'full_function': 'sklearn.metrics.ndcg_score(y_true, y_score, *, k=None, sample_weight=None, ignore_ties=False)',\n",
       "       'function_text': 'Compute Normalized Discounted Cumulative Gain. Sum the true scores ranked in the order induced by the predicted scores,\\nafter applying a logarithmic discount. Then divide by the best possible\\nscore (Ideal DCG, obtained for a perfect ranking) to obtain a score between\\n0 and 1. This ranking metric returns a high value if true labels are ranked high by\\ny_score.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.dcg_score.html#sklearn.metrics.dcg_score',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples, n_labels)',\n",
       "         'param_desc': 'True targets of multilabel classification, or true scores of entities\\nto be ranked. Negative values in y_true may result in an output\\nthat is not between 0 and 1.\\n'},\n",
       "        {'param_name': 'y_score',\n",
       "         'param_type': 'array-like of shape (n_samples, n_labels)',\n",
       "         'param_desc': 'Target scores, can either be probability estimates, confidence values,\\nor non-thresholded measure of decisions (as returned by\\n“decision_function” on some classifiers).\\n'},\n",
       "        {'param_name': 'k',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Only consider the highest k scores in the ranking. If None, use all\\noutputs.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights. If None, all samples are given the same weight.\\n'},\n",
       "        {'param_name': 'ignore_ties',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Assume that there are no ties in y_score (which is likely to be the\\ncase if y_score is continuous) for efficiency gains.\\n'}]},\n",
       "      'function_calling': {'name': 'ndcg_score',\n",
       "       'descriptions': 'Compute Normalized Discounted Cumulative Gain. Sum the true scores ranked in the order induced by the predicted scores,\\nafter applying a logarithmic discount. Then divide by the best possible\\nscore (Ideal DCG, obtained for a perfect ranking) to obtain a score between\\n0 and 1. This ranking metric returns a high value if true labels are ranked high by\\ny_score.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_labels). True targets of multilabel classification, or true scores of entities\\nto be ranked. Negative values in y_true may result in an output\\nthat is not between 0 and 1.\\n'},\n",
       "         'y_score': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_labels). Target scores, can either be probability estimates, confidence values,\\nor non-thresholded measure of decisions (as returned by\\n“decision_function” on some classifiers).\\n'},\n",
       "         'k': {'type': 'integer',\n",
       "          'description': 'int, default=None. Only consider the highest k scores in the ranking. If None, use all\\noutputs.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights. If None, all samples are given the same weight.\\n'},\n",
       "         'ignore_ties': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Assume that there are no ties in y_score (which is likely to be the\\ncase if y_score is continuous) for efficiency gains.\\n'}},\n",
       "        'required': ['y_true', 'y_score']}}},\n",
       "     {'func_name': 'precision_recall_fscore_support',\n",
       "      'func_desc': 'Compute precision, recall, F-measure and support for each class.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support',\n",
       "      'function_definitions': {'function_name': 'precision_recall_fscore_support',\n",
       "       'full_function': \"sklearn.metrics.precision_recall_fscore_support(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')\",\n",
       "       'function_text': \"Compute precision, recall, F-measure and support for each class. The precision is the ratio tp / (tp + fp) where tp is the number of\\ntrue positives and fp the number of false positives. The precision is\\nintuitively the ability of the classifier not to label a negative sample as\\npositive. The recall is the ratio tp / (tp + fn) where tp is the number of\\ntrue positives and fn the number of false negatives. The recall is\\nintuitively the ability of the classifier to find all the positive samples. The F-beta score can be interpreted as a weighted harmonic mean of\\nthe precision and recall, where an F-beta score reaches its best\\nvalue at 1 and worst score at 0. The F-beta score weights recall more than precision by a factor of\\nbeta. beta == 1.0 means recall and precision are equally important. The support is the number of occurrences of each class in y_true. Support beyond term:binary targets is achieved by treating multiclass\\nand multilabel data as a collection of binary problems, one for each\\nlabel. For the binary case, setting average='binary' will return\\nmetrics for pos_label. If average is not 'binary', pos_label is ignored\\nand metrics for both classes are computed, then averaged or both returned (when\\naverage=None). Similarly, for multiclass and multilabel targets,\\nmetrics for all labels are either returned or averaged depending on the average\\nparameter. Use labels specify the set of labels to calculate metrics for. Read more in the User Guide.\",\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-multiclass',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Estimated targets as returned by a classifier.\\n'},\n",
       "        {'param_name': 'beta',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'The strength of recall versus precision in the F-score.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like, default=None',\n",
       "         'param_desc': \"The set of labels to include when average != 'binary', and their\\norder if average is None. Labels present in the data can be\\nexcluded, for example in multiclass classification to exclude a “negative\\nclass”. Labels not present in the data can be included and will be\\n“assigned” 0 samples. For multilabel targets, labels are column indices.\\nBy default, all labels in y_true and y_pred are used in sorted order.\\n\"},\n",
       "        {'param_name': 'pos_label',\n",
       "         'param_type': 'int, float, bool or str, default=1',\n",
       "         'param_desc': \"The class to report if average='binary' and the data is binary,\\notherwise this parameter is ignored.\\nFor multiclass or multilabel targets, set labels=[pos_label] and\\naverage != 'binary' to report metrics for one label only.\\n\"},\n",
       "        {'param_name': 'average',\n",
       "         'param_type': '{‘binary’, ‘micro’, ‘macro’, ‘samples’, ‘weighted’},             default=None',\n",
       "         'param_desc': \"If None, the metrics for each class are returned. Otherwise, this\\ndetermines the type of averaging performed on the data:\\n\\n'binary':Only report results for the class specified by pos_label.\\nThis is applicable only if targets (y_{true,pred}) are binary.\\n\\n'micro':Calculate metrics globally by counting the total true positives,\\nfalse negatives and false positives.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average weighted\\nby support (the number of true instances for each label). This\\nalters ‘macro’ to account for label imbalance; it can result in an\\nF-score that is not between precision and recall.\\n\\n'samples':Calculate metrics for each instance, and find their average (only\\nmeaningful for multilabel classification where this differs from\\naccuracy_score).\\n\\n\\n\"},\n",
       "        {'param_name': 'warn_for',\n",
       "         'param_type': 'list, tuple or set, for internal use',\n",
       "         'param_desc': 'This determines which warnings will be made in the case that this\\nfunction is being used to return only one of its metrics.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'zero_division',\n",
       "         'param_type': '{“warn”, 0.0, 1.0, np.nan}, default=”warn”',\n",
       "         'param_desc': '\\nSets the value to return when there is a zero division:\\nrecall: when there are no positive labels\\nprecision: when there are no positive predictions\\nf-score: both\\n\\n\\n\\nNotes:\\n- If set to “warn”, this acts like 0, but a warning is also raised.\\n- If set to np.nan, such values will be excluded from the average.\\n\\nAdded in version 1.3: np.nan option was added.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'precision_recall_fscore_support',\n",
       "       'descriptions': \"Compute precision, recall, F-measure and support for each class. The precision is the ratio tp / (tp + fp) where tp is the number of\\ntrue positives and fp the number of false positives. The precision is\\nintuitively the ability of the classifier not to label a negative sample as\\npositive. The recall is the ratio tp / (tp + fn) where tp is the number of\\ntrue positives and fn the number of false negatives. The recall is\\nintuitively the ability of the classifier to find all the positive samples. The F-beta score can be interpreted as a weighted harmonic mean of\\nthe precision and recall, where an F-beta score reaches its best\\nvalue at 1 and worst score at 0. The F-beta score weights recall more than precision by a factor of\\nbeta. beta == 1.0 means recall and precision are equally important. The support is the number of occurrences of each class in y_true. Support beyond term:binary targets is achieved by treating multiclass\\nand multilabel data as a collection of binary problems, one for each\\nlabel. For the binary case, setting average='binary' will return\\nmetrics for pos_label. If average is not 'binary', pos_label is ignored\\nand metrics for both classes are computed, then averaged or both returned (when\\naverage=None). Similarly, for multiclass and multilabel targets,\\nmetrics for all labels are either returned or averaged depending on the average\\nparameter. Use labels specify the set of labels to calculate metrics for. Read more in the User Guide.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Estimated targets as returned by a classifier.\\n'},\n",
       "         'beta': {'type': 'number',\n",
       "          'description': 'float, default=1.0. The strength of recall versus precision in the F-score.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': \"array-like, default=None. The set of labels to include when average != 'binary', and their\\norder if average is None. Labels present in the data can be\\nexcluded, for example in multiclass classification to exclude a “negative\\nclass”. Labels not present in the data can be included and will be\\n“assigned” 0 samples. For multilabel targets, labels are column indices.\\nBy default, all labels in y_true and y_pred are used in sorted order.\\n\"},\n",
       "         'pos_label': {'type': 'integer',\n",
       "          'description': \"int, float, bool or str, default=1. The class to report if average='binary' and the data is binary,\\notherwise this parameter is ignored.\\nFor multiclass or multilabel targets, set labels=[pos_label] and\\naverage != 'binary' to report metrics for one label only.\\n\"},\n",
       "         'average': {'type': 'string',\n",
       "          'enum': ['binary', 'micro', 'macro', 'samples', 'weighted'],\n",
       "          'description': \"{‘binary’, ‘micro’, ‘macro’, ‘samples’, ‘weighted’},             default=None. If None, the metrics for each class are returned. Otherwise, this\\ndetermines the type of averaging performed on the data:\\n\\n'binary':Only report results for the class specified by pos_label.\\nThis is applicable only if targets (y_{true,pred}) are binary.\\n\\n'micro':Calculate metrics globally by counting the total true positives,\\nfalse negatives and false positives.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average weighted\\nby support (the number of true instances for each label). This\\nalters ‘macro’ to account for label imbalance; it can result in an\\nF-score that is not between precision and recall.\\n\\n'samples':Calculate metrics for each instance, and find their average (only\\nmeaningful for multilabel classification where this differs from\\naccuracy_score).\\n\\n\\n\"},\n",
       "         'warn_for': {'type': 'integer',\n",
       "          'description': 'list, tuple or set, for internal use. This determines which warnings will be made in the case that this\\nfunction is being used to return only one of its metrics.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'zero_division': {'type': 'string',\n",
       "          'enum': ['warn', '0.0', '1.0', 'np.nan'],\n",
       "          'description': '{“warn”, 0.0, 1.0, np.nan}, default=”warn”. \\nSets the value to return when there is a zero division:\\nrecall: when there are no positive labels\\nprecision: when there are no positive predictions\\nf-score: both\\n\\n\\n\\nNotes:\\n- If set to “warn”, this acts like 0, but a warning is also raised.\\n- If set to np.nan, such values will be excluded from the average.\\n\\nAdded in version 1.3: np.nan option was added.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'recall_score',\n",
       "      'func_desc': 'Compute the recall.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score',\n",
       "      'function_definitions': {'function_name': 'recall_score',\n",
       "       'full_function': \"sklearn.metrics.recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\",\n",
       "       'function_text': \"Compute the recall. The recall is the ratio tp / (tp + fn) where tp is the number of\\ntrue positives and fn the number of false negatives. The recall is\\nintuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0. Support beyond term:binary targets is achieved by treating multiclass\\nand multilabel data as a collection of binary problems, one for each\\nlabel. For the binary case, setting average='binary' will return\\nrecall for pos_label. If average is not 'binary', pos_label is ignored\\nand recall for both classes are computed then averaged or both returned (when\\naverage=None). Similarly, for multiclass and multilabel targets,\\nrecall for all labels are either returned or averaged depending on the average\\nparameter. Use labels specify the set of labels to calculate recall for. Read more in the User Guide.\",\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-multiclass',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Estimated targets as returned by a classifier.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like, default=None',\n",
       "         'param_desc': \"The set of labels to include when average != 'binary', and their\\norder if average is None. Labels present in the data can be\\nexcluded, for example in multiclass classification to exclude a “negative\\nclass”. Labels not present in the data can be included and will be\\n“assigned” 0 samples. For multilabel targets, labels are column indices.\\nBy default, all labels in y_true and y_pred are used in sorted order.\\n\\nChanged in version 0.17: Parameter labels improved for multiclass problem.\\n\\n\"},\n",
       "        {'param_name': 'pos_label',\n",
       "         'param_type': 'int, float, bool or str, default=1',\n",
       "         'param_desc': \"The class to report if average='binary' and the data is binary,\\notherwise this parameter is ignored.\\nFor multiclass or multilabel targets, set labels=[pos_label] and\\naverage != 'binary' to report metrics for one label only.\\n\"},\n",
       "        {'param_name': 'average',\n",
       "         'param_type': '{‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None,             default=’binary’',\n",
       "         'param_desc': \"This parameter is required for multiclass/multilabel targets.\\nIf None, the scores for each class are returned. Otherwise, this\\ndetermines the type of averaging performed on the data:\\n\\n'binary':Only report results for the class specified by pos_label.\\nThis is applicable only if targets (y_{true,pred}) are binary.\\n\\n'micro':Calculate metrics globally by counting the total true positives,\\nfalse negatives and false positives.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average weighted\\nby support (the number of true instances for each label). This\\nalters ‘macro’ to account for label imbalance; it can result in an\\nF-score that is not between precision and recall. Weighted recall\\nis equal to accuracy.\\n\\n'samples':Calculate metrics for each instance, and find their average (only\\nmeaningful for multilabel classification where this differs from\\naccuracy_score).\\n\\n\\n\"},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'zero_division',\n",
       "         'param_type': '{“warn”, 0.0, 1.0, np.nan}, default=”warn”',\n",
       "         'param_desc': 'Sets the value to return when there is a zero division.\\nNotes:\\n- If set to “warn”, this acts like 0, but a warning is also raised.\\n- If set to np.nan, such values will be excluded from the average.\\n\\nAdded in version 1.3: np.nan option was added.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'recall_score',\n",
       "       'descriptions': \"Compute the recall. The recall is the ratio tp / (tp + fn) where tp is the number of\\ntrue positives and fn the number of false negatives. The recall is\\nintuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0. Support beyond term:binary targets is achieved by treating multiclass\\nand multilabel data as a collection of binary problems, one for each\\nlabel. For the binary case, setting average='binary' will return\\nrecall for pos_label. If average is not 'binary', pos_label is ignored\\nand recall for both classes are computed then averaged or both returned (when\\naverage=None). Similarly, for multiclass and multilabel targets,\\nrecall for all labels are either returned or averaged depending on the average\\nparameter. Use labels specify the set of labels to calculate recall for. Read more in the User Guide.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Estimated targets as returned by a classifier.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': \"array-like, default=None. The set of labels to include when average != 'binary', and their\\norder if average is None. Labels present in the data can be\\nexcluded, for example in multiclass classification to exclude a “negative\\nclass”. Labels not present in the data can be included and will be\\n“assigned” 0 samples. For multilabel targets, labels are column indices.\\nBy default, all labels in y_true and y_pred are used in sorted order.\\n\\nChanged in version 0.17: Parameter labels improved for multiclass problem.\\n\\n\"},\n",
       "         'pos_label': {'type': 'integer',\n",
       "          'description': \"int, float, bool or str, default=1. The class to report if average='binary' and the data is binary,\\notherwise this parameter is ignored.\\nFor multiclass or multilabel targets, set labels=[pos_label] and\\naverage != 'binary' to report metrics for one label only.\\n\"},\n",
       "         'average': {'type': 'string',\n",
       "          'enum': ['micro', 'macro', 'samples', 'weighted', 'binary'],\n",
       "          'description': \"{‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None,             default=’binary’. This parameter is required for multiclass/multilabel targets.\\nIf None, the scores for each class are returned. Otherwise, this\\ndetermines the type of averaging performed on the data:\\n\\n'binary':Only report results for the class specified by pos_label.\\nThis is applicable only if targets (y_{true,pred}) are binary.\\n\\n'micro':Calculate metrics globally by counting the total true positives,\\nfalse negatives and false positives.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average weighted\\nby support (the number of true instances for each label). This\\nalters ‘macro’ to account for label imbalance; it can result in an\\nF-score that is not between precision and recall. Weighted recall\\nis equal to accuracy.\\n\\n'samples':Calculate metrics for each instance, and find their average (only\\nmeaningful for multilabel classification where this differs from\\naccuracy_score).\\n\\n\\n\"},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'zero_division': {'type': 'string',\n",
       "          'enum': ['warn', '0.0', '1.0', 'np.nan'],\n",
       "          'description': '{“warn”, 0.0, 1.0, np.nan}, default=”warn”. Sets the value to return when there is a zero division.\\nNotes:\\n- If set to “warn”, this acts like 0, but a warning is also raised.\\n- If set to np.nan, such values will be excluded from the average.\\n\\nAdded in version 1.3: np.nan option was added.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'roc_curve',\n",
       "      'func_desc': 'Compute Receiver operating characteristic (ROC).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve',\n",
       "      'function_definitions': {'function_name': 'roc_curve',\n",
       "       'full_function': 'sklearn.metrics.roc_curve(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=True)',\n",
       "       'function_text': 'Compute Receiver operating characteristic (ROC). Note: this implementation is restricted to the binary classification task. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'True binary labels. If labels are not either {-1, 1} or {0, 1}, then\\npos_label should be explicitly given.\\n'},\n",
       "        {'param_name': 'y_score',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Target scores, can either be probability estimates of the positive\\nclass, confidence values, or non-thresholded measure of decisions\\n(as returned by “decision_function” on some classifiers).\\n'},\n",
       "        {'param_name': 'pos_label',\n",
       "         'param_type': 'int, float, bool or str, default=None',\n",
       "         'param_desc': 'The label of the positive class.\\nWhen pos_label=None, if y_true is in {-1, 1} or {0, 1},\\npos_label is set to 1, otherwise an error will be raised.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'drop_intermediate',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to drop some suboptimal thresholds which would not appear\\non a plotted ROC curve. This is useful in order to create lighter\\nROC curves.\\n\\nAdded in version 0.17: parameter drop_intermediate.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'roc_curve',\n",
       "       'descriptions': 'Compute Receiver operating characteristic (ROC). Note: this implementation is restricted to the binary classification task. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). True binary labels. If labels are not either {-1, 1} or {0, 1}, then\\npos_label should be explicitly given.\\n'},\n",
       "         'y_score': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Target scores, can either be probability estimates of the positive\\nclass, confidence values, or non-thresholded measure of decisions\\n(as returned by “decision_function” on some classifiers).\\n'},\n",
       "         'pos_label': {'type': 'integer',\n",
       "          'description': 'int, float, bool or str, default=None. The label of the positive class.\\nWhen pos_label=None, if y_true is in {-1, 1} or {0, 1},\\npos_label is set to 1, otherwise an error will be raised.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'drop_intermediate': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to drop some suboptimal thresholds which would not appear\\non a plotted ROC curve. This is useful in order to create lighter\\nROC curves.\\n\\nAdded in version 0.17: parameter drop_intermediate.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_score']}}},\n",
       "     {'func_name': 'zero_one_loss',\n",
       "      'func_desc': 'Zero-one classification loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss',\n",
       "      'function_definitions': {'function_name': 'zero_one_loss',\n",
       "       'full_function': 'sklearn.metrics.zero_one_loss(y_true, y_pred, *, normalize=True, sample_weight=None)',\n",
       "       'function_text': 'Zero-one classification loss. If normalize is True, return the fraction of misclassifications\\n(float), else it returns the number of misclassifications (int). The best\\nperformance is 0. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#zero-one-loss',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Ground truth (correct) labels.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Predicted labels, as returned by a classifier.\\n'},\n",
       "        {'param_name': 'normalize',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, return the number of misclassifications.\\nOtherwise, return the fraction of misclassifications.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'zero_one_loss',\n",
       "       'descriptions': 'Zero-one classification loss. If normalize is True, return the fraction of misclassifications\\n(float), else it returns the number of misclassifications (int). The best\\nperformance is 0. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Ground truth (correct) labels.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Predicted labels, as returned by a classifier.\\n'},\n",
       "         'normalize': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, return the number of misclassifications.\\nOtherwise, return the fraction of misclassifications.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'auc',\n",
       "      'func_desc': 'Compute Area Under the Curve (AUC) using the trapezoidal rule.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html#sklearn.metrics.auc',\n",
       "      'function_definitions': {'function_name': 'auc',\n",
       "       'full_function': 'sklearn.metrics.auc(x, y)',\n",
       "       'function_text': 'Compute Area Under the Curve (AUC) using the trapezoidal rule. This is a general function, given points on a curve.  For computing the\\narea under the ROC-curve, see roc_auc_score.  For an alternative\\nway to summarize a precision-recall curve, see\\naverage_precision_score.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score',\n",
       "       'parameter_names_desc': [{'param_name': 'x',\n",
       "         'param_type': 'array-like of shape (n,)',\n",
       "         'param_desc': 'X coordinates. These must be either monotonic increasing or monotonic\\ndecreasing.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n,)',\n",
       "         'param_desc': 'Y coordinates.\\n'}]},\n",
       "      'function_calling': {'name': 'auc',\n",
       "       'descriptions': 'Compute Area Under the Curve (AUC) using the trapezoidal rule. This is a general function, given points on a curve.  For computing the\\narea under the ROC-curve, see roc_auc_score.  For an alternative\\nway to summarize a precision-recall curve, see\\naverage_precision_score.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'x': {'type': 'array',\n",
       "          'description': 'array-like of shape (n,). X coordinates. These must be either monotonic increasing or monotonic\\ndecreasing.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n,). Y coordinates.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'balanced_accuracy_score',\n",
       "      'func_desc': 'Compute the balanced accuracy.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score',\n",
       "      'function_definitions': {'function_name': 'balanced_accuracy_score',\n",
       "       'full_function': 'sklearn.metrics.balanced_accuracy_score(y_true, y_pred, *, sample_weight=None, adjusted=False)',\n",
       "       'function_text': 'Compute the balanced accuracy. The balanced accuracy in binary and multiclass classification problems to\\ndeal with imbalanced datasets. It is defined as the average of recall\\nobtained on each class. The best value is 1 and the worst value is 0 when adjusted=False. Read more in the User Guide. Added in version 0.20.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Estimated targets as returned by a classifier.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'adjusted',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When true, the result is adjusted for chance, so that random\\nperformance would score 0, while keeping perfect performance at a score\\nof 1.\\n'}]},\n",
       "      'function_calling': {'name': 'balanced_accuracy_score',\n",
       "       'descriptions': 'Compute the balanced accuracy. The balanced accuracy in binary and multiclass classification problems to\\ndeal with imbalanced datasets. It is defined as the average of recall\\nobtained on each class. The best value is 1 and the worst value is 0 when adjusted=False. Read more in the User Guide. Added in version 0.20.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Estimated targets as returned by a classifier.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'adjusted': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When true, the result is adjusted for chance, so that random\\nperformance would score 0, while keeping perfect performance at a score\\nof 1.\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'class_likelihood_ratios',\n",
       "      'func_desc': 'Compute binary classification positive and negative likelihood ratios.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html#sklearn.metrics.class_likelihood_ratios',\n",
       "      'function_definitions': {'function_name': 'class_likelihood_ratios',\n",
       "       'full_function': 'sklearn.metrics.class_likelihood_ratios(y_true, y_pred, *, labels=None, sample_weight=None, raise_warning=True)',\n",
       "       'function_text': 'Compute binary classification positive and negative likelihood ratios. The positive likelihood ratio is LR+ = sensitivity / (1 - specificity)\\nwhere the sensitivity or recall is the ratio tp / (tp + fn) and the\\nspecificity is tn / (tn + fp). The negative likelihood ratio is LR- = (1\\n- sensitivity) / specificity. Here tp is the number of true positives,\\nfp the number of false positives, tn is the number of true negatives and\\nfn the number of false negatives. Both class likelihood ratios can be used\\nto obtain post-test probabilities given a pre-test probability. LR+ ranges from 1 to infinity. A LR+ of 1 indicates that the probability\\nof predicting the positive class is the same for samples belonging to either\\nclass; therefore, the test is useless. The greater LR+ is, the more a\\npositive prediction is likely to be a true positive when compared with the\\npre-test probability. A value of LR+ lower than 1 is invalid as it would\\nindicate that the odds of a sample being a true positive decrease with\\nrespect to the pre-test odds. LR- ranges from 0 to 1. The closer it is to 0, the lower the probability\\nof a given sample to be a false negative. A LR- of 1 means the test is\\nuseless because the odds of having the condition did not change after the\\ntest. A value of LR- greater than 1 invalidates the classifier as it\\nindicates an increase in the odds of a sample belonging to the positive\\nclass after being classified as negative. This is the case when the\\nclassifier systematically predicts the opposite of the true label. A typical application in medicine is to identify the positive/negative class\\nto the presence/absence of a disease, respectively; the classifier being a\\ndiagnostic test; the pre-test probability of an individual having the\\ndisease can be the prevalence of such disease (proportion of a particular\\npopulation found to be affected by a medical condition); and the post-test\\nprobabilities would be the probability that the condition is truly present\\ngiven a positive test result. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#class-likelihood-ratios',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'cohen_kappa_score',\n",
       "      'func_desc': \"Compute Cohen's kappa: a statistic that measures inter-annotator agreement.\",\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score',\n",
       "      'function_definitions': {'function_name': 'cohen_kappa_score',\n",
       "       'full_function': 'sklearn.metrics.cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None)',\n",
       "       'function_text': 'Compute Cohen’s kappa: a statistic that measures inter-annotator agreement. This function computes Cohen’s kappa [1], a score that expresses the level\\nof agreement between two annotators on a classification problem. It is\\ndefined as where \\\\(p_o\\\\) is the empirical probability of agreement on the label\\nassigned to any sample (the observed agreement ratio), and \\\\(p_e\\\\) is\\nthe expected agreement when both annotators assign labels randomly.\\n\\\\(p_e\\\\) is estimated using a per-annotator empirical prior over the\\nclass labels [2]. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#r219a3b9132e1-1',\n",
       "       'parameter_names_desc': [{'param_name': 'y1',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Labels assigned by the first annotator.\\n'},\n",
       "        {'param_name': 'y2',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Labels assigned by the second annotator. The kappa statistic is\\nsymmetric, so swapping y1 and y2 doesn’t change the value.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like of shape (n_classes,), default=None',\n",
       "         'param_desc': 'List of labels to index the matrix. This may be used to select a\\nsubset of labels. If None, all labels that appear at least once in\\ny1 or y2 are used.\\n'},\n",
       "        {'param_name': 'weights',\n",
       "         'param_type': '{‘linear’, ‘quadratic’}, default=None',\n",
       "         'param_desc': 'Weighting type to calculate the score. None means no weighted;\\n“linear” means linear weighted; “quadratic” means quadratic weighted.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'cohen_kappa_score',\n",
       "       'descriptions': 'Compute Cohen’s kappa: a statistic that measures inter-annotator agreement. This function computes Cohen’s kappa [1], a score that expresses the level\\nof agreement between two annotators on a classification problem. It is\\ndefined as where \\\\(p_o\\\\) is the empirical probability of agreement on the label\\nassigned to any sample (the observed agreement ratio), and \\\\(p_e\\\\) is\\nthe expected agreement when both annotators assign labels randomly.\\n\\\\(p_e\\\\) is estimated using a per-annotator empirical prior over the\\nclass labels [2]. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y1': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Labels assigned by the first annotator.\\n'},\n",
       "         'y2': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Labels assigned by the second annotator. The kappa statistic is\\nsymmetric, so swapping y1 and y2 doesn’t change the value.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_classes,), default=None. List of labels to index the matrix. This may be used to select a\\nsubset of labels. If None, all labels that appear at least once in\\ny1 or y2 are used.\\n'},\n",
       "         'weights': {'type': 'string',\n",
       "          'enum': ['linear', 'quadratic'],\n",
       "          'description': '{‘linear’, ‘quadratic’}, default=None. Weighting type to calculate the score. None means no weighted;\\n“linear” means linear weighted; “quadratic” means quadratic weighted.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': ['y1', 'y2']}}},\n",
       "     {'func_name': 'd2_log_loss_score',\n",
       "      'func_desc': '\\\\(D^2\\\\) score function, fraction of log loss explained.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_log_loss_score.html#sklearn.metrics.d2_log_loss_score',\n",
       "      'function_definitions': {'function_name': 'd2_log_loss_score',\n",
       "       'full_function': 'sklearn.metrics.d2_log_loss_score(y_true, y_pred, *, sample_weight=None, labels=None)',\n",
       "       'function_text': '\\\\(D^2\\\\) score function, fraction of log loss explained. Best possible score is 1.0 and it can be negative (because the model can be\\narbitrarily worse). A model that always predicts the per-class proportions\\nof y_true, disregarding the input features, gets a D^2 score of 0.0. Read more in the User Guide. Added in version 1.5.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#d2-score-classification',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like or label indicator matrix',\n",
       "         'param_desc': 'The actuals labels for the n_samples samples.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples, n_classes) or (n_samples,)',\n",
       "         'param_desc': 'Predicted probabilities, as returned by a classifier’s\\npredict_proba method. If y_pred.shape = (n_samples,)\\nthe probabilities provided are assumed to be that of the\\npositive class. The labels in y_pred are assumed to be\\nordered alphabetically, as done by\\nLabelBinarizer.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like, default=None',\n",
       "         'param_desc': 'If not provided, labels will be inferred from y_true. If labels\\nis None and y_pred has shape (n_samples,) the labels are\\nassumed to be binary and are inferred from y_true.\\n'}]},\n",
       "      'function_calling': {'name': 'd2_log_loss_score',\n",
       "       'descriptions': '\\\\(D^2\\\\) score function, fraction of log loss explained. Best possible score is 1.0 and it can be negative (because the model can be\\narbitrarily worse). A model that always predicts the per-class proportions\\nof y_true, disregarding the input features, gets a D^2 score of 0.0. Read more in the User Guide. Added in version 1.5.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like or label indicator matrix. The actuals labels for the n_samples samples.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_classes) or (n_samples,). Predicted probabilities, as returned by a classifier’s\\npredict_proba method. If y_pred.shape = (n_samples,)\\nthe probabilities provided are assumed to be that of the\\npositive class. The labels in y_pred are assumed to be\\nordered alphabetically, as done by\\nLabelBinarizer.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': 'array-like, default=None. If not provided, labels will be inferred from y_true. If labels\\nis None and y_pred has shape (n_samples,) the labels are\\nassumed to be binary and are inferred from y_true.\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'det_curve',\n",
       "      'func_desc': 'Compute error rates for different probability thresholds.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.det_curve.html#sklearn.metrics.det_curve',\n",
       "      'function_definitions': {'function_name': 'det_curve',\n",
       "       'full_function': 'sklearn.metrics.det_curve(y_true, y_score, pos_label=None, sample_weight=None)',\n",
       "       'function_text': 'Compute error rates for different probability thresholds. Note This metric is used for evaluation of ranking and error tradeoffs of\\na binary classification task. Read more in the User Guide. Added in version 0.24.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#det-curve',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'ndarray of shape (n_samples,)',\n",
       "         'param_desc': 'True binary labels. If labels are not either {-1, 1} or {0, 1}, then\\npos_label should be explicitly given.\\n'},\n",
       "        {'param_name': 'y_score',\n",
       "         'param_type': 'ndarray of shape of (n_samples,)',\n",
       "         'param_desc': 'Target scores, can either be probability estimates of the positive\\nclass, confidence values, or non-thresholded measure of decisions\\n(as returned by “decision_function” on some classifiers).\\n'},\n",
       "        {'param_name': 'pos_label',\n",
       "         'param_type': 'int, float, bool or str, default=None',\n",
       "         'param_desc': 'The label of the positive class.\\nWhen pos_label=None, if y_true is in {-1, 1} or {0, 1},\\npos_label is set to 1, otherwise an error will be raised.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'det_curve',\n",
       "       'descriptions': 'Compute error rates for different probability thresholds. Note This metric is used for evaluation of ranking and error tradeoffs of\\na binary classification task. Read more in the User Guide. Added in version 0.24.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples,). True binary labels. If labels are not either {-1, 1} or {0, 1}, then\\npos_label should be explicitly given.\\n'},\n",
       "         'y_score': {'type': 'array',\n",
       "          'description': 'ndarray of shape of (n_samples,). Target scores, can either be probability estimates of the positive\\nclass, confidence values, or non-thresholded measure of decisions\\n(as returned by “decision_function” on some classifiers).\\n'},\n",
       "         'pos_label': {'type': 'integer',\n",
       "          'description': 'int, float, bool or str, default=None. The label of the positive class.\\nWhen pos_label=None, if y_true is in {-1, 1} or {0, 1},\\npos_label is set to 1, otherwise an error will be raised.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'fbeta_score',\n",
       "      'func_desc': 'Compute the F-beta score.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score',\n",
       "      'function_definitions': {'function_name': 'fbeta_score',\n",
       "       'full_function': \"sklearn.metrics.fbeta_score(y_true, y_pred, *, beta, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\",\n",
       "       'function_text': \"Compute the F-beta score. The F-beta score is the weighted harmonic mean of precision and recall,\\nreaching its optimal value at 1 and its worst value at 0. The beta parameter represents the ratio of recall importance to\\nprecision importance. beta > 1 gives more weight to recall, while\\nbeta < 1 favors precision. For example, beta = 2 makes recall twice\\nas important as precision, while beta = 0.5 does the opposite.\\nAsymptotically, beta -> +inf considers only recall, and beta -> 0\\nonly precision. The formula for F-beta score is: Where \\\\(\\\\text{tp}\\\\) is the number of true positives, \\\\(\\\\text{fp}\\\\) is the\\nnumber of false positives, and \\\\(\\\\text{fn}\\\\) is the number of false negatives. Support beyond term:binary targets is achieved by treating multiclass\\nand multilabel data as a collection of binary problems, one for each\\nlabel. For the binary case, setting average='binary' will return\\nF-beta score for pos_label. If average is not 'binary', pos_label is\\nignored and F-beta score for both classes are computed, then averaged or both\\nreturned (when average=None). Similarly, for multiclass and\\nmultilabel targets, F-beta score for all labels are either returned or\\naveraged depending on the average parameter. Use labels specify the set of\\nlabels to calculate F-beta score for. Read more in the User Guide.\",\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-multiclass',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Estimated targets as returned by a classifier.\\n'},\n",
       "        {'param_name': 'beta',\n",
       "         'param_type': 'float',\n",
       "         'param_desc': 'Determines the weight of recall in the combined score.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like, default=None',\n",
       "         'param_desc': \"The set of labels to include when average != 'binary', and their\\norder if average is None. Labels present in the data can be\\nexcluded, for example in multiclass classification to exclude a “negative\\nclass”. Labels not present in the data can be included and will be\\n“assigned” 0 samples. For multilabel targets, labels are column indices.\\nBy default, all labels in y_true and y_pred are used in sorted order.\\n\\nChanged in version 0.17: Parameter labels improved for multiclass problem.\\n\\n\"},\n",
       "        {'param_name': 'pos_label',\n",
       "         'param_type': 'int, float, bool or str, default=1',\n",
       "         'param_desc': \"The class to report if average='binary' and the data is binary,\\notherwise this parameter is ignored.\\nFor multiclass or multilabel targets, set labels=[pos_label] and\\naverage != 'binary' to report metrics for one label only.\\n\"},\n",
       "        {'param_name': 'average',\n",
       "         'param_type': '{‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None,             default=’binary’',\n",
       "         'param_desc': \"This parameter is required for multiclass/multilabel targets.\\nIf None, the scores for each class are returned. Otherwise, this\\ndetermines the type of averaging performed on the data:\\n\\n'binary':Only report results for the class specified by pos_label.\\nThis is applicable only if targets (y_{true,pred}) are binary.\\n\\n'micro':Calculate metrics globally by counting the total true positives,\\nfalse negatives and false positives.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average weighted\\nby support (the number of true instances for each label). This\\nalters ‘macro’ to account for label imbalance; it can result in an\\nF-score that is not between precision and recall.\\n\\n'samples':Calculate metrics for each instance, and find their average (only\\nmeaningful for multilabel classification where this differs from\\naccuracy_score).\\n\\n\\n\"},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'zero_division',\n",
       "         'param_type': '{“warn”, 0.0, 1.0, np.nan}, default=”warn”',\n",
       "         'param_desc': 'Sets the value to return when there is a zero division, i.e. when all\\npredictions and labels are negative.\\nNotes:\\n- If set to “warn”, this acts like 0, but a warning is also raised.\\n- If set to np.nan, such values will be excluded from the average.\\n\\nAdded in version 1.3: np.nan option was added.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'fbeta_score',\n",
       "       'descriptions': \"Compute the F-beta score. The F-beta score is the weighted harmonic mean of precision and recall,\\nreaching its optimal value at 1 and its worst value at 0. The beta parameter represents the ratio of recall importance to\\nprecision importance. beta > 1 gives more weight to recall, while\\nbeta < 1 favors precision. For example, beta = 2 makes recall twice\\nas important as precision, while beta = 0.5 does the opposite.\\nAsymptotically, beta -> +inf considers only recall, and beta -> 0\\nonly precision. The formula for F-beta score is: Where \\\\(\\\\text{tp}\\\\) is the number of true positives, \\\\(\\\\text{fp}\\\\) is the\\nnumber of false positives, and \\\\(\\\\text{fn}\\\\) is the number of false negatives. Support beyond term:binary targets is achieved by treating multiclass\\nand multilabel data as a collection of binary problems, one for each\\nlabel. For the binary case, setting average='binary' will return\\nF-beta score for pos_label. If average is not 'binary', pos_label is\\nignored and F-beta score for both classes are computed, then averaged or both\\nreturned (when average=None). Similarly, for multiclass and\\nmultilabel targets, F-beta score for all labels are either returned or\\naveraged depending on the average parameter. Use labels specify the set of\\nlabels to calculate F-beta score for. Read more in the User Guide.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Estimated targets as returned by a classifier.\\n'},\n",
       "         'beta': {'type': 'number',\n",
       "          'description': 'float. Determines the weight of recall in the combined score.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': \"array-like, default=None. The set of labels to include when average != 'binary', and their\\norder if average is None. Labels present in the data can be\\nexcluded, for example in multiclass classification to exclude a “negative\\nclass”. Labels not present in the data can be included and will be\\n“assigned” 0 samples. For multilabel targets, labels are column indices.\\nBy default, all labels in y_true and y_pred are used in sorted order.\\n\\nChanged in version 0.17: Parameter labels improved for multiclass problem.\\n\\n\"},\n",
       "         'pos_label': {'type': 'integer',\n",
       "          'description': \"int, float, bool or str, default=1. The class to report if average='binary' and the data is binary,\\notherwise this parameter is ignored.\\nFor multiclass or multilabel targets, set labels=[pos_label] and\\naverage != 'binary' to report metrics for one label only.\\n\"},\n",
       "         'average': {'type': 'string',\n",
       "          'enum': ['micro', 'macro', 'samples', 'weighted', 'binary'],\n",
       "          'description': \"{‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None,             default=’binary’. This parameter is required for multiclass/multilabel targets.\\nIf None, the scores for each class are returned. Otherwise, this\\ndetermines the type of averaging performed on the data:\\n\\n'binary':Only report results for the class specified by pos_label.\\nThis is applicable only if targets (y_{true,pred}) are binary.\\n\\n'micro':Calculate metrics globally by counting the total true positives,\\nfalse negatives and false positives.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average weighted\\nby support (the number of true instances for each label). This\\nalters ‘macro’ to account for label imbalance; it can result in an\\nF-score that is not between precision and recall.\\n\\n'samples':Calculate metrics for each instance, and find their average (only\\nmeaningful for multilabel classification where this differs from\\naccuracy_score).\\n\\n\\n\"},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'zero_division': {'type': 'string',\n",
       "          'enum': ['warn', '0.0', '1.0', 'np.nan'],\n",
       "          'description': '{“warn”, 0.0, 1.0, np.nan}, default=”warn”. Sets the value to return when there is a zero division, i.e. when all\\npredictions and labels are negative.\\nNotes:\\n- If set to “warn”, this acts like 0, but a warning is also raised.\\n- If set to np.nan, such values will be excluded from the average.\\n\\nAdded in version 1.3: np.nan option was added.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'hinge_loss',\n",
       "      'func_desc': 'Average hinge loss (non-regularized).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss',\n",
       "      'function_definitions': {'function_name': 'hinge_loss',\n",
       "       'full_function': 'sklearn.metrics.hinge_loss(y_true, pred_decision, *, labels=None, sample_weight=None)',\n",
       "       'function_text': 'Average hinge loss (non-regularized). In binary class case, assuming labels in y_true are encoded with +1 and -1,\\nwhen a prediction mistake is made, margin = y_true * pred_decision is\\nalways negative (since the signs disagree), implying 1 - margin is\\nalways greater than 1.  The cumulated hinge loss is therefore an upper\\nbound of the number of mistakes made by the classifier. In multiclass case, the function expects that either all the labels are\\nincluded in y_true or an optional labels argument is provided which\\ncontains all the labels. The multilabel margin is calculated according\\nto Crammer-Singer’s method. As in the binary case, the cumulated hinge loss\\nis an upper bound of the number of mistakes made by the classifier. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#hinge-loss',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'True target, consisting of integers of two values. The positive label\\nmust be greater than the negative label.\\n'},\n",
       "        {'param_name': 'pred_decision',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_classes)',\n",
       "         'param_desc': 'Predicted decisions, as output by decision_function (floats).\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like, default=None',\n",
       "         'param_desc': 'Contains all the labels for the problem. Used in multiclass hinge loss.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'hinge_loss',\n",
       "       'descriptions': 'Average hinge loss (non-regularized). In binary class case, assuming labels in y_true are encoded with +1 and -1,\\nwhen a prediction mistake is made, margin = y_true * pred_decision is\\nalways negative (since the signs disagree), implying 1 - margin is\\nalways greater than 1.  The cumulated hinge loss is therefore an upper\\nbound of the number of mistakes made by the classifier. In multiclass case, the function expects that either all the labels are\\nincluded in y_true or an optional labels argument is provided which\\ncontains all the labels. The multilabel margin is calculated according\\nto Crammer-Singer’s method. As in the binary case, the cumulated hinge loss\\nis an upper bound of the number of mistakes made by the classifier. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). True target, consisting of integers of two values. The positive label\\nmust be greater than the negative label.\\n'},\n",
       "         'pred_decision': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_classes). Predicted decisions, as output by decision_function (floats).\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': 'array-like, default=None. Contains all the labels for the problem. Used in multiclass hinge loss.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': ['y_true', 'pred_decision']}}},\n",
       "     {'func_name': 'log_loss',\n",
       "      'func_desc': 'Log loss, aka logistic loss or cross-entropy loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss',\n",
       "      'function_definitions': {'function_name': 'log_loss',\n",
       "       'full_function': 'sklearn.metrics.log_loss(y_true, y_pred, *, normalize=True, sample_weight=None, labels=None)',\n",
       "       'function_text': 'Log loss, aka logistic loss or cross-entropy loss. This is the loss function used in (multinomial) logistic regression\\nand extensions of it such as neural networks, defined as the negative\\nlog-likelihood of a logistic model that returns y_pred probabilities\\nfor its training data y_true.\\nThe log loss is only defined for two or more labels.\\nFor a single sample with true label \\\\(y \\\\in \\\\{0,1\\\\}\\\\) and\\na probability estimate \\\\(p = \\\\operatorname{Pr}(y = 1)\\\\), the log\\nloss is: Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#log-loss',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like or label indicator matrix',\n",
       "         'param_desc': 'Ground truth (correct) labels for n_samples samples.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of float, shape = (n_samples, n_classes) or (n_samples,)',\n",
       "         'param_desc': 'Predicted probabilities, as returned by a classifier’s\\npredict_proba method. If y_pred.shape = (n_samples,)\\nthe probabilities provided are assumed to be that of the\\npositive class. The labels in y_pred are assumed to be\\nordered alphabetically, as done by\\nLabelBinarizer.\\ny_pred values are clipped to [eps, 1-eps] where eps is the machine\\nprecision for y_pred’s dtype.\\n'},\n",
       "        {'param_name': 'normalize',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If true, return the mean loss per sample.\\nOtherwise, return the sum of the per-sample losses.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like, default=None',\n",
       "         'param_desc': 'If not provided, labels will be inferred from y_true. If labels\\nis None and y_pred has shape (n_samples,) the labels are\\nassumed to be binary and are inferred from y_true.\\n\\nAdded in version 0.18.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'log_loss',\n",
       "       'descriptions': 'Log loss, aka logistic loss or cross-entropy loss. This is the loss function used in (multinomial) logistic regression\\nand extensions of it such as neural networks, defined as the negative\\nlog-likelihood of a logistic model that returns y_pred probabilities\\nfor its training data y_true.\\nThe log loss is only defined for two or more labels.\\nFor a single sample with true label \\\\(y \\\\in \\\\{0,1\\\\}\\\\) and\\na probability estimate \\\\(p = \\\\operatorname{Pr}(y = 1)\\\\), the log\\nloss is: Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like or label indicator matrix. Ground truth (correct) labels for n_samples samples.\\n'},\n",
       "         'y_pred': {'type': 'number',\n",
       "          'description': 'array-like of float, shape = (n_samples, n_classes) or (n_samples,). Predicted probabilities, as returned by a classifier’s\\npredict_proba method. If y_pred.shape = (n_samples,)\\nthe probabilities provided are assumed to be that of the\\npositive class. The labels in y_pred are assumed to be\\nordered alphabetically, as done by\\nLabelBinarizer.\\ny_pred values are clipped to [eps, 1-eps] where eps is the machine\\nprecision for y_pred’s dtype.\\n'},\n",
       "         'normalize': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If true, return the mean loss per sample.\\nOtherwise, return the sum of the per-sample losses.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': 'array-like, default=None. If not provided, labels will be inferred from y_true. If labels\\nis None and y_pred has shape (n_samples,) the labels are\\nassumed to be binary and are inferred from y_true.\\n\\nAdded in version 0.18.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'multilabel_confusion_matrix',\n",
       "      'func_desc': 'Compute a confusion matrix for each class or sample.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html#sklearn.metrics.multilabel_confusion_matrix',\n",
       "      'function_definitions': {'function_name': 'multilabel_confusion_matrix',\n",
       "       'full_function': 'sklearn.metrics.multilabel_confusion_matrix(y_true, y_pred, *, sample_weight=None, labels=None, samplewise=False)',\n",
       "       'function_text': 'Compute a confusion matrix for each class or sample. Added in version 0.21. Compute class-wise (default) or sample-wise (samplewise=True) multilabel\\nconfusion matrix to evaluate the accuracy of a classification, and output\\nconfusion matrices for each class or sample. In multilabel confusion matrix \\\\(MCM\\\\), the count of true negatives\\nis \\\\(MCM_{:,0,0}\\\\), false negatives is \\\\(MCM_{:,1,0}\\\\),\\ntrue positives is \\\\(MCM_{:,1,1}\\\\) and false positives is\\n\\\\(MCM_{:,0,1}\\\\). Multiclass data will be treated as if binarized under a one-vs-rest\\ntransformation. Returned confusion matrices will be in the order of\\nsorted unique labels in the union of (y_true, y_pred). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#multilabel-confusion-matrix',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)',\n",
       "         'param_desc': 'Estimated targets as returned by a classifier.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like of shape (n_classes,), default=None',\n",
       "         'param_desc': 'A list of classes or column indices to select some (or to force\\ninclusion of classes absent from the data).\\n'},\n",
       "        {'param_name': 'samplewise',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'In the multilabel case, this calculates a confusion matrix per sample.\\n'}]},\n",
       "      'function_calling': {'name': 'multilabel_confusion_matrix',\n",
       "       'descriptions': 'Compute a confusion matrix for each class or sample. Added in version 0.21. Compute class-wise (default) or sample-wise (samplewise=True) multilabel\\nconfusion matrix to evaluate the accuracy of a classification, and output\\nconfusion matrices for each class or sample. In multilabel confusion matrix \\\\(MCM\\\\), the count of true negatives\\nis \\\\(MCM_{:,0,0}\\\\), false negatives is \\\\(MCM_{:,1,0}\\\\),\\ntrue positives is \\\\(MCM_{:,1,1}\\\\) and false positives is\\n\\\\(MCM_{:,0,1}\\\\). Multiclass data will be treated as if binarized under a one-vs-rest\\ntransformation. Returned confusion matrices will be in the order of\\nsorted unique labels in the union of (y_true, y_pred). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,). Estimated targets as returned by a classifier.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_classes,), default=None. A list of classes or column indices to select some (or to force\\ninclusion of classes absent from the data).\\n'},\n",
       "         'samplewise': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. In the multilabel case, this calculates a confusion matrix per sample.\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'precision_recall_curve',\n",
       "      'func_desc': 'Compute precision-recall pairs for different probability thresholds.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve',\n",
       "      'function_definitions': {'function_name': 'precision_recall_curve',\n",
       "       'full_function': \"sklearn.metrics.precision_recall_curve(y_true, y_score=None, *, pos_label=None, sample_weight=None, drop_intermediate=False, probas_pred='deprecated')\",\n",
       "       'function_text': 'Compute precision-recall pairs for different probability thresholds. Note: this implementation is restricted to the binary classification task. The precision is the ratio tp / (tp + fp) where tp is the number of\\ntrue positives and fp the number of false positives. The precision is\\nintuitively the ability of the classifier not to label as positive a sample\\nthat is negative. The recall is the ratio tp / (tp + fn) where tp is the number of\\ntrue positives and fn the number of false negatives. The recall is\\nintuitively the ability of the classifier to find all the positive samples. The last precision and recall values are 1. and 0. respectively and do not\\nhave a corresponding threshold. This ensures that the graph starts on the\\ny axis. The first precision and recall values are precision=class balance and recall=1.0\\nwhich corresponds to a classifier that always predicts the positive class. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'True binary labels. If labels are not either {-1, 1} or {0, 1}, then\\npos_label should be explicitly given.\\n'},\n",
       "        {'param_name': 'y_score',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Target scores, can either be probability estimates of the positive\\nclass, or non-thresholded measure of decisions (as returned by\\ndecision_function on some classifiers).\\n'},\n",
       "        {'param_name': 'pos_label',\n",
       "         'param_type': 'int, float, bool or str, default=None',\n",
       "         'param_desc': 'The label of the positive class.\\nWhen pos_label=None, if y_true is in {-1, 1} or {0, 1},\\npos_label is set to 1, otherwise an error will be raised.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'drop_intermediate',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to drop some suboptimal thresholds which would not appear\\non a plotted precision-recall curve. This is useful in order to create\\nlighter precision-recall curves.\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "        {'param_name': 'probas_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Target scores, can either be probability estimates of the positive\\nclass, or non-thresholded measure of decisions (as returned by\\ndecision_function on some classifiers).\\n\\nDeprecated since version 1.5: probas_pred is deprecated and will be removed in 1.7. Use\\ny_score instead.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'precision_recall_curve',\n",
       "       'descriptions': 'Compute precision-recall pairs for different probability thresholds. Note: this implementation is restricted to the binary classification task. The precision is the ratio tp / (tp + fp) where tp is the number of\\ntrue positives and fp the number of false positives. The precision is\\nintuitively the ability of the classifier not to label as positive a sample\\nthat is negative. The recall is the ratio tp / (tp + fn) where tp is the number of\\ntrue positives and fn the number of false negatives. The recall is\\nintuitively the ability of the classifier to find all the positive samples. The last precision and recall values are 1. and 0. respectively and do not\\nhave a corresponding threshold. This ensures that the graph starts on the\\ny axis. The first precision and recall values are precision=class balance and recall=1.0\\nwhich corresponds to a classifier that always predicts the positive class. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). True binary labels. If labels are not either {-1, 1} or {0, 1}, then\\npos_label should be explicitly given.\\n'},\n",
       "         'y_score': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Target scores, can either be probability estimates of the positive\\nclass, or non-thresholded measure of decisions (as returned by\\ndecision_function on some classifiers).\\n'},\n",
       "         'pos_label': {'type': 'integer',\n",
       "          'description': 'int, float, bool or str, default=None. The label of the positive class.\\nWhen pos_label=None, if y_true is in {-1, 1} or {0, 1},\\npos_label is set to 1, otherwise an error will be raised.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'drop_intermediate': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to drop some suboptimal thresholds which would not appear\\non a plotted precision-recall curve. This is useful in order to create\\nlighter precision-recall curves.\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "         'probas_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Target scores, can either be probability estimates of the positive\\nclass, or non-thresholded measure of decisions (as returned by\\ndecision_function on some classifiers).\\n\\nDeprecated since version 1.5: probas_pred is deprecated and will be removed in 1.7. Use\\ny_score instead.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_score=None']}}},\n",
       "     {'func_name': 'precision_score',\n",
       "      'func_desc': 'Compute the precision.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score',\n",
       "      'function_definitions': {'function_name': 'precision_score',\n",
       "       'full_function': \"sklearn.metrics.precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\",\n",
       "       'function_text': \"Compute the precision. The precision is the ratio tp / (tp + fp) where tp is the number of\\ntrue positives and fp the number of false positives. The precision is\\nintuitively the ability of the classifier not to label as positive a sample\\nthat is negative. The best value is 1 and the worst value is 0. Support beyond term:binary targets is achieved by treating multiclass\\nand multilabel data as a collection of binary problems, one for each\\nlabel. For the binary case, setting average='binary' will return\\nprecision for pos_label. If average is not 'binary', pos_label is ignored\\nand precision for both classes are computed, then averaged or both returned (when\\naverage=None). Similarly, for multiclass and multilabel targets,\\nprecision for all labels are either returned or averaged depending on the\\naverage parameter. Use labels specify the set of labels to calculate precision\\nfor. Read more in the User Guide.\",\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-multiclass',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': '1d array-like, or label indicator array / sparse matrix',\n",
       "         'param_desc': 'Estimated targets as returned by a classifier.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like, default=None',\n",
       "         'param_desc': \"The set of labels to include when average != 'binary', and their\\norder if average is None. Labels present in the data can be\\nexcluded, for example in multiclass classification to exclude a “negative\\nclass”. Labels not present in the data can be included and will be\\n“assigned” 0 samples. For multilabel targets, labels are column indices.\\nBy default, all labels in y_true and y_pred are used in sorted order.\\n\\nChanged in version 0.17: Parameter labels improved for multiclass problem.\\n\\n\"},\n",
       "        {'param_name': 'pos_label',\n",
       "         'param_type': 'int, float, bool or str, default=1',\n",
       "         'param_desc': \"The class to report if average='binary' and the data is binary,\\notherwise this parameter is ignored.\\nFor multiclass or multilabel targets, set labels=[pos_label] and\\naverage != 'binary' to report metrics for one label only.\\n\"},\n",
       "        {'param_name': 'average',\n",
       "         'param_type': '{‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None,             default=’binary’',\n",
       "         'param_desc': \"This parameter is required for multiclass/multilabel targets.\\nIf None, the scores for each class are returned. Otherwise, this\\ndetermines the type of averaging performed on the data:\\n\\n'binary':Only report results for the class specified by pos_label.\\nThis is applicable only if targets (y_{true,pred}) are binary.\\n\\n'micro':Calculate metrics globally by counting the total true positives,\\nfalse negatives and false positives.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average weighted\\nby support (the number of true instances for each label). This\\nalters ‘macro’ to account for label imbalance; it can result in an\\nF-score that is not between precision and recall.\\n\\n'samples':Calculate metrics for each instance, and find their average (only\\nmeaningful for multilabel classification where this differs from\\naccuracy_score).\\n\\n\\n\"},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'zero_division',\n",
       "         'param_type': '{“warn”, 0.0, 1.0, np.nan}, default=”warn”',\n",
       "         'param_desc': 'Sets the value to return when there is a zero division.\\nNotes:\\n- If set to “warn”, this acts like 0, but a warning is also raised.\\n- If set to np.nan, such values will be excluded from the average.\\n\\nAdded in version 1.3: np.nan option was added.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'precision_score',\n",
       "       'descriptions': \"Compute the precision. The precision is the ratio tp / (tp + fp) where tp is the number of\\ntrue positives and fp the number of false positives. The precision is\\nintuitively the ability of the classifier not to label as positive a sample\\nthat is negative. The best value is 1 and the worst value is 0. Support beyond term:binary targets is achieved by treating multiclass\\nand multilabel data as a collection of binary problems, one for each\\nlabel. For the binary case, setting average='binary' will return\\nprecision for pos_label. If average is not 'binary', pos_label is ignored\\nand precision for both classes are computed, then averaged or both returned (when\\naverage=None). Similarly, for multiclass and multilabel targets,\\nprecision for all labels are either returned or averaged depending on the\\naverage parameter. Use labels specify the set of labels to calculate precision\\nfor. Read more in the User Guide.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': '1d array-like, or label indicator array / sparse matrix. Estimated targets as returned by a classifier.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': \"array-like, default=None. The set of labels to include when average != 'binary', and their\\norder if average is None. Labels present in the data can be\\nexcluded, for example in multiclass classification to exclude a “negative\\nclass”. Labels not present in the data can be included and will be\\n“assigned” 0 samples. For multilabel targets, labels are column indices.\\nBy default, all labels in y_true and y_pred are used in sorted order.\\n\\nChanged in version 0.17: Parameter labels improved for multiclass problem.\\n\\n\"},\n",
       "         'pos_label': {'type': 'integer',\n",
       "          'description': \"int, float, bool or str, default=1. The class to report if average='binary' and the data is binary,\\notherwise this parameter is ignored.\\nFor multiclass or multilabel targets, set labels=[pos_label] and\\naverage != 'binary' to report metrics for one label only.\\n\"},\n",
       "         'average': {'type': 'string',\n",
       "          'enum': ['micro', 'macro', 'samples', 'weighted', 'binary'],\n",
       "          'description': \"{‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None,             default=’binary’. This parameter is required for multiclass/multilabel targets.\\nIf None, the scores for each class are returned. Otherwise, this\\ndetermines the type of averaging performed on the data:\\n\\n'binary':Only report results for the class specified by pos_label.\\nThis is applicable only if targets (y_{true,pred}) are binary.\\n\\n'micro':Calculate metrics globally by counting the total true positives,\\nfalse negatives and false positives.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average weighted\\nby support (the number of true instances for each label). This\\nalters ‘macro’ to account for label imbalance; it can result in an\\nF-score that is not between precision and recall.\\n\\n'samples':Calculate metrics for each instance, and find their average (only\\nmeaningful for multilabel classification where this differs from\\naccuracy_score).\\n\\n\\n\"},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'zero_division': {'type': 'string',\n",
       "          'enum': ['warn', '0.0', '1.0', 'np.nan'],\n",
       "          'description': '{“warn”, 0.0, 1.0, np.nan}, default=”warn”. Sets the value to return when there is a zero division.\\nNotes:\\n- If set to “warn”, this acts like 0, but a warning is also raised.\\n- If set to np.nan, such values will be excluded from the average.\\n\\nAdded in version 1.3: np.nan option was added.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'roc_auc_score',\n",
       "      'func_desc': 'Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score',\n",
       "      'function_definitions': {'function_name': 'roc_auc_score',\n",
       "       'full_function': \"sklearn.metrics.roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)\",\n",
       "       'function_text': 'Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores. Note: this implementation can be used with binary, multiclass and\\nmultilabel classification, but some restrictions apply (see Parameters). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_classes)',\n",
       "         'param_desc': 'True labels or binary label indicators. The binary and multiclass cases\\nexpect labels with shape (n_samples,) while the multilabel case expects\\nbinary label indicators with shape (n_samples, n_classes).\\n'},\n",
       "        {'param_name': 'y_score',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_classes)',\n",
       "         'param_desc': 'Target scores.\\n\\nIn the binary case, it corresponds to an array of shape\\n(n_samples,). Both probability estimates and non-thresholded\\ndecision values can be provided. The probability estimates correspond\\nto the probability of the class with the greater label,\\ni.e. estimator.classes_[1] and thus\\nestimator.predict_proba(X, y)[:, 1]. The decision values\\ncorresponds to the output of estimator.decision_function(X, y).\\nSee more information in the User guide;\\nIn the multiclass case, it corresponds to an array of shape\\n(n_samples, n_classes) of probability estimates provided by the\\npredict_proba method. The probability estimates must\\nsum to 1 across the possible classes. In addition, the order of the\\nclass scores must correspond to the order of labels,\\nif provided, or else to the numerical or lexicographical order of\\nthe labels in y_true. See more information in the\\nUser guide;\\nIn the multilabel case, it corresponds to an array of shape\\n(n_samples, n_classes). Probability estimates are provided by the\\npredict_proba method and the non-thresholded decision values by\\nthe decision_function method. The probability estimates correspond\\nto the probability of the class with the greater label for each\\noutput of the classifier. See more information in the\\nUser guide.\\n\\n'},\n",
       "        {'param_name': 'average',\n",
       "         'param_type': '{‘micro’, ‘macro’, ‘samples’, ‘weighted’} or None,             default=’macro’',\n",
       "         'param_desc': \"If None, the scores for each class are returned.\\nOtherwise, this determines the type of averaging performed on the data.\\nNote: multiclass ROC AUC currently only handles the ‘macro’ and\\n‘weighted’ averages. For multiclass targets, average=None is only\\nimplemented for multi_class='ovr' and average='micro' is only\\nimplemented for multi_class='ovr'.\\n\\n'micro':Calculate metrics globally by considering each element of the label\\nindicator matrix as a label.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average, weighted\\nby support (the number of true instances for each label).\\n\\n'samples':Calculate metrics for each instance, and find their average.\\n\\n\\nWill be ignored when y_true is binary.\\n\"},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'max_fpr',\n",
       "         'param_type': 'float > 0 and <= 1, default=None',\n",
       "         'param_desc': 'If not None, the standardized partial AUC [2] over the range\\n[0, max_fpr] is returned. For the multiclass case, max_fpr,\\nshould be either equal to None or 1.0 as AUC ROC partial\\ncomputation currently is not supported for multiclass.\\n'},\n",
       "        {'param_name': 'multi_class',\n",
       "         'param_type': '{‘raise’, ‘ovr’, ‘ovo’}, default=’raise’',\n",
       "         'param_desc': \"Only used for multiclass targets. Determines the type of configuration\\nto use. The default value raises an error, so either\\n'ovr' or 'ovo' must be passed explicitly.\\n\\n'ovr':Stands for One-vs-rest. Computes the AUC of each class\\nagainst the rest [3] [4]. This\\ntreats the multiclass case in the same way as the multilabel case.\\nSensitive to class imbalance even when average == 'macro',\\nbecause class imbalance affects the composition of each of the\\n‘rest’ groupings.\\n\\n'ovo':Stands for One-vs-one. Computes the average AUC of all\\npossible pairwise combinations of classes [5].\\nInsensitive to class imbalance when\\naverage == 'macro'.\\n\\n\\n\"},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like of shape (n_classes,), default=None',\n",
       "         'param_desc': 'Only used for multiclass targets. List of labels that index the\\nclasses in y_score. If None, the numerical or lexicographical\\norder of the labels in y_true is used.\\n'}]},\n",
       "      'function_calling': {'name': 'roc_auc_score',\n",
       "       'descriptions': 'Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores. Note: this implementation can be used with binary, multiclass and\\nmultilabel classification, but some restrictions apply (see Parameters). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_classes). True labels or binary label indicators. The binary and multiclass cases\\nexpect labels with shape (n_samples,) while the multilabel case expects\\nbinary label indicators with shape (n_samples, n_classes).\\n'},\n",
       "         'y_score': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_classes). Target scores.\\n\\nIn the binary case, it corresponds to an array of shape\\n(n_samples,). Both probability estimates and non-thresholded\\ndecision values can be provided. The probability estimates correspond\\nto the probability of the class with the greater label,\\ni.e. estimator.classes_[1] and thus\\nestimator.predict_proba(X, y)[:, 1]. The decision values\\ncorresponds to the output of estimator.decision_function(X, y).\\nSee more information in the User guide;\\nIn the multiclass case, it corresponds to an array of shape\\n(n_samples, n_classes) of probability estimates provided by the\\npredict_proba method. The probability estimates must\\nsum to 1 across the possible classes. In addition, the order of the\\nclass scores must correspond to the order of labels,\\nif provided, or else to the numerical or lexicographical order of\\nthe labels in y_true. See more information in the\\nUser guide;\\nIn the multilabel case, it corresponds to an array of shape\\n(n_samples, n_classes). Probability estimates are provided by the\\npredict_proba method and the non-thresholded decision values by\\nthe decision_function method. The probability estimates correspond\\nto the probability of the class with the greater label for each\\noutput of the classifier. See more information in the\\nUser guide.\\n\\n'},\n",
       "         'average': {'type': 'string',\n",
       "          'enum': ['micro', 'macro', 'samples', 'weighted'],\n",
       "          'description': \"{‘micro’, ‘macro’, ‘samples’, ‘weighted’} or None,             default=’macro’. If None, the scores for each class are returned.\\nOtherwise, this determines the type of averaging performed on the data.\\nNote: multiclass ROC AUC currently only handles the ‘macro’ and\\n‘weighted’ averages. For multiclass targets, average=None is only\\nimplemented for multi_class='ovr' and average='micro' is only\\nimplemented for multi_class='ovr'.\\n\\n'micro':Calculate metrics globally by considering each element of the label\\nindicator matrix as a label.\\n\\n'macro':Calculate metrics for each label, and find their unweighted\\nmean.  This does not take label imbalance into account.\\n\\n'weighted':Calculate metrics for each label, and find their average, weighted\\nby support (the number of true instances for each label).\\n\\n'samples':Calculate metrics for each instance, and find their average.\\n\\n\\nWill be ignored when y_true is binary.\\n\"},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'max_fpr': {'type': 'number',\n",
       "          'description': 'float > 0 and <= 1, default=None. If not None, the standardized partial AUC [2] over the range\\n[0, max_fpr] is returned. For the multiclass case, max_fpr,\\nshould be either equal to None or 1.0 as AUC ROC partial\\ncomputation currently is not supported for multiclass.\\n'},\n",
       "         'multi_class': {'type': 'string',\n",
       "          'enum': ['raise', 'ovr', 'ovo'],\n",
       "          'description': \"{‘raise’, ‘ovr’, ‘ovo’}, default=’raise’. Only used for multiclass targets. Determines the type of configuration\\nto use. The default value raises an error, so either\\n'ovr' or 'ovo' must be passed explicitly.\\n\\n'ovr':Stands for One-vs-rest. Computes the AUC of each class\\nagainst the rest [3] [4]. This\\ntreats the multiclass case in the same way as the multilabel case.\\nSensitive to class imbalance even when average == 'macro',\\nbecause class imbalance affects the composition of each of the\\n‘rest’ groupings.\\n\\n'ovo':Stands for One-vs-one. Computes the average AUC of all\\npossible pairwise combinations of classes [5].\\nInsensitive to class imbalance when\\naverage == 'macro'.\\n\\n\\n\"},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_classes,), default=None. Only used for multiclass targets. List of labels that index the\\nclasses in y_score. If None, the numerical or lexicographical\\norder of the labels in y_true is used.\\n'}},\n",
       "        'required': ['y_true', 'y_score']}}},\n",
       "     {'func_name': 'top_k_accuracy_score',\n",
       "      'func_desc': 'Top-k Accuracy classification score.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html#sklearn.metrics.top_k_accuracy_score',\n",
       "      'function_definitions': {'function_name': 'top_k_accuracy_score',\n",
       "       'full_function': 'sklearn.metrics.top_k_accuracy_score(y_true, y_score, *, k=2, normalize=True, sample_weight=None, labels=None)',\n",
       "       'function_text': 'Top-k Accuracy classification score. This metric computes the number of times where the correct label is among\\nthe top k labels predicted (ranked by predicted scores). Note that the\\nmultilabel case isn’t covered here. Read more in the User Guide',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#top-k-accuracy-score',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'True labels.\\n'},\n",
       "        {'param_name': 'y_score',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_classes)',\n",
       "         'param_desc': 'Target scores. These can be either probability estimates or\\nnon-thresholded decision values (as returned by\\ndecision_function on some classifiers).\\nThe binary case expects scores with shape (n_samples,) while the\\nmulticlass case expects scores with shape (n_samples, n_classes).\\nIn the multiclass case, the order of the class scores must\\ncorrespond to the order of labels, if provided, or else to\\nthe numerical or lexicographical order of the labels in y_true.\\nIf y_true does not contain all the labels, labels must be\\nprovided.\\n'},\n",
       "        {'param_name': 'k',\n",
       "         'param_type': 'int, default=2',\n",
       "         'param_desc': 'Number of most likely outcomes considered to find the correct label.\\n'},\n",
       "        {'param_name': 'normalize',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, return the fraction of correctly classified samples.\\nOtherwise, return the number of correctly classified samples.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights. If None, all samples are given the same weight.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like of shape (n_classes,), default=None',\n",
       "         'param_desc': 'Multiclass only. List of labels that index the classes in y_score.\\nIf None, the numerical or lexicographical order of the labels in\\ny_true is used. If y_true does not contain all the labels,\\nlabels must be provided.\\n'}]},\n",
       "      'function_calling': {'name': 'top_k_accuracy_score',\n",
       "       'descriptions': 'Top-k Accuracy classification score. This metric computes the number of times where the correct label is among\\nthe top k labels predicted (ranked by predicted scores). Note that the\\nmultilabel case isn’t covered here. Read more in the User Guide',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). True labels.\\n'},\n",
       "         'y_score': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_classes). Target scores. These can be either probability estimates or\\nnon-thresholded decision values (as returned by\\ndecision_function on some classifiers).\\nThe binary case expects scores with shape (n_samples,) while the\\nmulticlass case expects scores with shape (n_samples, n_classes).\\nIn the multiclass case, the order of the class scores must\\ncorrespond to the order of labels, if provided, or else to\\nthe numerical or lexicographical order of the labels in y_true.\\nIf y_true does not contain all the labels, labels must be\\nprovided.\\n'},\n",
       "         'k': {'type': 'integer',\n",
       "          'description': 'int, default=2. Number of most likely outcomes considered to find the correct label.\\n'},\n",
       "         'normalize': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, return the fraction of correctly classified samples.\\nOtherwise, return the number of correctly classified samples.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights. If None, all samples are given the same weight.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_classes,), default=None. Multiclass only. List of labels that index the classes in y_score.\\nIf None, the numerical or lexicographical order of the labels in\\ny_true is used. If y_true does not contain all the labels,\\nlabels must be provided.\\n'}},\n",
       "        'required': ['y_true', 'y_score']}}}]},\n",
       "   {'Regression metrics': [{'func_name': 'd2_absolute_error_score',\n",
       "      'func_desc': '\\\\(D^2\\\\) regression score function, fraction of absolute error explained.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_absolute_error_score.html#sklearn.metrics.d2_absolute_error_score',\n",
       "      'function_definitions': {'function_name': 'd2_absolute_error_score',\n",
       "       'full_function': \"sklearn.metrics.d2_absolute_error_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\",\n",
       "       'function_text': '\\\\(D^2\\\\) regression score function, fraction of absolute error explained. Best possible score is 1.0 and it can be negative (because the model can be\\narbitrarily worse). A model that always uses the empirical median of y_true\\nas constant prediction, disregarding the input features,\\ngets a \\\\(D^2\\\\) score of 0.0. Read more in the User Guide. Added in version 1.1.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#d2-score',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'multioutput',\n",
       "         'param_type': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’',\n",
       "         'param_desc': 'Defines aggregating of multiple output values.\\nArray-like value defines weights used to average scores.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Scores of all outputs are averaged with uniform weight.\\n\\n\\n'}]},\n",
       "      'function_calling': {'name': 'd2_absolute_error_score',\n",
       "       'descriptions': '\\\\(D^2\\\\) regression score function, fraction of absolute error explained. Best possible score is 1.0 and it can be negative (because the model can be\\narbitrarily worse). A model that always uses the empirical median of y_true\\nas constant prediction, disregarding the input features,\\ngets a \\\\(D^2\\\\) score of 0.0. Read more in the User Guide. Added in version 1.1.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Estimated target values.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'multioutput': {'type': 'array',\n",
       "          'description': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’. Defines aggregating of multiple output values.\\nArray-like value defines weights used to average scores.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Scores of all outputs are averaged with uniform weight.\\n\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'd2_tweedie_score',\n",
       "      'func_desc': '\\\\(D^2\\\\) regression score function, fraction of Tweedie deviance explained.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_tweedie_score.html#sklearn.metrics.d2_tweedie_score',\n",
       "      'function_definitions': {'function_name': 'd2_tweedie_score',\n",
       "       'full_function': 'sklearn.metrics.d2_tweedie_score(y_true, y_pred, *, sample_weight=None, power=0)',\n",
       "       'function_text': '\\\\(D^2\\\\) regression score function, fraction of Tweedie deviance explained. Best possible score is 1.0 and it can be negative (because the model can be\\narbitrarily worse). A model that always uses the empirical mean of y_true as\\nconstant prediction, disregarding the input features, gets a D^2 score of 0.0. Read more in the User Guide. Added in version 1.0.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#d2-score',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'power',\n",
       "         'param_type': 'float, default=0',\n",
       "         'param_desc': 'Tweedie power parameter. Either power <= 0 or power >= 1.\\nThe higher p the less weight is given to extreme\\ndeviations between true and predicted targets.\\n\\npower < 0: Extreme stable distribution. Requires: y_pred > 0.\\npower = 0 : Normal distribution, output corresponds to r2_score.\\ny_true and y_pred can be any real numbers.\\npower = 1 : Poisson distribution. Requires: y_true >= 0 and\\ny_pred > 0.\\n1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\\nand y_pred > 0.\\npower = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\\npower = 3 : Inverse Gaussian distribution. Requires: y_true > 0\\nand y_pred > 0.\\notherwise : Positive stable distribution. Requires: y_true > 0\\nand y_pred > 0.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'd2_tweedie_score',\n",
       "       'descriptions': '\\\\(D^2\\\\) regression score function, fraction of Tweedie deviance explained. Best possible score is 1.0 and it can be negative (because the model can be\\narbitrarily worse). A model that always uses the empirical mean of y_true as\\nconstant prediction, disregarding the input features, gets a D^2 score of 0.0. Read more in the User Guide. Added in version 1.0.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Estimated target values.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'power': {'type': 'number',\n",
       "          'description': 'float, default=0. Tweedie power parameter. Either power <= 0 or power >= 1.\\nThe higher p the less weight is given to extreme\\ndeviations between true and predicted targets.\\n\\npower < 0: Extreme stable distribution. Requires: y_pred > 0.\\npower = 0 : Normal distribution, output corresponds to r2_score.\\ny_true and y_pred can be any real numbers.\\npower = 1 : Poisson distribution. Requires: y_true >= 0 and\\ny_pred > 0.\\n1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\\nand y_pred > 0.\\npower = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\\npower = 3 : Inverse Gaussian distribution. Requires: y_true > 0\\nand y_pred > 0.\\notherwise : Positive stable distribution. Requires: y_true > 0\\nand y_pred > 0.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'max_error',\n",
       "      'func_desc': 'The max_error metric calculates the maximum residual error.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.max_error.html#sklearn.metrics.max_error',\n",
       "      'function_definitions': {'function_name': 'max_error',\n",
       "       'full_function': 'sklearn.metrics.max_error(y_true, y_pred)',\n",
       "       'function_text': 'The max_error metric calculates the maximum residual error. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#max-error',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Estimated target values.\\n'}]},\n",
       "      'function_calling': {'name': 'max_error',\n",
       "       'descriptions': 'The max_error metric calculates the maximum residual error. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Estimated target values.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'mean_absolute_percentage_error',\n",
       "      'func_desc': 'Mean absolute percentage error (MAPE) regression loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html#sklearn.metrics.mean_absolute_percentage_error',\n",
       "      'function_definitions': {'function_name': 'mean_absolute_percentage_error',\n",
       "       'full_function': \"sklearn.metrics.mean_absolute_percentage_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\",\n",
       "       'function_text': 'Mean absolute percentage error (MAPE) regression loss. Note here that the output is not a percentage in the range [0, 100]\\nand a value of 100 does not mean 100% but 1e2. Furthermore, the output\\ncan be arbitrarily high when y_true is small (which is specific to the\\nmetric) or when abs(y_true - y_pred) is large (which is common for most\\nregression metrics). Read more in the\\nUser Guide. Added in version 0.24.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-percentage-error',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'multioutput',\n",
       "         'param_type': '{‘raw_values’, ‘uniform_average’} or array-like',\n",
       "         'param_desc': 'Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\nIf input is list then the shape must be (n_outputs,).\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'}]},\n",
       "      'function_calling': {'name': 'mean_absolute_percentage_error',\n",
       "       'descriptions': 'Mean absolute percentage error (MAPE) regression loss. Note here that the output is not a percentage in the range [0, 100]\\nand a value of 100 does not mean 100% but 1e2. Furthermore, the output\\ncan be arbitrarily high when y_true is small (which is specific to the\\nmetric) or when abs(y_true - y_pred) is large (which is common for most\\nregression metrics). Read more in the\\nUser Guide. Added in version 0.24.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Estimated target values.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'multioutput': {'type': 'array',\n",
       "          'description': '{‘raw_values’, ‘uniform_average’} or array-like. Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\nIf input is list then the shape must be (n_outputs,).\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'mean_pinball_loss',\n",
       "      'func_desc': 'Pinball loss for quantile regression.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_pinball_loss.html#sklearn.metrics.mean_pinball_loss',\n",
       "      'function_definitions': {'function_name': 'mean_pinball_loss',\n",
       "       'full_function': \"sklearn.metrics.mean_pinball_loss(y_true, y_pred, *, sample_weight=None, alpha=0.5, multioutput='uniform_average')\",\n",
       "       'function_text': 'Pinball loss for quantile regression. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#pinball-loss',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'alpha',\n",
       "         'param_type': 'float, slope of the pinball loss, default=0.5,',\n",
       "         'param_desc': 'This loss is equivalent to Mean absolute error when alpha=0.5,\\nalpha=0.95 is minimized by estimators of the 95th percentile.\\n'},\n",
       "        {'param_name': 'multioutput',\n",
       "         'param_type': '{‘raw_values’, ‘uniform_average’}  or array-like of shape             (n_outputs,), default=’uniform_average’',\n",
       "         'param_desc': 'Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'}]},\n",
       "      'function_calling': {'name': 'mean_pinball_loss',\n",
       "       'descriptions': 'Pinball loss for quantile regression. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Estimated target values.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'alpha': {'type': 'number',\n",
       "          'description': 'float, slope of the pinball loss, default=0.5,. This loss is equivalent to Mean absolute error when alpha=0.5,\\nalpha=0.95 is minimized by estimators of the 95th percentile.\\n'},\n",
       "         'multioutput': {'type': 'array',\n",
       "          'description': '{‘raw_values’, ‘uniform_average’}  or array-like of shape             (n_outputs,), default=’uniform_average’. Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'mean_squared_error',\n",
       "      'func_desc': 'Mean squared error regression loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error',\n",
       "      'function_definitions': {'function_name': 'mean_squared_error',\n",
       "       'full_function': \"sklearn.metrics.mean_squared_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', squared='deprecated')\",\n",
       "       'function_text': 'Mean squared error regression loss. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'multioutput',\n",
       "         'param_type': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’',\n",
       "         'param_desc': 'Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'},\n",
       "        {'param_name': 'squared',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True returns MSE value, if False returns RMSE value.\\n\\nDeprecated since version 1.4: squared is deprecated in 1.4 and will be removed in 1.6.\\nUse root_mean_squared_error\\ninstead to calculate the root mean squared error.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'mean_squared_error',\n",
       "       'descriptions': 'Mean squared error regression loss. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Estimated target values.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'multioutput': {'type': 'array',\n",
       "          'description': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’. Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'},\n",
       "         'squared': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True returns MSE value, if False returns RMSE value.\\n\\nDeprecated since version 1.4: squared is deprecated in 1.4 and will be removed in 1.6.\\nUse root_mean_squared_error\\ninstead to calculate the root mean squared error.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'mean_tweedie_deviance',\n",
       "      'func_desc': 'Mean Tweedie deviance regression loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_tweedie_deviance.html#sklearn.metrics.mean_tweedie_deviance',\n",
       "      'function_definitions': {'function_name': 'mean_tweedie_deviance',\n",
       "       'full_function': 'sklearn.metrics.mean_tweedie_deviance(y_true, y_pred, *, sample_weight=None, power=0)',\n",
       "       'function_text': 'Mean Tweedie deviance regression loss. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#mean-tweedie-deviance',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'power',\n",
       "         'param_type': 'float, default=0',\n",
       "         'param_desc': 'Tweedie power parameter. Either power <= 0 or power >= 1.\\nThe higher p the less weight is given to extreme\\ndeviations between true and predicted targets.\\n\\npower < 0: Extreme stable distribution. Requires: y_pred > 0.\\npower = 0 : Normal distribution, output corresponds to\\nmean_squared_error. y_true and y_pred can be any real numbers.\\npower = 1 : Poisson distribution. Requires: y_true >= 0 and\\ny_pred > 0.\\n1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\\nand y_pred > 0.\\npower = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\\npower = 3 : Inverse Gaussian distribution. Requires: y_true > 0\\nand y_pred > 0.\\notherwise : Positive stable distribution. Requires: y_true > 0\\nand y_pred > 0.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'mean_tweedie_deviance',\n",
       "       'descriptions': 'Mean Tweedie deviance regression loss. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Estimated target values.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'power': {'type': 'number',\n",
       "          'description': 'float, default=0. Tweedie power parameter. Either power <= 0 or power >= 1.\\nThe higher p the less weight is given to extreme\\ndeviations between true and predicted targets.\\n\\npower < 0: Extreme stable distribution. Requires: y_pred > 0.\\npower = 0 : Normal distribution, output corresponds to\\nmean_squared_error. y_true and y_pred can be any real numbers.\\npower = 1 : Poisson distribution. Requires: y_true >= 0 and\\ny_pred > 0.\\n1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\\nand y_pred > 0.\\npower = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\\npower = 3 : Inverse Gaussian distribution. Requires: y_true > 0\\nand y_pred > 0.\\notherwise : Positive stable distribution. Requires: y_true > 0\\nand y_pred > 0.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'r2_score',\n",
       "      'func_desc': '\\\\(R^2\\\\) (coefficient of determination) regression score function.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score',\n",
       "      'function_definitions': {'function_name': 'r2_score',\n",
       "       'full_function': \"sklearn.metrics.r2_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', force_finite=True)\",\n",
       "       'function_text': '\\\\(R^2\\\\) (coefficient of determination) regression score function. Best possible score is 1.0 and it can be negative (because the\\nmodel can be arbitrarily worse). In the general case when the true y is\\nnon-constant, a constant model that always predicts the average y\\ndisregarding the input features would get a \\\\(R^2\\\\) score of 0.0. In the particular case when y_true is constant, the \\\\(R^2\\\\) score\\nis not finite: it is either NaN (perfect predictions) or -Inf\\n(imperfect predictions). To prevent such non-finite numbers to pollute\\nhigher-level experiments such as a grid search cross-validation, by default\\nthese cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect\\npredictions) respectively. You can set force_finite to False to\\nprevent this fix from happening. Note: when the prediction residuals have zero mean, the \\\\(R^2\\\\) score\\nis identical to the\\nExplained Variance score. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'multioutput',\n",
       "         'param_type': '{‘raw_values’, ‘uniform_average’, ‘variance_weighted’},             array-like of shape (n_outputs,) or None, default=’uniform_average’',\n",
       "         'param_desc': 'Defines aggregating of multiple output scores.\\nArray-like value defines weights used to average scores.\\nDefault is “uniform_average”.\\n\\n‘raw_values’ :Returns a full set of scores in case of multioutput input.\\n\\n‘uniform_average’ :Scores of all outputs are averaged with uniform weight.\\n\\n‘variance_weighted’ :Scores of all outputs are averaged, weighted by the variances\\nof each individual output.\\n\\n\\n\\nChanged in version 0.19: Default value of multioutput is ‘uniform_average’.\\n\\n'},\n",
       "        {'param_name': 'force_finite',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Flag indicating if NaN and -Inf scores resulting from constant\\ndata should be replaced with real numbers (1.0 if prediction is\\nperfect, 0.0 otherwise). Default is True, a convenient setting\\nfor hyperparameters’ search procedures (e.g. grid search\\ncross-validation).\\n\\nAdded in version 1.1.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'r2_score',\n",
       "       'descriptions': '\\\\(R^2\\\\) (coefficient of determination) regression score function. Best possible score is 1.0 and it can be negative (because the\\nmodel can be arbitrarily worse). In the general case when the true y is\\nnon-constant, a constant model that always predicts the average y\\ndisregarding the input features would get a \\\\(R^2\\\\) score of 0.0. In the particular case when y_true is constant, the \\\\(R^2\\\\) score\\nis not finite: it is either NaN (perfect predictions) or -Inf\\n(imperfect predictions). To prevent such non-finite numbers to pollute\\nhigher-level experiments such as a grid search cross-validation, by default\\nthese cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect\\npredictions) respectively. You can set force_finite to False to\\nprevent this fix from happening. Note: when the prediction residuals have zero mean, the \\\\(R^2\\\\) score\\nis identical to the\\nExplained Variance score. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Estimated target values.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'multioutput': {'type': 'array',\n",
       "          'description': '{‘raw_values’, ‘uniform_average’, ‘variance_weighted’},             array-like of shape (n_outputs,) or None, default=’uniform_average’. Defines aggregating of multiple output scores.\\nArray-like value defines weights used to average scores.\\nDefault is “uniform_average”.\\n\\n‘raw_values’ :Returns a full set of scores in case of multioutput input.\\n\\n‘uniform_average’ :Scores of all outputs are averaged with uniform weight.\\n\\n‘variance_weighted’ :Scores of all outputs are averaged, weighted by the variances\\nof each individual output.\\n\\n\\n\\nChanged in version 0.19: Default value of multioutput is ‘uniform_average’.\\n\\n'},\n",
       "         'force_finite': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Flag indicating if NaN and -Inf scores resulting from constant\\ndata should be replaced with real numbers (1.0 if prediction is\\nperfect, 0.0 otherwise). Default is True, a convenient setting\\nfor hyperparameters’ search procedures (e.g. grid search\\ncross-validation).\\n\\nAdded in version 1.1.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'root_mean_squared_log_error',\n",
       "      'func_desc': 'Root mean squared logarithmic error regression loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.root_mean_squared_log_error.html#sklearn.metrics.root_mean_squared_log_error',\n",
       "      'function_definitions': {'function_name': 'root_mean_squared_log_error',\n",
       "       'full_function': \"sklearn.metrics.root_mean_squared_log_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\",\n",
       "       'function_text': 'Root mean squared logarithmic error regression loss. Read more in the User Guide. Added in version 1.4.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-log-error',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'multioutput',\n",
       "         'param_type': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’',\n",
       "         'param_desc': 'Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors when the input is of multioutput\\nformat.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'}]},\n",
       "      'function_calling': {'name': 'root_mean_squared_log_error',\n",
       "       'descriptions': 'Root mean squared logarithmic error regression loss. Read more in the User Guide. Added in version 1.4.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Estimated target values.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'multioutput': {'type': 'array',\n",
       "          'description': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’. Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors when the input is of multioutput\\nformat.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'd2_pinball_score',\n",
       "      'func_desc': '\\\\(D^2\\\\) regression score function, fraction of pinball loss explained.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_pinball_score.html#sklearn.metrics.d2_pinball_score',\n",
       "      'function_definitions': {'function_name': 'd2_pinball_score',\n",
       "       'full_function': \"sklearn.metrics.d2_pinball_score(y_true, y_pred, *, sample_weight=None, alpha=0.5, multioutput='uniform_average')\",\n",
       "       'function_text': '\\\\(D^2\\\\) regression score function, fraction of pinball loss explained. Best possible score is 1.0 and it can be negative (because the model can be\\narbitrarily worse). A model that always uses the empirical alpha-quantile of\\ny_true as constant prediction, disregarding the input features,\\ngets a \\\\(D^2\\\\) score of 0.0. Read more in the User Guide. Added in version 1.1.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#d2-score',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'alpha',\n",
       "         'param_type': 'float, default=0.5',\n",
       "         'param_desc': 'Slope of the pinball deviance. It determines the quantile level alpha\\nfor which the pinball deviance and also D2 are optimal.\\nThe default alpha=0.5 is equivalent to d2_absolute_error_score.\\n'},\n",
       "        {'param_name': 'multioutput',\n",
       "         'param_type': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’',\n",
       "         'param_desc': 'Defines aggregating of multiple output values.\\nArray-like value defines weights used to average scores.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Scores of all outputs are averaged with uniform weight.\\n\\n\\n'}]},\n",
       "      'function_calling': {'name': 'd2_pinball_score',\n",
       "       'descriptions': '\\\\(D^2\\\\) regression score function, fraction of pinball loss explained. Best possible score is 1.0 and it can be negative (because the model can be\\narbitrarily worse). A model that always uses the empirical alpha-quantile of\\ny_true as constant prediction, disregarding the input features,\\ngets a \\\\(D^2\\\\) score of 0.0. Read more in the User Guide. Added in version 1.1.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Estimated target values.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'alpha': {'type': 'number',\n",
       "          'description': 'float, default=0.5. Slope of the pinball deviance. It determines the quantile level alpha\\nfor which the pinball deviance and also D2 are optimal.\\nThe default alpha=0.5 is equivalent to d2_absolute_error_score.\\n'},\n",
       "         'multioutput': {'type': 'array',\n",
       "          'description': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’. Defines aggregating of multiple output values.\\nArray-like value defines weights used to average scores.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Scores of all outputs are averaged with uniform weight.\\n\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'explained_variance_score',\n",
       "      'func_desc': 'Explained variance regression score function.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score',\n",
       "      'function_definitions': {'function_name': 'explained_variance_score',\n",
       "       'full_function': \"sklearn.metrics.explained_variance_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', force_finite=True)\",\n",
       "       'function_text': 'Explained variance regression score function. Best possible score is 1.0, lower values are worse. In the particular case when y_true is constant, the explained variance\\nscore is not finite: it is either NaN (perfect predictions) or\\n-Inf (imperfect predictions). To prevent such non-finite numbers to\\npollute higher-level experiments such as a grid search cross-validation,\\nby default these cases are replaced with 1.0 (perfect predictions) or 0.0\\n(imperfect predictions) respectively. If force_finite\\nis set to False, this score falls back on the original \\\\(R^2\\\\)\\ndefinition. Note The Explained Variance score is similar to the\\nR^2 score, with the notable difference that it\\ndoes not account for systematic offsets in the prediction. Most often\\nthe R^2 score should be preferred. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.r2_score.html#sklearn.metrics.r2_score',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'multioutput',\n",
       "         'param_type': '{‘raw_values’, ‘uniform_average’, ‘variance_weighted’} or             array-like of shape (n_outputs,), default=’uniform_average’',\n",
       "         'param_desc': 'Defines aggregating of multiple output scores.\\nArray-like value defines weights used to average scores.\\n\\n‘raw_values’ :Returns a full set of scores in case of multioutput input.\\n\\n‘uniform_average’ :Scores of all outputs are averaged with uniform weight.\\n\\n‘variance_weighted’ :Scores of all outputs are averaged, weighted by the variances\\nof each individual output.\\n\\n\\n'},\n",
       "        {'param_name': 'force_finite',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Flag indicating if NaN and -Inf scores resulting from constant\\ndata should be replaced with real numbers (1.0 if prediction is\\nperfect, 0.0 otherwise). Default is True, a convenient setting\\nfor hyperparameters’ search procedures (e.g. grid search\\ncross-validation).\\n\\nAdded in version 1.1.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'explained_variance_score',\n",
       "       'descriptions': 'Explained variance regression score function. Best possible score is 1.0, lower values are worse. In the particular case when y_true is constant, the explained variance\\nscore is not finite: it is either NaN (perfect predictions) or\\n-Inf (imperfect predictions). To prevent such non-finite numbers to\\npollute higher-level experiments such as a grid search cross-validation,\\nby default these cases are replaced with 1.0 (perfect predictions) or 0.0\\n(imperfect predictions) respectively. If force_finite\\nis set to False, this score falls back on the original \\\\(R^2\\\\)\\ndefinition. Note The Explained Variance score is similar to the\\nR^2 score, with the notable difference that it\\ndoes not account for systematic offsets in the prediction. Most often\\nthe R^2 score should be preferred. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Estimated target values.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'multioutput': {'type': 'array',\n",
       "          'description': '{‘raw_values’, ‘uniform_average’, ‘variance_weighted’} or             array-like of shape (n_outputs,), default=’uniform_average’. Defines aggregating of multiple output scores.\\nArray-like value defines weights used to average scores.\\n\\n‘raw_values’ :Returns a full set of scores in case of multioutput input.\\n\\n‘uniform_average’ :Scores of all outputs are averaged with uniform weight.\\n\\n‘variance_weighted’ :Scores of all outputs are averaged, weighted by the variances\\nof each individual output.\\n\\n\\n'},\n",
       "         'force_finite': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Flag indicating if NaN and -Inf scores resulting from constant\\ndata should be replaced with real numbers (1.0 if prediction is\\nperfect, 0.0 otherwise). Default is True, a convenient setting\\nfor hyperparameters’ search procedures (e.g. grid search\\ncross-validation).\\n\\nAdded in version 1.1.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'mean_absolute_error',\n",
       "      'func_desc': 'Mean absolute error regression loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error',\n",
       "      'function_definitions': {'function_name': 'mean_absolute_error',\n",
       "       'full_function': \"sklearn.metrics.mean_absolute_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\",\n",
       "       'function_text': 'Mean absolute error regression loss. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'multioutput',\n",
       "         'param_type': '{‘raw_values’, ‘uniform_average’}  or array-like of shape             (n_outputs,), default=’uniform_average’',\n",
       "         'param_desc': 'Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'}]},\n",
       "      'function_calling': {'name': 'mean_absolute_error',\n",
       "       'descriptions': 'Mean absolute error regression loss. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Estimated target values.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'multioutput': {'type': 'array',\n",
       "          'description': '{‘raw_values’, ‘uniform_average’}  or array-like of shape             (n_outputs,), default=’uniform_average’. Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'mean_gamma_deviance',\n",
       "      'func_desc': 'Mean Gamma deviance regression loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_gamma_deviance.html#sklearn.metrics.mean_gamma_deviance',\n",
       "      'function_definitions': {'function_name': 'mean_gamma_deviance',\n",
       "       'full_function': 'sklearn.metrics.mean_gamma_deviance(y_true, y_pred, *, sample_weight=None)',\n",
       "       'function_text': 'Mean Gamma deviance regression loss. Gamma deviance is equivalent to the Tweedie deviance with\\nthe power parameter power=2. It is invariant to scaling of\\nthe target variable, and measures relative errors. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#mean-tweedie-deviance',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Ground truth (correct) target values. Requires y_true > 0.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Estimated target values. Requires y_pred > 0.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'mean_gamma_deviance',\n",
       "       'descriptions': 'Mean Gamma deviance regression loss. Gamma deviance is equivalent to the Tweedie deviance with\\nthe power parameter power=2. It is invariant to scaling of\\nthe target variable, and measures relative errors. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Ground truth (correct) target values. Requires y_true > 0.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Estimated target values. Requires y_pred > 0.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'mean_poisson_deviance',\n",
       "      'func_desc': 'Mean Poisson deviance regression loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_poisson_deviance.html#sklearn.metrics.mean_poisson_deviance',\n",
       "      'function_definitions': {'function_name': 'mean_poisson_deviance',\n",
       "       'full_function': 'sklearn.metrics.mean_poisson_deviance(y_true, y_pred, *, sample_weight=None)',\n",
       "       'function_text': 'Mean Poisson deviance regression loss. Poisson deviance is equivalent to the Tweedie deviance with\\nthe power parameter power=1. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#mean-tweedie-deviance',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Ground truth (correct) target values. Requires y_true >= 0.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Estimated target values. Requires y_pred > 0.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'mean_poisson_deviance',\n",
       "       'descriptions': 'Mean Poisson deviance regression loss. Poisson deviance is equivalent to the Tweedie deviance with\\nthe power parameter power=1. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Ground truth (correct) target values. Requires y_true >= 0.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Estimated target values. Requires y_pred > 0.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'mean_squared_log_error',\n",
       "      'func_desc': 'Mean squared logarithmic error regression loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error',\n",
       "      'function_definitions': {'function_name': 'mean_squared_log_error',\n",
       "       'full_function': \"sklearn.metrics.mean_squared_log_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', squared='deprecated')\",\n",
       "       'function_text': 'Mean squared logarithmic error regression loss. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-log-error',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'multioutput',\n",
       "         'param_type': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’',\n",
       "         'param_desc': 'Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors when the input is of multioutput\\nformat.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'},\n",
       "        {'param_name': 'squared',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True returns MSLE (mean squared log error) value.\\nIf False returns RMSLE (root mean squared log error) value.\\n\\nDeprecated since version 1.4: squared is deprecated in 1.4 and will be removed in 1.6.\\nUse root_mean_squared_log_error\\ninstead to calculate the root mean squared logarithmic error.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'mean_squared_log_error',\n",
       "       'descriptions': 'Mean squared logarithmic error regression loss. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Estimated target values.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'multioutput': {'type': 'array',\n",
       "          'description': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’. Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors when the input is of multioutput\\nformat.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'},\n",
       "         'squared': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True returns MSLE (mean squared log error) value.\\nIf False returns RMSLE (root mean squared log error) value.\\n\\nDeprecated since version 1.4: squared is deprecated in 1.4 and will be removed in 1.6.\\nUse root_mean_squared_log_error\\ninstead to calculate the root mean squared logarithmic error.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'median_absolute_error',\n",
       "      'func_desc': 'Median absolute error regression loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error',\n",
       "      'function_definitions': {'function_name': 'median_absolute_error',\n",
       "       'full_function': \"sklearn.metrics.median_absolute_error(y_true, y_pred, *, multioutput='uniform_average', sample_weight=None)\",\n",
       "       'function_text': 'Median absolute error regression loss. Median absolute error output is non-negative floating point. The best value\\nis 0.0. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#median-absolute-error',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'multioutput',\n",
       "         'param_type': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’',\n",
       "         'param_desc': 'Defines aggregating of multiple output values. Array-like value defines\\nweights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n\\nAdded in version 0.24.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'median_absolute_error',\n",
       "       'descriptions': 'Median absolute error regression loss. Median absolute error output is non-negative floating point. The best value\\nis 0.0. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Estimated target values.\\n'},\n",
       "         'multioutput': {'type': 'array',\n",
       "          'description': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’. Defines aggregating of multiple output values. Array-like value defines\\nweights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n\\nAdded in version 0.24.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}},\n",
       "     {'func_name': 'root_mean_squared_error',\n",
       "      'func_desc': 'Root mean squared error regression loss.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.root_mean_squared_error.html#sklearn.metrics.root_mean_squared_error',\n",
       "      'function_definitions': {'function_name': 'root_mean_squared_error',\n",
       "       'full_function': \"sklearn.metrics.root_mean_squared_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\",\n",
       "       'function_text': 'Root mean squared error regression loss. Read more in the User Guide. Added in version 1.4.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Ground truth (correct) target values.\\n'},\n",
       "        {'param_name': 'y_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Estimated target values.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'},\n",
       "        {'param_name': 'multioutput',\n",
       "         'param_type': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’',\n",
       "         'param_desc': 'Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'}]},\n",
       "      'function_calling': {'name': 'root_mean_squared_error',\n",
       "       'descriptions': 'Root mean squared error regression loss. Read more in the User Guide. Added in version 1.4.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Ground truth (correct) target values.\\n'},\n",
       "         'y_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs). Estimated target values.\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'},\n",
       "         'multioutput': {'type': 'array',\n",
       "          'description': '{‘raw_values’, ‘uniform_average’} or array-like of shape             (n_outputs,), default=’uniform_average’. Defines aggregating of multiple output values.\\nArray-like value defines weights used to average errors.\\n\\n‘raw_values’ :Returns a full set of errors in case of multioutput input.\\n\\n‘uniform_average’ :Errors of all outputs are averaged with uniform weight.\\n\\n\\n'}},\n",
       "        'required': ['y_true', 'y_pred']}}}]},\n",
       "   {'Multilabel ranking metrics': [{'func_name': 'coverage_error',\n",
       "      'func_desc': 'Coverage error measure.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.coverage_error.html#sklearn.metrics.coverage_error',\n",
       "      'function_definitions': {'function_name': 'coverage_error',\n",
       "       'full_function': 'sklearn.metrics.coverage_error(y_true, y_score, *, sample_weight=None)',\n",
       "       'function_text': 'Coverage error measure. Compute how far we need to go through the ranked scores to cover all\\ntrue labels. The best value is equal to the average number\\nof labels in y_true per sample. Ties in y_scores are broken by giving maximal rank that would have\\nbeen assigned to all tied values. Note: Our implementation’s score is 1 greater than the one given in\\nTsoumakas et al., 2010. This extends it to handle the degenerate case\\nin which an instance has 0 true labels. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#coverage-error',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': 'array-like of shape (n_samples, n_labels)',\n",
       "         'param_desc': 'True binary labels in binary indicator format.\\n'},\n",
       "        {'param_name': 'y_score',\n",
       "         'param_type': 'array-like of shape (n_samples, n_labels)',\n",
       "         'param_desc': 'Target scores, can either be probability estimates of the positive\\nclass, confidence values, or non-thresholded measure of decisions\\n(as returned by “decision_function” on some classifiers).\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'coverage_error',\n",
       "       'descriptions': 'Coverage error measure. Compute how far we need to go through the ranked scores to cover all\\ntrue labels. The best value is equal to the average number\\nof labels in y_true per sample. Ties in y_scores are broken by giving maximal rank that would have\\nbeen assigned to all tied values. Note: Our implementation’s score is 1 greater than the one given in\\nTsoumakas et al., 2010. This extends it to handle the degenerate case\\nin which an instance has 0 true labels. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_labels). True binary labels in binary indicator format.\\n'},\n",
       "         'y_score': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_labels). Target scores, can either be probability estimates of the positive\\nclass, confidence values, or non-thresholded measure of decisions\\n(as returned by “decision_function” on some classifiers).\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': ['y_true', 'y_score']}}},\n",
       "     {'func_name': 'label_ranking_loss',\n",
       "      'func_desc': 'Compute Ranking loss measure.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_loss.html#sklearn.metrics.label_ranking_loss',\n",
       "      'function_definitions': {'function_name': 'label_ranking_loss',\n",
       "       'full_function': 'sklearn.metrics.label_ranking_loss(y_true, y_score, *, sample_weight=None)',\n",
       "       'function_text': 'Compute Ranking loss measure. Compute the average number of label pairs that are incorrectly ordered\\ngiven y_score weighted by the size of the label set and the number of\\nlabels not in the label set. This is similar to the error set size, but weighted by the number of\\nrelevant and irrelevant labels. The best performance is achieved with\\na ranking loss of zero. Read more in the User Guide. Added in version 0.17: A function label_ranking_loss',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-loss',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_labels)',\n",
       "         'param_desc': 'True binary labels in binary indicator format.\\n'},\n",
       "        {'param_name': 'y_score',\n",
       "         'param_type': 'array-like of shape (n_samples, n_labels)',\n",
       "         'param_desc': 'Target scores, can either be probability estimates of the positive\\nclass, confidence values, or non-thresholded measure of decisions\\n(as returned by “decision_function” on some classifiers).\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n'}]},\n",
       "      'function_calling': {'name': 'label_ranking_loss',\n",
       "       'descriptions': 'Compute Ranking loss measure. Compute the average number of label pairs that are incorrectly ordered\\ngiven y_score weighted by the size of the label set and the number of\\nlabels not in the label set. This is similar to the error set size, but weighted by the number of\\nrelevant and irrelevant labels. The best performance is achieved with\\na ranking loss of zero. Read more in the User Guide. Added in version 0.17: A function label_ranking_loss',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_labels). True binary labels in binary indicator format.\\n'},\n",
       "         'y_score': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_labels). Target scores, can either be probability estimates of the positive\\nclass, confidence values, or non-thresholded measure of decisions\\n(as returned by “decision_function” on some classifiers).\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n'}},\n",
       "        'required': ['y_true', 'y_score']}}},\n",
       "     {'func_name': 'label_ranking_average_precision_score',\n",
       "      'func_desc': 'Compute ranking-based average precision.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html#sklearn.metrics.label_ranking_average_precision_score',\n",
       "      'function_definitions': {'function_name': 'label_ranking_average_precision_score',\n",
       "       'full_function': 'sklearn.metrics.label_ranking_average_precision_score(y_true, y_score, *, sample_weight=None)',\n",
       "       'function_text': 'Compute ranking-based average precision. Label ranking average precision (LRAP) is the average over each ground\\ntruth label assigned to each sample, of the ratio of true vs. total\\nlabels with lower score. This metric is used in multilabel ranking problem, where the goal\\nis to give better rank to the labels associated to each sample. The obtained score is always strictly greater than 0 and\\nthe best value is 1. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-average-precision',\n",
       "       'parameter_names_desc': [{'param_name': 'y_true',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_labels)',\n",
       "         'param_desc': 'True binary labels in binary indicator format.\\n'},\n",
       "        {'param_name': 'y_score',\n",
       "         'param_type': 'array-like of shape (n_samples, n_labels)',\n",
       "         'param_desc': 'Target scores, can either be probability estimates of the positive\\nclass, confidence values, or non-thresholded measure of decisions\\n(as returned by “decision_function” on some classifiers).\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Sample weights.\\n\\nAdded in version 0.20.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'label_ranking_average_precision_score',\n",
       "       'descriptions': 'Compute ranking-based average precision. Label ranking average precision (LRAP) is the average over each ground\\ntruth label assigned to each sample, of the ratio of true vs. total\\nlabels with lower score. This metric is used in multilabel ranking problem, where the goal\\nis to give better rank to the labels associated to each sample. The obtained score is always strictly greater than 0 and\\nthe best value is 1. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y_true': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_labels). True binary labels in binary indicator format.\\n'},\n",
       "         'y_score': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_labels). Target scores, can either be probability estimates of the positive\\nclass, confidence values, or non-thresholded measure of decisions\\n(as returned by “decision_function” on some classifiers).\\n'},\n",
       "         'sample_weight': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Sample weights.\\n\\nAdded in version 0.20.\\n\\n'}},\n",
       "        'required': ['y_true', 'y_score']}}}]},\n",
       "   {'Clustering metrics': [{'func_name': 'adjusted_mutual_info_score',\n",
       "      'func_desc': 'Adjusted Mutual Information between two clusterings.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score',\n",
       "      'function_definitions': {'function_name': 'adjusted_mutual_info_score',\n",
       "       'full_function': \"sklearn.metrics.adjusted_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\",\n",
       "       'function_text': 'Adjusted Mutual Information between two clusterings. Adjusted Mutual Information (AMI) is an adjustment of the Mutual\\nInformation (MI) score to account for chance. It accounts for the fact that\\nthe MI is generally higher for two clusterings with a larger number of\\nclusters, regardless of whether there is actually more information shared.\\nFor two clusterings \\\\(U\\\\) and \\\\(V\\\\), the AMI is given as: This metric is independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore value in any way. This metric is furthermore symmetric: switching \\\\(U\\\\) (label_true)\\nwith \\\\(V\\\\) (labels_pred) will return the same score value. This can\\nbe useful to measure the agreement of two independent label assignments\\nstrategies on the same dataset when the real ground truth is not known. Be mindful that this function is an order of magnitude slower than other\\nmetrics, such as the Adjusted Rand Index. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#mutual-info-score',\n",
       "       'parameter_names_desc': [{'param_name': 'labels_true',\n",
       "         'param_type': 'int array-like of shape (n_samples,)',\n",
       "         'param_desc': 'A clustering of the data into disjoint subsets, called \\\\(U\\\\) in\\nthe above formula.\\n'},\n",
       "        {'param_name': 'labels_pred',\n",
       "         'param_type': 'int array-like of shape (n_samples,)',\n",
       "         'param_desc': 'A clustering of the data into disjoint subsets, called \\\\(V\\\\) in\\nthe above formula.\\n'},\n",
       "        {'param_name': 'average_method',\n",
       "         'param_type': '{‘min’, ‘geometric’, ‘arithmetic’, ‘max’}, default=’arithmetic’',\n",
       "         'param_desc': 'How to compute the normalizer in the denominator.\\n\\nAdded in version 0.20.\\n\\n\\nChanged in version 0.22: The default value of average_method changed from ‘max’ to\\n‘arithmetic’.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'adjusted_mutual_info_score',\n",
       "       'descriptions': 'Adjusted Mutual Information between two clusterings. Adjusted Mutual Information (AMI) is an adjustment of the Mutual\\nInformation (MI) score to account for chance. It accounts for the fact that\\nthe MI is generally higher for two clusterings with a larger number of\\nclusters, regardless of whether there is actually more information shared.\\nFor two clusterings \\\\(U\\\\) and \\\\(V\\\\), the AMI is given as: This metric is independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore value in any way. This metric is furthermore symmetric: switching \\\\(U\\\\) (label_true)\\nwith \\\\(V\\\\) (labels_pred) will return the same score value. This can\\nbe useful to measure the agreement of two independent label assignments\\nstrategies on the same dataset when the real ground truth is not known. Be mindful that this function is an order of magnitude slower than other\\nmetrics, such as the Adjusted Rand Index. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'labels_true': {'type': 'integer',\n",
       "          'description': 'int array-like of shape (n_samples,). A clustering of the data into disjoint subsets, called \\\\(U\\\\) in\\nthe above formula.\\n'},\n",
       "         'labels_pred': {'type': 'integer',\n",
       "          'description': 'int array-like of shape (n_samples,). A clustering of the data into disjoint subsets, called \\\\(V\\\\) in\\nthe above formula.\\n'},\n",
       "         'average_method': {'type': 'string',\n",
       "          'enum': ['min', 'geometric', 'arithmetic', 'max'],\n",
       "          'description': '{‘min’, ‘geometric’, ‘arithmetic’, ‘max’}, default=’arithmetic’. How to compute the normalizer in the denominator.\\n\\nAdded in version 0.20.\\n\\n\\nChanged in version 0.22: The default value of average_method changed from ‘max’ to\\n‘arithmetic’.\\n\\n'}},\n",
       "        'required': ['labels_true', 'labels_pred']}}},\n",
       "     {'func_name': 'calinski_harabasz_score',\n",
       "      'func_desc': 'Compute the Calinski and Harabasz score.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score',\n",
       "      'function_definitions': {'function_name': 'calinski_harabasz_score',\n",
       "       'full_function': 'sklearn.metrics.calinski_harabasz_score(X, labels)',\n",
       "       'function_text': 'Compute the Calinski and Harabasz score. It is also known as the Variance Ratio Criterion. The score is defined as ratio of the sum of between-cluster dispersion and\\nof within-cluster dispersion. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'A list of n_features-dimensional data points. Each row corresponds\\nto a single data point.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Predicted labels for each sample.\\n'}]},\n",
       "      'function_calling': {'name': 'calinski_harabasz_score',\n",
       "       'descriptions': 'Compute the Calinski and Harabasz score. It is also known as the Variance Ratio Criterion. The score is defined as ratio of the sum of between-cluster dispersion and\\nof within-cluster dispersion. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). A list of n_features-dimensional data points. Each row corresponds\\nto a single data point.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Predicted labels for each sample.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'cluster.pair_confusion_matrix',\n",
       "      'func_desc': 'Pair confusion matrix arising from two clusterings.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cluster.pair_confusion_matrix.html#sklearn.metrics.cluster.pair_confusion_matrix',\n",
       "      'function_definitions': {'function_name': 'pair_confusion_matrix',\n",
       "       'full_function': 'sklearn.metrics.cluster.pair_confusion_matrix(labels_true, labels_pred)',\n",
       "       'function_text': 'Pair confusion matrix arising from two clusterings. The pair confusion matrix \\\\(C\\\\) computes a 2 by 2 similarity matrix\\nbetween two clusterings by considering all pairs of samples and counting\\npairs that are assigned into the same or into different clusters under\\nthe true and predicted clusterings [1]. Considering a pair of samples that is clustered together a positive pair,\\nthen as in binary classification the count of true negatives is\\n\\\\(C_{00}\\\\), false negatives is \\\\(C_{10}\\\\), true positives is\\n\\\\(C_{11}\\\\) and false positives is \\\\(C_{01}\\\\). Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#r9ca8fd06d29a-1',\n",
       "       'parameter_names_desc': [{'param_name': 'labels_true',\n",
       "         'param_type': 'array-like of shape (n_samples,), dtype=integral',\n",
       "         'param_desc': 'Ground truth class labels to be used as a reference.\\n'},\n",
       "        {'param_name': 'labels_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,), dtype=integral',\n",
       "         'param_desc': 'Cluster labels to evaluate.\\n'}]},\n",
       "      'function_calling': {'name': 'pair_confusion_matrix',\n",
       "       'descriptions': 'Pair confusion matrix arising from two clusterings. The pair confusion matrix \\\\(C\\\\) computes a 2 by 2 similarity matrix\\nbetween two clusterings by considering all pairs of samples and counting\\npairs that are assigned into the same or into different clusters under\\nthe true and predicted clusterings [1]. Considering a pair of samples that is clustered together a positive pair,\\nthen as in binary classification the count of true negatives is\\n\\\\(C_{00}\\\\), false negatives is \\\\(C_{10}\\\\), true positives is\\n\\\\(C_{11}\\\\) and false positives is \\\\(C_{01}\\\\). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'labels_true': {'type': 'integer',\n",
       "          'description': 'array-like of shape (n_samples,), dtype=integral. Ground truth class labels to be used as a reference.\\n'},\n",
       "         'labels_pred': {'type': 'integer',\n",
       "          'description': 'array-like of shape (n_samples,), dtype=integral. Cluster labels to evaluate.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'davies_bouldin_score',\n",
       "      'func_desc': 'Compute the Davies-Bouldin score.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score',\n",
       "      'function_definitions': {'function_name': 'davies_bouldin_score',\n",
       "       'full_function': 'sklearn.metrics.davies_bouldin_score(X, labels)',\n",
       "       'function_text': 'Compute the Davies-Bouldin score. The score is defined as the average similarity measure of each cluster with\\nits most similar cluster, where similarity is the ratio of within-cluster\\ndistances to between-cluster distances. Thus, clusters which are farther\\napart and less dispersed will result in a better score. The minimum score is zero, with lower values indicating better clustering. Read more in the User Guide. Added in version 0.20.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'A list of n_features-dimensional data points. Each row corresponds\\nto a single data point.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Predicted labels for each sample.\\n'}]},\n",
       "      'function_calling': {'name': 'davies_bouldin_score',\n",
       "       'descriptions': 'Compute the Davies-Bouldin score. The score is defined as the average similarity measure of each cluster with\\nits most similar cluster, where similarity is the ratio of within-cluster\\ndistances to between-cluster distances. Thus, clusters which are farther\\napart and less dispersed will result in a better score. The minimum score is zero, with lower values indicating better clustering. Read more in the User Guide. Added in version 0.20.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). A list of n_features-dimensional data points. Each row corresponds\\nto a single data point.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Predicted labels for each sample.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'homogeneity_completeness_v_measure',\n",
       "      'func_desc': 'Compute the homogeneity and completeness and V-Measure scores at once.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html#sklearn.metrics.homogeneity_completeness_v_measure',\n",
       "      'function_definitions': {'function_name': 'homogeneity_completeness_v_measure',\n",
       "       'full_function': 'sklearn.metrics.homogeneity_completeness_v_measure(labels_true, labels_pred, *, beta=1.0)',\n",
       "       'function_text': 'Compute the homogeneity and completeness and V-Measure scores at once. Those metrics are based on normalized conditional entropy measures of\\nthe clustering labeling to evaluate given the knowledge of a Ground\\nTruth class labels of the same samples. A clustering result satisfies homogeneity if all of its clusters\\ncontain only data points which are members of a single class. A clustering result satisfies completeness if all the data points\\nthat are members of a given class are elements of the same cluster. Both scores have positive values between 0.0 and 1.0, larger values\\nbeing desirable. Those 3 metrics are independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore values in any way. V-Measure is furthermore symmetric: swapping labels_true and\\nlabel_pred will give the same score. This does not hold for\\nhomogeneity and completeness. V-Measure is identical to\\nnormalized_mutual_info_score with the arithmetic averaging\\nmethod. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score',\n",
       "       'parameter_names_desc': [{'param_name': 'labels_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Ground truth class labels to be used as a reference.\\n'},\n",
       "        {'param_name': 'labels_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Cluster labels to evaluate.\\n'},\n",
       "        {'param_name': 'beta',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'Ratio of weight attributed to homogeneity vs completeness.\\nIf beta is greater than 1, completeness is weighted more\\nstrongly in the calculation. If beta is less than 1,\\nhomogeneity is weighted more strongly.\\n'}]},\n",
       "      'function_calling': {'name': 'homogeneity_completeness_v_measure',\n",
       "       'descriptions': 'Compute the homogeneity and completeness and V-Measure scores at once. Those metrics are based on normalized conditional entropy measures of\\nthe clustering labeling to evaluate given the knowledge of a Ground\\nTruth class labels of the same samples. A clustering result satisfies homogeneity if all of its clusters\\ncontain only data points which are members of a single class. A clustering result satisfies completeness if all the data points\\nthat are members of a given class are elements of the same cluster. Both scores have positive values between 0.0 and 1.0, larger values\\nbeing desirable. Those 3 metrics are independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore values in any way. V-Measure is furthermore symmetric: swapping labels_true and\\nlabel_pred will give the same score. This does not hold for\\nhomogeneity and completeness. V-Measure is identical to\\nnormalized_mutual_info_score with the arithmetic averaging\\nmethod. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'labels_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Ground truth class labels to be used as a reference.\\n'},\n",
       "         'labels_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Cluster labels to evaluate.\\n'},\n",
       "         'beta': {'type': 'number',\n",
       "          'description': 'float, default=1.0. Ratio of weight attributed to homogeneity vs completeness.\\nIf beta is greater than 1, completeness is weighted more\\nstrongly in the calculation. If beta is less than 1,\\nhomogeneity is weighted more strongly.\\n'}},\n",
       "        'required': ['labels_true', 'labels_pred']}}},\n",
       "     {'func_name': 'mutual_info_score',\n",
       "      'func_desc': 'Mutual Information between two clusterings.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html#sklearn.metrics.mutual_info_score',\n",
       "      'function_definitions': {'function_name': 'mutual_info_score',\n",
       "       'full_function': 'sklearn.metrics.mutual_info_score(labels_true, labels_pred, *, contingency=None)',\n",
       "       'function_text': 'Mutual Information between two clusterings. The Mutual Information is a measure of the similarity between two labels\\nof the same data. Where \\\\(|U_i|\\\\) is the number of the samples\\nin cluster \\\\(U_i\\\\) and \\\\(|V_j|\\\\) is the number of the\\nsamples in cluster \\\\(V_j\\\\), the Mutual Information\\nbetween clusterings \\\\(U\\\\) and \\\\(V\\\\) is given as: This metric is independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore value in any way. This metric is furthermore symmetric: switching \\\\(U\\\\) (i.e\\nlabel_true) with \\\\(V\\\\) (i.e. label_pred) will return the\\nsame score value. This can be useful to measure the agreement of two\\nindependent label assignments strategies on the same dataset when the\\nreal ground truth is not known. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#mutual-info-score',\n",
       "       'parameter_names_desc': [{'param_name': 'labels_true',\n",
       "         'param_type': 'array-like of shape (n_samples,), dtype=integral',\n",
       "         'param_desc': 'A clustering of the data into disjoint subsets, called \\\\(U\\\\) in\\nthe above formula.\\n'},\n",
       "        {'param_name': 'labels_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,), dtype=integral',\n",
       "         'param_desc': 'A clustering of the data into disjoint subsets, called \\\\(V\\\\) in\\nthe above formula.\\n'},\n",
       "        {'param_name': 'contingency',\n",
       "         'param_type': '{array-like, sparse matrix} of shape             (n_classes_true, n_classes_pred), default=None',\n",
       "         'param_desc': 'A contingency matrix given by the\\ncontingency_matrix function. If value\\nis None, it will be computed, otherwise the given value is used,\\nwith labels_true and labels_pred ignored.\\n'}]},\n",
       "      'function_calling': {'name': 'mutual_info_score',\n",
       "       'descriptions': 'Mutual Information between two clusterings. The Mutual Information is a measure of the similarity between two labels\\nof the same data. Where \\\\(|U_i|\\\\) is the number of the samples\\nin cluster \\\\(U_i\\\\) and \\\\(|V_j|\\\\) is the number of the\\nsamples in cluster \\\\(V_j\\\\), the Mutual Information\\nbetween clusterings \\\\(U\\\\) and \\\\(V\\\\) is given as: This metric is independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore value in any way. This metric is furthermore symmetric: switching \\\\(U\\\\) (i.e\\nlabel_true) with \\\\(V\\\\) (i.e. label_pred) will return the\\nsame score value. This can be useful to measure the agreement of two\\nindependent label assignments strategies on the same dataset when the\\nreal ground truth is not known. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'labels_true': {'type': 'integer',\n",
       "          'description': 'array-like of shape (n_samples,), dtype=integral. A clustering of the data into disjoint subsets, called \\\\(U\\\\) in\\nthe above formula.\\n'},\n",
       "         'labels_pred': {'type': 'integer',\n",
       "          'description': 'array-like of shape (n_samples,), dtype=integral. A clustering of the data into disjoint subsets, called \\\\(V\\\\) in\\nthe above formula.\\n'},\n",
       "         'contingency': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape             (n_classes_true, n_classes_pred), default=None. A contingency matrix given by the\\ncontingency_matrix function. If value\\nis None, it will be computed, otherwise the given value is used,\\nwith labels_true and labels_pred ignored.\\n'}},\n",
       "        'required': ['labels_true', 'labels_pred']}}},\n",
       "     {'func_name': 'rand_score',\n",
       "      'func_desc': 'Rand index.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html#sklearn.metrics.rand_score',\n",
       "      'function_definitions': {'function_name': 'rand_score',\n",
       "       'full_function': 'sklearn.metrics.rand_score(labels_true, labels_pred)',\n",
       "       'function_text': 'Rand index. The Rand Index computes a similarity measure between two clusterings\\nby considering all pairs of samples and counting pairs that are\\nassigned in the same or different clusters in the predicted and\\ntrue clusterings [1] [2]. The raw RI score [3] is: RI = (number of agreeing pairs) / (number of pairs) Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#rbedd61930922-1',\n",
       "       'parameter_names_desc': [{'param_name': 'labels_true',\n",
       "         'param_type': 'array-like of shape (n_samples,), dtype=integral',\n",
       "         'param_desc': 'Ground truth class labels to be used as a reference.\\n'},\n",
       "        {'param_name': 'labels_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,), dtype=integral',\n",
       "         'param_desc': 'Cluster labels to evaluate.\\n'}]},\n",
       "      'function_calling': {'name': 'rand_score',\n",
       "       'descriptions': 'Rand index. The Rand Index computes a similarity measure between two clusterings\\nby considering all pairs of samples and counting pairs that are\\nassigned in the same or different clusters in the predicted and\\ntrue clusterings [1] [2]. The raw RI score [3] is: RI = (number of agreeing pairs) / (number of pairs) Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'labels_true': {'type': 'integer',\n",
       "          'description': 'array-like of shape (n_samples,), dtype=integral. Ground truth class labels to be used as a reference.\\n'},\n",
       "         'labels_pred': {'type': 'integer',\n",
       "          'description': 'array-like of shape (n_samples,), dtype=integral. Cluster labels to evaluate.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'silhouette_score',\n",
       "      'func_desc': 'Compute the mean Silhouette Coefficient of all samples.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score',\n",
       "      'function_definitions': {'function_name': 'silhouette_score',\n",
       "       'full_function': \"sklearn.metrics.silhouette_score(X, labels, *, metric='euclidean', sample_size=None, random_state=None, **kwds)\",\n",
       "       'function_text': 'Compute the mean Silhouette Coefficient of all samples. The Silhouette Coefficient is calculated using the mean intra-cluster\\ndistance (a) and the mean nearest-cluster distance (b) for each\\nsample.  The Silhouette Coefficient for a sample is (b - a) / max(a,\\nb).  To clarify, b is the distance between a sample and the nearest\\ncluster that the sample is not a part of.\\nNote that Silhouette Coefficient is only defined if number of labels\\nis 2 <= n_labels <= n_samples - 1. This function returns the mean Silhouette Coefficient over all samples.\\nTo obtain the values for each sample, use silhouette_samples. The best value is 1 and the worst value is -1. Values near 0 indicate\\noverlapping clusters. Negative values generally indicate that a sample has\\nbeen assigned to the wrong cluster, as a different cluster is more similar. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.silhouette_samples.html#sklearn.metrics.silhouette_samples',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             “precomputed” or (n_samples_a, n_features) otherwise',\n",
       "         'param_desc': 'An array of pairwise distances between samples, or a feature array.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Predicted labels for each sample.\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'str or callable, default=’euclidean’',\n",
       "         'param_desc': 'The metric to use when calculating distance between instances in a\\nfeature array. If metric is a string, it must be one of the options\\nallowed by pairwise_distances. If X is\\nthe distance array itself, use metric=\"precomputed\".\\n'},\n",
       "        {'param_name': 'sample_size',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The size of the sample to use when computing the Silhouette Coefficient\\non a random subset of the data.\\nIf sample_size is None, no sampling is used.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for selecting a subset of samples.\\nUsed when sample_size is not None.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': '**kwds',\n",
       "         'param_type': 'optional keyword parameters',\n",
       "         'param_desc': 'Any further parameters are passed directly to the distance function.\\nIf using a scipy.spatial.distance metric, the parameters are still\\nmetric dependent. See the scipy docs for usage examples.\\n'}]},\n",
       "      'function_calling': {'name': 'silhouette_score',\n",
       "       'descriptions': 'Compute the mean Silhouette Coefficient of all samples. The Silhouette Coefficient is calculated using the mean intra-cluster\\ndistance (a) and the mean nearest-cluster distance (b) for each\\nsample.  The Silhouette Coefficient for a sample is (b - a) / max(a,\\nb).  To clarify, b is the distance between a sample and the nearest\\ncluster that the sample is not a part of.\\nNote that Silhouette Coefficient is only defined if number of labels\\nis 2 <= n_labels <= n_samples - 1. This function returns the mean Silhouette Coefficient over all samples.\\nTo obtain the values for each sample, use silhouette_samples. The best value is 1 and the worst value is -1. Values near 0 indicate\\noverlapping clusters. Negative values generally indicate that a sample has\\nbeen assigned to the wrong cluster, as a different cluster is more similar. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             “precomputed” or (n_samples_a, n_features) otherwise. An array of pairwise distances between samples, or a feature array.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Predicted labels for each sample.\\n'},\n",
       "         'metric': {'type': 'string',\n",
       "          'description': 'str or callable, default=’euclidean’. The metric to use when calculating distance between instances in a\\nfeature array. If metric is a string, it must be one of the options\\nallowed by pairwise_distances. If X is\\nthe distance array itself, use metric=\"precomputed\".\\n'},\n",
       "         'sample_size': {'type': 'integer',\n",
       "          'description': 'int, default=None. The size of the sample to use when computing the Silhouette Coefficient\\non a random subset of the data.\\nIf sample_size is None, no sampling is used.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for selecting a subset of samples.\\nUsed when sample_size is not None.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['X', 'labels']}}},\n",
       "     {'func_name': 'adjusted_rand_score',\n",
       "      'func_desc': 'Rand index adjusted for chance.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score',\n",
       "      'function_definitions': {'function_name': 'adjusted_rand_score',\n",
       "       'full_function': 'sklearn.metrics.adjusted_rand_score(labels_true, labels_pred)',\n",
       "       'function_text': 'Rand index adjusted for chance. The Rand Index computes a similarity measure between two clusterings\\nby considering all pairs of samples and counting pairs that are\\nassigned in the same or different clusters in the predicted and\\ntrue clusterings. The raw RI score is then “adjusted for chance” into the ARI score\\nusing the following scheme: The adjusted Rand index is thus ensured to have a value close to\\n0.0 for random labeling independently of the number of clusters and\\nsamples and exactly 1.0 when the clusterings are identical (up to\\na permutation). The adjusted Rand index is bounded below by -0.5 for\\nespecially discordant clusterings. ARI is a symmetric measure: Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#adjusted-rand-score',\n",
       "       'parameter_names_desc': [{'param_name': 'labels_true',\n",
       "         'param_type': 'array-like of shape (n_samples,), dtype=int',\n",
       "         'param_desc': 'Ground truth class labels to be used as a reference.\\n'},\n",
       "        {'param_name': 'labels_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,), dtype=int',\n",
       "         'param_desc': 'Cluster labels to evaluate.\\n'}]},\n",
       "      'function_calling': {'name': 'adjusted_rand_score',\n",
       "       'descriptions': 'Rand index adjusted for chance. The Rand Index computes a similarity measure between two clusterings\\nby considering all pairs of samples and counting pairs that are\\nassigned in the same or different clusters in the predicted and\\ntrue clusterings. The raw RI score is then “adjusted for chance” into the ARI score\\nusing the following scheme: The adjusted Rand index is thus ensured to have a value close to\\n0.0 for random labeling independently of the number of clusters and\\nsamples and exactly 1.0 when the clusterings are identical (up to\\na permutation). The adjusted Rand index is bounded below by -0.5 for\\nespecially discordant clusterings. ARI is a symmetric measure: Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'labels_true': {'type': 'integer',\n",
       "          'description': 'array-like of shape (n_samples,), dtype=int. Ground truth class labels to be used as a reference.\\n'},\n",
       "         'labels_pred': {'type': 'integer',\n",
       "          'description': 'array-like of shape (n_samples,), dtype=int. Cluster labels to evaluate.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'cluster.contingency_matrix',\n",
       "      'func_desc': 'Build a contingency matrix describing the relationship between labels.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cluster.contingency_matrix.html#sklearn.metrics.cluster.contingency_matrix',\n",
       "      'function_definitions': {'function_name': 'contingency_matrix',\n",
       "       'full_function': \"sklearn.metrics.cluster.contingency_matrix(labels_true, labels_pred, *, eps=None, sparse=False, dtype=<class 'numpy.int64'>)\",\n",
       "       'function_text': 'Build a contingency matrix describing the relationship between labels.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'labels_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Ground truth class labels to be used as a reference.\\n'},\n",
       "        {'param_name': 'labels_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Cluster labels to evaluate.\\n'},\n",
       "        {'param_name': 'eps',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': 'If a float, that value is added to all values in the contingency\\nmatrix. This helps to stop NaN propagation.\\nIf None, nothing is adjusted.\\n'},\n",
       "        {'param_name': 'sparse',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, return a sparse CSR continency matrix. If eps is not\\nNone and sparse is True will raise ValueError.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "        {'param_name': 'dtype',\n",
       "         'param_type': 'numeric type, default=np.int64',\n",
       "         'param_desc': 'Output dtype. Ignored if eps is not None.\\n\\nAdded in version 0.24.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'contingency_matrix',\n",
       "       'descriptions': 'Build a contingency matrix describing the relationship between labels.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'labels_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Ground truth class labels to be used as a reference.\\n'},\n",
       "         'labels_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Cluster labels to evaluate.\\n'},\n",
       "         'eps': {'type': 'number',\n",
       "          'description': 'float, default=None. If a float, that value is added to all values in the contingency\\nmatrix. This helps to stop NaN propagation.\\nIf None, nothing is adjusted.\\n'},\n",
       "         'sparse': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, return a sparse CSR continency matrix. If eps is not\\nNone and sparse is True will raise ValueError.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "         'dtype': {'type': 'integer',\n",
       "          'description': 'numeric type, default=np.int64. Output dtype. Ignored if eps is not None.\\n\\nAdded in version 0.24.\\n\\n'}},\n",
       "        'required': ['labels_true', 'labels_pred']}}},\n",
       "     {'func_name': 'completeness_score',\n",
       "      'func_desc': 'Compute completeness metric of a cluster labeling given a ground truth.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score',\n",
       "      'function_definitions': {'function_name': 'completeness_score',\n",
       "       'full_function': 'sklearn.metrics.completeness_score(labels_true, labels_pred)',\n",
       "       'function_text': 'Compute completeness metric of a cluster labeling given a ground truth. A clustering result satisfies completeness if all the data points\\nthat are members of a given class are elements of the same cluster. This metric is independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore value in any way. This metric is not symmetric: switching label_true with label_pred\\nwill return the homogeneity_score which will be different in\\ngeneral. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score',\n",
       "       'parameter_names_desc': [{'param_name': 'labels_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Ground truth class labels to be used as a reference.\\n'},\n",
       "        {'param_name': 'labels_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Cluster labels to evaluate.\\n'}]},\n",
       "      'function_calling': {'name': 'completeness_score',\n",
       "       'descriptions': 'Compute completeness metric of a cluster labeling given a ground truth. A clustering result satisfies completeness if all the data points\\nthat are members of a given class are elements of the same cluster. This metric is independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore value in any way. This metric is not symmetric: switching label_true with label_pred\\nwill return the homogeneity_score which will be different in\\ngeneral. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'labels_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Ground truth class labels to be used as a reference.\\n'},\n",
       "         'labels_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Cluster labels to evaluate.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'fowlkes_mallows_score',\n",
       "      'func_desc': 'Measure the similarity of two clusterings of a set of points.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html#sklearn.metrics.fowlkes_mallows_score',\n",
       "      'function_definitions': {'function_name': 'fowlkes_mallows_score',\n",
       "       'full_function': 'sklearn.metrics.fowlkes_mallows_score(labels_true, labels_pred, *, sparse=False)',\n",
       "       'function_text': 'Measure the similarity of two clusterings of a set of points. Added in version 0.18. The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\\nthe precision and recall: Where TP is the number of True Positive (i.e. the number of pair of\\npoints that belongs in the same clusters in both labels_true and\\nlabels_pred), FP is the number of False Positive (i.e. the\\nnumber of pair of points that belongs in the same clusters in\\nlabels_true and not in labels_pred) and FN is the number of\\nFalse Negative (i.e. the number of pair of points that belongs in the\\nsame clusters in labels_pred and not in labels_True). The score ranges from 0 to 1. A high value indicates a good similarity\\nbetween two clusters. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#fowlkes-mallows-scores',\n",
       "       'parameter_names_desc': [{'param_name': 'labels_true',\n",
       "         'param_type': 'array-like of shape (n_samples,), dtype=int',\n",
       "         'param_desc': 'A clustering of the data into disjoint subsets.\\n'},\n",
       "        {'param_name': 'labels_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,), dtype=int',\n",
       "         'param_desc': 'A clustering of the data into disjoint subsets.\\n'},\n",
       "        {'param_name': 'sparse',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Compute contingency matrix internally with sparse matrix.\\n'}]},\n",
       "      'function_calling': {'name': 'fowlkes_mallows_score',\n",
       "       'descriptions': 'Measure the similarity of two clusterings of a set of points. Added in version 0.18. The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\\nthe precision and recall: Where TP is the number of True Positive (i.e. the number of pair of\\npoints that belongs in the same clusters in both labels_true and\\nlabels_pred), FP is the number of False Positive (i.e. the\\nnumber of pair of points that belongs in the same clusters in\\nlabels_true and not in labels_pred) and FN is the number of\\nFalse Negative (i.e. the number of pair of points that belongs in the\\nsame clusters in labels_pred and not in labels_True). The score ranges from 0 to 1. A high value indicates a good similarity\\nbetween two clusters. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'labels_true': {'type': 'integer',\n",
       "          'description': 'array-like of shape (n_samples,), dtype=int. A clustering of the data into disjoint subsets.\\n'},\n",
       "         'labels_pred': {'type': 'integer',\n",
       "          'description': 'array-like of shape (n_samples,), dtype=int. A clustering of the data into disjoint subsets.\\n'},\n",
       "         'sparse': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Compute contingency matrix internally with sparse matrix.\\n'}},\n",
       "        'required': ['labels_true', 'labels_pred']}}},\n",
       "     {'func_name': 'homogeneity_score',\n",
       "      'func_desc': 'Homogeneity metric of a cluster labeling given a ground truth.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score',\n",
       "      'function_definitions': {'function_name': 'homogeneity_score',\n",
       "       'full_function': 'sklearn.metrics.homogeneity_score(labels_true, labels_pred)',\n",
       "       'function_text': 'Homogeneity metric of a cluster labeling given a ground truth. A clustering result satisfies homogeneity if all of its clusters\\ncontain only data points which are members of a single class. This metric is independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore value in any way. This metric is not symmetric: switching label_true with label_pred\\nwill return the completeness_score which will be different in\\ngeneral. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score',\n",
       "       'parameter_names_desc': [{'param_name': 'labels_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Ground truth class labels to be used as a reference.\\n'},\n",
       "        {'param_name': 'labels_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Cluster labels to evaluate.\\n'}]},\n",
       "      'function_calling': {'name': 'homogeneity_score',\n",
       "       'descriptions': 'Homogeneity metric of a cluster labeling given a ground truth. A clustering result satisfies homogeneity if all of its clusters\\ncontain only data points which are members of a single class. This metric is independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore value in any way. This metric is not symmetric: switching label_true with label_pred\\nwill return the completeness_score which will be different in\\ngeneral. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'labels_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Ground truth class labels to be used as a reference.\\n'},\n",
       "         'labels_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Cluster labels to evaluate.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'normalized_mutual_info_score',\n",
       "      'func_desc': 'Normalized Mutual Information between two clusterings.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score',\n",
       "      'function_definitions': {'function_name': 'normalized_mutual_info_score',\n",
       "       'full_function': \"sklearn.metrics.normalized_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\",\n",
       "       'function_text': 'Normalized Mutual Information between two clusterings. Normalized Mutual Information (NMI) is a normalization of the Mutual\\nInformation (MI) score to scale the results between 0 (no mutual\\ninformation) and 1 (perfect correlation). In this function, mutual\\ninformation is normalized by some generalized mean of H(labels_true)\\nand H(labels_pred)), defined by the average_method. This measure is not adjusted for chance. Therefore\\nadjusted_mutual_info_score might be preferred. This metric is independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore value in any way. This metric is furthermore symmetric: switching label_true with\\nlabel_pred will return the same score value. This can be useful to\\nmeasure the agreement of two independent label assignments strategies\\non the same dataset when the real ground truth is not known. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score',\n",
       "       'parameter_names_desc': [{'param_name': 'labels_true',\n",
       "         'param_type': 'int array-like of shape (n_samples,)',\n",
       "         'param_desc': 'A clustering of the data into disjoint subsets.\\n'},\n",
       "        {'param_name': 'labels_pred',\n",
       "         'param_type': 'int array-like of shape (n_samples,)',\n",
       "         'param_desc': 'A clustering of the data into disjoint subsets.\\n'},\n",
       "        {'param_name': 'average_method',\n",
       "         'param_type': '{‘min’, ‘geometric’, ‘arithmetic’, ‘max’}, default=’arithmetic’',\n",
       "         'param_desc': 'How to compute the normalizer in the denominator.\\n\\nAdded in version 0.20.\\n\\n\\nChanged in version 0.22: The default value of average_method changed from ‘geometric’ to\\n‘arithmetic’.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'normalized_mutual_info_score',\n",
       "       'descriptions': 'Normalized Mutual Information between two clusterings. Normalized Mutual Information (NMI) is a normalization of the Mutual\\nInformation (MI) score to scale the results between 0 (no mutual\\ninformation) and 1 (perfect correlation). In this function, mutual\\ninformation is normalized by some generalized mean of H(labels_true)\\nand H(labels_pred)), defined by the average_method. This measure is not adjusted for chance. Therefore\\nadjusted_mutual_info_score might be preferred. This metric is independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore value in any way. This metric is furthermore symmetric: switching label_true with\\nlabel_pred will return the same score value. This can be useful to\\nmeasure the agreement of two independent label assignments strategies\\non the same dataset when the real ground truth is not known. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'labels_true': {'type': 'integer',\n",
       "          'description': 'int array-like of shape (n_samples,). A clustering of the data into disjoint subsets.\\n'},\n",
       "         'labels_pred': {'type': 'integer',\n",
       "          'description': 'int array-like of shape (n_samples,). A clustering of the data into disjoint subsets.\\n'},\n",
       "         'average_method': {'type': 'string',\n",
       "          'enum': ['min', 'geometric', 'arithmetic', 'max'],\n",
       "          'description': '{‘min’, ‘geometric’, ‘arithmetic’, ‘max’}, default=’arithmetic’. How to compute the normalizer in the denominator.\\n\\nAdded in version 0.20.\\n\\n\\nChanged in version 0.22: The default value of average_method changed from ‘geometric’ to\\n‘arithmetic’.\\n\\n'}},\n",
       "        'required': ['labels_true', 'labels_pred']}}},\n",
       "     {'func_name': 'silhouette_samples',\n",
       "      'func_desc': 'Compute the Silhouette Coefficient for each sample.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html#sklearn.metrics.silhouette_samples',\n",
       "      'function_definitions': {'function_name': 'silhouette_samples',\n",
       "       'full_function': \"sklearn.metrics.silhouette_samples(X, labels, *, metric='euclidean', **kwds)\",\n",
       "       'function_text': 'Compute the Silhouette Coefficient for each sample. The Silhouette Coefficient is a measure of how well samples are clustered\\nwith samples that are similar to themselves. Clustering models with a high\\nSilhouette Coefficient are said to be dense, where samples in the same\\ncluster are similar to each other, and well separated, where samples in\\ndifferent clusters are not very similar to each other. The Silhouette Coefficient is calculated using the mean intra-cluster\\ndistance (a) and the mean nearest-cluster distance (b) for each\\nsample.  The Silhouette Coefficient for a sample is (b - a) / max(a,\\nb).\\nNote that Silhouette Coefficient is only defined if number of labels\\nis 2 <= n_labels <= n_samples - 1. This function returns the Silhouette Coefficient for each sample. The best value is 1 and the worst value is -1. Values near 0 indicate\\noverlapping clusters. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             “precomputed” or (n_samples_a, n_features) otherwise',\n",
       "         'param_desc': 'An array of pairwise distances between samples, or a feature array. If\\na sparse matrix is provided, CSR format should be favoured avoiding\\nan additional copy.\\n'},\n",
       "        {'param_name': 'labels',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Label values for each sample.\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'str or callable, default=’euclidean’',\n",
       "         'param_desc': 'The metric to use when calculating distance between instances in a\\nfeature array. If metric is a string, it must be one of the options\\nallowed by pairwise_distances.\\nIf X is the distance array itself, use “precomputed” as the metric.\\nPrecomputed distance matrices must have 0 along the diagonal.\\n'},\n",
       "        {'param_name': '**kwds',\n",
       "         'param_type': 'optional keyword parameters',\n",
       "         'param_desc': 'Any further parameters are passed directly to the distance function.\\nIf using a scipy.spatial.distance metric, the parameters are still\\nmetric dependent. See the scipy docs for usage examples.\\n'}]},\n",
       "      'function_calling': {'name': 'silhouette_samples',\n",
       "       'descriptions': 'Compute the Silhouette Coefficient for each sample. The Silhouette Coefficient is a measure of how well samples are clustered\\nwith samples that are similar to themselves. Clustering models with a high\\nSilhouette Coefficient are said to be dense, where samples in the same\\ncluster are similar to each other, and well separated, where samples in\\ndifferent clusters are not very similar to each other. The Silhouette Coefficient is calculated using the mean intra-cluster\\ndistance (a) and the mean nearest-cluster distance (b) for each\\nsample.  The Silhouette Coefficient for a sample is (b - a) / max(a,\\nb).\\nNote that Silhouette Coefficient is only defined if number of labels\\nis 2 <= n_labels <= n_samples - 1. This function returns the Silhouette Coefficient for each sample. The best value is 1 and the worst value is -1. Values near 0 indicate\\noverlapping clusters. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             “precomputed” or (n_samples_a, n_features) otherwise. An array of pairwise distances between samples, or a feature array. If\\na sparse matrix is provided, CSR format should be favoured avoiding\\nan additional copy.\\n'},\n",
       "         'labels': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Label values for each sample.\\n'},\n",
       "         'metric': {'type': 'string',\n",
       "          'description': 'str or callable, default=’euclidean’. The metric to use when calculating distance between instances in a\\nfeature array. If metric is a string, it must be one of the options\\nallowed by pairwise_distances.\\nIf X is the distance array itself, use “precomputed” as the metric.\\nPrecomputed distance matrices must have 0 along the diagonal.\\n'}},\n",
       "        'required': ['X', 'labels']}}},\n",
       "     {'func_name': 'v_measure_score',\n",
       "      'func_desc': 'V-measure cluster labeling given a ground truth.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score',\n",
       "      'function_definitions': {'function_name': 'v_measure_score',\n",
       "       'full_function': 'sklearn.metrics.v_measure_score(labels_true, labels_pred, *, beta=1.0)',\n",
       "       'function_text': \"V-measure cluster labeling given a ground truth. This score is identical to normalized_mutual_info_score with\\nthe 'arithmetic' option for averaging. The V-measure is the harmonic mean between homogeneity and completeness: This metric is independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore value in any way. This metric is furthermore symmetric: switching label_true with\\nlabel_pred will return the same score value. This can be useful to\\nmeasure the agreement of two independent label assignments strategies\\non the same dataset when the real ground truth is not known. Read more in the User Guide.\",\n",
       "       'func_text_user_guide': 'sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score',\n",
       "       'parameter_names_desc': [{'param_name': 'labels_true',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Ground truth class labels to be used as a reference.\\n'},\n",
       "        {'param_name': 'labels_pred',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Cluster labels to evaluate.\\n'},\n",
       "        {'param_name': 'beta',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'Ratio of weight attributed to homogeneity vs completeness.\\nIf beta is greater than 1, completeness is weighted more\\nstrongly in the calculation. If beta is less than 1,\\nhomogeneity is weighted more strongly.\\n'}]},\n",
       "      'function_calling': {'name': 'v_measure_score',\n",
       "       'descriptions': \"V-measure cluster labeling given a ground truth. This score is identical to normalized_mutual_info_score with\\nthe 'arithmetic' option for averaging. The V-measure is the harmonic mean between homogeneity and completeness: This metric is independent of the absolute values of the labels:\\na permutation of the class or cluster label values won’t change the\\nscore value in any way. This metric is furthermore symmetric: switching label_true with\\nlabel_pred will return the same score value. This can be useful to\\nmeasure the agreement of two independent label assignments strategies\\non the same dataset when the real ground truth is not known. Read more in the User Guide.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'labels_true': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Ground truth class labels to be used as a reference.\\n'},\n",
       "         'labels_pred': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Cluster labels to evaluate.\\n'},\n",
       "         'beta': {'type': 'number',\n",
       "          'description': 'float, default=1.0. Ratio of weight attributed to homogeneity vs completeness.\\nIf beta is greater than 1, completeness is weighted more\\nstrongly in the calculation. If beta is less than 1,\\nhomogeneity is weighted more strongly.\\n'}},\n",
       "        'required': ['labels_true', 'labels_pred']}}}]},\n",
       "   {'Biclustering metrics': [{'func_name': 'consensus_score',\n",
       "      'func_desc': 'The similarity of two sets of biclusters.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.consensus_score.html#sklearn.metrics.consensus_score',\n",
       "      'function_definitions': {'function_name': 'consensus_score',\n",
       "       'full_function': \"sklearn.metrics.consensus_score(a, b, *, similarity='jaccard')\",\n",
       "       'function_text': 'The similarity of two sets of biclusters. Similarity between individual biclusters is computed. Then the\\nbest matching between sets is found using the Hungarian algorithm.\\nThe final score is the sum of similarities divided by the size of\\nthe larger set. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/biclustering.html#biclustering',\n",
       "       'parameter_names_desc': [{'param_name': 'a',\n",
       "         'param_type': 'tuple (rows, columns)',\n",
       "         'param_desc': 'Tuple of row and column indicators for a set of biclusters.\\n'},\n",
       "        {'param_name': 'b',\n",
       "         'param_type': 'tuple (rows, columns)',\n",
       "         'param_desc': 'Another set of biclusters like a.\\n'},\n",
       "        {'param_name': 'similarity',\n",
       "         'param_type': '‘jaccard’ or callable, default=’jaccard’',\n",
       "         'param_desc': 'May be the string “jaccard” to use the Jaccard coefficient, or\\nany function that takes four arguments, each of which is a 1d\\nindicator vector: (a_rows, a_columns, b_rows, b_columns).\\n'}]},\n",
       "      'function_calling': {'name': 'consensus_score',\n",
       "       'descriptions': 'The similarity of two sets of biclusters. Similarity between individual biclusters is computed. Then the\\nbest matching between sets is found using the Hungarian algorithm.\\nThe final score is the sum of similarities divided by the size of\\nthe larger set. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'a': {'type': 'array',\n",
       "          'description': 'tuple (rows, columns). Tuple of row and column indicators for a set of biclusters.\\n'},\n",
       "         'b': {'type': 'array',\n",
       "          'description': 'tuple (rows, columns). Another set of biclusters like a.\\n'},\n",
       "         'similarity': {'type': 'object',\n",
       "          'description': '‘jaccard’ or callable, default=’jaccard’. May be the string “jaccard” to use the Jaccard coefficient, or\\nany function that takes four arguments, each of which is a 1d\\nindicator vector: (a_rows, a_columns, b_rows, b_columns).\\n'}},\n",
       "        'required': ['a', 'b']}}}]},\n",
       "   {'Distance metrics': [{'func_name': 'DistanceMetric',\n",
       "      'func_desc': 'Uniform interface for fast distance metric functions.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html#sklearn.metrics.DistanceMetric',\n",
       "      'function_definitions': {'function_name': 'DistanceMetric',\n",
       "       'full_function': 'class sklearn.metrics.DistanceMetric#',\n",
       "       'function_text': \"Uniform interface for fast distance metric functions. The DistanceMetric class provides a convenient way to compute pairwise distances\\nbetween samples. It supports various distance metrics, such as Euclidean distance,\\nManhattan distance, and more. The pairwise method can be used to compute pairwise distances between samples in\\nthe input arrays. It returns a distance matrix representing the distances between\\nall pairs of samples. The get_metric method allows you to retrieve a specific metric using its\\nstring identifier. Examples Available Metrics The following lists the string metric identifiers and the associated\\ndistance metric classes: Metrics intended for real-valued vector spaces: identifier class name args distance function “euclidean” EuclideanDistance sqrt(sum((x - y)^2)) “manhattan” ManhattanDistance sum(|x - y|) “chebyshev” ChebyshevDistance max(|x - y|) “minkowski” MinkowskiDistance p, w sum(w * |x - y|^p)^(1/p) “seuclidean” SEuclideanDistance V sqrt(sum((x - y)^2 / V)) “mahalanobis” MahalanobisDistance V or VI sqrt((x - y)' V^-1 (x - y)) Metrics intended for two-dimensional vector spaces:  Note that the haversine\\ndistance metric requires data in the form of [latitude, longitude] and both\\ninputs and outputs are in units of radians. identifier class name distance function “haversine” HaversineDistance 2 arcsin(sqrt(sin^2(0.5*dx) + cos(x1)cos(x2)sin^2(0.5*dy))) Metrics intended for integer-valued vector spaces:  Though intended\\nfor integer-valued vectors, these are also valid metrics in the case of\\nreal-valued vectors. identifier class name distance function “hamming” HammingDistance N_unequal(x, y) / N_tot “canberra” CanberraDistance sum(|x - y| / (|x| + |y|)) “braycurtis” BrayCurtisDistance sum(|x - y|) / (sum(|x|) + sum(|y|)) Metrics intended for boolean-valued vector spaces:  Any nonzero entry\\nis evaluated to “True”.  In the listings below, the following\\nabbreviations are used: N  : number of dimensions NTT : number of dims in which both values are True NTF : number of dims in which the first value is True, second is False NFT : number of dims in which the first value is False, second is True NFF : number of dims in which both values are False NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT identifier class name distance function “jaccard” JaccardDistance NNEQ / NNZ “matching” MatchingDistance NNEQ / N “dice” DiceDistance NNEQ / (NTT + NNZ) “kulsinski” KulsinskiDistance (NNEQ + N - NTT) / (NNEQ + N) “rogerstanimoto” RogersTanimotoDistance 2 * NNEQ / (N + NNEQ) “russellrao” RussellRaoDistance (N - NTT) / N “sokalmichener” SokalMichenerDistance 2 * NNEQ / (N + NNEQ) “sokalsneath” SokalSneathDistance NNEQ / (NNEQ + 0.5 * NTT) User-defined distance: identifier class name args “pyfunc” PyFuncDistance func Here func is a function which takes two one-dimensional numpy\\narrays, and returns a distance.  Note that in order to be used within\\nthe BallTree, the distance must be a true metric:\\ni.e. it must satisfy the following properties Non-negativity: d(x, y) >= 0 Identity: d(x, y) = 0 if and only if x == y Symmetry: d(x, y) = d(y, x) Triangle Inequality: d(x, y) + d(y, z) >= d(x, z) Because of the Python object overhead involved in calling the python\\nfunction, this will be fairly slow, but it will have the same\\nscaling as other distances. Get the given distance metric from the string identifier. See the docstring of DistanceMetric for a list of available metrics.\",\n",
       "       'func_text_user_guide': '#sklearn.metrics.DistanceMetric.get_metric',\n",
       "       'parameter_names_desc': [{'param_name': 'metric',\n",
       "         'param_type': 'str or class name',\n",
       "         'param_desc': 'The string identifier or class name of the desired distance metric.\\nSee the documentation of the DistanceMetric class for a list of\\navailable metrics.\\n'},\n",
       "        {'param_name': 'dtype',\n",
       "         'param_type': '{np.float32, np.float64}, default=np.float64',\n",
       "         'param_desc': 'The data type of the input on which the metric will be applied.\\nThis affects the precision of the computed distances.\\nBy default, it is set to np.float64.\\n'}]},\n",
       "      'function_calling': {'name': 'DistanceMetric',\n",
       "       'descriptions': \"Uniform interface for fast distance metric functions. The DistanceMetric class provides a convenient way to compute pairwise distances\\nbetween samples. It supports various distance metrics, such as Euclidean distance,\\nManhattan distance, and more. The pairwise method can be used to compute pairwise distances between samples in\\nthe input arrays. It returns a distance matrix representing the distances between\\nall pairs of samples. The get_metric method allows you to retrieve a specific metric using its\\nstring identifier. Examples Available Metrics The following lists the string metric identifiers and the associated\\ndistance metric classes: Metrics intended for real-valued vector spaces: identifier class name args distance function “euclidean” EuclideanDistance sqrt(sum((x - y)^2)) “manhattan” ManhattanDistance sum(|x - y|) “chebyshev” ChebyshevDistance max(|x - y|) “minkowski” MinkowskiDistance p, w sum(w * |x - y|^p)^(1/p) “seuclidean” SEuclideanDistance V sqrt(sum((x - y)^2 / V)) “mahalanobis” MahalanobisDistance V or VI sqrt((x - y)' V^-1 (x - y)) Metrics intended for two-dimensional vector spaces:  Note that the haversine\\ndistance metric requires data in the form of [latitude, longitude] and both\\ninputs and outputs are in units of radians. identifier class name distance function “haversine” HaversineDistance 2 arcsin(sqrt(sin^2(0.5*dx) + cos(x1)cos(x2)sin^2(0.5*dy))) Metrics intended for integer-valued vector spaces:  Though intended\\nfor integer-valued vectors, these are also valid metrics in the case of\\nreal-valued vectors. identifier class name distance function “hamming” HammingDistance N_unequal(x, y) / N_tot “canberra” CanberraDistance sum(|x - y| / (|x| + |y|)) “braycurtis” BrayCurtisDistance sum(|x - y|) / (sum(|x|) + sum(|y|)) Metrics intended for boolean-valued vector spaces:  Any nonzero entry\\nis evaluated to “True”.  In the listings below, the following\\nabbreviations are used: N  : number of dimensions NTT : number of dims in which both values are True NTF : number of dims in which the first value is True, second is False NFT : number of dims in which the first value is False, second is True NFF : number of dims in which both values are False NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT identifier class name distance function “jaccard” JaccardDistance NNEQ / NNZ “matching” MatchingDistance NNEQ / N “dice” DiceDistance NNEQ / (NTT + NNZ) “kulsinski” KulsinskiDistance (NNEQ + N - NTT) / (NNEQ + N) “rogerstanimoto” RogersTanimotoDistance 2 * NNEQ / (N + NNEQ) “russellrao” RussellRaoDistance (N - NTT) / N “sokalmichener” SokalMichenerDistance 2 * NNEQ / (N + NNEQ) “sokalsneath” SokalSneathDistance NNEQ / (NNEQ + 0.5 * NTT) User-defined distance: identifier class name args “pyfunc” PyFuncDistance func Here func is a function which takes two one-dimensional numpy\\narrays, and returns a distance.  Note that in order to be used within\\nthe BallTree, the distance must be a true metric:\\ni.e. it must satisfy the following properties Non-negativity: d(x, y) >= 0 Identity: d(x, y) = 0 if and only if x == y Symmetry: d(x, y) = d(y, x) Triangle Inequality: d(x, y) + d(y, z) >= d(x, z) Because of the Python object overhead involved in calling the python\\nfunction, this will be fairly slow, but it will have the same\\nscaling as other distances. Get the given distance metric from the string identifier. See the docstring of DistanceMetric for a list of available metrics.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'metric': {'type': 'string',\n",
       "          'description': 'str or class name. The string identifier or class name of the desired distance metric.\\nSee the documentation of the DistanceMetric class for a list of\\navailable metrics.\\n'},\n",
       "         'dtype': {'type': 'number',\n",
       "          'description': '{np.float32, np.float64}, default=np.float64. The data type of the input on which the metric will be applied.\\nThis affects the precision of the computed distances.\\nBy default, it is set to np.float64.\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Pairwise metrics': [{'func_name': 'pairwise.additive_chi2_kernel',\n",
       "      'func_desc': 'Compute the additive chi-squared kernel between observations in X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.additive_chi2_kernel.html#sklearn.metrics.pairwise.additive_chi2_kernel',\n",
       "      'function_definitions': {'function_name': 'additive_chi2_kernel',\n",
       "       'full_function': 'sklearn.metrics.pairwise.additive_chi2_kernel(X, Y=None)',\n",
       "       'function_text': 'Compute the additive chi-squared kernel between observations in X and Y. The chi-squared kernel is computed between each pair of rows in X and Y.  X\\nand Y have to be non-negative. This kernel is most commonly applied to\\nhistograms. The chi-squared kernel is given by: It can be interpreted as a weighted difference per entry. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#chi2-kernel',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'A feature array.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': 'array-like of shape (n_samples_Y, n_features), default=None',\n",
       "         'param_desc': 'An optional second feature array. If None, uses Y=X.\\n'}]},\n",
       "      'function_calling': {'name': 'additive_chi2_kernel',\n",
       "       'descriptions': 'Compute the additive chi-squared kernel between observations in X and Y. The chi-squared kernel is computed between each pair of rows in X and Y.  X\\nand Y have to be non-negative. This kernel is most commonly applied to\\nhistograms. The chi-squared kernel is given by: It can be interpreted as a weighted difference per entry. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples_X, n_features). A feature array.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples_Y, n_features), default=None. An optional second feature array. If None, uses Y=X.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise.cosine_distances',\n",
       "      'func_desc': 'Compute cosine distance between samples in X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_distances.html#sklearn.metrics.pairwise.cosine_distances',\n",
       "      'function_definitions': {'function_name': 'cosine_distances',\n",
       "       'full_function': 'sklearn.metrics.pairwise.cosine_distances(X, Y=None)',\n",
       "       'function_text': 'Compute cosine distance between samples in X and Y. Cosine distance is defined as 1.0 minus the cosine similarity. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#metrics',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'Matrix X.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None',\n",
       "         'param_desc': 'Matrix Y.\\n'}]},\n",
       "      'function_calling': {'name': 'cosine_distances',\n",
       "       'descriptions': 'Compute cosine distance between samples in X and Y. Cosine distance is defined as 1.0 minus the cosine similarity. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, n_features). Matrix X.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None. Matrix Y.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise.distance_metrics',\n",
       "      'func_desc': 'Valid metrics for pairwise_distances.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.distance_metrics.html#sklearn.metrics.pairwise.distance_metrics',\n",
       "      'function_definitions': {'function_name': 'distance_metrics',\n",
       "       'full_function': 'sklearn.metrics.pairwise.distance_metrics()',\n",
       "       'function_text': 'Valid metrics for pairwise_distances. This function simply returns the valid pairwise distance metrics.\\nIt exists to allow for a description of the mapping for\\neach of the valid strings. The valid distance metrics, and the function they map to, are: metric Function ‘cityblock’ metrics.pairwise.manhattan_distances ‘cosine’ metrics.pairwise.cosine_distances ‘euclidean’ metrics.pairwise.euclidean_distances ‘haversine’ metrics.pairwise.haversine_distances ‘l1’ metrics.pairwise.manhattan_distances ‘l2’ metrics.pairwise.euclidean_distances ‘manhattan’ metrics.pairwise.manhattan_distances ‘nan_euclidean’ metrics.pairwise.nan_euclidean_distances Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#metrics',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'pairwise.haversine_distances',\n",
       "      'func_desc': 'Compute the Haversine distance between samples in X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.haversine_distances.html#sklearn.metrics.pairwise.haversine_distances',\n",
       "      'function_definitions': {'function_name': 'haversine_distances',\n",
       "       'full_function': 'sklearn.metrics.pairwise.haversine_distances(X, Y=None)',\n",
       "       'function_text': 'Compute the Haversine distance between samples in X and Y. The Haversine (or great circle) distance is the angular distance between\\ntwo points on the surface of a sphere. The first coordinate of each point\\nis assumed to be the latitude, the second is the longitude, given\\nin radians. The dimension of the data must be 2.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, 2)',\n",
       "         'param_desc': 'A feature array.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, 2), default=None',\n",
       "         'param_desc': 'An optional second feature array. If None, uses Y=X.\\n'}]},\n",
       "      'function_calling': {'name': 'haversine_distances',\n",
       "       'descriptions': 'Compute the Haversine distance between samples in X and Y. The Haversine (or great circle) distance is the angular distance between\\ntwo points on the surface of a sphere. The first coordinate of each point\\nis assumed to be the latitude, the second is the longitude, given\\nin radians. The dimension of the data must be 2.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, 2). A feature array.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, 2), default=None. An optional second feature array. If None, uses Y=X.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise.laplacian_kernel',\n",
       "      'func_desc': 'Compute the laplacian kernel between X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.laplacian_kernel.html#sklearn.metrics.pairwise.laplacian_kernel',\n",
       "      'function_definitions': {'function_name': 'laplacian_kernel',\n",
       "       'full_function': 'sklearn.metrics.pairwise.laplacian_kernel(X, Y=None, gamma=None)',\n",
       "       'function_text': 'Compute the laplacian kernel between X and Y. The laplacian kernel is defined as: for each pair of rows x in X and y in Y.\\nRead more in the User Guide. Added in version 0.17.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#laplacian-kernel',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'A feature array.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None',\n",
       "         'param_desc': 'An optional second feature array. If None, uses Y=X.\\n'},\n",
       "        {'param_name': 'gamma',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': 'If None, defaults to 1.0 / n_features. Otherwise it should be strictly positive.\\n'}]},\n",
       "      'function_calling': {'name': 'laplacian_kernel',\n",
       "       'descriptions': 'Compute the laplacian kernel between X and Y. The laplacian kernel is defined as: for each pair of rows x in X and y in Y.\\nRead more in the User Guide. Added in version 0.17.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, n_features). A feature array.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None. An optional second feature array. If None, uses Y=X.\\n'},\n",
       "         'gamma': {'type': 'number',\n",
       "          'description': 'float, default=None. If None, defaults to 1.0 / n_features. Otherwise it should be strictly positive.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise.manhattan_distances',\n",
       "      'func_desc': 'Compute the L1 distances between the vectors in X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.manhattan_distances.html#sklearn.metrics.pairwise.manhattan_distances',\n",
       "      'function_definitions': {'function_name': 'manhattan_distances',\n",
       "       'full_function': 'sklearn.metrics.pairwise.manhattan_distances(X, Y=None)',\n",
       "       'function_text': 'Compute the L1 distances between the vectors in X and Y. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#metrics',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'An array where each row is a sample and each column is a feature.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None',\n",
       "         'param_desc': 'An array where each row is a sample and each column is a feature.\\nIf None, method uses Y=X.\\n'}]},\n",
       "      'function_calling': {'name': 'manhattan_distances',\n",
       "       'descriptions': 'Compute the L1 distances between the vectors in X and Y. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, n_features). An array where each row is a sample and each column is a feature.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None. An array where each row is a sample and each column is a feature.\\nIf None, method uses Y=X.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise.paired_cosine_distances',\n",
       "      'func_desc': 'Compute the paired cosine distances between X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.paired_cosine_distances.html#sklearn.metrics.pairwise.paired_cosine_distances',\n",
       "      'function_definitions': {'function_name': 'paired_cosine_distances',\n",
       "       'full_function': 'sklearn.metrics.pairwise.paired_cosine_distances(X, Y)',\n",
       "       'function_text': 'Compute the paired cosine distances between X and Y. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#metrics',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'An array where each row is a sample and each column is a feature.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'An array where each row is a sample and each column is a feature.\\n'}]},\n",
       "      'function_calling': {'name': 'paired_cosine_distances',\n",
       "       'descriptions': 'Compute the paired cosine distances between X and Y. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). An array where each row is a sample and each column is a feature.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). An array where each row is a sample and each column is a feature.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise.paired_euclidean_distances',\n",
       "      'func_desc': 'Compute the paired euclidean distances between X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.paired_euclidean_distances.html#sklearn.metrics.pairwise.paired_euclidean_distances',\n",
       "      'function_definitions': {'function_name': 'paired_euclidean_distances',\n",
       "       'full_function': 'sklearn.metrics.pairwise.paired_euclidean_distances(X, Y)',\n",
       "       'function_text': 'Compute the paired euclidean distances between X and Y. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#metrics',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Input array/matrix X.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Input array/matrix Y.\\n'}]},\n",
       "      'function_calling': {'name': 'paired_euclidean_distances',\n",
       "       'descriptions': 'Compute the paired euclidean distances between X and Y. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Input array/matrix X.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Input array/matrix Y.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise.pairwise_kernels',\n",
       "      'func_desc': 'Compute the kernel between arrays X and optional array Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_kernels.html#sklearn.metrics.pairwise.pairwise_kernels',\n",
       "      'function_definitions': {'function_name': 'pairwise_kernels',\n",
       "       'full_function': \"sklearn.metrics.pairwise.pairwise_kernels(X, Y=None, metric='linear', *, filter_params=False, n_jobs=None, **kwds)\",\n",
       "       'function_text': 'Compute the kernel between arrays X and optional array Y. This method takes either a vector array or a kernel matrix, and returns\\na kernel matrix. If the input is a vector array, the kernels are\\ncomputed. If the input is a kernel matrix, it is returned instead. This method provides a safe way to take a kernel matrix as input, while\\npreserving compatibility with many other algorithms that take a vector\\narray. If Y is given (default is None), then the returned matrix is the pairwise\\nkernel between the arrays from both X and Y. [‘additive_chi2’, ‘chi2’, ‘linear’, ‘poly’, ‘polynomial’, ‘rbf’,\\n‘laplacian’, ‘sigmoid’, ‘cosine’] Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#metrics',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix}  of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)',\n",
       "         'param_desc': 'Array of pairwise kernels between samples, or a feature array.\\nThe shape of the array should be (n_samples_X, n_samples_X) if\\nmetric == “precomputed” and (n_samples_X, n_features) otherwise.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None',\n",
       "         'param_desc': 'A second feature array only if X has shape (n_samples_X, n_features).\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'str or callable, default=”linear”',\n",
       "         'param_desc': 'The metric to use when calculating kernel between instances in a\\nfeature array. If metric is a string, it must be one of the metrics\\nin pairwise.PAIRWISE_KERNEL_FUNCTIONS.\\nIf metric is “precomputed”, X is assumed to be a kernel matrix.\\nAlternatively, if metric is a callable function, it is called on each\\npair of instances (rows) and the resulting value recorded. The callable\\nshould take two rows from X as input and return the corresponding\\nkernel value as a single number. This means that callables from\\nsklearn.metrics.pairwise are not allowed, as they operate on\\nmatrices, not single samples. Use the string identifying the kernel\\ninstead.\\n'},\n",
       "        {'param_name': 'filter_params',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to filter invalid parameters or not.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of jobs to use for the computation. This works by breaking\\ndown the pairwise matrix into n_jobs even slices and computing them in\\nparallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': '**kwds',\n",
       "         'param_type': 'optional keyword parameters',\n",
       "         'param_desc': 'Any further parameters are passed directly to the kernel function.\\n'}]},\n",
       "      'function_calling': {'name': 'pairwise_kernels',\n",
       "       'descriptions': 'Compute the kernel between arrays X and optional array Y. This method takes either a vector array or a kernel matrix, and returns\\na kernel matrix. If the input is a vector array, the kernels are\\ncomputed. If the input is a kernel matrix, it is returned instead. This method provides a safe way to take a kernel matrix as input, while\\npreserving compatibility with many other algorithms that take a vector\\narray. If Y is given (default is None), then the returned matrix is the pairwise\\nkernel between the arrays from both X and Y. [‘additive_chi2’, ‘chi2’, ‘linear’, ‘poly’, ‘polynomial’, ‘rbf’,\\n‘laplacian’, ‘sigmoid’, ‘cosine’] Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix}  of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features). Array of pairwise kernels between samples, or a feature array.\\nThe shape of the array should be (n_samples_X, n_samples_X) if\\nmetric == “precomputed” and (n_samples_X, n_features) otherwise.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None. A second feature array only if X has shape (n_samples_X, n_features).\\n'},\n",
       "         'metric': {'type': 'string',\n",
       "          'description': 'str or callable, default=”linear”. The metric to use when calculating kernel between instances in a\\nfeature array. If metric is a string, it must be one of the metrics\\nin pairwise.PAIRWISE_KERNEL_FUNCTIONS.\\nIf metric is “precomputed”, X is assumed to be a kernel matrix.\\nAlternatively, if metric is a callable function, it is called on each\\npair of instances (rows) and the resulting value recorded. The callable\\nshould take two rows from X as input and return the corresponding\\nkernel value as a single number. This means that callables from\\nsklearn.metrics.pairwise are not allowed, as they operate on\\nmatrices, not single samples. Use the string identifying the kernel\\ninstead.\\n'},\n",
       "         'filter_params': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to filter invalid parameters or not.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of jobs to use for the computation. This works by breaking\\ndown the pairwise matrix into n_jobs even slices and computing them in\\nparallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'}},\n",
       "        'required': ['X', 'Y=None', \"metric='linear'\"]}}},\n",
       "     {'func_name': 'pairwise.rbf_kernel',\n",
       "      'func_desc': 'Compute the rbf (gaussian) kernel between X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.rbf_kernel.html#sklearn.metrics.pairwise.rbf_kernel',\n",
       "      'function_definitions': {'function_name': 'rbf_kernel',\n",
       "       'full_function': 'sklearn.metrics.pairwise.rbf_kernel(X, Y=None, gamma=None)',\n",
       "       'function_text': 'Compute the rbf (gaussian) kernel between X and Y. K(x, y) = exp(-gamma ||x-y||^2) for each pair of rows x in X and y in Y. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#rbf-kernel',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'A feature array.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None',\n",
       "         'param_desc': 'An optional second feature array. If None, uses Y=X.\\n'},\n",
       "        {'param_name': 'gamma',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': 'If None, defaults to 1.0 / n_features.\\n'}]},\n",
       "      'function_calling': {'name': 'rbf_kernel',\n",
       "       'descriptions': 'Compute the rbf (gaussian) kernel between X and Y. K(x, y) = exp(-gamma ||x-y||^2) for each pair of rows x in X and y in Y. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, n_features). A feature array.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None. An optional second feature array. If None, uses Y=X.\\n'},\n",
       "         'gamma': {'type': 'number',\n",
       "          'description': 'float, default=None. If None, defaults to 1.0 / n_features.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise_distances',\n",
       "      'func_desc': 'Compute the distance matrix from a vector array X and optional Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances',\n",
       "      'function_definitions': {'function_name': 'pairwise_distances',\n",
       "       'full_function': \"sklearn.metrics.pairwise_distances(X, Y=None, metric='euclidean', *, n_jobs=None, force_all_finite=True, **kwds)\",\n",
       "       'function_text': \"Compute the distance matrix from a vector array X and optional Y. This method takes either a vector array or a distance matrix, and returns\\na distance matrix.\\nIf the input is a vector array, the distances are computed.\\nIf the input is a distances matrix, it is returned instead.\\nIf the input is a collection of non-numeric data (e.g. a list of strings or a\\nboolean array), a custom metric must be passed. This method provides a safe way to take a distance matrix as input, while\\npreserving compatibility with many other algorithms that take a vector\\narray. If Y is given (default is None), then the returned matrix is the pairwise\\ndistance between the arrays from both X and Y. Valid values for metric are: From scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,\\n‘manhattan’]. These metrics support sparse matrix\\ninputs.\\n[‘nan_euclidean’] but it does not yet support sparse matrices. From scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,\\n‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’,\\n‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’,\\n‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’]\\nSee the documentation for scipy.spatial.distance for details on these\\nmetrics. These metrics do not support sparse matrix inputs. Note 'kulsinski' is deprecated from SciPy 1.9 and will be removed in SciPy 1.11. Note 'matching' has been removed in SciPy 1.9 (use 'hamming' instead). Note that in the case of ‘cityblock’, ‘cosine’ and ‘euclidean’ (which are\\nvalid scipy.spatial.distance metrics), the scikit-learn implementation\\nwill be used, which is faster and has support for sparse matrices (except\\nfor ‘cityblock’). For a verbose description of the metrics from\\nscikit-learn, see sklearn.metrics.pairwise.distance_metrics\\nfunction. Read more in the User Guide.\",\n",
       "       'func_text_user_guide': 'sklearn.metrics.pairwise.distance_metrics.html#sklearn.metrics.pairwise.distance_metrics',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)',\n",
       "         'param_desc': 'Array of pairwise distances between samples, or a feature array.\\nThe shape of the array should be (n_samples_X, n_samples_X) if\\nmetric == “precomputed” and (n_samples_X, n_features) otherwise.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None',\n",
       "         'param_desc': 'An optional second feature array. Only allowed if\\nmetric != “precomputed”.\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'str or callable, default=’euclidean’',\n",
       "         'param_desc': 'The metric to use when calculating distance between instances in a\\nfeature array. If metric is a string, it must be one of the options\\nallowed by scipy.spatial.distance.pdist for its metric parameter, or\\na metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\\nIf metric is “precomputed”, X is assumed to be a distance matrix.\\nAlternatively, if metric is a callable function, it is called on each\\npair of instances (rows) and the resulting value recorded. The callable\\nshould take two arrays from X as input and return a value indicating\\nthe distance between them.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of jobs to use for the computation. This works by breaking\\ndown the pairwise matrix into n_jobs even slices and computing them in\\nparallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\nThe “euclidean” and “cosine” metrics rely heavily on BLAS which is already\\nmultithreaded. So, increasing n_jobs would likely cause oversubscription\\nand quickly degrade performance.\\n'},\n",
       "        {'param_name': 'force_all_finite',\n",
       "         'param_type': 'bool or ‘allow-nan’, default=True',\n",
       "         'param_desc': \"Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\\nfor a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. The\\npossibilities are:\\n\\nTrue: Force all values of array to be finite.\\nFalse: accepts np.inf, np.nan, pd.NA in array.\\n‘allow-nan’: accepts only np.nan and pd.NA values in array. Values\\ncannot be infinite.\\n\\n\\nAdded in version 0.22: force_all_finite accepts the string 'allow-nan'.\\n\\n\\nChanged in version 0.23: Accepts pd.NA and converts it into np.nan.\\n\\n\"},\n",
       "        {'param_name': '**kwds',\n",
       "         'param_type': 'optional keyword parameters',\n",
       "         'param_desc': 'Any further parameters are passed directly to the distance function.\\nIf using a scipy.spatial.distance metric, the parameters are still\\nmetric dependent. See the scipy docs for usage examples.\\n'}]},\n",
       "      'function_calling': {'name': 'pairwise_distances',\n",
       "       'descriptions': \"Compute the distance matrix from a vector array X and optional Y. This method takes either a vector array or a distance matrix, and returns\\na distance matrix.\\nIf the input is a vector array, the distances are computed.\\nIf the input is a distances matrix, it is returned instead.\\nIf the input is a collection of non-numeric data (e.g. a list of strings or a\\nboolean array), a custom metric must be passed. This method provides a safe way to take a distance matrix as input, while\\npreserving compatibility with many other algorithms that take a vector\\narray. If Y is given (default is None), then the returned matrix is the pairwise\\ndistance between the arrays from both X and Y. Valid values for metric are: From scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,\\n‘manhattan’]. These metrics support sparse matrix\\ninputs.\\n[‘nan_euclidean’] but it does not yet support sparse matrices. From scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,\\n‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’,\\n‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’,\\n‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’]\\nSee the documentation for scipy.spatial.distance for details on these\\nmetrics. These metrics do not support sparse matrix inputs. Note 'kulsinski' is deprecated from SciPy 1.9 and will be removed in SciPy 1.11. Note 'matching' has been removed in SciPy 1.9 (use 'hamming' instead). Note that in the case of ‘cityblock’, ‘cosine’ and ‘euclidean’ (which are\\nvalid scipy.spatial.distance metrics), the scikit-learn implementation\\nwill be used, which is faster and has support for sparse matrices (except\\nfor ‘cityblock’). For a verbose description of the metrics from\\nscikit-learn, see sklearn.metrics.pairwise.distance_metrics\\nfunction. Read more in the User Guide.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features). Array of pairwise distances between samples, or a feature array.\\nThe shape of the array should be (n_samples_X, n_samples_X) if\\nmetric == “precomputed” and (n_samples_X, n_features) otherwise.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None. An optional second feature array. Only allowed if\\nmetric != “precomputed”.\\n'},\n",
       "         'metric': {'type': 'string',\n",
       "          'description': 'str or callable, default=’euclidean’. The metric to use when calculating distance between instances in a\\nfeature array. If metric is a string, it must be one of the options\\nallowed by scipy.spatial.distance.pdist for its metric parameter, or\\na metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\\nIf metric is “precomputed”, X is assumed to be a distance matrix.\\nAlternatively, if metric is a callable function, it is called on each\\npair of instances (rows) and the resulting value recorded. The callable\\nshould take two arrays from X as input and return a value indicating\\nthe distance between them.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of jobs to use for the computation. This works by breaking\\ndown the pairwise matrix into n_jobs even slices and computing them in\\nparallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\nThe “euclidean” and “cosine” metrics rely heavily on BLAS which is already\\nmultithreaded. So, increasing n_jobs would likely cause oversubscription\\nand quickly degrade performance.\\n'},\n",
       "         'force_all_finite': {'type': 'boolean',\n",
       "          'description': \"bool or ‘allow-nan’, default=True. Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\\nfor a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. The\\npossibilities are:\\n\\nTrue: Force all values of array to be finite.\\nFalse: accepts np.inf, np.nan, pd.NA in array.\\n‘allow-nan’: accepts only np.nan and pd.NA values in array. Values\\ncannot be infinite.\\n\\n\\nAdded in version 0.22: force_all_finite accepts the string 'allow-nan'.\\n\\n\\nChanged in version 0.23: Accepts pd.NA and converts it into np.nan.\\n\\n\"}},\n",
       "        'required': ['X', 'Y=None', \"metric='euclidean'\"]}}},\n",
       "     {'func_name': 'pairwise_distances_argmin_min',\n",
       "      'func_desc': 'Compute minimum distances between one point and a set of points.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_argmin_min.html#sklearn.metrics.pairwise_distances_argmin_min',\n",
       "      'function_definitions': {'function_name': 'pairwise_distances_argmin_min',\n",
       "       'full_function': \"sklearn.metrics.pairwise_distances_argmin_min(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\",\n",
       "       'function_text': 'Compute minimum distances between one point and a set of points. This function computes for each row in X, the index of the row of Y which\\nis closest (according to the specified distance). The minimal distances are\\nalso returned. This is mostly equivalent to calling: pairwise_distances(X, Y=Y, metric=metric).min(axis=axis)) but uses much less memory, and is faster for large arrays.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'Array containing points.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features)',\n",
       "         'param_desc': 'Array containing points.\\n'},\n",
       "        {'param_name': 'axis',\n",
       "         'param_type': 'int, default=1',\n",
       "         'param_desc': 'Axis along which the argmin and distances are to be computed.\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'str or callable, default=’euclidean’',\n",
       "         'param_desc': \"Metric to use for distance computation. Any metric from scikit-learn\\nor scipy.spatial.distance can be used.\\nIf metric is a callable function, it is called on each\\npair of instances (rows) and the resulting value recorded. The callable\\nshould take two arrays as input and return one value indicating the\\ndistance between them. This works for Scipy’s metrics, but is less\\nefficient than passing the metric name as a string.\\nDistance matrices are not supported.\\nValid values for metric are:\\n\\nfrom scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,\\n‘manhattan’]\\nfrom scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,\\n‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,\\n‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,\\n‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,\\n‘yule’]\\n\\nSee the documentation for scipy.spatial.distance for details on these\\nmetrics.\\n\\nNote\\n'kulsinski' is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\\n\\n\\nNote\\n'matching' has been removed in SciPy 1.9 (use 'hamming' instead).\\n\\n\"},\n",
       "        {'param_name': 'metric_kwargs',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Keyword arguments to pass to specified metric function.\\n'}]},\n",
       "      'function_calling': {'name': 'pairwise_distances_argmin_min',\n",
       "       'descriptions': 'Compute minimum distances between one point and a set of points. This function computes for each row in X, the index of the row of Y which\\nis closest (according to the specified distance). The minimal distances are\\nalso returned. This is mostly equivalent to calling: pairwise_distances(X, Y=Y, metric=metric).min(axis=axis)) but uses much less memory, and is faster for large arrays.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, n_features). Array containing points.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features). Array containing points.\\n'},\n",
       "         'axis': {'type': 'integer',\n",
       "          'description': 'int, default=1. Axis along which the argmin and distances are to be computed.\\n'},\n",
       "         'metric': {'type': 'string',\n",
       "          'description': \"str or callable, default=’euclidean’. Metric to use for distance computation. Any metric from scikit-learn\\nor scipy.spatial.distance can be used.\\nIf metric is a callable function, it is called on each\\npair of instances (rows) and the resulting value recorded. The callable\\nshould take two arrays as input and return one value indicating the\\ndistance between them. This works for Scipy’s metrics, but is less\\nefficient than passing the metric name as a string.\\nDistance matrices are not supported.\\nValid values for metric are:\\n\\nfrom scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,\\n‘manhattan’]\\nfrom scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,\\n‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,\\n‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,\\n‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,\\n‘yule’]\\n\\nSee the documentation for scipy.spatial.distance for details on these\\nmetrics.\\n\\nNote\\n'kulsinski' is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\\n\\n\\nNote\\n'matching' has been removed in SciPy 1.9 (use 'hamming' instead).\\n\\n\"},\n",
       "         'metric_kwargs': {'type': 'array',\n",
       "          'description': 'dict, default=None. Keyword arguments to pass to specified metric function.\\n'}},\n",
       "        'required': ['X', 'Y']}}},\n",
       "     {'func_name': 'pairwise.chi2_kernel',\n",
       "      'func_desc': 'Compute the exponential chi-squared kernel between X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.chi2_kernel.html#sklearn.metrics.pairwise.chi2_kernel',\n",
       "      'function_definitions': {'function_name': 'chi2_kernel',\n",
       "       'full_function': 'sklearn.metrics.pairwise.chi2_kernel(X, Y=None, gamma=1.0)',\n",
       "       'function_text': 'Compute the exponential chi-squared kernel between X and Y. The chi-squared kernel is computed between each pair of rows in X and Y.  X\\nand Y have to be non-negative. This kernel is most commonly applied to\\nhistograms. The chi-squared kernel is given by: It can be interpreted as a weighted difference per entry. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#chi2-kernel',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'A feature array.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': 'array-like of shape (n_samples_Y, n_features), default=None',\n",
       "         'param_desc': 'An optional second feature array. If None, uses Y=X.\\n'},\n",
       "        {'param_name': 'gamma',\n",
       "         'param_type': 'float, default=1',\n",
       "         'param_desc': 'Scaling parameter of the chi2 kernel.\\n'}]},\n",
       "      'function_calling': {'name': 'chi2_kernel',\n",
       "       'descriptions': 'Compute the exponential chi-squared kernel between X and Y. The chi-squared kernel is computed between each pair of rows in X and Y.  X\\nand Y have to be non-negative. This kernel is most commonly applied to\\nhistograms. The chi-squared kernel is given by: It can be interpreted as a weighted difference per entry. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples_X, n_features). A feature array.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples_Y, n_features), default=None. An optional second feature array. If None, uses Y=X.\\n'},\n",
       "         'gamma': {'type': 'number',\n",
       "          'description': 'float, default=1. Scaling parameter of the chi2 kernel.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise.cosine_similarity',\n",
       "      'func_desc': 'Compute cosine similarity between samples in X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity',\n",
       "      'function_definitions': {'function_name': 'cosine_similarity',\n",
       "       'full_function': 'sklearn.metrics.pairwise.cosine_similarity(X, Y=None, dense_output=True)',\n",
       "       'function_text': 'Compute cosine similarity between samples in X and Y. Cosine similarity, or the cosine kernel, computes similarity as the\\nnormalized dot product of X and Y: K(X, Y) = <X, Y> / (||X||*||Y||) On L2-normalized data, this function is equivalent to linear_kernel. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'Input data.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None',\n",
       "         'param_desc': 'Input data. If None, the output will be the pairwise\\nsimilarities between all samples in X.\\n'},\n",
       "        {'param_name': 'dense_output',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to return dense output even when the input is sparse. If\\nFalse, the output is sparse if both input arrays are sparse.\\n\\nAdded in version 0.17: parameter dense_output for dense output.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'cosine_similarity',\n",
       "       'descriptions': 'Compute cosine similarity between samples in X and Y. Cosine similarity, or the cosine kernel, computes similarity as the\\nnormalized dot product of X and Y: K(X, Y) = <X, Y> / (||X||*||Y||) On L2-normalized data, this function is equivalent to linear_kernel. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, n_features). Input data.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None. Input data. If None, the output will be the pairwise\\nsimilarities between all samples in X.\\n'},\n",
       "         'dense_output': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to return dense output even when the input is sparse. If\\nFalse, the output is sparse if both input arrays are sparse.\\n\\nAdded in version 0.17: parameter dense_output for dense output.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise.euclidean_distances',\n",
       "      'func_desc': 'Compute the distance matrix between each pair from a vector array X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html#sklearn.metrics.pairwise.euclidean_distances',\n",
       "      'function_definitions': {'function_name': 'euclidean_distances',\n",
       "       'full_function': 'sklearn.metrics.pairwise.euclidean_distances(X, Y=None, *, Y_norm_squared=None, squared=False, X_norm_squared=None)',\n",
       "       'function_text': 'Compute the distance matrix between each pair from a vector array X and Y. For efficiency reasons, the euclidean distance between a pair of row\\nvector x and y is computed as: This formulation has two advantages over other ways of computing distances.\\nFirst, it is computationally efficient when dealing with sparse data.\\nSecond, if one argument varies but the other remains unchanged, then\\ndot(x, x) and/or dot(y, y) can be pre-computed. However, this is not the most precise way of doing this computation,\\nbecause this equation potentially suffers from “catastrophic cancellation”.\\nAlso, the distance matrix returned by this function may not be exactly\\nsymmetric as required by, e.g., scipy.spatial.distance functions. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#metrics',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'An array where each row is a sample and each column is a feature.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None',\n",
       "         'param_desc': 'An array where each row is a sample and each column is a feature.\\nIf None, method uses Y=X.\\n'},\n",
       "        {'param_name': 'Y_norm_squared',\n",
       "         'param_type': 'array-like of shape (n_samples_Y,) or (n_samples_Y, 1)             or (1, n_samples_Y), default=None',\n",
       "         'param_desc': 'Pre-computed dot-products of vectors in Y (e.g.,\\n(Y**2).sum(axis=1))\\nMay be ignored in some cases, see the note below.\\n'},\n",
       "        {'param_name': 'squared',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Return squared Euclidean distances.\\n'},\n",
       "        {'param_name': 'X_norm_squared',\n",
       "         'param_type': 'array-like of shape (n_samples_X,) or (n_samples_X, 1)             or (1, n_samples_X), default=None',\n",
       "         'param_desc': 'Pre-computed dot-products of vectors in X (e.g.,\\n(X**2).sum(axis=1))\\nMay be ignored in some cases, see the note below.\\n'}]},\n",
       "      'function_calling': {'name': 'euclidean_distances',\n",
       "       'descriptions': 'Compute the distance matrix between each pair from a vector array X and Y. For efficiency reasons, the euclidean distance between a pair of row\\nvector x and y is computed as: This formulation has two advantages over other ways of computing distances.\\nFirst, it is computationally efficient when dealing with sparse data.\\nSecond, if one argument varies but the other remains unchanged, then\\ndot(x, x) and/or dot(y, y) can be pre-computed. However, this is not the most precise way of doing this computation,\\nbecause this equation potentially suffers from “catastrophic cancellation”.\\nAlso, the distance matrix returned by this function may not be exactly\\nsymmetric as required by, e.g., scipy.spatial.distance functions. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, n_features). An array where each row is a sample and each column is a feature.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None. An array where each row is a sample and each column is a feature.\\nIf None, method uses Y=X.\\n'},\n",
       "         'Y_norm_squared': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples_Y,) or (n_samples_Y, 1)             or (1, n_samples_Y), default=None. Pre-computed dot-products of vectors in Y (e.g.,\\n(Y**2).sum(axis=1))\\nMay be ignored in some cases, see the note below.\\n'},\n",
       "         'squared': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Return squared Euclidean distances.\\n'},\n",
       "         'X_norm_squared': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples_X,) or (n_samples_X, 1)             or (1, n_samples_X), default=None. Pre-computed dot-products of vectors in X (e.g.,\\n(X**2).sum(axis=1))\\nMay be ignored in some cases, see the note below.\\n'}},\n",
       "        'required': ['X', 'Y=None']}}},\n",
       "     {'func_name': 'pairwise.kernel_metrics',\n",
       "      'func_desc': 'Valid metrics for pairwise_kernels.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.kernel_metrics.html#sklearn.metrics.pairwise.kernel_metrics',\n",
       "      'function_definitions': {'function_name': 'kernel_metrics',\n",
       "       'full_function': 'sklearn.metrics.pairwise.kernel_metrics()',\n",
       "       'function_text': 'Valid metrics for pairwise_kernels. This function simply returns the valid pairwise distance metrics.\\nIt exists, however, to allow for a verbose description of the mapping for\\neach of the valid strings. metric Function ‘additive_chi2’ sklearn.pairwise.additive_chi2_kernel ‘chi2’ sklearn.pairwise.chi2_kernel ‘linear’ sklearn.pairwise.linear_kernel ‘poly’ sklearn.pairwise.polynomial_kernel ‘polynomial’ sklearn.pairwise.polynomial_kernel ‘rbf’ sklearn.pairwise.rbf_kernel ‘laplacian’ sklearn.pairwise.laplacian_kernel ‘sigmoid’ sklearn.pairwise.sigmoid_kernel ‘cosine’ sklearn.pairwise.cosine_similarity Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#metrics',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'pairwise.linear_kernel',\n",
       "      'func_desc': 'Compute the linear kernel between X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel',\n",
       "      'function_definitions': {'function_name': 'linear_kernel',\n",
       "       'full_function': 'sklearn.metrics.pairwise.linear_kernel(X, Y=None, dense_output=True)',\n",
       "       'function_text': 'Compute the linear kernel between X and Y. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#linear-kernel',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'A feature array.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None',\n",
       "         'param_desc': 'An optional second feature array. If None, uses Y=X.\\n'},\n",
       "        {'param_name': 'dense_output',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to return dense output even when the input is sparse. If\\nFalse, the output is sparse if both input arrays are sparse.\\n\\nAdded in version 0.20.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'linear_kernel',\n",
       "       'descriptions': 'Compute the linear kernel between X and Y. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, n_features). A feature array.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None. An optional second feature array. If None, uses Y=X.\\n'},\n",
       "         'dense_output': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to return dense output even when the input is sparse. If\\nFalse, the output is sparse if both input arrays are sparse.\\n\\nAdded in version 0.20.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise.nan_euclidean_distances',\n",
       "      'func_desc': 'Calculate the euclidean distances in the presence of missing values.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.nan_euclidean_distances.html#sklearn.metrics.pairwise.nan_euclidean_distances',\n",
       "      'function_definitions': {'function_name': 'nan_euclidean_distances',\n",
       "       'full_function': 'sklearn.metrics.pairwise.nan_euclidean_distances(X, Y=None, *, squared=False, missing_values=nan, copy=True)',\n",
       "       'function_text': 'Calculate the euclidean distances in the presence of missing values. Compute the euclidean distance between each pair of samples in X and Y,\\nwhere Y=X is assumed if Y=None. When calculating the distance between a\\npair of samples, this formulation ignores feature coordinates with a\\nmissing value in either sample and scales up the weight of the remaining\\ncoordinates: dist(x,y) = sqrt(weight * sq. distance from present coordinates)\\nwhere,\\nweight = Total # of coordinates / # of present coordinates For example, the distance between [3, na, na, 6] and [1, na, 4, 5]\\nis: If all the coordinates are missing or if there are no common present\\ncoordinates then NaN is returned for that pair. Read more in the User Guide. Added in version 0.22.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#metrics',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'An array where each row is a sample and each column is a feature.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': 'array-like of shape (n_samples_Y, n_features), default=None',\n",
       "         'param_desc': 'An array where each row is a sample and each column is a feature.\\nIf None, method uses Y=X.\\n'},\n",
       "        {'param_name': 'squared',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Return squared Euclidean distances.\\n'},\n",
       "        {'param_name': 'missing_values',\n",
       "         'param_type': 'np.nan, float or int, default=np.nan',\n",
       "         'param_desc': 'Representation of missing value.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Make and use a deep copy of X and Y (if Y exists).\\n'}]},\n",
       "      'function_calling': {'name': 'nan_euclidean_distances',\n",
       "       'descriptions': 'Calculate the euclidean distances in the presence of missing values. Compute the euclidean distance between each pair of samples in X and Y,\\nwhere Y=X is assumed if Y=None. When calculating the distance between a\\npair of samples, this formulation ignores feature coordinates with a\\nmissing value in either sample and scales up the weight of the remaining\\ncoordinates: dist(x,y) = sqrt(weight * sq. distance from present coordinates)\\nwhere,\\nweight = Total # of coordinates / # of present coordinates For example, the distance between [3, na, na, 6] and [1, na, 4, 5]\\nis: If all the coordinates are missing or if there are no common present\\ncoordinates then NaN is returned for that pair. Read more in the User Guide. Added in version 0.22.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples_X, n_features). An array where each row is a sample and each column is a feature.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples_Y, n_features), default=None. An array where each row is a sample and each column is a feature.\\nIf None, method uses Y=X.\\n'},\n",
       "         'squared': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Return squared Euclidean distances.\\n'},\n",
       "         'missing_values': {'type': 'integer',\n",
       "          'description': 'np.nan, float or int, default=np.nan. Representation of missing value.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Make and use a deep copy of X and Y (if Y exists).\\n'}},\n",
       "        'required': ['X', 'Y=None']}}},\n",
       "     {'func_name': 'pairwise.paired_distances',\n",
       "      'func_desc': 'Compute the paired distances between X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.paired_distances.html#sklearn.metrics.pairwise.paired_distances',\n",
       "      'function_definitions': {'function_name': 'paired_distances',\n",
       "       'full_function': \"sklearn.metrics.pairwise.paired_distances(X, Y, *, metric='euclidean', **kwds)\",\n",
       "       'function_text': 'Compute the paired distances between X and Y. Compute the distances between (X[0], Y[0]), (X[1], Y[1]), etc… Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#metrics',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'ndarray of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Array 1 for distance computation.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': 'ndarray of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Array 2 for distance computation.\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'str or callable, default=”euclidean”',\n",
       "         'param_desc': 'The metric to use when calculating distance between instances in a\\nfeature array. If metric is a string, it must be one of the options\\nspecified in PAIRED_DISTANCES, including “euclidean”,\\n“manhattan”, or “cosine”.\\nAlternatively, if metric is a callable function, it is called on each\\npair of instances (rows) and the resulting value recorded. The callable\\nshould take two arrays from X as input and return a value indicating\\nthe distance between them.\\n'},\n",
       "        {'param_name': '**kwds',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Unused parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'paired_distances',\n",
       "       'descriptions': 'Compute the paired distances between X and Y. Compute the distances between (X[0], Y[0]), (X[1], Y[1]), etc… Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples, n_features). Array 1 for distance computation.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples, n_features). Array 2 for distance computation.\\n'},\n",
       "         'metric': {'type': 'string',\n",
       "          'description': 'str or callable, default=”euclidean”. The metric to use when calculating distance between instances in a\\nfeature array. If metric is a string, it must be one of the options\\nspecified in PAIRED_DISTANCES, including “euclidean”,\\n“manhattan”, or “cosine”.\\nAlternatively, if metric is a callable function, it is called on each\\npair of instances (rows) and the resulting value recorded. The callable\\nshould take two arrays from X as input and return a value indicating\\nthe distance between them.\\n'}},\n",
       "        'required': ['X', 'Y']}}},\n",
       "     {'func_name': 'pairwise.paired_manhattan_distances',\n",
       "      'func_desc': 'Compute the paired L1 distances between X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.paired_manhattan_distances.html#sklearn.metrics.pairwise.paired_manhattan_distances',\n",
       "      'function_definitions': {'function_name': 'paired_manhattan_distances',\n",
       "       'full_function': 'sklearn.metrics.pairwise.paired_manhattan_distances(X, Y)',\n",
       "       'function_text': 'Compute the paired L1 distances between X and Y. Distances are calculated between (X[0], Y[0]), (X[1], Y[1]), …,\\n(X[n_samples], Y[n_samples]). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#metrics',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'An array-like where each row is a sample and each column is a feature.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'An array-like where each row is a sample and each column is a feature.\\n'}]},\n",
       "      'function_calling': {'name': 'paired_manhattan_distances',\n",
       "       'descriptions': 'Compute the paired L1 distances between X and Y. Distances are calculated between (X[0], Y[0]), (X[1], Y[1]), …,\\n(X[n_samples], Y[n_samples]). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). An array-like where each row is a sample and each column is a feature.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). An array-like where each row is a sample and each column is a feature.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise.polynomial_kernel',\n",
       "      'func_desc': 'Compute the polynomial kernel between X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.polynomial_kernel.html#sklearn.metrics.pairwise.polynomial_kernel',\n",
       "      'function_definitions': {'function_name': 'polynomial_kernel',\n",
       "       'full_function': 'sklearn.metrics.pairwise.polynomial_kernel(X, Y=None, degree=3, gamma=None, coef0=1)',\n",
       "       'function_text': 'Compute the polynomial kernel between X and Y. K(X, Y) = (gamma <X, Y> + coef0) ^ degree Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#polynomial-kernel',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'A feature array.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None',\n",
       "         'param_desc': 'An optional second feature array. If None, uses Y=X.\\n'},\n",
       "        {'param_name': 'degree',\n",
       "         'param_type': 'float, default=3',\n",
       "         'param_desc': 'Kernel degree.\\n'},\n",
       "        {'param_name': 'gamma',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': 'Coefficient of the vector inner product. If None, defaults to 1.0 / n_features.\\n'},\n",
       "        {'param_name': 'coef0',\n",
       "         'param_type': 'float, default=1',\n",
       "         'param_desc': 'Constant offset added to scaled inner product.\\n'}]},\n",
       "      'function_calling': {'name': 'polynomial_kernel',\n",
       "       'descriptions': 'Compute the polynomial kernel between X and Y. K(X, Y) = (gamma <X, Y> + coef0) ^ degree Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, n_features). A feature array.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None. An optional second feature array. If None, uses Y=X.\\n'},\n",
       "         'degree': {'type': 'number',\n",
       "          'description': 'float, default=3. Kernel degree.\\n'},\n",
       "         'gamma': {'type': 'number',\n",
       "          'description': 'float, default=None. Coefficient of the vector inner product. If None, defaults to 1.0 / n_features.\\n'},\n",
       "         'coef0': {'type': 'number',\n",
       "          'description': 'float, default=1. Constant offset added to scaled inner product.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise.sigmoid_kernel',\n",
       "      'func_desc': 'Compute the sigmoid kernel between X and Y.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.sigmoid_kernel.html#sklearn.metrics.pairwise.sigmoid_kernel',\n",
       "      'function_definitions': {'function_name': 'sigmoid_kernel',\n",
       "       'full_function': 'sklearn.metrics.pairwise.sigmoid_kernel(X, Y=None, gamma=None, coef0=1)',\n",
       "       'function_text': 'Compute the sigmoid kernel between X and Y. K(X, Y) = tanh(gamma <X, Y> + coef0) Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/metrics.html#sigmoid-kernel',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'A feature array.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None',\n",
       "         'param_desc': 'An optional second feature array. If None, uses Y=X.\\n'},\n",
       "        {'param_name': 'gamma',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': 'Coefficient of the vector inner product. If None, defaults to 1.0 / n_features.\\n'},\n",
       "        {'param_name': 'coef0',\n",
       "         'param_type': 'float, default=1',\n",
       "         'param_desc': 'Constant offset added to scaled inner product.\\n'}]},\n",
       "      'function_calling': {'name': 'sigmoid_kernel',\n",
       "       'descriptions': 'Compute the sigmoid kernel between X and Y. K(X, Y) = tanh(gamma <X, Y> + coef0) Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, n_features). A feature array.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None. An optional second feature array. If None, uses Y=X.\\n'},\n",
       "         'gamma': {'type': 'number',\n",
       "          'description': 'float, default=None. Coefficient of the vector inner product. If None, defaults to 1.0 / n_features.\\n'},\n",
       "         'coef0': {'type': 'number',\n",
       "          'description': 'float, default=1. Constant offset added to scaled inner product.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'pairwise_distances_argmin',\n",
       "      'func_desc': 'Compute minimum distances between one point and a set of points.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_argmin.html#sklearn.metrics.pairwise_distances_argmin',\n",
       "      'function_definitions': {'function_name': 'pairwise_distances_argmin',\n",
       "       'full_function': \"sklearn.metrics.pairwise_distances_argmin(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\",\n",
       "       'function_text': 'Compute minimum distances between one point and a set of points. This function computes for each row in X, the index of the row of Y which\\nis closest (according to the specified distance). This is mostly equivalent to calling: pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis) but uses much less memory, and is faster for large arrays. This function works with dense 2D arrays only.',\n",
       "       'func_text_user_guide': 'sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, n_features)',\n",
       "         'param_desc': 'Array containing points.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features)',\n",
       "         'param_desc': 'Arrays containing points.\\n'},\n",
       "        {'param_name': 'axis',\n",
       "         'param_type': 'int, default=1',\n",
       "         'param_desc': 'Axis along which the argmin and distances are to be computed.\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'str or callable, default=”euclidean”',\n",
       "         'param_desc': \"Metric to use for distance computation. Any metric from scikit-learn\\nor scipy.spatial.distance can be used.\\nIf metric is a callable function, it is called on each\\npair of instances (rows) and the resulting value recorded. The callable\\nshould take two arrays as input and return one value indicating the\\ndistance between them. This works for Scipy’s metrics, but is less\\nefficient than passing the metric name as a string.\\nDistance matrices are not supported.\\nValid values for metric are:\\n\\nfrom scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,\\n‘manhattan’]\\nfrom scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,\\n‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,\\n‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,\\n‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,\\n‘yule’]\\n\\nSee the documentation for scipy.spatial.distance for details on these\\nmetrics.\\n\\nNote\\n'kulsinski' is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\\n\\n\\nNote\\n'matching' has been removed in SciPy 1.9 (use 'hamming' instead).\\n\\n\"},\n",
       "        {'param_name': 'metric_kwargs',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Keyword arguments to pass to specified metric function.\\n'}]},\n",
       "      'function_calling': {'name': 'pairwise_distances_argmin',\n",
       "       'descriptions': 'Compute minimum distances between one point and a set of points. This function computes for each row in X, the index of the row of Y which\\nis closest (according to the specified distance). This is mostly equivalent to calling: pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis) but uses much less memory, and is faster for large arrays. This function works with dense 2D arrays only.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, n_features). Array containing points.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features). Arrays containing points.\\n'},\n",
       "         'axis': {'type': 'integer',\n",
       "          'description': 'int, default=1. Axis along which the argmin and distances are to be computed.\\n'},\n",
       "         'metric': {'type': 'string',\n",
       "          'description': \"str or callable, default=”euclidean”. Metric to use for distance computation. Any metric from scikit-learn\\nor scipy.spatial.distance can be used.\\nIf metric is a callable function, it is called on each\\npair of instances (rows) and the resulting value recorded. The callable\\nshould take two arrays as input and return one value indicating the\\ndistance between them. This works for Scipy’s metrics, but is less\\nefficient than passing the metric name as a string.\\nDistance matrices are not supported.\\nValid values for metric are:\\n\\nfrom scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,\\n‘manhattan’]\\nfrom scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,\\n‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,\\n‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,\\n‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,\\n‘yule’]\\n\\nSee the documentation for scipy.spatial.distance for details on these\\nmetrics.\\n\\nNote\\n'kulsinski' is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\\n\\n\\nNote\\n'matching' has been removed in SciPy 1.9 (use 'hamming' instead).\\n\\n\"},\n",
       "         'metric_kwargs': {'type': 'array',\n",
       "          'description': 'dict, default=None. Keyword arguments to pass to specified metric function.\\n'}},\n",
       "        'required': ['X', 'Y']}}},\n",
       "     {'func_name': 'pairwise_distances_chunked',\n",
       "      'func_desc': 'Generate a distance matrix chunk by chunk with optional reduction.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_chunked.html#sklearn.metrics.pairwise_distances_chunked',\n",
       "      'function_definitions': {'function_name': 'pairwise_distances_chunked',\n",
       "       'full_function': \"sklearn.metrics.pairwise_distances_chunked(X, Y=None, *, reduce_func=None, metric='euclidean', n_jobs=None, working_memory=None, **kwds)\",\n",
       "       'function_text': 'Generate a distance matrix chunk by chunk with optional reduction. In cases where not all of a pairwise distance matrix needs to be\\nstored at once, this is used to calculate pairwise distances in\\nworking_memory-sized chunks.  If reduce_func is given, it is\\nrun on each chunk and its return values are concatenated into lists,\\narrays or sparse matrices.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-n_jobs',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)',\n",
       "         'param_desc': 'Array of pairwise distances between samples, or a feature array.\\nThe shape the array should be (n_samples_X, n_samples_X) if\\nmetric=’precomputed’ and (n_samples_X, n_features) otherwise.\\n'},\n",
       "        {'param_name': 'Y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None',\n",
       "         'param_desc': 'An optional second feature array. Only allowed if\\nmetric != “precomputed”.\\n'},\n",
       "        {'param_name': 'reduce_func',\n",
       "         'param_type': 'callable, default=None',\n",
       "         'param_desc': 'The function which is applied on each chunk of the distance matrix,\\nreducing it to needed values.  reduce_func(D_chunk, start)\\nis called repeatedly, where D_chunk is a contiguous vertical\\nslice of the pairwise distance matrix, starting at row start.\\nIt should return one of: None; an array, a list, or a sparse matrix\\nof length D_chunk.shape[0]; or a tuple of such objects.\\nReturning None is useful for in-place operations, rather than\\nreductions.\\nIf None, pairwise_distances_chunked returns a generator of vertical\\nchunks of the distance matrix.\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'str or callable, default=’euclidean’',\n",
       "         'param_desc': 'The metric to use when calculating distance between instances in a\\nfeature array. If metric is a string, it must be one of the options\\nallowed by scipy.spatial.distance.pdist for its metric parameter,\\nor a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\\nIf metric is “precomputed”, X is assumed to be a distance matrix.\\nAlternatively, if metric is a callable function, it is called on\\neach pair of instances (rows) and the resulting value recorded.\\nThe callable should take two arrays from X as input and return a\\nvalue indicating the distance between them.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of jobs to use for the computation. This works by\\nbreaking down the pairwise matrix into n_jobs even slices and\\ncomputing them in parallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': 'working_memory',\n",
       "         'param_type': 'float, default=None',\n",
       "         'param_desc': \"The sought maximum memory for temporary distance matrix chunks.\\nWhen None (default), the value of\\nsklearn.get_config()['working_memory'] is used.\\n\"},\n",
       "        {'param_name': '**kwds',\n",
       "         'param_type': 'optional keyword parameters',\n",
       "         'param_desc': 'Any further parameters are passed directly to the distance function.\\nIf using a scipy.spatial.distance metric, the parameters are still\\nmetric dependent. See the scipy docs for usage examples.\\n'}]},\n",
       "      'function_calling': {'name': 'pairwise_distances_chunked',\n",
       "       'descriptions': 'Generate a distance matrix chunk by chunk with optional reduction. In cases where not all of a pairwise distance matrix needs to be\\nstored at once, this is used to calculate pairwise distances in\\nworking_memory-sized chunks.  If reduce_func is given, it is\\nrun on each chunk and its return values are concatenated into lists,\\narrays or sparse matrices.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features). Array of pairwise distances between samples, or a feature array.\\nThe shape the array should be (n_samples_X, n_samples_X) if\\nmetric=’precomputed’ and (n_samples_X, n_features) otherwise.\\n'},\n",
       "         'Y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None. An optional second feature array. Only allowed if\\nmetric != “precomputed”.\\n'},\n",
       "         'reduce_func': {'type': 'object',\n",
       "          'description': 'callable, default=None. The function which is applied on each chunk of the distance matrix,\\nreducing it to needed values.  reduce_func(D_chunk, start)\\nis called repeatedly, where D_chunk is a contiguous vertical\\nslice of the pairwise distance matrix, starting at row start.\\nIt should return one of: None; an array, a list, or a sparse matrix\\nof length D_chunk.shape[0]; or a tuple of such objects.\\nReturning None is useful for in-place operations, rather than\\nreductions.\\nIf None, pairwise_distances_chunked returns a generator of vertical\\nchunks of the distance matrix.\\n'},\n",
       "         'metric': {'type': 'string',\n",
       "          'description': 'str or callable, default=’euclidean’. The metric to use when calculating distance between instances in a\\nfeature array. If metric is a string, it must be one of the options\\nallowed by scipy.spatial.distance.pdist for its metric parameter,\\nor a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\\nIf metric is “precomputed”, X is assumed to be a distance matrix.\\nAlternatively, if metric is a callable function, it is called on\\neach pair of instances (rows) and the resulting value recorded.\\nThe callable should take two arrays from X as input and return a\\nvalue indicating the distance between them.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of jobs to use for the computation. This works by\\nbreaking down the pairwise matrix into n_jobs even slices and\\ncomputing them in parallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "         'working_memory': {'type': 'number',\n",
       "          'description': \"float, default=None. The sought maximum memory for temporary distance matrix chunks.\\nWhen None (default), the value of\\nsklearn.get_config()['working_memory'] is used.\\n\"}},\n",
       "        'required': ['X', 'Y=None']}}}]},\n",
       "   {'Plotting': [{'func_name': 'ConfusionMatrixDisplay',\n",
       "      'func_desc': 'Confusion Matrix visualization.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay',\n",
       "      'function_definitions': {'function_name': 'ConfusionMatrixDisplay',\n",
       "       'full_function': 'class sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix, *, display_labels=None)',\n",
       "       'function_text': 'Confusion Matrix visualization. It is recommend to use\\nfrom_estimator or\\nfrom_predictions to\\ncreate a ConfusionMatrixDisplay. All parameters are stored as\\nattributes. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#sklearn.metrics.ConfusionMatrixDisplay.from_estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'include_values',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Includes values in confusion matrix.\\n'},\n",
       "        {'param_name': 'cmap',\n",
       "         'param_type': 'str or matplotlib Colormap, default=’viridis’',\n",
       "         'param_desc': 'Colormap recognized by matplotlib.\\n'},\n",
       "        {'param_name': 'xticks_rotation',\n",
       "         'param_type': '{‘vertical’, ‘horizontal’} or float,                          default=’horizontal’',\n",
       "         'param_desc': 'Rotation of xtick labels.\\n'},\n",
       "        {'param_name': 'values_format',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'Format specification for values in confusion matrix. If None,\\nthe format specification is ‘d’ or ‘.2g’ whichever is shorter.\\n'},\n",
       "        {'param_name': 'ax',\n",
       "         'param_type': 'matplotlib axes, default=None',\n",
       "         'param_desc': 'Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "        {'param_name': 'colorbar',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether or not to add a colorbar to the plot.\\n'},\n",
       "        {'param_name': 'im_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Dict with keywords passed to matplotlib.pyplot.imshow call.\\n'},\n",
       "        {'param_name': 'text_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Dict with keywords passed to matplotlib.pyplot.text call.\\n\\nAdded in version 1.2.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'ConfusionMatrixDisplay',\n",
       "       'descriptions': 'Confusion Matrix visualization. It is recommend to use\\nfrom_estimator or\\nfrom_predictions to\\ncreate a ConfusionMatrixDisplay. All parameters are stored as\\nattributes. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'include_values': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Includes values in confusion matrix.\\n'},\n",
       "         'cmap': {'type': 'string',\n",
       "          'description': 'str or matplotlib Colormap, default=’viridis’. Colormap recognized by matplotlib.\\n'},\n",
       "         'xticks_rotation': {'type': 'number',\n",
       "          'description': '{‘vertical’, ‘horizontal’} or float,                          default=’horizontal’. Rotation of xtick labels.\\n'},\n",
       "         'values_format': {'type': 'string',\n",
       "          'description': 'str, default=None. Format specification for values in confusion matrix. If None,\\nthe format specification is ‘d’ or ‘.2g’ whichever is shorter.\\n'},\n",
       "         'ax': {'type': 'object',\n",
       "          'description': 'matplotlib axes, default=None. Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "         'colorbar': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether or not to add a colorbar to the plot.\\n'},\n",
       "         'im_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Dict with keywords passed to matplotlib.pyplot.imshow call.\\n'},\n",
       "         'text_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Dict with keywords passed to matplotlib.pyplot.text call.\\n\\nAdded in version 1.2.\\n\\n'}},\n",
       "        'required': ['confusion_matrix']}}},\n",
       "     {'func_name': 'PrecisionRecallDisplay',\n",
       "      'func_desc': 'Precision Recall visualization.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html#sklearn.metrics.PrecisionRecallDisplay',\n",
       "      'function_definitions': {'function_name': 'PrecisionRecallDisplay',\n",
       "       'full_function': 'class sklearn.metrics.PrecisionRecallDisplay(precision, recall, *, average_precision=None, estimator_name=None, pos_label=None, prevalence_pos_label=None)',\n",
       "       'function_text': 'Precision Recall visualization. It is recommend to use\\nfrom_estimator or\\nfrom_predictions to create\\na PrecisionRecallDisplay. All parameters are\\nstored as attributes. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#sklearn.metrics.PrecisionRecallDisplay.from_estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'ax',\n",
       "         'param_type': 'Matplotlib Axes, default=None',\n",
       "         'param_desc': 'Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "        {'param_name': 'name',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'Name of precision recall curve for labeling. If None, use\\nestimator_name if not None, otherwise no labeling is shown.\\n'},\n",
       "        {'param_name': 'plot_chance_level',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to plot the chance level. The chance level is the prevalence\\nof the positive label computed from the data passed during\\nfrom_estimator or from_predictions call.\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "        {'param_name': 'chance_level_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Keyword arguments to be passed to matplotlib’s plot for rendering\\nthe chance level line.\\n\\nAdded in version 1.3.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'PrecisionRecallDisplay',\n",
       "       'descriptions': 'Precision Recall visualization. It is recommend to use\\nfrom_estimator or\\nfrom_predictions to create\\na PrecisionRecallDisplay. All parameters are\\nstored as attributes. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'ax': {'type': 'object',\n",
       "          'description': 'Matplotlib Axes, default=None. Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "         'name': {'type': 'string',\n",
       "          'description': 'str, default=None. Name of precision recall curve for labeling. If None, use\\nestimator_name if not None, otherwise no labeling is shown.\\n'},\n",
       "         'plot_chance_level': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to plot the chance level. The chance level is the prevalence\\nof the positive label computed from the data passed during\\nfrom_estimator or from_predictions call.\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "         'chance_level_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Keyword arguments to be passed to matplotlib’s plot for rendering\\nthe chance level line.\\n\\nAdded in version 1.3.\\n\\n'}},\n",
       "        'required': ['precision', 'recall']}}},\n",
       "     {'func_name': 'RocCurveDisplay',\n",
       "      'func_desc': 'ROC Curve visualization.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html#sklearn.metrics.RocCurveDisplay',\n",
       "      'function_definitions': {'function_name': 'RocCurveDisplay',\n",
       "       'full_function': 'class sklearn.metrics.RocCurveDisplay(*, fpr, tpr, roc_auc=None, estimator_name=None, pos_label=None)',\n",
       "       'function_text': 'ROC Curve visualization. It is recommend to use\\nfrom_estimator or\\nfrom_predictions to create\\na RocCurveDisplay. All parameters are\\nstored as attributes. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#sklearn.metrics.RocCurveDisplay.from_estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'ax',\n",
       "         'param_type': 'matplotlib axes, default=None',\n",
       "         'param_desc': 'Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "        {'param_name': 'name',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'Name of ROC Curve for labeling. If None, use estimator_name if\\nnot None, otherwise no labeling is shown.\\n'},\n",
       "        {'param_name': 'plot_chance_level',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to plot the chance level.\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "        {'param_name': 'chance_level_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Keyword arguments to be passed to matplotlib’s plot for rendering\\nthe chance level line.\\n\\nAdded in version 1.3.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'RocCurveDisplay',\n",
       "       'descriptions': 'ROC Curve visualization. It is recommend to use\\nfrom_estimator or\\nfrom_predictions to create\\na RocCurveDisplay. All parameters are\\nstored as attributes. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'ax': {'type': 'object',\n",
       "          'description': 'matplotlib axes, default=None. Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "         'name': {'type': 'string',\n",
       "          'description': 'str, default=None. Name of ROC Curve for labeling. If None, use estimator_name if\\nnot None, otherwise no labeling is shown.\\n'},\n",
       "         'plot_chance_level': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to plot the chance level.\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "         'chance_level_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Keyword arguments to be passed to matplotlib’s plot for rendering\\nthe chance level line.\\n\\nAdded in version 1.3.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'DetCurveDisplay',\n",
       "      'func_desc': 'DET curve visualization.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DetCurveDisplay.html#sklearn.metrics.DetCurveDisplay',\n",
       "      'function_definitions': {'function_name': 'DetCurveDisplay',\n",
       "       'full_function': 'class sklearn.metrics.DetCurveDisplay(*, fpr, fnr, estimator_name=None, pos_label=None)',\n",
       "       'function_text': 'DET curve visualization. It is recommend to use from_estimator\\nor from_predictions to create a\\nvisualizer. All parameters are stored as attributes. Read more in the User Guide. Added in version 0.24.',\n",
       "       'func_text_user_guide': '#sklearn.metrics.DetCurveDisplay.from_estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'ax',\n",
       "         'param_type': 'matplotlib axes, default=None',\n",
       "         'param_desc': 'Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "        {'param_name': 'name',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'Name of DET curve for labeling. If None, use estimator_name if\\nit is not None, otherwise no labeling is shown.\\n'}]},\n",
       "      'function_calling': {'name': 'DetCurveDisplay',\n",
       "       'descriptions': 'DET curve visualization. It is recommend to use from_estimator\\nor from_predictions to create a\\nvisualizer. All parameters are stored as attributes. Read more in the User Guide. Added in version 0.24.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'ax': {'type': 'object',\n",
       "          'description': 'matplotlib axes, default=None. Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "         'name': {'type': 'string',\n",
       "          'description': 'str, default=None. Name of DET curve for labeling. If None, use estimator_name if\\nit is not None, otherwise no labeling is shown.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'PredictionErrorDisplay',\n",
       "      'func_desc': 'Visualization of the prediction error of a regression model.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PredictionErrorDisplay.html#sklearn.metrics.PredictionErrorDisplay',\n",
       "      'function_definitions': {'function_name': 'PredictionErrorDisplay',\n",
       "       'full_function': 'class sklearn.metrics.PredictionErrorDisplay(*, y_true, y_pred)',\n",
       "       'function_text': 'Visualization of the prediction error of a regression model. This tool can display “residuals vs predicted” or “actual vs predicted”\\nusing scatter plots to qualitatively assess the behavior of a regressor,\\npreferably on held-out data points. See the details in the docstrings of\\nfrom_estimator or\\nfrom_predictions to\\ncreate a visualizer. All parameters are stored as attributes. For general information regarding scikit-learn visualization tools, read\\nmore in the Visualization Guide.\\nFor details regarding interpreting these plots, refer to the\\nModel Evaluation Guide. Added in version 1.2.',\n",
       "       'func_text_user_guide': '#sklearn.metrics.PredictionErrorDisplay.from_estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'ax',\n",
       "         'param_type': 'matplotlib axes, default=None',\n",
       "         'param_desc': 'Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "        {'param_name': 'kind',\n",
       "         'param_type': '{“actual_vs_predicted”, “residual_vs_predicted”},                 default=”residual_vs_predicted”',\n",
       "         'param_desc': 'The type of plot to draw:\\n\\n“actual_vs_predicted” draws the observed values (y-axis) vs.\\nthe predicted values (x-axis).\\n“residual_vs_predicted” draws the residuals, i.e. difference\\nbetween observed and predicted values, (y-axis) vs. the predicted\\nvalues (x-axis).\\n\\n'},\n",
       "        {'param_name': 'scatter_kwargs',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Dictionary with keywords passed to the matplotlib.pyplot.scatter\\ncall.\\n'},\n",
       "        {'param_name': 'line_kwargs',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Dictionary with keyword passed to the matplotlib.pyplot.plot\\ncall to draw the optimal line.\\n'}]},\n",
       "      'function_calling': {'name': 'PredictionErrorDisplay',\n",
       "       'descriptions': 'Visualization of the prediction error of a regression model. This tool can display “residuals vs predicted” or “actual vs predicted”\\nusing scatter plots to qualitatively assess the behavior of a regressor,\\npreferably on held-out data points. See the details in the docstrings of\\nfrom_estimator or\\nfrom_predictions to\\ncreate a visualizer. All parameters are stored as attributes. For general information regarding scikit-learn visualization tools, read\\nmore in the Visualization Guide.\\nFor details regarding interpreting these plots, refer to the\\nModel Evaluation Guide. Added in version 1.2.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'ax': {'type': 'object',\n",
       "          'description': 'matplotlib axes, default=None. Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "         'kind': {'type': 'array',\n",
       "          'description': '{“actual_vs_predicted”, “residual_vs_predicted”},                 default=”residual_vs_predicted”. The type of plot to draw:\\n\\n“actual_vs_predicted” draws the observed values (y-axis) vs.\\nthe predicted values (x-axis).\\n“residual_vs_predicted” draws the residuals, i.e. difference\\nbetween observed and predicted values, (y-axis) vs. the predicted\\nvalues (x-axis).\\n\\n'},\n",
       "         'scatter_kwargs': {'type': 'array',\n",
       "          'description': 'dict, default=None. Dictionary with keywords passed to the matplotlib.pyplot.scatter\\ncall.\\n'},\n",
       "         'line_kwargs': {'type': 'array',\n",
       "          'description': 'dict, default=None. Dictionary with keyword passed to the matplotlib.pyplot.plot\\ncall to draw the optimal line.\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.metrics',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.metrics.html'},\n",
       " 'sklearn.mixture.html': {'functions': [{'defaults': [{'func_name': 'BayesianGaussianMixture',\n",
       "      'func_desc': 'Variational Bayesian estimation of a Gaussian mixture.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture',\n",
       "      'function_definitions': {'function_name': 'BayesianGaussianMixture',\n",
       "       'full_function': \"class sklearn.mixture.BayesianGaussianMixture(*, n_components=1, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weight_concentration_prior_type='dirichlet_process', weight_concentration_prior=None, mean_precision_prior=None, mean_prior=None, degrees_of_freedom_prior=None, covariance_prior=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10)\",\n",
       "       'function_text': 'Variational Bayesian estimation of a Gaussian mixture. This class allows to infer an approximate posterior distribution over the\\nparameters of a Gaussian mixture distribution. The effective number of\\ncomponents can be inferred from the data. This class implements two types of prior for the weights distribution: a\\nfinite mixture model with Dirichlet distribution and an infinite mixture\\nmodel with the Dirichlet Process. In practice Dirichlet Process inference\\nalgorithm is approximated and uses a truncated distribution with a fixed\\nmaximum number of components (called the Stick-breaking representation).\\nThe number of components actually used almost always depends on the data. Added in version 0.18. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/mixture.html#bgmm',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'BayesianGaussianMixture',\n",
       "       'descriptions': 'Variational Bayesian estimation of a Gaussian mixture. This class allows to infer an approximate posterior distribution over the\\nparameters of a Gaussian mixture distribution. The effective number of\\ncomponents can be inferred from the data. This class implements two types of prior for the weights distribution: a\\nfinite mixture model with Dirichlet distribution and an infinite mixture\\nmodel with the Dirichlet Process. In practice Dirichlet Process inference\\nalgorithm is approximated and uses a truncated distribution with a fixed\\nmaximum number of components (called the Stick-breaking representation).\\nThe number of components actually used almost always depends on the data. Added in version 0.18. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object', 'properties': {}, 'required': []}}},\n",
       "     {'func_name': 'GaussianMixture',\n",
       "      'func_desc': 'Gaussian Mixture.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture',\n",
       "      'function_definitions': {'function_name': 'GaussianMixture',\n",
       "       'full_function': \"class sklearn.mixture.GaussianMixture(n_components=1, *, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10)\",\n",
       "       'function_text': 'Gaussian Mixture. Representation of a Gaussian mixture model probability distribution.\\nThis class allows to estimate the parameters of a Gaussian mixture\\ndistribution. Read more in the User Guide. Added in version 0.18.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/mixture.html#gmm',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'GaussianMixture',\n",
       "       'descriptions': 'Gaussian Mixture. Representation of a Gaussian mixture model probability distribution.\\nThis class allows to estimate the parameters of a Gaussian mixture\\ndistribution. Read more in the User Guide. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {},\n",
       "        'required': ['n_components=1']}}}]}],\n",
       "  'name': 'sklearn.mixture',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.mixture.html'},\n",
       " 'sklearn.model_selection.html': {'functions': [{'Splitters': [{'func_name': 'GroupKFold',\n",
       "      'func_desc': 'K-fold iterator variant with non-overlapping groups.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold',\n",
       "      'function_definitions': {'function_name': 'GroupKFold',\n",
       "       'full_function': 'class sklearn.model_selection.GroupKFold(n_splits=5)',\n",
       "       'function_text': 'K-fold iterator variant with non-overlapping groups. Each group will appear exactly once in the test set across all folds (the\\nnumber of distinct groups has to be at least equal to the number of folds). The folds are approximately balanced in the sense that the number of\\nsamples is approximately the same in each test fold. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#group-k-fold',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'The target variable for supervised learning problems.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Group labels for the samples used while splitting the dataset into\\ntrain/test set.\\n'}]},\n",
       "      'function_calling': {'name': 'GroupKFold',\n",
       "       'descriptions': 'K-fold iterator variant with non-overlapping groups. Each group will appear exactly once in the test set across all folds (the\\nnumber of distinct groups has to be at least equal to the number of folds). The folds are approximately balanced in the sense that the number of\\nsamples is approximately the same in each test fold. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. The target variable for supervised learning problems.\\n'},\n",
       "         'groups': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Group labels for the samples used while splitting the dataset into\\ntrain/test set.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'KFold',\n",
       "      'func_desc': 'K-Fold cross-validator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold',\n",
       "      'function_definitions': {'function_name': 'KFold',\n",
       "       'full_function': 'class sklearn.model_selection.KFold(n_splits=5, *, shuffle=False, random_state=None)',\n",
       "       'function_text': 'K-Fold cross-validator. Provides train/test indices to split data in train/test sets. Split\\ndataset into k consecutive folds (without shuffling by default). Each fold is then used once as a validation while the k - 1 remaining\\nfolds form the training set. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#k-fold',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'The target variable for supervised learning problems.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'Always ignored, exists for compatibility.\\n'}]},\n",
       "      'function_calling': {'name': 'KFold',\n",
       "       'descriptions': 'K-Fold cross-validator. Provides train/test indices to split data in train/test sets. Split\\ndataset into k consecutive folds (without shuffling by default). Each fold is then used once as a validation while the k - 1 remaining\\nfolds form the training set. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). The target variable for supervised learning problems.\\n'},\n",
       "         'groups': {'type': 'object',\n",
       "          'description': 'object. Always ignored, exists for compatibility.\\n'}},\n",
       "        'required': ['n_splits=5']}}},\n",
       "     {'func_name': 'LeaveOneOut',\n",
       "      'func_desc': 'Leave-One-Out cross-validator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut',\n",
       "      'function_definitions': {'function_name': 'LeaveOneOut',\n",
       "       'full_function': 'class sklearn.model_selection.LeaveOneOut',\n",
       "       'function_text': 'Leave-One-Out cross-validator. Provides train/test indices to split data in train/test sets. Each\\nsample is used once as a test set (singleton) while the remaining\\nsamples form the training set. Note: LeaveOneOut() is equivalent to KFold(n_splits=n) and\\nLeavePOut(p=1) where n is the number of samples. Due to the high number of test sets (which is the same as the\\nnumber of samples) this cross-validation method can be very costly.\\nFor large datasets one should favor KFold, ShuffleSplit\\nor StratifiedKFold. Read more in the User Guide. See also For splitting the data according to explicit, domain-specific stratification of the dataset. K-fold iterator variant with non-overlapping groups. Examples Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works.',\n",
       "       'func_text_user_guide': 'sklearn.model_selection.KFold.html#sklearn.model_selection.KFold',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'LeavePOut',\n",
       "      'func_desc': 'Leave-P-Out cross-validator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePOut.html#sklearn.model_selection.LeavePOut',\n",
       "      'function_definitions': {'function_name': 'LeavePOut',\n",
       "       'full_function': 'class sklearn.model_selection.LeavePOut(p)',\n",
       "       'function_text': 'Leave-P-Out cross-validator. Provides train/test indices to split data in train/test sets. This results\\nin testing on all distinct samples of size p, while the remaining n - p\\nsamples form the training set in each iteration. Note: LeavePOut(p) is NOT equivalent to\\nKFold(n_splits=n_samples // p) which creates non-overlapping test sets. Due to the high number of iterations which grows combinatorically with the\\nnumber of samples this cross-validation method can be very costly. For\\nlarge datasets one should favor KFold, StratifiedKFold\\nor ShuffleSplit. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.model_selection.KFold.html#sklearn.model_selection.KFold',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'The target variable for supervised learning problems.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'Always ignored, exists for compatibility.\\n'}]},\n",
       "      'function_calling': {'name': 'LeavePOut',\n",
       "       'descriptions': 'Leave-P-Out cross-validator. Provides train/test indices to split data in train/test sets. This results\\nin testing on all distinct samples of size p, while the remaining n - p\\nsamples form the training set in each iteration. Note: LeavePOut(p) is NOT equivalent to\\nKFold(n_splits=n_samples // p) which creates non-overlapping test sets. Due to the high number of iterations which grows combinatorically with the\\nnumber of samples this cross-validation method can be very costly. For\\nlarge datasets one should favor KFold, StratifiedKFold\\nor ShuffleSplit. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). The target variable for supervised learning problems.\\n'},\n",
       "         'groups': {'type': 'object',\n",
       "          'description': 'object. Always ignored, exists for compatibility.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'RepeatedKFold',\n",
       "      'func_desc': 'Repeated K-Fold cross validator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold',\n",
       "      'function_definitions': {'function_name': 'RepeatedKFold',\n",
       "       'full_function': 'class sklearn.model_selection.RepeatedKFold(*, n_splits=5, n_repeats=10, random_state=None)',\n",
       "       'function_text': 'Repeated K-Fold cross validator. Repeats K-Fold n times with different randomization in each repetition. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#repeated-k-fold',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'The target variable for supervised learning problems.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'Always ignored, exists for compatibility.\\n'}]},\n",
       "      'function_calling': {'name': 'RepeatedKFold',\n",
       "       'descriptions': 'Repeated K-Fold cross validator. Repeats K-Fold n times with different randomization in each repetition. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). The target variable for supervised learning problems.\\n'},\n",
       "         'groups': {'type': 'object',\n",
       "          'description': 'object. Always ignored, exists for compatibility.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'ShuffleSplit',\n",
       "      'func_desc': 'Random permutation cross-validator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit',\n",
       "      'function_definitions': {'function_name': 'ShuffleSplit',\n",
       "       'full_function': 'class sklearn.model_selection.ShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None)',\n",
       "       'function_text': 'Random permutation cross-validator. Yields indices to split data into training and test sets. Note: contrary to other cross-validation strategies, random splits\\ndo not guarantee that all folds will be different, although this is\\nstill very likely for sizeable datasets. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#shufflesplit',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'The target variable for supervised learning problems.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'Always ignored, exists for compatibility.\\n'}]},\n",
       "      'function_calling': {'name': 'ShuffleSplit',\n",
       "       'descriptions': 'Random permutation cross-validator. Yields indices to split data into training and test sets. Note: contrary to other cross-validation strategies, random splits\\ndo not guarantee that all folds will be different, although this is\\nstill very likely for sizeable datasets. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). The target variable for supervised learning problems.\\n'},\n",
       "         'groups': {'type': 'object',\n",
       "          'description': 'object. Always ignored, exists for compatibility.\\n'}},\n",
       "        'required': ['n_splits=10']}}},\n",
       "     {'func_name': 'StratifiedKFold',\n",
       "      'func_desc': 'Stratified K-Fold cross-validator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold',\n",
       "      'function_definitions': {'function_name': 'StratifiedKFold',\n",
       "       'full_function': 'class sklearn.model_selection.StratifiedKFold(n_splits=5, *, shuffle=False, random_state=None)',\n",
       "       'function_text': 'Stratified K-Fold cross-validator. Provides train/test indices to split data in train/test sets. This cross-validation object is a variation of KFold that returns\\nstratified folds. The folds are made by preserving the percentage of\\nsamples for each class. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\nNote that providing y is sufficient to generate the splits and\\nhence np.zeros(n_samples) may be used as a placeholder for\\nX instead of actual training data.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'The target variable for supervised learning problems.\\nStratification is done based on the y labels.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'Always ignored, exists for compatibility.\\n'}]},\n",
       "      'function_calling': {'name': 'StratifiedKFold',\n",
       "       'descriptions': 'Stratified K-Fold cross-validator. Provides train/test indices to split data in train/test sets. This cross-validation object is a variation of KFold that returns\\nstratified folds. The folds are made by preserving the percentage of\\nsamples for each class. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\nNote that providing y is sufficient to generate the splits and\\nhence np.zeros(n_samples) may be used as a placeholder for\\nX instead of actual training data.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). The target variable for supervised learning problems.\\nStratification is done based on the y labels.\\n'},\n",
       "         'groups': {'type': 'object',\n",
       "          'description': 'object. Always ignored, exists for compatibility.\\n'}},\n",
       "        'required': ['n_splits=5']}}},\n",
       "     {'func_name': 'TimeSeriesSplit',\n",
       "      'func_desc': 'Time Series cross-validator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html#sklearn.model_selection.TimeSeriesSplit',\n",
       "      'function_definitions': {'function_name': 'TimeSeriesSplit',\n",
       "       'full_function': 'class sklearn.model_selection.TimeSeriesSplit(n_splits=5, *, max_train_size=None, test_size=None, gap=0)',\n",
       "       'function_text': 'Time Series cross-validator. Provides train/test indices to split time series data samples\\nthat are observed at fixed time intervals, in train/test sets.\\nIn each split, test indices must be higher than before, and thus shuffling\\nin cross validator is inappropriate. This cross-validation object is a variation of KFold.\\nIn the kth split, it returns first k folds as train set and the\\n(k+1)th fold as test set. Note that unlike standard cross-validation methods, successive\\ntraining sets are supersets of those that come before them. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Added in version 0.18.',\n",
       "       'func_text_user_guide': 'sklearn.model_selection.KFold.html#sklearn.model_selection.KFold',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Always ignored, exists for compatibility.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Always ignored, exists for compatibility.\\n'}]},\n",
       "      'function_calling': {'name': 'TimeSeriesSplit',\n",
       "       'descriptions': 'Time Series cross-validator. Provides train/test indices to split time series data samples\\nthat are observed at fixed time intervals, in train/test sets.\\nIn each split, test indices must be higher than before, and thus shuffling\\nin cross validator is inappropriate. This cross-validation object is a variation of KFold.\\nIn the kth split, it returns first k folds as train set and the\\n(k+1)th fold as test set. Note that unlike standard cross-validation methods, successive\\ntraining sets are supersets of those that come before them. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Always ignored, exists for compatibility.\\n'},\n",
       "         'groups': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Always ignored, exists for compatibility.\\n'}},\n",
       "        'required': ['n_splits=5']}}},\n",
       "     {'func_name': 'train_test_split',\n",
       "      'func_desc': 'Split arrays or matrices into random train and test subsets.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split',\n",
       "      'function_definitions': {'function_name': 'train_test_split',\n",
       "       'full_function': 'sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)',\n",
       "       'function_text': 'Split arrays or matrices into random train and test subsets. Quick utility that wraps input validation,\\nnext(ShuffleSplit().split(X, y)), and application to input data\\ninto a single call for splitting (and optionally subsampling) data into a\\none-liner. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation',\n",
       "       'parameter_names_desc': [{'param_name': '*arrays',\n",
       "         'param_type': 'sequence of indexables with same length / shape[0]',\n",
       "         'param_desc': 'Allowed inputs are lists, numpy arrays, scipy-sparse\\nmatrices or pandas dataframes.\\n'},\n",
       "        {'param_name': 'test_size',\n",
       "         'param_type': 'float or int, default=None',\n",
       "         'param_desc': 'If float, should be between 0.0 and 1.0 and represent the proportion\\nof the dataset to include in the test split. If int, represents the\\nabsolute number of test samples. If None, the value is set to the\\ncomplement of the train size. If train_size is also None, it will\\nbe set to 0.25.\\n'},\n",
       "        {'param_name': 'train_size',\n",
       "         'param_type': 'float or int, default=None',\n",
       "         'param_desc': 'If float, should be between 0.0 and 1.0 and represent the\\nproportion of the dataset to include in the train split. If\\nint, represents the absolute number of train samples. If None,\\nthe value is automatically set to the complement of the test size.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Controls the shuffling applied to the data before applying the split.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether or not to shuffle the data before splitting. If shuffle=False\\nthen stratify must be None.\\n'},\n",
       "        {'param_name': 'stratify',\n",
       "         'param_type': 'array-like, default=None',\n",
       "         'param_desc': 'If not None, data is split in a stratified fashion, using this as\\nthe class labels.\\nRead more in the User Guide.\\n'}]},\n",
       "      'function_calling': {'name': 'train_test_split',\n",
       "       'descriptions': 'Split arrays or matrices into random train and test subsets. Quick utility that wraps input validation,\\nnext(ShuffleSplit().split(X, y)), and application to input data\\ninto a single call for splitting (and optionally subsampling) data into a\\none-liner. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'*arrays': {'type': 'array',\n",
       "          'description': 'sequence of indexables with same length / shape[0]. Allowed inputs are lists, numpy arrays, scipy-sparse\\nmatrices or pandas dataframes.\\n'},\n",
       "         'test_size': {'type': 'integer',\n",
       "          'description': 'float or int, default=None. If float, should be between 0.0 and 1.0 and represent the proportion\\nof the dataset to include in the test split. If int, represents the\\nabsolute number of test samples. If None, the value is set to the\\ncomplement of the train size. If train_size is also None, it will\\nbe set to 0.25.\\n'},\n",
       "         'train_size': {'type': 'integer',\n",
       "          'description': 'float or int, default=None. If float, should be between 0.0 and 1.0 and represent the\\nproportion of the dataset to include in the train split. If\\nint, represents the absolute number of train samples. If None,\\nthe value is automatically set to the complement of the test size.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Controls the shuffling applied to the data before applying the split.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether or not to shuffle the data before splitting. If shuffle=False\\nthen stratify must be None.\\n'},\n",
       "         'stratify': {'type': 'array',\n",
       "          'description': 'array-like, default=None. If not None, data is split in a stratified fashion, using this as\\nthe class labels.\\nRead more in the User Guide.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'GroupShuffleSplit',\n",
       "      'func_desc': 'Shuffle-Group(s)-Out cross-validation iterator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupShuffleSplit.html#sklearn.model_selection.GroupShuffleSplit',\n",
       "      'function_definitions': {'function_name': 'GroupShuffleSplit',\n",
       "       'full_function': 'class sklearn.model_selection.GroupShuffleSplit(n_splits=5, *, test_size=None, train_size=None, random_state=None)',\n",
       "       'function_text': 'Shuffle-Group(s)-Out cross-validation iterator. Provides randomized train/test indices to split data according to a\\nthird-party provided group. This group information can be used to encode\\narbitrary domain specific stratifications of the samples as integers. For instance the groups could be the year of collection of the samples\\nand thus allow for cross-validation against time-based splits. The difference between LeavePGroupsOut and GroupShuffleSplit is that\\nthe former generates splits using all subsets of size p unique groups,\\nwhereas GroupShuffleSplit generates a user-determined number of random\\ntest splits, each with a user-determined fraction of unique groups. For example, a less computationally intensive alternative to\\nLeavePGroupsOut(p=10) would be\\nGroupShuffleSplit(test_size=10, n_splits=100). Note: The parameters test_size and train_size refer to groups, and\\nnot to samples, as in ShuffleSplit. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#group-shuffle-split',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'The target variable for supervised learning problems.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Group labels for the samples used while splitting the dataset into\\ntrain/test set.\\n'}]},\n",
       "      'function_calling': {'name': 'GroupShuffleSplit',\n",
       "       'descriptions': 'Shuffle-Group(s)-Out cross-validation iterator. Provides randomized train/test indices to split data according to a\\nthird-party provided group. This group information can be used to encode\\narbitrary domain specific stratifications of the samples as integers. For instance the groups could be the year of collection of the samples\\nand thus allow for cross-validation against time-based splits. The difference between LeavePGroupsOut and GroupShuffleSplit is that\\nthe former generates splits using all subsets of size p unique groups,\\nwhereas GroupShuffleSplit generates a user-determined number of random\\ntest splits, each with a user-determined fraction of unique groups. For example, a less computationally intensive alternative to\\nLeavePGroupsOut(p=10) would be\\nGroupShuffleSplit(test_size=10, n_splits=100). Note: The parameters test_size and train_size refer to groups, and\\nnot to samples, as in ShuffleSplit. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. The target variable for supervised learning problems.\\n'},\n",
       "         'groups': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Group labels for the samples used while splitting the dataset into\\ntrain/test set.\\n'}},\n",
       "        'required': ['n_splits=5']}}},\n",
       "     {'func_name': 'LeaveOneGroupOut',\n",
       "      'func_desc': 'Leave One Group Out cross-validator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut',\n",
       "      'function_definitions': {'function_name': 'LeaveOneGroupOut',\n",
       "       'full_function': 'class sklearn.model_selection.LeaveOneGroupOut',\n",
       "       'function_text': 'Leave One Group Out cross-validator. Provides train/test indices to split data such that each training set is\\ncomprised of all samples except ones belonging to one specific group.\\nArbitrary domain specific group information is provided an array integers\\nthat encodes the group of each sample. For instance the groups could be the year of collection of the samples\\nand thus allow for cross-validation against time-based splits. Read more in the User Guide. See also K-fold iterator variant with non-overlapping groups. Notes Splits are ordered according to the index of the group left out. The first\\nsplit has testing set consisting of the group whose index in groups is\\nlowest, and so on. Examples Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#leave-one-group-out',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'LeavePGroupsOut',\n",
       "      'func_desc': 'Leave P Group(s) Out cross-validator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePGroupsOut.html#sklearn.model_selection.LeavePGroupsOut',\n",
       "      'function_definitions': {'function_name': 'LeavePGroupsOut',\n",
       "       'full_function': 'class sklearn.model_selection.LeavePGroupsOut(n_groups)',\n",
       "       'function_text': 'Leave P Group(s) Out cross-validator. Provides train/test indices to split data according to a third-party\\nprovided group. This group information can be used to encode arbitrary\\ndomain specific stratifications of the samples as integers. For instance the groups could be the year of collection of the samples\\nand thus allow for cross-validation against time-based splits. The difference between LeavePGroupsOut and LeaveOneGroupOut is that\\nthe former builds the test sets with all the samples assigned to\\np different values of the groups while the latter uses samples\\nall assigned the same groups. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#leave-p-groups-out',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'The target variable for supervised learning problems.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Group labels for the samples used while splitting the dataset into\\ntrain/test set.\\n'}]},\n",
       "      'function_calling': {'name': 'LeavePGroupsOut',\n",
       "       'descriptions': 'Leave P Group(s) Out cross-validator. Provides train/test indices to split data according to a third-party\\nprovided group. This group information can be used to encode arbitrary\\ndomain specific stratifications of the samples as integers. For instance the groups could be the year of collection of the samples\\nand thus allow for cross-validation against time-based splits. The difference between LeavePGroupsOut and LeaveOneGroupOut is that\\nthe former builds the test sets with all the samples assigned to\\np different values of the groups while the latter uses samples\\nall assigned the same groups. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. The target variable for supervised learning problems.\\n'},\n",
       "         'groups': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Group labels for the samples used while splitting the dataset into\\ntrain/test set.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'PredefinedSplit',\n",
       "      'func_desc': 'Predefined split cross-validator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit',\n",
       "      'function_definitions': {'function_name': 'PredefinedSplit',\n",
       "       'full_function': 'class sklearn.model_selection.PredefinedSplit(test_fold)',\n",
       "       'function_text': 'Predefined split cross-validator. Provides train/test indices to split data into train/test sets using a\\npredefined scheme specified by the user with the test_fold parameter. Read more in the User Guide. Added in version 0.16.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#predefined-split',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'Always ignored, exists for compatibility.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'Always ignored, exists for compatibility.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'Always ignored, exists for compatibility.\\n'}]},\n",
       "      'function_calling': {'name': 'PredefinedSplit',\n",
       "       'descriptions': 'Predefined split cross-validator. Provides train/test indices to split data into train/test sets using a\\npredefined scheme specified by the user with the test_fold parameter. Read more in the User Guide. Added in version 0.16.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'object',\n",
       "          'description': 'object. Always ignored, exists for compatibility.\\n'},\n",
       "         'y': {'type': 'object',\n",
       "          'description': 'object. Always ignored, exists for compatibility.\\n'},\n",
       "         'groups': {'type': 'object',\n",
       "          'description': 'object. Always ignored, exists for compatibility.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'RepeatedStratifiedKFold',\n",
       "      'func_desc': 'Repeated Stratified K-Fold cross validator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html#sklearn.model_selection.RepeatedStratifiedKFold',\n",
       "      'function_definitions': {'function_name': 'RepeatedStratifiedKFold',\n",
       "       'full_function': 'class sklearn.model_selection.RepeatedStratifiedKFold(*, n_splits=5, n_repeats=10, random_state=None)',\n",
       "       'function_text': 'Repeated Stratified K-Fold cross validator. Repeats Stratified K-Fold n times with different randomization in each\\nrepetition. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#repeated-k-fold',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'The target variable for supervised learning problems.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'Always ignored, exists for compatibility.\\n'}]},\n",
       "      'function_calling': {'name': 'RepeatedStratifiedKFold',\n",
       "       'descriptions': 'Repeated Stratified K-Fold cross validator. Repeats Stratified K-Fold n times with different randomization in each\\nrepetition. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). The target variable for supervised learning problems.\\n'},\n",
       "         'groups': {'type': 'object',\n",
       "          'description': 'object. Always ignored, exists for compatibility.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'StratifiedGroupKFold',\n",
       "      'func_desc': 'Stratified K-Fold iterator variant with non-overlapping groups.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedGroupKFold.html#sklearn.model_selection.StratifiedGroupKFold',\n",
       "      'function_definitions': {'function_name': 'StratifiedGroupKFold',\n",
       "       'full_function': 'class sklearn.model_selection.StratifiedGroupKFold(n_splits=5, shuffle=False, random_state=None)',\n",
       "       'function_text': 'Stratified K-Fold iterator variant with non-overlapping groups. This cross-validation object is a variation of StratifiedKFold attempts to\\nreturn stratified folds with non-overlapping groups. The folds are made by\\npreserving the percentage of samples for each class. Each group will appear exactly once in the test set across all folds (the\\nnumber of distinct groups has to be at least equal to the number of folds). The difference between GroupKFold\\nand StratifiedGroupKFold is that\\nthe former attempts to create balanced folds such that the number of\\ndistinct groups is approximately the same in each fold, whereas\\nStratifiedGroupKFold attempts to create folds which preserve the\\npercentage of samples for each class as much as possible given the\\nconstraint of non-overlapping groups between splits. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'func_text_user_guide': 'sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'The target variable for supervised learning problems.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Group labels for the samples used while splitting the dataset into\\ntrain/test set.\\n'}]},\n",
       "      'function_calling': {'name': 'StratifiedGroupKFold',\n",
       "       'descriptions': 'Stratified K-Fold iterator variant with non-overlapping groups. This cross-validation object is a variation of StratifiedKFold attempts to\\nreturn stratified folds with non-overlapping groups. The folds are made by\\npreserving the percentage of samples for each class. Each group will appear exactly once in the test set across all folds (the\\nnumber of distinct groups has to be at least equal to the number of folds). The difference between GroupKFold\\nand StratifiedGroupKFold is that\\nthe former attempts to create balanced folds such that the number of\\ndistinct groups is approximately the same in each fold, whereas\\nStratifiedGroupKFold attempts to create folds which preserve the\\npercentage of samples for each class as much as possible given the\\nconstraint of non-overlapping groups between splits. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. The target variable for supervised learning problems.\\n'},\n",
       "         'groups': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Group labels for the samples used while splitting the dataset into\\ntrain/test set.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'StratifiedShuffleSplit',\n",
       "      'func_desc': 'Stratified ShuffleSplit cross-validator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit',\n",
       "      'function_definitions': {'function_name': 'StratifiedShuffleSplit',\n",
       "       'full_function': 'class sklearn.model_selection.StratifiedShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None)',\n",
       "       'function_text': 'Stratified ShuffleSplit cross-validator. Provides train/test indices to split data in train/test sets. This cross-validation object is a merge of StratifiedKFold and\\nShuffleSplit, which returns stratified randomized folds. The folds\\nare made by preserving the percentage of samples for each class. Note: like the ShuffleSplit strategy, stratified random splits\\ndo not guarantee that all folds will be different, although this is\\nstill very likely for sizeable datasets. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#stratified-shuffle-split',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\nNote that providing y is sufficient to generate the splits and\\nhence np.zeros(n_samples) may be used as a placeholder for\\nX instead of actual training data.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_labels)',\n",
       "         'param_desc': 'The target variable for supervised learning problems.\\nStratification is done based on the y labels.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'Always ignored, exists for compatibility.\\n'}]},\n",
       "      'function_calling': {'name': 'StratifiedShuffleSplit',\n",
       "       'descriptions': 'Stratified ShuffleSplit cross-validator. Provides train/test indices to split data in train/test sets. This cross-validation object is a merge of StratifiedKFold and\\nShuffleSplit, which returns stratified randomized folds. The folds\\nare made by preserving the percentage of samples for each class. Note: like the ShuffleSplit strategy, stratified random splits\\ndo not guarantee that all folds will be different, although this is\\nstill very likely for sizeable datasets. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Training data, where n_samples is the number of samples\\nand n_features is the number of features.\\nNote that providing y is sufficient to generate the splits and\\nhence np.zeros(n_samples) may be used as a placeholder for\\nX instead of actual training data.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_labels). The target variable for supervised learning problems.\\nStratification is done based on the y labels.\\n'},\n",
       "         'groups': {'type': 'object',\n",
       "          'description': 'object. Always ignored, exists for compatibility.\\n'}},\n",
       "        'required': ['n_splits=10']}}},\n",
       "     {'func_name': 'check_cv',\n",
       "      'func_desc': 'Input checker utility for building a cross-validator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.check_cv.html#sklearn.model_selection.check_cv',\n",
       "      'function_definitions': {'function_name': 'check_cv',\n",
       "       'full_function': 'sklearn.model_selection.check_cv(cv=5, y=None, *, classifier=False)',\n",
       "       'function_text': 'Input checker utility for building a cross-validator.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-CV-splitter',\n",
       "       'parameter_names_desc': [{'param_name': 'cv',\n",
       "         'param_type': 'int, cross-validation generator, iterable or None, default=5',\n",
       "         'param_desc': 'Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n- None, to use the default 5-fold cross validation,\\n- integer, to specify the number of folds.\\n- CV splitter,\\n- An iterable that generates (train, test) splits as arrays of indices.\\nFor integer/None inputs, if classifier is True and y is either\\nbinary or multiclass, StratifiedKFold is used. In all other\\ncases, KFold is used.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value changed from 3-fold to 5-fold.\\n\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like, default=None',\n",
       "         'param_desc': 'The target variable for supervised learning problems.\\n'},\n",
       "        {'param_name': 'classifier',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether the task is a classification task, in which case\\nstratified KFold will be used.\\n'}]},\n",
       "      'function_calling': {'name': 'check_cv',\n",
       "       'descriptions': 'Input checker utility for building a cross-validator.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'cv': {'type': 'integer',\n",
       "          'description': 'int, cross-validation generator, iterable or None, default=5. Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n- None, to use the default 5-fold cross validation,\\n- integer, to specify the number of folds.\\n- CV splitter,\\n- An iterable that generates (train, test) splits as arrays of indices.\\nFor integer/None inputs, if classifier is True and y is either\\nbinary or multiclass, StratifiedKFold is used. In all other\\ncases, KFold is used.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value changed from 3-fold to 5-fold.\\n\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like, default=None. The target variable for supervised learning problems.\\n'},\n",
       "         'classifier': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether the task is a classification task, in which case\\nstratified KFold will be used.\\n'}},\n",
       "        'required': ['cv=5', 'y=None']}}}]},\n",
       "   {'Hyper-parameter optimizers': [{'func_name': 'GridSearchCV',\n",
       "      'func_desc': 'Exhaustive search over specified parameter values for an estimator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV',\n",
       "      'function_definitions': {'function_name': 'GridSearchCV',\n",
       "       'full_function': \"class sklearn.model_selection.GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\",\n",
       "       'function_text': 'Exhaustive search over specified parameter values for an estimator. Important members are fit, predict. GridSearchCV implements a “fit” and a “score” method.\\nIt also implements “score_samples”, “predict”, “predict_proba”,\\n“decision_function”, “transform” and “inverse_transform” if they are\\nimplemented in the estimator used. The parameters of the estimator used to apply these methods are optimized\\nby cross-validated grid-search over a parameter grid. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/grid_search.html#grid-search',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'indexable, length n_samples',\n",
       "         'param_desc': 'Must fulfill the input assumptions of the\\nunderlying estimator.\\n'}]},\n",
       "      'function_calling': {'name': 'GridSearchCV',\n",
       "       'descriptions': 'Exhaustive search over specified parameter values for an estimator. Important members are fit, predict. GridSearchCV implements a “fit” and a “score” method.\\nIt also implements “score_samples”, “predict”, “predict_proba”,\\n“decision_function”, “transform” and “inverse_transform” if they are\\nimplemented in the estimator used. The parameters of the estimator used to apply these methods are optimized\\nby cross-validated grid-search over a parameter grid. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'indexable, length n_samples. Must fulfill the input assumptions of the\\nunderlying estimator.\\n'}},\n",
       "        'required': ['estimator', 'param_grid']}}},\n",
       "     {'func_name': 'HalvingRandomSearchCV',\n",
       "      'func_desc': 'Randomized search on hyper parameters.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html#sklearn.model_selection.HalvingRandomSearchCV',\n",
       "      'function_definitions': {'function_name': 'HalvingRandomSearchCV',\n",
       "       'full_function': \"class sklearn.model_selection.HalvingRandomSearchCV(estimator, param_distributions, *, n_candidates='exhaust', factor=3, resource='n_samples', max_resources='auto', min_resources='smallest', aggressive_elimination=False, cv=5, scoring=None, refit=True, error_score=nan, return_train_score=True, random_state=None, n_jobs=None, verbose=0)\",\n",
       "       'function_text': 'Randomized search on hyper parameters. The search strategy starts evaluating all the candidates with a small\\namount of resources and iteratively selects the best candidates, using more\\nand more resources. The candidates are sampled at random from the parameter space and the\\nnumber of sampled candidates is determined by n_candidates. Read more in the User guide. Note This estimator is still experimental for now: the predictions\\nand the API might change without any deprecation cycle. To use it,\\nyou need to explicitly import enable_halving_search_cv:',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/grid_search.html#successive-halving-user-guide',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'indexable, length n_samples',\n",
       "         'param_desc': 'Must fulfill the input assumptions of the\\nunderlying estimator.\\n'}]},\n",
       "      'function_calling': {'name': 'HalvingRandomSearchCV',\n",
       "       'descriptions': 'Randomized search on hyper parameters. The search strategy starts evaluating all the candidates with a small\\namount of resources and iteratively selects the best candidates, using more\\nand more resources. The candidates are sampled at random from the parameter space and the\\nnumber of sampled candidates is determined by n_candidates. Read more in the User guide. Note This estimator is still experimental for now: the predictions\\nand the API might change without any deprecation cycle. To use it,\\nyou need to explicitly import enable_halving_search_cv:',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'indexable, length n_samples. Must fulfill the input assumptions of the\\nunderlying estimator.\\n'}},\n",
       "        'required': ['estimator', 'param_distributions']}}},\n",
       "     {'func_name': 'ParameterSampler',\n",
       "      'func_desc': 'Generator on parameters sampled from given distributions.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterSampler.html#sklearn.model_selection.ParameterSampler',\n",
       "      'function_definitions': {'function_name': 'ParameterSampler',\n",
       "       'full_function': 'class sklearn.model_selection.ParameterSampler(param_distributions, n_iter, *, random_state=None)',\n",
       "       'function_text': 'Generator on parameters sampled from given distributions. Non-deterministic iterable over random candidate combinations for hyper-\\nparameter search. If all parameters are presented as a list,\\nsampling without replacement is performed. If at least one parameter\\nis given as a distribution, sampling with replacement is used.\\nIt is highly recommended to use continuous distributions for continuous\\nparameters. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/grid_search.html#grid-search',\n",
       "       'parameter_names_desc': [{'param_name': 'param_distributions',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Dictionary with parameters names (str) as keys and distributions\\nor lists of parameters to try. Distributions must provide a rvs\\nmethod for sampling (such as those from scipy.stats.distributions).\\nIf a list is given, it is sampled uniformly.\\nIf a list of dicts is given, first a dict is sampled uniformly, and\\nthen a parameter is sampled using that dict as above.\\n'},\n",
       "        {'param_name': 'n_iter',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Number of parameter settings that are produced.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Pseudo random number generator state used for random uniform sampling\\nfrom lists of possible values instead of scipy.stats distributions.\\nPass an int for reproducible output across multiple\\nfunction calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'ParameterSampler',\n",
       "       'descriptions': 'Generator on parameters sampled from given distributions. Non-deterministic iterable over random candidate combinations for hyper-\\nparameter search. If all parameters are presented as a list,\\nsampling without replacement is performed. If at least one parameter\\nis given as a distribution, sampling with replacement is used.\\nIt is highly recommended to use continuous distributions for continuous\\nparameters. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'param_distributions': {'type': 'array',\n",
       "          'description': 'dict. Dictionary with parameters names (str) as keys and distributions\\nor lists of parameters to try. Distributions must provide a rvs\\nmethod for sampling (such as those from scipy.stats.distributions).\\nIf a list is given, it is sampled uniformly.\\nIf a list of dicts is given, first a dict is sampled uniformly, and\\nthen a parameter is sampled using that dict as above.\\n'},\n",
       "         'n_iter': {'type': 'integer',\n",
       "          'description': 'int. Number of parameter settings that are produced.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Pseudo random number generator state used for random uniform sampling\\nfrom lists of possible values instead of scipy.stats distributions.\\nPass an int for reproducible output across multiple\\nfunction calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['param_distributions', 'n_iter']}}},\n",
       "     {'func_name': 'HalvingGridSearchCV',\n",
       "      'func_desc': 'Search over specified parameter values with successive halving.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV',\n",
       "      'function_definitions': {'function_name': 'HalvingGridSearchCV',\n",
       "       'full_function': \"class sklearn.model_selection.HalvingGridSearchCV(estimator, param_grid, *, factor=3, resource='n_samples', max_resources='auto', min_resources='exhaust', aggressive_elimination=False, cv=5, scoring=None, refit=True, error_score=nan, return_train_score=True, random_state=None, n_jobs=None, verbose=0)\",\n",
       "       'function_text': 'Search over specified parameter values with successive halving. The search strategy starts evaluating all the candidates with a small\\namount of resources and iteratively selects the best candidates, using\\nmore and more resources. Read more in the User guide. Note This estimator is still experimental for now: the predictions\\nand the API might change without any deprecation cycle. To use it,\\nyou need to explicitly import enable_halving_search_cv:',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/grid_search.html#successive-halving-user-guide',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'indexable, length n_samples',\n",
       "         'param_desc': 'Must fulfill the input assumptions of the\\nunderlying estimator.\\n'}]},\n",
       "      'function_calling': {'name': 'HalvingGridSearchCV',\n",
       "       'descriptions': 'Search over specified parameter values with successive halving. The search strategy starts evaluating all the candidates with a small\\namount of resources and iteratively selects the best candidates, using\\nmore and more resources. Read more in the User guide. Note This estimator is still experimental for now: the predictions\\nand the API might change without any deprecation cycle. To use it,\\nyou need to explicitly import enable_halving_search_cv:',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'indexable, length n_samples. Must fulfill the input assumptions of the\\nunderlying estimator.\\n'}},\n",
       "        'required': ['estimator', 'param_grid']}}},\n",
       "     {'func_name': 'ParameterGrid',\n",
       "      'func_desc': 'Grid of parameters with a discrete number of values for each.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html#sklearn.model_selection.ParameterGrid',\n",
       "      'function_definitions': {'function_name': 'ParameterGrid',\n",
       "       'full_function': 'class sklearn.model_selection.ParameterGrid(param_grid)',\n",
       "       'function_text': 'Grid of parameters with a discrete number of values for each. Can be used to iterate over parameter value combinations with the\\nPython built-in function iter.\\nThe order of the generated parameter combinations is deterministic. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/grid_search.html#grid-search',\n",
       "       'parameter_names_desc': [{'param_name': 'param_grid',\n",
       "         'param_type': 'dict of str to sequence, or sequence of such',\n",
       "         'param_desc': 'The parameter grid to explore, as a dictionary mapping estimator\\nparameters to sequences of allowed values.\\nAn empty dict signifies default parameters.\\nA sequence of dicts signifies a sequence of grids to search, and is\\nuseful to avoid exploring parameter combinations that make no sense\\nor have no effect. See the examples below.\\n'}]},\n",
       "      'function_calling': {'name': 'ParameterGrid',\n",
       "       'descriptions': 'Grid of parameters with a discrete number of values for each. Can be used to iterate over parameter value combinations with the\\nPython built-in function iter.\\nThe order of the generated parameter combinations is deterministic. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'param_grid': {'type': 'string',\n",
       "          'description': 'dict of str to sequence, or sequence of such. The parameter grid to explore, as a dictionary mapping estimator\\nparameters to sequences of allowed values.\\nAn empty dict signifies default parameters.\\nA sequence of dicts signifies a sequence of grids to search, and is\\nuseful to avoid exploring parameter combinations that make no sense\\nor have no effect. See the examples below.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'RandomizedSearchCV',\n",
       "      'func_desc': 'Randomized search on hyper parameters.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV',\n",
       "      'function_definitions': {'function_name': 'RandomizedSearchCV',\n",
       "       'full_function': \"class sklearn.model_selection.RandomizedSearchCV(estimator, param_distributions, *, n_iter=10, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score=nan, return_train_score=False)\",\n",
       "       'function_text': 'Randomized search on hyper parameters. RandomizedSearchCV implements a “fit” and a “score” method.\\nIt also implements “score_samples”, “predict”, “predict_proba”,\\n“decision_function”, “transform” and “inverse_transform” if they are\\nimplemented in the estimator used. The parameters of the estimator used to apply these methods are optimized\\nby cross-validated search over parameter settings. In contrast to GridSearchCV, not all parameter values are tried out, but\\nrather a fixed number of parameter settings is sampled from the specified\\ndistributions. The number of parameter settings that are tried is\\ngiven by n_iter. If all parameters are presented as a list,\\nsampling without replacement is performed. If at least one parameter\\nis given as a distribution, sampling with replacement is used.\\nIt is highly recommended to use continuous distributions for continuous\\nparameters. Read more in the User Guide. Added in version 0.14.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'indexable, length n_samples',\n",
       "         'param_desc': 'Must fulfill the input assumptions of the\\nunderlying estimator.\\n'}]},\n",
       "      'function_calling': {'name': 'RandomizedSearchCV',\n",
       "       'descriptions': 'Randomized search on hyper parameters. RandomizedSearchCV implements a “fit” and a “score” method.\\nIt also implements “score_samples”, “predict”, “predict_proba”,\\n“decision_function”, “transform” and “inverse_transform” if they are\\nimplemented in the estimator used. The parameters of the estimator used to apply these methods are optimized\\nby cross-validated search over parameter settings. In contrast to GridSearchCV, not all parameter values are tried out, but\\nrather a fixed number of parameter settings is sampled from the specified\\ndistributions. The number of parameter settings that are tried is\\ngiven by n_iter. If all parameters are presented as a list,\\nsampling without replacement is performed. If at least one parameter\\nis given as a distribution, sampling with replacement is used.\\nIt is highly recommended to use continuous distributions for continuous\\nparameters. Read more in the User Guide. Added in version 0.14.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'indexable, length n_samples. Must fulfill the input assumptions of the\\nunderlying estimator.\\n'}},\n",
       "        'required': ['estimator', 'param_distributions']}}}]},\n",
       "   {'Post-fit model tuning': [{'func_name': 'FixedThresholdClassifier',\n",
       "      'func_desc': 'Binary classifier that manually sets the decision threshold.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.FixedThresholdClassifier.html#sklearn.model_selection.FixedThresholdClassifier',\n",
       "      'function_definitions': {'function_name': 'FixedThresholdClassifier',\n",
       "       'full_function': \"class sklearn.model_selection.FixedThresholdClassifier(estimator, *, threshold='auto', pos_label=None, response_method='auto')\",\n",
       "       'function_text': 'Binary classifier that manually sets the decision threshold. This classifier allows to change the default decision threshold used for\\nconverting posterior probability estimates (i.e. output of predict_proba) or\\ndecision scores (i.e. output of decision_function) into a class label. Here, the threshold is not optimized and is set to a constant value. Read more in the User Guide. Added in version 1.5.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/classification_threshold.html#fixedthresholdclassifier',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'FixedThresholdClassifier',\n",
       "       'descriptions': 'Binary classifier that manually sets the decision threshold. This classifier allows to change the default decision threshold used for\\nconverting posterior probability estimates (i.e. output of predict_proba) or\\ndecision scores (i.e. output of decision_function) into a class label. Here, the threshold is not optimized and is set to a constant value. Read more in the User Guide. Added in version 1.5.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['estimator']}}},\n",
       "     {'func_name': 'TunedThresholdClassifierCV',\n",
       "      'func_desc': 'Classifier that post-tunes the decision threshold using cross-validation.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TunedThresholdClassifierCV.html#sklearn.model_selection.TunedThresholdClassifierCV',\n",
       "      'function_definitions': {'function_name': 'TunedThresholdClassifierCV',\n",
       "       'full_function': \"class sklearn.model_selection.TunedThresholdClassifierCV(estimator, *, scoring='balanced_accuracy', response_method='auto', thresholds=100, cv=None, refit=True, n_jobs=None, random_state=None, store_cv_results=False)\",\n",
       "       'function_text': 'Classifier that post-tunes the decision threshold using cross-validation. This estimator post-tunes the decision threshold (cut-off point) that is\\nused for converting posterior probability estimates (i.e. output of\\npredict_proba) or decision scores (i.e. output of decision_function)\\ninto a class label. The tuning is done by optimizing a binary metric,\\npotentially constrained by a another metric. Read more in the User Guide. Added in version 1.5.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/classification_threshold.html#tunedthresholdclassifiercv',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'TunedThresholdClassifierCV',\n",
       "       'descriptions': 'Classifier that post-tunes the decision threshold using cross-validation. This estimator post-tunes the decision threshold (cut-off point) that is\\nused for converting posterior probability estimates (i.e. output of\\npredict_proba) or decision scores (i.e. output of decision_function)\\ninto a class label. The tuning is done by optimizing a binary metric,\\npotentially constrained by a another metric. Read more in the User Guide. Added in version 1.5.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['estimator']}}}]},\n",
       "   {'Model validation': [{'func_name': 'cross_val_predict',\n",
       "      'func_desc': 'Generate cross-validated estimates for each input data point.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict',\n",
       "      'function_definitions': {'function_name': 'cross_val_predict',\n",
       "       'full_function': \"sklearn.model_selection.cross_val_predict(estimator, X, y=None, *, groups=None, cv=None, n_jobs=None, verbose=0, fit_params=None, params=None, pre_dispatch='2*n_jobs', method='predict')\",\n",
       "       'function_text': 'Generate cross-validated estimates for each input data point. The data is split according to the cv parameter. Each sample belongs\\nto exactly one test set, and its prediction is computed with an\\nestimator fitted on the corresponding training set. Passing these predictions into an evaluation metric may not be a valid\\nway to measure generalization performance. Results can differ from\\ncross_validate and cross_val_score unless all tests sets\\nhave equal size and the metric decomposes over samples. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'estimator',\n",
       "         'param_desc': 'The estimator instance to use to fit the data. It must implement a fit\\nmethod and the method given by the method parameter.\\n'},\n",
       "        {'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to fit. Can be, for example a list, or an array at least 2d.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples,) or (n_samples, n_outputs),             default=None',\n",
       "         'param_desc': 'The target variable to try to predict in the case of\\nsupervised learning.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': \"Group labels for the samples used while splitting the dataset into\\ntrain/test set. Only used in conjunction with a “Group” cv\\ninstance (e.g., GroupKFold).\\n\\nChanged in version 1.4: groups can only be passed if metadata routing is not enabled\\nvia sklearn.set_config(enable_metadata_routing=True). When routing\\nis enabled, pass groups alongside other metadata via the params\\nargument instead. E.g.:\\ncross_val_predict(..., params={'groups': groups}).\\n\\n\"},\n",
       "        {'param_name': 'cv',\n",
       "         'param_type': 'int, cross-validation generator or an iterable, default=None',\n",
       "         'param_desc': 'Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\nNone, to use the default 5-fold cross validation,\\nint, to specify the number of folds in a (Stratified)KFold,\\nCV splitter,\\nAn iterable that generates (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, if the estimator is a classifier and y is\\neither binary or multiclass, StratifiedKFold is used. In all\\nother cases, KFold is used. These splitters are instantiated\\nwith shuffle=False so the splits will be the same across calls.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\\n\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of jobs to run in parallel. Training the estimator and\\npredicting are parallelized over the cross-validation splits.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'The verbosity level.\\n'},\n",
       "        {'param_name': 'fit_params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Parameters to pass to the fit method of the estimator.\\n\\nDeprecated since version 1.4: This parameter is deprecated and will be removed in version 1.6. Use\\nparams instead.\\n\\n'},\n",
       "        {'param_name': 'params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Parameters to pass to the underlying estimator’s fit and the CV\\nsplitter.\\n\\nAdded in version 1.4.\\n\\n'},\n",
       "        {'param_name': 'pre_dispatch',\n",
       "         'param_type': 'int or str, default=’2*n_jobs’',\n",
       "         'param_desc': 'Controls the number of jobs that get dispatched during parallel\\nexecution. Reducing this number can be useful to avoid an\\nexplosion of memory consumption when more jobs get dispatched\\nthan CPUs can process. This parameter can be:\\n\\n\\nNone, in which case all the jobs are immediately\\ncreated and spawned. Use this for lightweight and\\nfast-running jobs, to avoid delays due to on-demand\\nspawning of the jobs\\nAn int, giving the exact number of total jobs that are\\nspawned\\nA str, giving an expression as a function of n_jobs,\\nas in ‘2*n_jobs’\\n\\n\\n'},\n",
       "        {'param_name': 'method',\n",
       "         'param_type': '{‘predict’, ‘predict_proba’, ‘predict_log_proba’,               ‘decision_function’}, default=’predict’',\n",
       "         'param_desc': 'The method to be invoked by estimator.\\n'}]},\n",
       "      'function_calling': {'name': 'cross_val_predict',\n",
       "       'descriptions': 'Generate cross-validated estimates for each input data point. The data is split according to the cv parameter. Each sample belongs\\nto exactly one test set, and its prediction is computed with an\\nestimator fitted on the corresponding training set. Passing these predictions into an evaluation metric may not be a valid\\nway to measure generalization performance. Results can differ from\\ncross_validate and cross_val_score unless all tests sets\\nhave equal size and the metric decomposes over samples. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'estimator. The estimator instance to use to fit the data. It must implement a fit\\nmethod and the method given by the method parameter.\\n'},\n",
       "         'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data to fit. Can be, for example a list, or an array at least 2d.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples,) or (n_samples, n_outputs),             default=None. The target variable to try to predict in the case of\\nsupervised learning.\\n'},\n",
       "         'groups': {'type': 'array',\n",
       "          'description': \"array-like of shape (n_samples,), default=None. Group labels for the samples used while splitting the dataset into\\ntrain/test set. Only used in conjunction with a “Group” cv\\ninstance (e.g., GroupKFold).\\n\\nChanged in version 1.4: groups can only be passed if metadata routing is not enabled\\nvia sklearn.set_config(enable_metadata_routing=True). When routing\\nis enabled, pass groups alongside other metadata via the params\\nargument instead. E.g.:\\ncross_val_predict(..., params={'groups': groups}).\\n\\n\"},\n",
       "         'cv': {'type': 'integer',\n",
       "          'description': 'int, cross-validation generator or an iterable, default=None. Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\nNone, to use the default 5-fold cross validation,\\nint, to specify the number of folds in a (Stratified)KFold,\\nCV splitter,\\nAn iterable that generates (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, if the estimator is a classifier and y is\\neither binary or multiclass, StratifiedKFold is used. In all\\nother cases, KFold is used. These splitters are instantiated\\nwith shuffle=False so the splits will be the same across calls.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\\n\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of jobs to run in parallel. Training the estimator and\\npredicting are parallelized over the cross-validation splits.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'int, default=0. The verbosity level.\\n'},\n",
       "         'fit_params': {'type': 'array',\n",
       "          'description': 'dict, default=None. Parameters to pass to the fit method of the estimator.\\n\\nDeprecated since version 1.4: This parameter is deprecated and will be removed in version 1.6. Use\\nparams instead.\\n\\n'},\n",
       "         'params': {'type': 'array',\n",
       "          'description': 'dict, default=None. Parameters to pass to the underlying estimator’s fit and the CV\\nsplitter.\\n\\nAdded in version 1.4.\\n\\n'},\n",
       "         'pre_dispatch': {'type': 'integer',\n",
       "          'description': 'int or str, default=’2*n_jobs’. Controls the number of jobs that get dispatched during parallel\\nexecution. Reducing this number can be useful to avoid an\\nexplosion of memory consumption when more jobs get dispatched\\nthan CPUs can process. This parameter can be:\\n\\n\\nNone, in which case all the jobs are immediately\\ncreated and spawned. Use this for lightweight and\\nfast-running jobs, to avoid delays due to on-demand\\nspawning of the jobs\\nAn int, giving the exact number of total jobs that are\\nspawned\\nA str, giving an expression as a function of n_jobs,\\nas in ‘2*n_jobs’\\n\\n\\n'},\n",
       "         'method': {'type': 'array',\n",
       "          'description': '{‘predict’, ‘predict_proba’, ‘predict_log_proba’,               ‘decision_function’}, default=’predict’. The method to be invoked by estimator.\\n'}},\n",
       "        'required': ['estimator', 'X', 'y=None']}}},\n",
       "     {'func_name': 'cross_validate',\n",
       "      'func_desc': 'Evaluate metric(s) by cross-validation and also record fit/score times.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate',\n",
       "      'function_definitions': {'function_name': 'cross_validate',\n",
       "       'full_function': \"sklearn.model_selection.cross_validate(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, params=None, pre_dispatch='2*n_jobs', return_train_score=False, return_estimator=False, return_indices=False, error_score=nan)\",\n",
       "       'function_text': 'Evaluate metric(s) by cross-validation and also record fit/score times. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'estimator object implementing ‘fit’',\n",
       "         'param_desc': 'The object to use to fit the data.\\n'},\n",
       "        {'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to fit. Can be for example a list, or an array.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs), default=None',\n",
       "         'param_desc': 'The target variable to try to predict in the case of\\nsupervised learning.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': \"Group labels for the samples used while splitting the dataset into\\ntrain/test set. Only used in conjunction with a “Group” cv\\ninstance (e.g., GroupKFold).\\n\\nChanged in version 1.4: groups can only be passed if metadata routing is not enabled\\nvia sklearn.set_config(enable_metadata_routing=True). When routing\\nis enabled, pass groups alongside other metadata via the params\\nargument instead. E.g.:\\ncross_validate(..., params={'groups': groups}).\\n\\n\"},\n",
       "        {'param_name': 'scoring',\n",
       "         'param_type': 'str, callable, list, tuple, or dict, default=None',\n",
       "         'param_desc': 'Strategy to evaluate the performance of the cross-validated model on\\nthe test set.\\nIf scoring represents a single score, one can use:\\n\\na single string (see The scoring parameter: defining model evaluation rules);\\na callable (see Defining your scoring strategy from metric functions) that returns a single value.\\n\\nIf scoring represents multiple scores, one can use:\\n\\na list or tuple of unique strings;\\na callable returning a dictionary where the keys are the metric\\nnames and the values are the metric scores;\\na dictionary with metric names as keys and callables a values.\\n\\nSee Specifying multiple metrics for evaluation for an example.\\n'},\n",
       "        {'param_name': 'cv',\n",
       "         'param_type': 'int, cross-validation generator or an iterable, default=None',\n",
       "         'param_desc': 'Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\nNone, to use the default 5-fold cross validation,\\nint, to specify the number of folds in a (Stratified)KFold,\\nCV splitter,\\nAn iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, if the estimator is a classifier and y is\\neither binary or multiclass, StratifiedKFold is used. In all\\nother cases, KFold is used. These splitters are instantiated\\nwith shuffle=False so the splits will be the same across calls.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\\n\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of jobs to run in parallel. Training the estimator and computing\\nthe score are parallelized over the cross-validation splits.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'The verbosity level.\\n'},\n",
       "        {'param_name': 'fit_params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Parameters to pass to the fit method of the estimator.\\n\\nDeprecated since version 1.4: This parameter is deprecated and will be removed in version 1.6. Use\\nparams instead.\\n\\n'},\n",
       "        {'param_name': 'params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Parameters to pass to the underlying estimator’s fit, the scorer,\\nand the CV splitter.\\n\\nAdded in version 1.4.\\n\\n'},\n",
       "        {'param_name': 'pre_dispatch',\n",
       "         'param_type': 'int or str, default=’2*n_jobs’',\n",
       "         'param_desc': 'Controls the number of jobs that get dispatched during parallel\\nexecution. Reducing this number can be useful to avoid an\\nexplosion of memory consumption when more jobs get dispatched\\nthan CPUs can process. This parameter can be:\\n\\n\\nAn int, giving the exact number of total jobs that are\\nspawned\\nA str, giving an expression as a function of n_jobs,\\nas in ‘2*n_jobs’\\n\\n\\n'},\n",
       "        {'param_name': 'return_train_score',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to include train scores.\\nComputing training scores is used to get insights on how different\\nparameter settings impact the overfitting/underfitting trade-off.\\nHowever computing the scores on the training set can be computationally\\nexpensive and is not strictly required to select the parameters that\\nyield the best generalization performance.\\n\\nAdded in version 0.19.\\n\\n\\nChanged in version 0.21: Default value was changed from True to False\\n\\n'},\n",
       "        {'param_name': 'return_estimator',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to return the estimators fitted on each split.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'return_indices',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to return the train-test indices selected for each split.\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "        {'param_name': 'error_score',\n",
       "         'param_type': '‘raise’ or numeric, default=np.nan',\n",
       "         'param_desc': 'Value to assign to the score if an error occurs in estimator fitting.\\nIf set to ‘raise’, the error is raised.\\nIf a numeric value is given, FitFailedWarning is raised.\\n\\nAdded in version 0.20.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'cross_validate',\n",
       "       'descriptions': 'Evaluate metric(s) by cross-validation and also record fit/score times. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'estimator object implementing ‘fit’. The object to use to fit the data.\\n'},\n",
       "         'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data to fit. Can be for example a list, or an array.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs), default=None. The target variable to try to predict in the case of\\nsupervised learning.\\n'},\n",
       "         'groups': {'type': 'array',\n",
       "          'description': \"array-like of shape (n_samples,), default=None. Group labels for the samples used while splitting the dataset into\\ntrain/test set. Only used in conjunction with a “Group” cv\\ninstance (e.g., GroupKFold).\\n\\nChanged in version 1.4: groups can only be passed if metadata routing is not enabled\\nvia sklearn.set_config(enable_metadata_routing=True). When routing\\nis enabled, pass groups alongside other metadata via the params\\nargument instead. E.g.:\\ncross_validate(..., params={'groups': groups}).\\n\\n\"},\n",
       "         'scoring': {'type': 'string',\n",
       "          'description': 'str, callable, list, tuple, or dict, default=None. Strategy to evaluate the performance of the cross-validated model on\\nthe test set.\\nIf scoring represents a single score, one can use:\\n\\na single string (see The scoring parameter: defining model evaluation rules);\\na callable (see Defining your scoring strategy from metric functions) that returns a single value.\\n\\nIf scoring represents multiple scores, one can use:\\n\\na list or tuple of unique strings;\\na callable returning a dictionary where the keys are the metric\\nnames and the values are the metric scores;\\na dictionary with metric names as keys and callables a values.\\n\\nSee Specifying multiple metrics for evaluation for an example.\\n'},\n",
       "         'cv': {'type': 'integer',\n",
       "          'description': 'int, cross-validation generator or an iterable, default=None. Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\nNone, to use the default 5-fold cross validation,\\nint, to specify the number of folds in a (Stratified)KFold,\\nCV splitter,\\nAn iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, if the estimator is a classifier and y is\\neither binary or multiclass, StratifiedKFold is used. In all\\nother cases, KFold is used. These splitters are instantiated\\nwith shuffle=False so the splits will be the same across calls.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\\n\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of jobs to run in parallel. Training the estimator and computing\\nthe score are parallelized over the cross-validation splits.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'int, default=0. The verbosity level.\\n'},\n",
       "         'fit_params': {'type': 'array',\n",
       "          'description': 'dict, default=None. Parameters to pass to the fit method of the estimator.\\n\\nDeprecated since version 1.4: This parameter is deprecated and will be removed in version 1.6. Use\\nparams instead.\\n\\n'},\n",
       "         'params': {'type': 'array',\n",
       "          'description': 'dict, default=None. Parameters to pass to the underlying estimator’s fit, the scorer,\\nand the CV splitter.\\n\\nAdded in version 1.4.\\n\\n'},\n",
       "         'pre_dispatch': {'type': 'integer',\n",
       "          'description': 'int or str, default=’2*n_jobs’. Controls the number of jobs that get dispatched during parallel\\nexecution. Reducing this number can be useful to avoid an\\nexplosion of memory consumption when more jobs get dispatched\\nthan CPUs can process. This parameter can be:\\n\\n\\nAn int, giving the exact number of total jobs that are\\nspawned\\nA str, giving an expression as a function of n_jobs,\\nas in ‘2*n_jobs’\\n\\n\\n'},\n",
       "         'return_train_score': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to include train scores.\\nComputing training scores is used to get insights on how different\\nparameter settings impact the overfitting/underfitting trade-off.\\nHowever computing the scores on the training set can be computationally\\nexpensive and is not strictly required to select the parameters that\\nyield the best generalization performance.\\n\\nAdded in version 0.19.\\n\\n\\nChanged in version 0.21: Default value was changed from True to False\\n\\n'},\n",
       "         'return_estimator': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to return the estimators fitted on each split.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'return_indices': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to return the train-test indices selected for each split.\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "         'error_score': {'type': 'object',\n",
       "          'description': '‘raise’ or numeric, default=np.nan. Value to assign to the score if an error occurs in estimator fitting.\\nIf set to ‘raise’, the error is raised.\\nIf a numeric value is given, FitFailedWarning is raised.\\n\\nAdded in version 0.20.\\n\\n'}},\n",
       "        'required': ['estimator', 'X', 'y=None']}}},\n",
       "     {'func_name': 'permutation_test_score',\n",
       "      'func_desc': 'Evaluate the significance of a cross-validated score with permutations.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html#sklearn.model_selection.permutation_test_score',\n",
       "      'function_definitions': {'function_name': 'permutation_test_score',\n",
       "       'full_function': 'sklearn.model_selection.permutation_test_score(estimator, X, y, *, groups=None, cv=None, n_permutations=100, n_jobs=None, random_state=0, verbose=0, scoring=None, fit_params=None)',\n",
       "       'function_text': 'Evaluate the significance of a cross-validated score with permutations. Permutes targets to generate ‘randomized data’ and compute the empirical\\np-value against the null hypothesis that features and targets are\\nindependent. The p-value represents the fraction of randomized data sets where the\\nestimator performed as well or better than in the original data. A small\\np-value suggests that there is a real dependency between features and\\ntargets which has been used by the estimator to give good predictions.\\nA large p-value may be due to lack of real dependency between features\\nand targets or the estimator was not able to use the dependency to\\ngive good predictions. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#permutation-test-score',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'estimator object implementing ‘fit’',\n",
       "         'param_desc': 'The object to use to fit the data.\\n'},\n",
       "        {'param_name': 'X',\n",
       "         'param_type': 'array-like of shape at least 2D',\n",
       "         'param_desc': 'The data to fit.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs) or None',\n",
       "         'param_desc': 'The target variable to try to predict in the case of\\nsupervised learning.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Labels to constrain permutation within groups, i.e. y values\\nare permuted among samples with the same group identifier.\\nWhen not specified, y values are permuted among all samples.\\nWhen a grouped cross-validator is used, the group labels are\\nalso passed on to the split method of the cross-validator. The\\ncross-validator uses them for grouping the samples  while splitting\\nthe dataset into train/test set.\\n'},\n",
       "        {'param_name': 'cv',\n",
       "         'param_type': 'int, cross-validation generator or an iterable, default=None',\n",
       "         'param_desc': 'Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\nNone, to use the default 5-fold cross validation,\\nint, to specify the number of folds in a (Stratified)KFold,\\nCV splitter,\\nAn iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, if the estimator is a classifier and y is\\neither binary or multiclass, StratifiedKFold is used. In all\\nother cases, KFold is used. These splitters are instantiated\\nwith shuffle=False so the splits will be the same across calls.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\\n\\n'},\n",
       "        {'param_name': 'n_permutations',\n",
       "         'param_type': 'int, default=100',\n",
       "         'param_desc': 'Number of times to permute y.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of jobs to run in parallel. Training the estimator and computing\\nthe cross-validated score are parallelized over the permutations.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=0',\n",
       "         'param_desc': 'Pass an int for reproducible output for permutation of\\ny values among samples. See Glossary.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'The verbosity level.\\n'},\n",
       "        {'param_name': 'scoring',\n",
       "         'param_type': 'str or callable, default=None',\n",
       "         'param_desc': 'A single str (see The scoring parameter: defining model evaluation rules) or a callable\\n(see Defining your scoring strategy from metric functions) to evaluate the predictions on the test set.\\nIf None the estimator’s score method is used.\\n'},\n",
       "        {'param_name': 'fit_params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Parameters to pass to the fit method of the estimator.\\n\\nAdded in version 0.24.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'permutation_test_score',\n",
       "       'descriptions': 'Evaluate the significance of a cross-validated score with permutations. Permutes targets to generate ‘randomized data’ and compute the empirical\\np-value against the null hypothesis that features and targets are\\nindependent. The p-value represents the fraction of randomized data sets where the\\nestimator performed as well or better than in the original data. A small\\np-value suggests that there is a real dependency between features and\\ntargets which has been used by the estimator to give good predictions.\\nA large p-value may be due to lack of real dependency between features\\nand targets or the estimator was not able to use the dependency to\\ngive good predictions. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'estimator object implementing ‘fit’. The object to use to fit the data.\\n'},\n",
       "         'X': {'type': 'array',\n",
       "          'description': 'array-like of shape at least 2D. The data to fit.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs) or None. The target variable to try to predict in the case of\\nsupervised learning.\\n'},\n",
       "         'groups': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Labels to constrain permutation within groups, i.e. y values\\nare permuted among samples with the same group identifier.\\nWhen not specified, y values are permuted among all samples.\\nWhen a grouped cross-validator is used, the group labels are\\nalso passed on to the split method of the cross-validator. The\\ncross-validator uses them for grouping the samples  while splitting\\nthe dataset into train/test set.\\n'},\n",
       "         'cv': {'type': 'integer',\n",
       "          'description': 'int, cross-validation generator or an iterable, default=None. Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\nNone, to use the default 5-fold cross validation,\\nint, to specify the number of folds in a (Stratified)KFold,\\nCV splitter,\\nAn iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, if the estimator is a classifier and y is\\neither binary or multiclass, StratifiedKFold is used. In all\\nother cases, KFold is used. These splitters are instantiated\\nwith shuffle=False so the splits will be the same across calls.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\\n\\n'},\n",
       "         'n_permutations': {'type': 'integer',\n",
       "          'description': 'int, default=100. Number of times to permute y.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of jobs to run in parallel. Training the estimator and computing\\nthe cross-validated score are parallelized over the permutations.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=0. Pass an int for reproducible output for permutation of\\ny values among samples. See Glossary.\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'int, default=0. The verbosity level.\\n'},\n",
       "         'scoring': {'type': 'string',\n",
       "          'description': 'str or callable, default=None. A single str (see The scoring parameter: defining model evaluation rules) or a callable\\n(see Defining your scoring strategy from metric functions) to evaluate the predictions on the test set.\\nIf None the estimator’s score method is used.\\n'},\n",
       "         'fit_params': {'type': 'array',\n",
       "          'description': 'dict, default=None. Parameters to pass to the fit method of the estimator.\\n\\nAdded in version 0.24.\\n\\n'}},\n",
       "        'required': ['estimator', 'X', 'y']}}},\n",
       "     {'func_name': 'cross_val_score',\n",
       "      'func_desc': 'Evaluate a score by cross-validation.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score',\n",
       "      'function_definitions': {'function_name': 'cross_val_score',\n",
       "       'full_function': \"sklearn.model_selection.cross_val_score(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, params=None, pre_dispatch='2*n_jobs', error_score=nan)\",\n",
       "       'function_text': 'Evaluate a score by cross-validation. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'estimator object implementing ‘fit’',\n",
       "         'param_desc': 'The object to use to fit the data.\\n'},\n",
       "        {'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to fit. Can be for example a list, or an array.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None',\n",
       "         'param_desc': 'The target variable to try to predict in the case of\\nsupervised learning.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': \"Group labels for the samples used while splitting the dataset into\\ntrain/test set. Only used in conjunction with a “Group” cv\\ninstance (e.g., GroupKFold).\\n\\nChanged in version 1.4: groups can only be passed if metadata routing is not enabled\\nvia sklearn.set_config(enable_metadata_routing=True). When routing\\nis enabled, pass groups alongside other metadata via the params\\nargument instead. E.g.:\\ncross_val_score(..., params={'groups': groups}).\\n\\n\"},\n",
       "        {'param_name': 'scoring',\n",
       "         'param_type': 'str or callable, default=None',\n",
       "         'param_desc': 'A str (see model evaluation documentation) or\\na scorer callable object / function with signature\\nscorer(estimator, X, y) which should return only\\na single value.\\nSimilar to cross_validate\\nbut only a single metric is permitted.\\nIf None, the estimator’s default scorer (if available) is used.\\n'},\n",
       "        {'param_name': 'cv',\n",
       "         'param_type': 'int, cross-validation generator or an iterable, default=None',\n",
       "         'param_desc': 'Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\nNone, to use the default 5-fold cross validation,\\nint, to specify the number of folds in a (Stratified)KFold,\\nCV splitter,\\nAn iterable that generates (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, if the estimator is a classifier and y is\\neither binary or multiclass, StratifiedKFold is used. In all\\nother cases, KFold is used. These splitters are instantiated\\nwith shuffle=False so the splits will be the same across calls.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\\n\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of jobs to run in parallel. Training the estimator and computing\\nthe score are parallelized over the cross-validation splits.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'The verbosity level.\\n'},\n",
       "        {'param_name': 'fit_params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Parameters to pass to the fit method of the estimator.\\n\\nDeprecated since version 1.4: This parameter is deprecated and will be removed in version 1.6. Use\\nparams instead.\\n\\n'},\n",
       "        {'param_name': 'params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Parameters to pass to the underlying estimator’s fit, the scorer,\\nand the CV splitter.\\n\\nAdded in version 1.4.\\n\\n'},\n",
       "        {'param_name': 'pre_dispatch',\n",
       "         'param_type': 'int or str, default=’2*n_jobs’',\n",
       "         'param_desc': 'Controls the number of jobs that get dispatched during parallel\\nexecution. Reducing this number can be useful to avoid an\\nexplosion of memory consumption when more jobs get dispatched\\nthan CPUs can process. This parameter can be:\\n\\n\\nNone, in which case all the jobs are immediately\\ncreated and spawned. Use this for lightweight and\\nfast-running jobs, to avoid delays due to on-demand\\nspawning of the jobs\\nAn int, giving the exact number of total jobs that are\\nspawned\\nA str, giving an expression as a function of n_jobs,\\nas in ‘2*n_jobs’\\n\\n\\n'},\n",
       "        {'param_name': 'error_score',\n",
       "         'param_type': '‘raise’ or numeric, default=np.nan',\n",
       "         'param_desc': 'Value to assign to the score if an error occurs in estimator fitting.\\nIf set to ‘raise’, the error is raised.\\nIf a numeric value is given, FitFailedWarning is raised.\\n\\nAdded in version 0.20.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'cross_val_score',\n",
       "       'descriptions': 'Evaluate a score by cross-validation. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'estimator object implementing ‘fit’. The object to use to fit the data.\\n'},\n",
       "         'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data to fit. Can be for example a list, or an array.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None. The target variable to try to predict in the case of\\nsupervised learning.\\n'},\n",
       "         'groups': {'type': 'array',\n",
       "          'description': \"array-like of shape (n_samples,), default=None. Group labels for the samples used while splitting the dataset into\\ntrain/test set. Only used in conjunction with a “Group” cv\\ninstance (e.g., GroupKFold).\\n\\nChanged in version 1.4: groups can only be passed if metadata routing is not enabled\\nvia sklearn.set_config(enable_metadata_routing=True). When routing\\nis enabled, pass groups alongside other metadata via the params\\nargument instead. E.g.:\\ncross_val_score(..., params={'groups': groups}).\\n\\n\"},\n",
       "         'scoring': {'type': 'string',\n",
       "          'description': 'str or callable, default=None. A str (see model evaluation documentation) or\\na scorer callable object / function with signature\\nscorer(estimator, X, y) which should return only\\na single value.\\nSimilar to cross_validate\\nbut only a single metric is permitted.\\nIf None, the estimator’s default scorer (if available) is used.\\n'},\n",
       "         'cv': {'type': 'integer',\n",
       "          'description': 'int, cross-validation generator or an iterable, default=None. Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\nNone, to use the default 5-fold cross validation,\\nint, to specify the number of folds in a (Stratified)KFold,\\nCV splitter,\\nAn iterable that generates (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, if the estimator is a classifier and y is\\neither binary or multiclass, StratifiedKFold is used. In all\\nother cases, KFold is used. These splitters are instantiated\\nwith shuffle=False so the splits will be the same across calls.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\\n\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of jobs to run in parallel. Training the estimator and computing\\nthe score are parallelized over the cross-validation splits.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'int, default=0. The verbosity level.\\n'},\n",
       "         'fit_params': {'type': 'array',\n",
       "          'description': 'dict, default=None. Parameters to pass to the fit method of the estimator.\\n\\nDeprecated since version 1.4: This parameter is deprecated and will be removed in version 1.6. Use\\nparams instead.\\n\\n'},\n",
       "         'params': {'type': 'array',\n",
       "          'description': 'dict, default=None. Parameters to pass to the underlying estimator’s fit, the scorer,\\nand the CV splitter.\\n\\nAdded in version 1.4.\\n\\n'},\n",
       "         'pre_dispatch': {'type': 'integer',\n",
       "          'description': 'int or str, default=’2*n_jobs’. Controls the number of jobs that get dispatched during parallel\\nexecution. Reducing this number can be useful to avoid an\\nexplosion of memory consumption when more jobs get dispatched\\nthan CPUs can process. This parameter can be:\\n\\n\\nNone, in which case all the jobs are immediately\\ncreated and spawned. Use this for lightweight and\\nfast-running jobs, to avoid delays due to on-demand\\nspawning of the jobs\\nAn int, giving the exact number of total jobs that are\\nspawned\\nA str, giving an expression as a function of n_jobs,\\nas in ‘2*n_jobs’\\n\\n\\n'},\n",
       "         'error_score': {'type': 'object',\n",
       "          'description': '‘raise’ or numeric, default=np.nan. Value to assign to the score if an error occurs in estimator fitting.\\nIf set to ‘raise’, the error is raised.\\nIf a numeric value is given, FitFailedWarning is raised.\\n\\nAdded in version 0.20.\\n\\n'}},\n",
       "        'required': ['estimator', 'X', 'y=None']}}},\n",
       "     {'func_name': 'learning_curve',\n",
       "      'func_desc': 'Learning curve.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve',\n",
       "      'function_definitions': {'function_name': 'learning_curve',\n",
       "       'full_function': \"sklearn.model_selection.learning_curve(estimator, X, y, *, groups=None, train_sizes=array([0.1, 0.33, 0.55, 0.78, 1.]), cv=None, scoring=None, exploit_incremental_learning=False, n_jobs=None, pre_dispatch='all', verbose=0, shuffle=False, random_state=None, error_score=nan, return_times=False, fit_params=None)\",\n",
       "       'function_text': 'Learning curve. Determines cross-validated training and test scores for different training\\nset sizes. A cross-validation generator splits the whole dataset k times in training\\nand test data. Subsets of the training set with varying sizes will be used\\nto train the estimator and a score for each training subset size and the\\ntest set will be computed. Afterwards, the scores will be averaged over\\nall k runs for each training subset size. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'object type that implements the “fit” method',\n",
       "         'param_desc': 'An object of that type which is cloned for each validation. It must\\nalso implement “predict” unless scoring is a callable that doesn’t\\nrely on “predict” to compute a score.\\n'},\n",
       "        {'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training vector, where n_samples is the number of samples and\\nn_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs) or None',\n",
       "         'param_desc': 'Target relative to X for classification or regression;\\nNone for unsupervised learning.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Group labels for the samples used while splitting the dataset into\\ntrain/test set. Only used in conjunction with a “Group” cv\\ninstance (e.g., GroupKFold).\\n'},\n",
       "        {'param_name': 'train_sizes',\n",
       "         'param_type': 'array-like of shape (n_ticks,),             default=np.linspace(0.1, 1.0, 5)',\n",
       "         'param_desc': 'Relative or absolute numbers of training examples that will be used to\\ngenerate the learning curve. If the dtype is float, it is regarded as a\\nfraction of the maximum size of the training set (that is determined\\nby the selected validation method), i.e. it has to be within (0, 1].\\nOtherwise it is interpreted as absolute sizes of the training sets.\\nNote that for classification the number of samples usually have to\\nbe big enough to contain at least one sample from each class.\\n'},\n",
       "        {'param_name': 'cv',\n",
       "         'param_type': 'int, cross-validation generator or an iterable, default=None',\n",
       "         'param_desc': 'Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\nNone, to use the default 5-fold cross validation,\\nint, to specify the number of folds in a (Stratified)KFold,\\nCV splitter,\\nAn iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, if the estimator is a classifier and y is\\neither binary or multiclass, StratifiedKFold is used. In all\\nother cases, KFold is used. These splitters are instantiated\\nwith shuffle=False so the splits will be the same across calls.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\\n\\n'},\n",
       "        {'param_name': 'scoring',\n",
       "         'param_type': 'str or callable, default=None',\n",
       "         'param_desc': 'A str (see model evaluation documentation) or\\na scorer callable object / function with signature\\nscorer(estimator, X, y).\\n'},\n",
       "        {'param_name': 'exploit_incremental_learning',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If the estimator supports incremental learning, this will be\\nused to speed up fitting for different training set sizes.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of jobs to run in parallel. Training the estimator and computing\\nthe score are parallelized over the different training and test sets.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': 'pre_dispatch',\n",
       "         'param_type': 'int or str, default=’all’',\n",
       "         'param_desc': 'Number of predispatched jobs for parallel execution (default is\\nall). The option can reduce the allocated memory. The str can\\nbe an expression like ‘2*n_jobs’.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Controls the verbosity: the higher, the more messages.\\n'},\n",
       "        {'param_name': 'shuffle',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to shuffle training data before taking prefixes of it\\nbased on``train_sizes``.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Used when shuffle is True. Pass an int for reproducible\\noutput across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'error_score',\n",
       "         'param_type': '‘raise’ or numeric, default=np.nan',\n",
       "         'param_desc': 'Value to assign to the score if an error occurs in estimator fitting.\\nIf set to ‘raise’, the error is raised.\\nIf a numeric value is given, FitFailedWarning is raised.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'return_times',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to return the fit and score times.\\n'},\n",
       "        {'param_name': 'fit_params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Parameters to pass to the fit method of the estimator.\\n\\nAdded in version 0.24.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'learning_curve',\n",
       "       'descriptions': 'Learning curve. Determines cross-validated training and test scores for different training\\nset sizes. A cross-validation generator splits the whole dataset k times in training\\nand test data. Subsets of the training set with varying sizes will be used\\nto train the estimator and a score for each training subset size and the\\ntest set will be computed. Afterwards, the scores will be averaged over\\nall k runs for each training subset size. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'object type that implements the “fit” method. An object of that type which is cloned for each validation. It must\\nalso implement “predict” unless scoring is a callable that doesn’t\\nrely on “predict” to compute a score.\\n'},\n",
       "         'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Training vector, where n_samples is the number of samples and\\nn_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs) or None. Target relative to X for classification or regression;\\nNone for unsupervised learning.\\n'},\n",
       "         'groups': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Group labels for the samples used while splitting the dataset into\\ntrain/test set. Only used in conjunction with a “Group” cv\\ninstance (e.g., GroupKFold).\\n'},\n",
       "         'train_sizes': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_ticks,),             default=np.linspace(0.1, 1.0, 5). Relative or absolute numbers of training examples that will be used to\\ngenerate the learning curve. If the dtype is float, it is regarded as a\\nfraction of the maximum size of the training set (that is determined\\nby the selected validation method), i.e. it has to be within (0, 1].\\nOtherwise it is interpreted as absolute sizes of the training sets.\\nNote that for classification the number of samples usually have to\\nbe big enough to contain at least one sample from each class.\\n'},\n",
       "         'cv': {'type': 'integer',\n",
       "          'description': 'int, cross-validation generator or an iterable, default=None. Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\nNone, to use the default 5-fold cross validation,\\nint, to specify the number of folds in a (Stratified)KFold,\\nCV splitter,\\nAn iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, if the estimator is a classifier and y is\\neither binary or multiclass, StratifiedKFold is used. In all\\nother cases, KFold is used. These splitters are instantiated\\nwith shuffle=False so the splits will be the same across calls.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\\n\\n'},\n",
       "         'scoring': {'type': 'string',\n",
       "          'description': 'str or callable, default=None. A str (see model evaluation documentation) or\\na scorer callable object / function with signature\\nscorer(estimator, X, y).\\n'},\n",
       "         'exploit_incremental_learning': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If the estimator supports incremental learning, this will be\\nused to speed up fitting for different training set sizes.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of jobs to run in parallel. Training the estimator and computing\\nthe score are parallelized over the different training and test sets.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "         'pre_dispatch': {'type': 'integer',\n",
       "          'description': 'int or str, default=’all’. Number of predispatched jobs for parallel execution (default is\\nall). The option can reduce the allocated memory. The str can\\nbe an expression like ‘2*n_jobs’.\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'int, default=0. Controls the verbosity: the higher, the more messages.\\n'},\n",
       "         'shuffle': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to shuffle training data before taking prefixes of it\\nbased on``train_sizes``.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Used when shuffle is True. Pass an int for reproducible\\noutput across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'error_score': {'type': 'object',\n",
       "          'description': '‘raise’ or numeric, default=np.nan. Value to assign to the score if an error occurs in estimator fitting.\\nIf set to ‘raise’, the error is raised.\\nIf a numeric value is given, FitFailedWarning is raised.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'return_times': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to return the fit and score times.\\n'},\n",
       "         'fit_params': {'type': 'array',\n",
       "          'description': 'dict, default=None. Parameters to pass to the fit method of the estimator.\\n\\nAdded in version 0.24.\\n\\n'}},\n",
       "        'required': ['estimator', 'X', 'y']}}},\n",
       "     {'func_name': 'validation_curve',\n",
       "      'func_desc': 'Validation curve.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html#sklearn.model_selection.validation_curve',\n",
       "      'function_definitions': {'function_name': 'validation_curve',\n",
       "       'full_function': \"sklearn.model_selection.validation_curve(estimator, X, y, *, param_name, param_range, groups=None, cv=None, scoring=None, n_jobs=None, pre_dispatch='all', verbose=0, error_score=nan, fit_params=None)\",\n",
       "       'function_text': 'Validation curve. Determine training and test scores for varying parameter values. Compute scores for an estimator with different values of a specified\\nparameter. This is similar to grid search with one parameter. However, this\\nwill also compute training scores and is merely a utility for plotting the\\nresults. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/learning_curve.html#validation-curve',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'object type that implements the “fit” method',\n",
       "         'param_desc': 'An object of that type which is cloned for each validation. It must\\nalso implement “predict” unless scoring is a callable that doesn’t\\nrely on “predict” to compute a score.\\n'},\n",
       "        {'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training vector, where n_samples is the number of samples and\\nn_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,) or (n_samples, n_outputs) or None',\n",
       "         'param_desc': 'Target relative to X for classification or regression;\\nNone for unsupervised learning.\\n'},\n",
       "        {'param_name': 'param_name',\n",
       "         'param_type': 'str',\n",
       "         'param_desc': 'Name of the parameter that will be varied.\\n'},\n",
       "        {'param_name': 'param_range',\n",
       "         'param_type': 'array-like of shape (n_values,)',\n",
       "         'param_desc': 'The values of the parameter that will be evaluated.\\n'},\n",
       "        {'param_name': 'groups',\n",
       "         'param_type': 'array-like of shape (n_samples,), default=None',\n",
       "         'param_desc': 'Group labels for the samples used while splitting the dataset into\\ntrain/test set. Only used in conjunction with a “Group” cv\\ninstance (e.g., GroupKFold).\\n'},\n",
       "        {'param_name': 'cv',\n",
       "         'param_type': 'int, cross-validation generator or an iterable, default=None',\n",
       "         'param_desc': 'Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\nNone, to use the default 5-fold cross validation,\\nint, to specify the number of folds in a (Stratified)KFold,\\nCV splitter,\\nAn iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, if the estimator is a classifier and y is\\neither binary or multiclass, StratifiedKFold is used. In all\\nother cases, KFold is used. These splitters are instantiated\\nwith shuffle=False so the splits will be the same across calls.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\\n\\n'},\n",
       "        {'param_name': 'scoring',\n",
       "         'param_type': 'str or callable, default=None',\n",
       "         'param_desc': 'A str (see model evaluation documentation) or\\na scorer callable object / function with signature\\nscorer(estimator, X, y).\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of jobs to run in parallel. Training the estimator and computing\\nthe score are parallelized over the combinations of each parameter\\nvalue and each cross-validation split.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "        {'param_name': 'pre_dispatch',\n",
       "         'param_type': 'int or str, default=’all’',\n",
       "         'param_desc': 'Number of predispatched jobs for parallel execution (default is\\nall). The option can reduce the allocated memory. The str can\\nbe an expression like ‘2*n_jobs’.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Controls the verbosity: the higher, the more messages.\\n'},\n",
       "        {'param_name': 'error_score',\n",
       "         'param_type': '‘raise’ or numeric, default=np.nan',\n",
       "         'param_desc': 'Value to assign to the score if an error occurs in estimator fitting.\\nIf set to ‘raise’, the error is raised.\\nIf a numeric value is given, FitFailedWarning is raised.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'fit_params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Parameters to pass to the fit method of the estimator.\\n\\nAdded in version 0.24.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'validation_curve',\n",
       "       'descriptions': 'Validation curve. Determine training and test scores for varying parameter values. Compute scores for an estimator with different values of a specified\\nparameter. This is similar to grid search with one parameter. However, this\\nwill also compute training scores and is merely a utility for plotting the\\nresults. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'object type that implements the “fit” method. An object of that type which is cloned for each validation. It must\\nalso implement “predict” unless scoring is a callable that doesn’t\\nrely on “predict” to compute a score.\\n'},\n",
       "         'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Training vector, where n_samples is the number of samples and\\nn_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,) or (n_samples, n_outputs) or None. Target relative to X for classification or regression;\\nNone for unsupervised learning.\\n'},\n",
       "         'param_name': {'type': 'string',\n",
       "          'description': 'str. Name of the parameter that will be varied.\\n'},\n",
       "         'param_range': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_values,). The values of the parameter that will be evaluated.\\n'},\n",
       "         'groups': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,), default=None. Group labels for the samples used while splitting the dataset into\\ntrain/test set. Only used in conjunction with a “Group” cv\\ninstance (e.g., GroupKFold).\\n'},\n",
       "         'cv': {'type': 'integer',\n",
       "          'description': 'int, cross-validation generator or an iterable, default=None. Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\nNone, to use the default 5-fold cross validation,\\nint, to specify the number of folds in a (Stratified)KFold,\\nCV splitter,\\nAn iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, if the estimator is a classifier and y is\\neither binary or multiclass, StratifiedKFold is used. In all\\nother cases, KFold is used. These splitters are instantiated\\nwith shuffle=False so the splits will be the same across calls.\\nRefer User Guide for the various\\ncross-validation strategies that can be used here.\\n\\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\\n\\n'},\n",
       "         'scoring': {'type': 'string',\n",
       "          'description': 'str or callable, default=None. A str (see model evaluation documentation) or\\na scorer callable object / function with signature\\nscorer(estimator, X, y).\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of jobs to run in parallel. Training the estimator and computing\\nthe score are parallelized over the combinations of each parameter\\nvalue and each cross-validation split.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'},\n",
       "         'pre_dispatch': {'type': 'integer',\n",
       "          'description': 'int or str, default=’all’. Number of predispatched jobs for parallel execution (default is\\nall). The option can reduce the allocated memory. The str can\\nbe an expression like ‘2*n_jobs’.\\n'},\n",
       "         'verbose': {'type': 'integer',\n",
       "          'description': 'int, default=0. Controls the verbosity: the higher, the more messages.\\n'},\n",
       "         'error_score': {'type': 'object',\n",
       "          'description': '‘raise’ or numeric, default=np.nan. Value to assign to the score if an error occurs in estimator fitting.\\nIf set to ‘raise’, the error is raised.\\nIf a numeric value is given, FitFailedWarning is raised.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'fit_params': {'type': 'array',\n",
       "          'description': 'dict, default=None. Parameters to pass to the fit method of the estimator.\\n\\nAdded in version 0.24.\\n\\n'}},\n",
       "        'required': ['estimator', 'X', 'y']}}}]},\n",
       "   {'Visualization': [{'func_name': 'LearningCurveDisplay',\n",
       "      'func_desc': 'Learning Curve visualization.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LearningCurveDisplay.html#sklearn.model_selection.LearningCurveDisplay',\n",
       "      'function_definitions': {'function_name': 'LearningCurveDisplay',\n",
       "       'full_function': 'class sklearn.model_selection.LearningCurveDisplay(*, train_sizes, train_scores, test_scores, score_name=None)',\n",
       "       'function_text': 'Learning Curve visualization. It is recommended to use\\nfrom_estimator to\\ncreate a LearningCurveDisplay instance.\\nAll parameters are stored as attributes. Read more in the User Guide for general information\\nabout the visualization API and\\ndetailed documentation regarding the learning\\ncurve visualization. Added in version 1.2.',\n",
       "       'func_text_user_guide': '#sklearn.model_selection.LearningCurveDisplay.from_estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'ax',\n",
       "         'param_type': 'matplotlib Axes, default=None',\n",
       "         'param_desc': 'Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "        {'param_name': 'negate_score',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether or not to negate the scores obtained through\\nlearning_curve. This is\\nparticularly useful when using the error denoted by neg_* in\\nscikit-learn.\\n'},\n",
       "        {'param_name': 'score_name',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'The name of the score used to decorate the y-axis of the plot. It will\\noverride the name inferred from the scoring parameter. If score is\\nNone, we use \"Score\" if negate_score is False and \"Negative score\"\\notherwise. If scoring is a string or a callable, we infer the name. We\\nreplace _ by spaces and capitalize the first letter. We remove neg_ and\\nreplace it by \"Negative\" if negate_score is\\nFalse or just remove it otherwise.\\n'},\n",
       "        {'param_name': 'score_type',\n",
       "         'param_type': '{“test”, “train”, “both”}, default=”both”',\n",
       "         'param_desc': 'The type of score to plot. Can be one of \"test\", \"train\", or\\n\"both\".\\n'},\n",
       "        {'param_name': 'std_display_style',\n",
       "         'param_type': '{“errorbar”, “fill_between”} or None, default=”fill_between”',\n",
       "         'param_desc': 'The style used to display the score standard deviation around the\\nmean score. If None, no standard deviation representation is\\ndisplayed.\\n'},\n",
       "        {'param_name': 'line_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Additional keyword arguments passed to the plt.plot used to draw\\nthe mean score.\\n'},\n",
       "        {'param_name': 'fill_between_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Additional keyword arguments passed to the plt.fill_between used\\nto draw the score standard deviation.\\n'},\n",
       "        {'param_name': 'errorbar_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Additional keyword arguments passed to the plt.errorbar used to\\ndraw mean score and standard deviation score.\\n'}]},\n",
       "      'function_calling': {'name': 'LearningCurveDisplay',\n",
       "       'descriptions': 'Learning Curve visualization. It is recommended to use\\nfrom_estimator to\\ncreate a LearningCurveDisplay instance.\\nAll parameters are stored as attributes. Read more in the User Guide for general information\\nabout the visualization API and\\ndetailed documentation regarding the learning\\ncurve visualization. Added in version 1.2.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'ax': {'type': 'object',\n",
       "          'description': 'matplotlib Axes, default=None. Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "         'negate_score': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether or not to negate the scores obtained through\\nlearning_curve. This is\\nparticularly useful when using the error denoted by neg_* in\\nscikit-learn.\\n'},\n",
       "         'score_name': {'type': 'string',\n",
       "          'description': 'str, default=None. The name of the score used to decorate the y-axis of the plot. It will\\noverride the name inferred from the scoring parameter. If score is\\nNone, we use \"Score\" if negate_score is False and \"Negative score\"\\notherwise. If scoring is a string or a callable, we infer the name. We\\nreplace _ by spaces and capitalize the first letter. We remove neg_ and\\nreplace it by \"Negative\" if negate_score is\\nFalse or just remove it otherwise.\\n'},\n",
       "         'score_type': {'type': 'string',\n",
       "          'enum': ['test', 'train', 'both'],\n",
       "          'description': '{“test”, “train”, “both”}, default=”both”. The type of score to plot. Can be one of \"test\", \"train\", or\\n\"both\".\\n'},\n",
       "         'std_display_style': {'type': 'string',\n",
       "          'enum': ['errorbar', 'fill_between'],\n",
       "          'description': '{“errorbar”, “fill_between”} or None, default=”fill_between”. The style used to display the score standard deviation around the\\nmean score. If None, no standard deviation representation is\\ndisplayed.\\n'},\n",
       "         'line_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Additional keyword arguments passed to the plt.plot used to draw\\nthe mean score.\\n'},\n",
       "         'fill_between_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Additional keyword arguments passed to the plt.fill_between used\\nto draw the score standard deviation.\\n'},\n",
       "         'errorbar_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Additional keyword arguments passed to the plt.errorbar used to\\ndraw mean score and standard deviation score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'ValidationCurveDisplay',\n",
       "      'func_desc': 'Validation Curve visualization.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ValidationCurveDisplay.html#sklearn.model_selection.ValidationCurveDisplay',\n",
       "      'function_definitions': {'function_name': 'ValidationCurveDisplay',\n",
       "       'full_function': 'class sklearn.model_selection.ValidationCurveDisplay(*, param_name, param_range, train_scores, test_scores, score_name=None)',\n",
       "       'function_text': 'Validation Curve visualization. It is recommended to use\\nfrom_estimator to\\ncreate a ValidationCurveDisplay instance.\\nAll parameters are stored as attributes. Read more in the User Guide for general information\\nabout the visualization API and detailed documentation regarding the validation curve visualization. Added in version 1.3.',\n",
       "       'func_text_user_guide': '#sklearn.model_selection.ValidationCurveDisplay.from_estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'ax',\n",
       "         'param_type': 'matplotlib Axes, default=None',\n",
       "         'param_desc': 'Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "        {'param_name': 'negate_score',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether or not to negate the scores obtained through\\nvalidation_curve. This is\\nparticularly useful when using the error denoted by neg_* in\\nscikit-learn.\\n'},\n",
       "        {'param_name': 'score_name',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'The name of the score used to decorate the y-axis of the plot. It will\\noverride the name inferred from the scoring parameter. If score is\\nNone, we use \"Score\" if negate_score is False and \"Negative score\"\\notherwise. If scoring is a string or a callable, we infer the name. We\\nreplace _ by spaces and capitalize the first letter. We remove neg_ and\\nreplace it by \"Negative\" if negate_score is\\nFalse or just remove it otherwise.\\n'},\n",
       "        {'param_name': 'score_type',\n",
       "         'param_type': '{“test”, “train”, “both”}, default=”both”',\n",
       "         'param_desc': 'The type of score to plot. Can be one of \"test\", \"train\", or\\n\"both\".\\n'},\n",
       "        {'param_name': 'std_display_style',\n",
       "         'param_type': '{“errorbar”, “fill_between”} or None, default=”fill_between”',\n",
       "         'param_desc': 'The style used to display the score standard deviation around the\\nmean score. If None, no standard deviation representation is\\ndisplayed.\\n'},\n",
       "        {'param_name': 'line_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Additional keyword arguments passed to the plt.plot used to draw\\nthe mean score.\\n'},\n",
       "        {'param_name': 'fill_between_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Additional keyword arguments passed to the plt.fill_between used\\nto draw the score standard deviation.\\n'},\n",
       "        {'param_name': 'errorbar_kw',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Additional keyword arguments passed to the plt.errorbar used to\\ndraw mean score and standard deviation score.\\n'}]},\n",
       "      'function_calling': {'name': 'ValidationCurveDisplay',\n",
       "       'descriptions': 'Validation Curve visualization. It is recommended to use\\nfrom_estimator to\\ncreate a ValidationCurveDisplay instance.\\nAll parameters are stored as attributes. Read more in the User Guide for general information\\nabout the visualization API and detailed documentation regarding the validation curve visualization. Added in version 1.3.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'ax': {'type': 'object',\n",
       "          'description': 'matplotlib Axes, default=None. Axes object to plot on. If None, a new figure and axes is\\ncreated.\\n'},\n",
       "         'negate_score': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether or not to negate the scores obtained through\\nvalidation_curve. This is\\nparticularly useful when using the error denoted by neg_* in\\nscikit-learn.\\n'},\n",
       "         'score_name': {'type': 'string',\n",
       "          'description': 'str, default=None. The name of the score used to decorate the y-axis of the plot. It will\\noverride the name inferred from the scoring parameter. If score is\\nNone, we use \"Score\" if negate_score is False and \"Negative score\"\\notherwise. If scoring is a string or a callable, we infer the name. We\\nreplace _ by spaces and capitalize the first letter. We remove neg_ and\\nreplace it by \"Negative\" if negate_score is\\nFalse or just remove it otherwise.\\n'},\n",
       "         'score_type': {'type': 'string',\n",
       "          'enum': ['test', 'train', 'both'],\n",
       "          'description': '{“test”, “train”, “both”}, default=”both”. The type of score to plot. Can be one of \"test\", \"train\", or\\n\"both\".\\n'},\n",
       "         'std_display_style': {'type': 'string',\n",
       "          'enum': ['errorbar', 'fill_between'],\n",
       "          'description': '{“errorbar”, “fill_between”} or None, default=”fill_between”. The style used to display the score standard deviation around the\\nmean score. If None, no standard deviation representation is\\ndisplayed.\\n'},\n",
       "         'line_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Additional keyword arguments passed to the plt.plot used to draw\\nthe mean score.\\n'},\n",
       "         'fill_between_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Additional keyword arguments passed to the plt.fill_between used\\nto draw the score standard deviation.\\n'},\n",
       "         'errorbar_kw': {'type': 'array',\n",
       "          'description': 'dict, default=None. Additional keyword arguments passed to the plt.errorbar used to\\ndraw mean score and standard deviation score.\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.model_selection',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.model_selection.html'},\n",
       " 'sklearn.multiclass.html': {'functions': [{'defaults': [{'func_name': 'OneVsOneClassifier',\n",
       "      'func_desc': 'One-vs-one multiclass strategy.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier',\n",
       "      'function_definitions': {'function_name': 'OneVsOneClassifier',\n",
       "       'full_function': 'class sklearn.multiclass.OneVsOneClassifier(estimator, *, n_jobs=None)',\n",
       "       'function_text': 'One-vs-one multiclass strategy. This strategy consists in fitting one classifier per class pair.\\nAt prediction time, the class which received the most votes is selected.\\nSince it requires to fit n_classes * (n_classes - 1) / 2 classifiers,\\nthis method is usually slower than one-vs-the-rest, due to its\\nO(n_classes^2) complexity. However, this method may be advantageous for\\nalgorithms such as kernel algorithms which don’t scale well with\\nn_samples. This is because each individual learning problem only involves\\na small subset of the data whereas, with one-vs-the-rest, the complete\\ndataset is used n_classes times. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/multiclass.html#ovo-classification',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'OneVsOneClassifier',\n",
       "       'descriptions': 'One-vs-one multiclass strategy. This strategy consists in fitting one classifier per class pair.\\nAt prediction time, the class which received the most votes is selected.\\nSince it requires to fit n_classes * (n_classes - 1) / 2 classifiers,\\nthis method is usually slower than one-vs-the-rest, due to its\\nO(n_classes^2) complexity. However, this method may be advantageous for\\nalgorithms such as kernel algorithms which don’t scale well with\\nn_samples. This is because each individual learning problem only involves\\na small subset of the data whereas, with one-vs-the-rest, the complete\\ndataset is used n_classes times. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['estimator']}}},\n",
       "     {'func_name': 'OutputCodeClassifier',\n",
       "      'func_desc': '(Error-Correcting) Output-Code multiclass strategy.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OutputCodeClassifier.html#sklearn.multiclass.OutputCodeClassifier',\n",
       "      'function_definitions': {'function_name': 'OutputCodeClassifier',\n",
       "       'full_function': 'class sklearn.multiclass.OutputCodeClassifier(estimator, *, code_size=1.5, random_state=None, n_jobs=None)',\n",
       "       'function_text': '(Error-Correcting) Output-Code multiclass strategy. Output-code based strategies consist in representing each class with a\\nbinary code (an array of 0s and 1s). At fitting time, one binary\\nclassifier per bit in the code book is fitted.  At prediction time, the\\nclassifiers are used to project new points in the class space and the class\\nclosest to the points is chosen. The main advantage of these strategies is\\nthat the number of classifiers used can be controlled by the user, either\\nfor compressing the model (0 < code_size < 1) or for making the model more\\nrobust to errors (code_size > 1). See the documentation for more details. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/multiclass.html#ecoc',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'OutputCodeClassifier',\n",
       "       'descriptions': '(Error-Correcting) Output-Code multiclass strategy. Output-code based strategies consist in representing each class with a\\nbinary code (an array of 0s and 1s). At fitting time, one binary\\nclassifier per bit in the code book is fitted.  At prediction time, the\\nclassifiers are used to project new points in the class space and the class\\nclosest to the points is chosen. The main advantage of these strategies is\\nthat the number of classifiers used can be controlled by the user, either\\nfor compressing the model (0 < code_size < 1) or for making the model more\\nrobust to errors (code_size > 1). See the documentation for more details. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['estimator']}}},\n",
       "     {'func_name': 'OneVsRestClassifier',\n",
       "      'func_desc': 'One-vs-the-rest (OvR) multiclass strategy.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier',\n",
       "      'function_definitions': {'function_name': 'OneVsRestClassifier',\n",
       "       'full_function': 'class sklearn.multiclass.OneVsRestClassifier(estimator, *, n_jobs=None, verbose=0)',\n",
       "       'function_text': 'One-vs-the-rest (OvR) multiclass strategy. Also known as one-vs-all, this strategy consists in fitting one classifier\\nper class. For each classifier, the class is fitted against all the other\\nclasses. In addition to its computational efficiency (only n_classes\\nclassifiers are needed), one advantage of this approach is its\\ninterpretability. Since each class is represented by one and one classifier\\nonly, it is possible to gain knowledge about the class by inspecting its\\ncorresponding classifier. This is the most commonly used strategy for\\nmulticlass classification and is a fair default choice. OneVsRestClassifier can also be used for multilabel classification. To use\\nthis feature, provide an indicator matrix for the target y when calling\\n.fit. In other words, the target labels should be formatted as a 2D\\nbinary (0/1) matrix, where [i, j] == 1 indicates the presence of label j\\nin sample i. This estimator uses the binary relevance method to perform\\nmultilabel classification, which involves training one binary classifier\\nindependently for each label. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/multiclass.html#ovr-classification',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'OneVsRestClassifier',\n",
       "       'descriptions': 'One-vs-the-rest (OvR) multiclass strategy. Also known as one-vs-all, this strategy consists in fitting one classifier\\nper class. For each classifier, the class is fitted against all the other\\nclasses. In addition to its computational efficiency (only n_classes\\nclassifiers are needed), one advantage of this approach is its\\ninterpretability. Since each class is represented by one and one classifier\\nonly, it is possible to gain knowledge about the class by inspecting its\\ncorresponding classifier. This is the most commonly used strategy for\\nmulticlass classification and is a fair default choice. OneVsRestClassifier can also be used for multilabel classification. To use\\nthis feature, provide an indicator matrix for the target y when calling\\n.fit. In other words, the target labels should be formatted as a 2D\\nbinary (0/1) matrix, where [i, j] == 1 indicates the presence of label j\\nin sample i. This estimator uses the binary relevance method to perform\\nmultilabel classification, which involves training one binary classifier\\nindependently for each label. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['estimator']}}}]}],\n",
       "  'name': 'sklearn.multiclass',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.multiclass.html'},\n",
       " 'sklearn.multioutput.html': {'functions': [{'defaults': [{'func_name': 'ClassifierChain',\n",
       "      'func_desc': 'A multi-label model that arranges binary classifiers into a chain.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.ClassifierChain.html#sklearn.multioutput.ClassifierChain',\n",
       "      'function_definitions': {'function_name': 'ClassifierChain',\n",
       "       'full_function': \"class sklearn.multioutput.ClassifierChain(base_estimator, *, order=None, cv=None, chain_method='predict', random_state=None, verbose=False)\",\n",
       "       'function_text': 'A multi-label model that arranges binary classifiers into a chain. Each model makes a prediction in the order specified by the chain using\\nall of the available features provided to the model plus the predictions\\nof models that are earlier in the chain. For an example of how to use ClassifierChain and benefit from its\\nensemble, see\\nClassifierChain on a yeast dataset example. Read more in the User Guide. Added in version 0.19.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/multioutput/plot_classifier_chain_yeast.html#sphx-glr-auto-examples-multioutput-plot-classifier-chain-yeast-py',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'ClassifierChain',\n",
       "       'descriptions': 'A multi-label model that arranges binary classifiers into a chain. Each model makes a prediction in the order specified by the chain using\\nall of the available features provided to the model plus the predictions\\nof models that are earlier in the chain. For an example of how to use ClassifierChain and benefit from its\\nensemble, see\\nClassifierChain on a yeast dataset example. Read more in the User Guide. Added in version 0.19.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['base_estimator']}}},\n",
       "     {'func_name': 'MultiOutputRegressor',\n",
       "      'func_desc': 'Multi target regression.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputRegressor.html#sklearn.multioutput.MultiOutputRegressor',\n",
       "      'function_definitions': {'function_name': 'MultiOutputRegressor',\n",
       "       'full_function': 'class sklearn.multioutput.MultiOutputRegressor(estimator, *, n_jobs=None)',\n",
       "       'function_text': 'Multi target regression. This strategy consists of fitting one regressor per target. This is a\\nsimple strategy for extending regressors that do not natively support\\nmulti-target regression. Added in version 0.18.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-fit',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'MultiOutputRegressor',\n",
       "       'descriptions': 'Multi target regression. This strategy consists of fitting one regressor per target. This is a\\nsimple strategy for extending regressors that do not natively support\\nmulti-target regression. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['estimator']}}},\n",
       "     {'func_name': 'MultiOutputClassifier',\n",
       "      'func_desc': 'Multi target classification.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html#sklearn.multioutput.MultiOutputClassifier',\n",
       "      'function_definitions': {'function_name': 'MultiOutputClassifier',\n",
       "       'full_function': 'class sklearn.multioutput.MultiOutputClassifier(estimator, *, n_jobs=None)',\n",
       "       'function_text': 'Multi target classification. This strategy consists of fitting one classifier per target. This is a\\nsimple strategy for extending classifiers that do not natively support\\nmulti-target classification.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-fit',\n",
       "       'parameter_names_desc': [{'param_name': 'classes',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for classes parameter in partial_fit.\\n'},\n",
       "        {'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in partial_fit.\\n'}]},\n",
       "      'function_calling': {'name': 'MultiOutputClassifier',\n",
       "       'descriptions': 'Multi target classification. This strategy consists of fitting one classifier per target. This is a\\nsimple strategy for extending classifiers that do not natively support\\nmulti-target classification.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'classes': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for classes parameter in partial_fit.\\n'},\n",
       "         'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in partial_fit.\\n'}},\n",
       "        'required': ['estimator']}}},\n",
       "     {'func_name': 'RegressorChain',\n",
       "      'func_desc': 'A multi-label model that arranges regressions into a chain.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.RegressorChain.html#sklearn.multioutput.RegressorChain',\n",
       "      'function_definitions': {'function_name': 'RegressorChain',\n",
       "       'full_function': 'class sklearn.multioutput.RegressorChain(base_estimator, *, order=None, cv=None, random_state=None, verbose=False)',\n",
       "       'function_text': 'A multi-label model that arranges regressions into a chain. Each model makes a prediction in the order specified by the chain using\\nall of the available features provided to the model plus the predictions\\nof models that are earlier in the chain. Read more in the User Guide. Added in version 0.20.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/multiclass.html#regressorchain',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'RegressorChain',\n",
       "       'descriptions': 'A multi-label model that arranges regressions into a chain. Each model makes a prediction in the order specified by the chain using\\nall of the available features provided to the model plus the predictions\\nof models that are earlier in the chain. Read more in the User Guide. Added in version 0.20.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['base_estimator']}}}]}],\n",
       "  'name': 'sklearn.multioutput',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.multioutput.html'},\n",
       " 'sklearn.naive_bayes.html': {'functions': [{'defaults': [{'func_name': 'BernoulliNB',\n",
       "      'func_desc': 'Naive Bayes classifier for multivariate Bernoulli models.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB',\n",
       "      'function_definitions': {'function_name': 'BernoulliNB',\n",
       "       'full_function': 'class sklearn.naive_bayes.BernoulliNB(*, alpha=1.0, force_alpha=True, binarize=0.0, fit_prior=True, class_prior=None)',\n",
       "       'function_text': 'Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The\\ndifference is that while MultinomialNB works with occurrence counts,\\nBernoulliNB is designed for binary/boolean features. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'BernoulliNB',\n",
       "       'descriptions': 'Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The\\ndifference is that while MultinomialNB works with occurrence counts,\\nBernoulliNB is designed for binary/boolean features. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'ComplementNB',\n",
       "      'func_desc': 'The Complement Naive Bayes classifier described in Rennie et al. (2003).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html#sklearn.naive_bayes.ComplementNB',\n",
       "      'function_definitions': {'function_name': 'ComplementNB',\n",
       "       'full_function': 'class sklearn.naive_bayes.ComplementNB(*, alpha=1.0, force_alpha=True, fit_prior=True, class_prior=None, norm=False)',\n",
       "       'function_text': 'The Complement Naive Bayes classifier described in Rennie et al. (2003). The Complement Naive Bayes classifier was designed to correct the “severe\\nassumptions” made by the standard Multinomial Naive Bayes classifier. It is\\nparticularly suited for imbalanced data sets. Read more in the User Guide. Added in version 0.20.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/naive_bayes.html#complement-naive-bayes',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'ComplementNB',\n",
       "       'descriptions': 'The Complement Naive Bayes classifier described in Rennie et al. (2003). The Complement Naive Bayes classifier was designed to correct the “severe\\nassumptions” made by the standard Multinomial Naive Bayes classifier. It is\\nparticularly suited for imbalanced data sets. Read more in the User Guide. Added in version 0.20.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'MultinomialNB',\n",
       "      'func_desc': 'Naive Bayes classifier for multinomial models.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB',\n",
       "      'function_definitions': {'function_name': 'MultinomialNB',\n",
       "       'full_function': 'class sklearn.naive_bayes.MultinomialNB(*, alpha=1.0, force_alpha=True, fit_prior=True, class_prior=None)',\n",
       "       'function_text': 'Naive Bayes classifier for multinomial models. The multinomial Naive Bayes classifier is suitable for classification with\\ndiscrete features (e.g., word counts for text classification). The\\nmultinomial distribution normally requires integer feature counts. However,\\nin practice, fractional counts such as tf-idf may also work. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'MultinomialNB',\n",
       "       'descriptions': 'Naive Bayes classifier for multinomial models. The multinomial Naive Bayes classifier is suitable for classification with\\ndiscrete features (e.g., word counts for text classification). The\\nmultinomial distribution normally requires integer feature counts. However,\\nin practice, fractional counts such as tf-idf may also work. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'CategoricalNB',\n",
       "      'func_desc': 'Naive Bayes classifier for categorical features.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html#sklearn.naive_bayes.CategoricalNB',\n",
       "      'function_definitions': {'function_name': 'CategoricalNB',\n",
       "       'full_function': 'class sklearn.naive_bayes.CategoricalNB(*, alpha=1.0, force_alpha=True, fit_prior=True, class_prior=None, min_categories=None)',\n",
       "       'function_text': 'Naive Bayes classifier for categorical features. The categorical Naive Bayes classifier is suitable for classification with\\ndiscrete features that are categorically distributed. The categories of\\neach feature are drawn from a categorical distribution. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/naive_bayes.html#categorical-naive-bayes',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'CategoricalNB',\n",
       "       'descriptions': 'Naive Bayes classifier for categorical features. The categorical Naive Bayes classifier is suitable for classification with\\ndiscrete features that are categorically distributed. The categories of\\neach feature are drawn from a categorical distribution. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'GaussianNB',\n",
       "      'func_desc': 'Gaussian Naive Bayes (GaussianNB).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB',\n",
       "      'function_definitions': {'function_name': 'GaussianNB',\n",
       "       'full_function': 'class sklearn.naive_bayes.GaussianNB(*, priors=None, var_smoothing=1e-09)',\n",
       "       'function_text': 'Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit.\\nFor details on algorithm used to update feature means and variance online,\\nsee Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque: http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#sklearn.naive_bayes.GaussianNB.partial_fit',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'GaussianNB',\n",
       "       'descriptions': 'Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit.\\nFor details on algorithm used to update feature means and variance online,\\nsee Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque: http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.naive_bayes',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.naive_bayes.html'},\n",
       " 'sklearn.neighbors.html': {'functions': [{'defaults': [{'func_name': 'BallTree',\n",
       "      'func_desc': 'BallTree for fast generalized N-point problems',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree',\n",
       "      'function_definitions': {'function_name': 'BallTree',\n",
       "       'full_function': 'class sklearn.neighbors.BallTree#',\n",
       "       'function_text': 'BallTree for fast generalized N-point problems Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-neighbors',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'An array of points to query.  Last dimension should match dimension\\nof training data.\\n'},\n",
       "        {'param_name': 'r',\n",
       "         'param_type': 'array-like',\n",
       "         'param_desc': 'A one-dimensional array of distances\\n'},\n",
       "        {'param_name': 'dualtree',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, use a dualtree algorithm.  Otherwise, use a single-tree\\nalgorithm.  Dual tree algorithms can have better scaling for\\nlarge N.\\n'}]},\n",
       "      'function_calling': {'name': 'BallTree',\n",
       "       'descriptions': 'BallTree for fast generalized N-point problems Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). An array of points to query.  Last dimension should match dimension\\nof training data.\\n'},\n",
       "         'r': {'type': 'array',\n",
       "          'description': 'array-like. A one-dimensional array of distances\\n'},\n",
       "         'dualtree': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, use a dualtree algorithm.  Otherwise, use a single-tree\\nalgorithm.  Dual tree algorithms can have better scaling for\\nlarge N.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'KNeighborsClassifier',\n",
       "      'func_desc': 'Classifier implementing the k-nearest neighbors vote.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier',\n",
       "      'function_definitions': {'function_name': 'KNeighborsClassifier',\n",
       "       'full_function': \"class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\",\n",
       "       'function_text': 'Classifier implementing the k-nearest neighbors vote. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neighbors.html#classification',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'KNeighborsClassifier',\n",
       "       'descriptions': 'Classifier implementing the k-nearest neighbors vote. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['n_neighbors=5']}}},\n",
       "     {'func_name': 'KNeighborsTransformer',\n",
       "      'func_desc': 'Transform X into a (weighted) graph of k nearest neighbors.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer',\n",
       "      'function_definitions': {'function_name': 'KNeighborsTransformer',\n",
       "       'full_function': \"class sklearn.neighbors.KNeighborsTransformer(*, mode='distance', n_neighbors=5, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\",\n",
       "       'function_text': 'Transform X into a (weighted) graph of k nearest neighbors. The transformed data is a sparse graph as returned by kneighbors_graph. Read more in the User Guide. Added in version 0.22.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neighbors.html#neighbors-transformer',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples_transform, n_features)',\n",
       "         'param_desc': 'Sample data.\\n'}]},\n",
       "      'function_calling': {'name': 'KNeighborsTransformer',\n",
       "       'descriptions': 'Transform X into a (weighted) graph of k nearest neighbors. The transformed data is a sparse graph as returned by kneighbors_graph. Read more in the User Guide. Added in version 0.22.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples_transform, n_features). Sample data.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'LocalOutlierFactor',\n",
       "      'func_desc': 'Unsupervised Outlier Detection using the Local Outlier Factor (LOF).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor',\n",
       "      'function_definitions': {'function_name': 'LocalOutlierFactor',\n",
       "       'full_function': \"class sklearn.neighbors.LocalOutlierFactor(n_neighbors=20, *, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, contamination='auto', novelty=False, n_jobs=None)\",\n",
       "       'function_text': 'Unsupervised Outlier Detection using the Local Outlier Factor (LOF). The anomaly score of each sample is called the Local Outlier Factor.\\nIt measures the local deviation of the density of a given sample with respect\\nto its neighbors.\\nIt is local in that the anomaly score depends on how isolated the object\\nis with respect to the surrounding neighborhood.\\nMore precisely, locality is given by k-nearest neighbors, whose distance\\nis used to estimate the local density.\\nBy comparing the local density of a sample to the local densities of its\\nneighbors, one can identify samples that have a substantially lower density\\nthan their neighbors. These are considered outliers. Added in version 0.19.',\n",
       "       'func_text_user_guide': '#sklearn.neighbors.LocalOutlierFactor.kneighbors',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'LocalOutlierFactor',\n",
       "       'descriptions': 'Unsupervised Outlier Detection using the Local Outlier Factor (LOF). The anomaly score of each sample is called the Local Outlier Factor.\\nIt measures the local deviation of the density of a given sample with respect\\nto its neighbors.\\nIt is local in that the anomaly score depends on how isolated the object\\nis with respect to the surrounding neighborhood.\\nMore precisely, locality is given by k-nearest neighbors, whose distance\\nis used to estimate the local density.\\nBy comparing the local density of a sample to the local densities of its\\nneighbors, one can identify samples that have a substantially lower density\\nthan their neighbors. These are considered outliers. Added in version 0.19.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {},\n",
       "        'required': ['n_neighbors=20']}}},\n",
       "     {'func_name': 'NearestNeighbors',\n",
       "      'func_desc': 'Unsupervised learner for implementing neighbor searches.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors',\n",
       "      'function_definitions': {'function_name': 'NearestNeighbors',\n",
       "       'full_function': \"class sklearn.neighbors.NearestNeighbors(*, n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\",\n",
       "       'function_text': 'Unsupervised learner for implementing neighbor searches. Read more in the User Guide. Added in version 0.9.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-neighbors',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'NearestNeighbors',\n",
       "       'descriptions': 'Unsupervised learner for implementing neighbor searches. Read more in the User Guide. Added in version 0.9.',\n",
       "       'parameters': {'type': 'object', 'properties': {}, 'required': []}}},\n",
       "     {'func_name': 'RadiusNeighborsClassifier',\n",
       "      'func_desc': 'Classifier implementing a vote among neighbors within a given radius.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier',\n",
       "      'function_definitions': {'function_name': 'RadiusNeighborsClassifier',\n",
       "       'full_function': \"class sklearn.neighbors.RadiusNeighborsClassifier(radius=1.0, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', outlier_label=None, metric_params=None, n_jobs=None)\",\n",
       "       'function_text': 'Classifier implementing a vote among neighbors within a given radius. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neighbors.html#classification',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'RadiusNeighborsClassifier',\n",
       "       'descriptions': 'Classifier implementing a vote among neighbors within a given radius. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['radius=1.0']}}},\n",
       "     {'func_name': 'RadiusNeighborsTransformer',\n",
       "      'func_desc': 'Transform X into a (weighted) graph of neighbors nearer than a radius.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsTransformer.html#sklearn.neighbors.RadiusNeighborsTransformer',\n",
       "      'function_definitions': {'function_name': 'RadiusNeighborsTransformer',\n",
       "       'full_function': \"class sklearn.neighbors.RadiusNeighborsTransformer(*, mode='distance', radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\",\n",
       "       'function_text': 'Transform X into a (weighted) graph of neighbors nearer than a radius. The transformed data is a sparse graph as returned by\\nradius_neighbors_graph. Read more in the User Guide. Added in version 0.22.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neighbors.html#neighbors-transformer',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples_transform, n_features)',\n",
       "         'param_desc': 'Sample data.\\n'}]},\n",
       "      'function_calling': {'name': 'RadiusNeighborsTransformer',\n",
       "       'descriptions': 'Transform X into a (weighted) graph of neighbors nearer than a radius. The transformed data is a sparse graph as returned by\\nradius_neighbors_graph. Read more in the User Guide. Added in version 0.22.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples_transform, n_features). Sample data.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'radius_neighbors_graph',\n",
       "      'func_desc': 'Compute the (weighted) graph of Neighbors for points in X.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.radius_neighbors_graph.html#sklearn.neighbors.radius_neighbors_graph',\n",
       "      'function_definitions': {'function_name': 'radius_neighbors_graph',\n",
       "       'full_function': \"sklearn.neighbors.radius_neighbors_graph(X, radius, *, mode='connectivity', metric='minkowski', p=2, metric_params=None, include_self=False, n_jobs=None)\",\n",
       "       'function_text': 'Compute the (weighted) graph of Neighbors for points in X. Neighborhoods are restricted the points at a distance lower than\\nradius. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-neighbors',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Sample data.\\n'},\n",
       "        {'param_name': 'radius',\n",
       "         'param_type': 'float',\n",
       "         'param_desc': 'Radius of neighborhoods.\\n'},\n",
       "        {'param_name': 'mode',\n",
       "         'param_type': '{‘connectivity’, ‘distance’}, default=’connectivity’',\n",
       "         'param_desc': 'Type of returned matrix: ‘connectivity’ will return the connectivity\\nmatrix with ones and zeros, and ‘distance’ will return the distances\\nbetween neighbors according to the given metric.\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'str, default=’minkowski’',\n",
       "         'param_desc': 'Metric to use for distance computation. Default is “minkowski”, which\\nresults in the standard Euclidean distance when p = 2. See the\\ndocumentation of scipy.spatial.distance and\\nthe metrics listed in\\ndistance_metrics for valid metric\\nvalues.\\n'},\n",
       "        {'param_name': 'p',\n",
       "         'param_type': 'float, default=2',\n",
       "         'param_desc': 'Power parameter for the Minkowski metric. When p = 1, this is\\nequivalent to using manhattan_distance (l1), and euclidean_distance\\n(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\\n'},\n",
       "        {'param_name': 'metric_params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Additional keyword arguments for the metric function.\\n'},\n",
       "        {'param_name': 'include_self',\n",
       "         'param_type': 'bool or ‘auto’, default=False',\n",
       "         'param_desc': 'Whether or not to mark each sample as the first nearest neighbor to\\nitself. If ‘auto’, then True is used for mode=’connectivity’ and False\\nfor mode=’distance’.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of parallel jobs to run for neighbors search.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'}]},\n",
       "      'function_calling': {'name': 'radius_neighbors_graph',\n",
       "       'descriptions': 'Compute the (weighted) graph of Neighbors for points in X. Neighborhoods are restricted the points at a distance lower than\\nradius. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Sample data.\\n'},\n",
       "         'radius': {'type': 'number',\n",
       "          'description': 'float. Radius of neighborhoods.\\n'},\n",
       "         'mode': {'type': 'string',\n",
       "          'enum': ['connectivity', 'distance'],\n",
       "          'description': '{‘connectivity’, ‘distance’}, default=’connectivity’. Type of returned matrix: ‘connectivity’ will return the connectivity\\nmatrix with ones and zeros, and ‘distance’ will return the distances\\nbetween neighbors according to the given metric.\\n'},\n",
       "         'metric': {'type': 'string',\n",
       "          'description': 'str, default=’minkowski’. Metric to use for distance computation. Default is “minkowski”, which\\nresults in the standard Euclidean distance when p = 2. See the\\ndocumentation of scipy.spatial.distance and\\nthe metrics listed in\\ndistance_metrics for valid metric\\nvalues.\\n'},\n",
       "         'p': {'type': 'number',\n",
       "          'description': 'float, default=2. Power parameter for the Minkowski metric. When p = 1, this is\\nequivalent to using manhattan_distance (l1), and euclidean_distance\\n(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\\n'},\n",
       "         'metric_params': {'type': 'array',\n",
       "          'description': 'dict, default=None. Additional keyword arguments for the metric function.\\n'},\n",
       "         'include_self': {'type': 'boolean',\n",
       "          'description': 'bool or ‘auto’, default=False. Whether or not to mark each sample as the first nearest neighbor to\\nitself. If ‘auto’, then True is used for mode=’connectivity’ and False\\nfor mode=’distance’.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of parallel jobs to run for neighbors search.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'}},\n",
       "        'required': ['X', 'radius']}}},\n",
       "     {'func_name': 'KDTree',\n",
       "      'func_desc': 'KDTree for fast generalized N-point problems',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree',\n",
       "      'function_definitions': {'function_name': 'KDTree',\n",
       "       'full_function': 'class sklearn.neighbors.KDTree#',\n",
       "       'function_text': 'KDTree for fast generalized N-point problems Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-neighbors',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'An array of points to query.  Last dimension should match dimension\\nof training data.\\n'},\n",
       "        {'param_name': 'r',\n",
       "         'param_type': 'array-like',\n",
       "         'param_desc': 'A one-dimensional array of distances\\n'},\n",
       "        {'param_name': 'dualtree',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, use a dualtree algorithm.  Otherwise, use a single-tree\\nalgorithm.  Dual tree algorithms can have better scaling for\\nlarge N.\\n'}]},\n",
       "      'function_calling': {'name': 'KDTree',\n",
       "       'descriptions': 'KDTree for fast generalized N-point problems Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). An array of points to query.  Last dimension should match dimension\\nof training data.\\n'},\n",
       "         'r': {'type': 'array',\n",
       "          'description': 'array-like. A one-dimensional array of distances\\n'},\n",
       "         'dualtree': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, use a dualtree algorithm.  Otherwise, use a single-tree\\nalgorithm.  Dual tree algorithms can have better scaling for\\nlarge N.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'KNeighborsRegressor',\n",
       "      'func_desc': 'Regression based on k-nearest neighbors.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor',\n",
       "      'function_definitions': {'function_name': 'KNeighborsRegressor',\n",
       "       'full_function': \"class sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\",\n",
       "       'function_text': 'Regression based on k-nearest neighbors. The target is predicted by local interpolation of the targets\\nassociated of the nearest neighbors in the training set. Read more in the User Guide. Added in version 0.9.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neighbors.html#regression',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'KNeighborsRegressor',\n",
       "       'descriptions': 'Regression based on k-nearest neighbors. The target is predicted by local interpolation of the targets\\nassociated of the nearest neighbors in the training set. Read more in the User Guide. Added in version 0.9.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['n_neighbors=5']}}},\n",
       "     {'func_name': 'KernelDensity',\n",
       "      'func_desc': 'Kernel Density Estimation.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity',\n",
       "      'function_definitions': {'function_name': 'KernelDensity',\n",
       "       'full_function': \"class sklearn.neighbors.KernelDensity(*, bandwidth=1.0, algorithm='auto', kernel='gaussian', metric='euclidean', atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)\",\n",
       "       'function_text': 'Kernel Density Estimation. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/density.html#kernel-density',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'KernelDensity',\n",
       "       'descriptions': 'Kernel Density Estimation. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object', 'properties': {}, 'required': []}}},\n",
       "     {'func_name': 'NearestCentroid',\n",
       "      'func_desc': 'Nearest centroid classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid',\n",
       "      'function_definitions': {'function_name': 'NearestCentroid',\n",
       "       'full_function': \"class sklearn.neighbors.NearestCentroid(metric='euclidean', *, shrink_threshold=None)\",\n",
       "       'function_text': 'Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to\\nthe class with the nearest centroid. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neighbors.html#nearest-centroid-classifier',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'NearestCentroid',\n",
       "       'descriptions': 'Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to\\nthe class with the nearest centroid. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': [\"metric='euclidean'\"]}}},\n",
       "     {'func_name': 'NeighborhoodComponentsAnalysis',\n",
       "      'func_desc': 'Neighborhood Components Analysis.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis',\n",
       "      'function_definitions': {'function_name': 'NeighborhoodComponentsAnalysis',\n",
       "       'full_function': \"class sklearn.neighbors.NeighborhoodComponentsAnalysis(n_components=None, *, init='auto', warm_start=False, max_iter=50, tol=1e-05, callback=None, verbose=0, random_state=None)\",\n",
       "       'function_text': 'Neighborhood Components Analysis. Neighborhood Component Analysis (NCA) is a machine learning algorithm for\\nmetric learning. It learns a linear transformation in a supervised fashion\\nto improve the classification accuracy of a stochastic nearest neighbors\\nrule in the transformed space. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neighbors.html#nca',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'RadiusNeighborsRegressor',\n",
       "      'func_desc': 'Regression based on neighbors within a fixed radius.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor',\n",
       "      'function_definitions': {'function_name': 'RadiusNeighborsRegressor',\n",
       "       'full_function': \"class sklearn.neighbors.RadiusNeighborsRegressor(radius=1.0, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\",\n",
       "       'function_text': 'Regression based on neighbors within a fixed radius. The target is predicted by local interpolation of the targets\\nassociated of the nearest neighbors in the training set. Read more in the User Guide. Added in version 0.9.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neighbors.html#regression',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'RadiusNeighborsRegressor',\n",
       "       'descriptions': 'Regression based on neighbors within a fixed radius. The target is predicted by local interpolation of the targets\\nassociated of the nearest neighbors in the training set. Read more in the User Guide. Added in version 0.9.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['radius=1.0']}}},\n",
       "     {'func_name': 'kneighbors_graph',\n",
       "      'func_desc': 'Compute the (weighted) graph of k-Neighbors for points in X.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph',\n",
       "      'function_definitions': {'function_name': 'kneighbors_graph',\n",
       "       'full_function': \"sklearn.neighbors.kneighbors_graph(X, n_neighbors, *, mode='connectivity', metric='minkowski', p=2, metric_params=None, include_self=False, n_jobs=None)\",\n",
       "       'function_text': 'Compute the (weighted) graph of k-Neighbors for points in X. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-neighbors',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Sample data.\\n'},\n",
       "        {'param_name': 'n_neighbors',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Number of neighbors for each sample.\\n'},\n",
       "        {'param_name': 'mode',\n",
       "         'param_type': '{‘connectivity’, ‘distance’}, default=’connectivity’',\n",
       "         'param_desc': 'Type of returned matrix: ‘connectivity’ will return the connectivity\\nmatrix with ones and zeros, and ‘distance’ will return the distances\\nbetween neighbors according to the given metric.\\n'},\n",
       "        {'param_name': 'metric',\n",
       "         'param_type': 'str, default=’minkowski’',\n",
       "         'param_desc': 'Metric to use for distance computation. Default is “minkowski”, which\\nresults in the standard Euclidean distance when p = 2. See the\\ndocumentation of scipy.spatial.distance and\\nthe metrics listed in\\ndistance_metrics for valid metric\\nvalues.\\n'},\n",
       "        {'param_name': 'p',\n",
       "         'param_type': 'float, default=2',\n",
       "         'param_desc': 'Power parameter for the Minkowski metric. When p = 1, this is equivalent\\nto using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\\nFor arbitrary p, minkowski_distance (l_p) is used. This parameter is expected\\nto be positive.\\n'},\n",
       "        {'param_name': 'metric_params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Additional keyword arguments for the metric function.\\n'},\n",
       "        {'param_name': 'include_self',\n",
       "         'param_type': 'bool or ‘auto’, default=False',\n",
       "         'param_desc': 'Whether or not to mark each sample as the first nearest neighbor to\\nitself. If ‘auto’, then True is used for mode=’connectivity’ and False\\nfor mode=’distance’.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The number of parallel jobs to run for neighbors search.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'}]},\n",
       "      'function_calling': {'name': 'kneighbors_graph',\n",
       "       'descriptions': 'Compute the (weighted) graph of k-Neighbors for points in X. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Sample data.\\n'},\n",
       "         'n_neighbors': {'type': 'integer',\n",
       "          'description': 'int. Number of neighbors for each sample.\\n'},\n",
       "         'mode': {'type': 'string',\n",
       "          'enum': ['connectivity', 'distance'],\n",
       "          'description': '{‘connectivity’, ‘distance’}, default=’connectivity’. Type of returned matrix: ‘connectivity’ will return the connectivity\\nmatrix with ones and zeros, and ‘distance’ will return the distances\\nbetween neighbors according to the given metric.\\n'},\n",
       "         'metric': {'type': 'string',\n",
       "          'description': 'str, default=’minkowski’. Metric to use for distance computation. Default is “minkowski”, which\\nresults in the standard Euclidean distance when p = 2. See the\\ndocumentation of scipy.spatial.distance and\\nthe metrics listed in\\ndistance_metrics for valid metric\\nvalues.\\n'},\n",
       "         'p': {'type': 'number',\n",
       "          'description': 'float, default=2. Power parameter for the Minkowski metric. When p = 1, this is equivalent\\nto using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\\nFor arbitrary p, minkowski_distance (l_p) is used. This parameter is expected\\nto be positive.\\n'},\n",
       "         'metric_params': {'type': 'array',\n",
       "          'description': 'dict, default=None. Additional keyword arguments for the metric function.\\n'},\n",
       "         'include_self': {'type': 'boolean',\n",
       "          'description': 'bool or ‘auto’, default=False. Whether or not to mark each sample as the first nearest neighbor to\\nitself. If ‘auto’, then True is used for mode=’connectivity’ and False\\nfor mode=’distance’.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. The number of parallel jobs to run for neighbors search.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n'}},\n",
       "        'required': ['X', 'n_neighbors']}}},\n",
       "     {'func_name': 'sort_graph_by_row_values',\n",
       "      'func_desc': 'Sort a sparse graph such that each row is stored with increasing values.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.sort_graph_by_row_values.html#sklearn.neighbors.sort_graph_by_row_values',\n",
       "      'function_definitions': {'function_name': 'sort_graph_by_row_values',\n",
       "       'full_function': 'sklearn.neighbors.sort_graph_by_row_values(graph, copy=False, warn_when_not_sorted=True)',\n",
       "       'function_text': 'Sort a sparse graph such that each row is stored with increasing values. Added in version 1.2.',\n",
       "       'func_text_user_guide': 'sklearn.exceptions.EfficiencyWarning.html#sklearn.exceptions.EfficiencyWarning',\n",
       "       'parameter_names_desc': [{'param_name': 'graph',\n",
       "         'param_type': 'sparse matrix of shape (n_samples, n_samples)',\n",
       "         'param_desc': 'Distance matrix to other samples, where only non-zero elements are\\nconsidered neighbors. Matrix is converted to CSR format if not already.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the graph is copied before sorting. If False, the sorting is\\nperformed inplace. If the graph is not of CSR format, copy must be\\nTrue to allow the conversion to CSR format, otherwise an error is\\nraised.\\n'},\n",
       "        {'param_name': 'warn_when_not_sorted',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, a EfficiencyWarning is raised\\nwhen the input graph is not sorted by row values.\\n'}]},\n",
       "      'function_calling': {'name': 'sort_graph_by_row_values',\n",
       "       'descriptions': 'Sort a sparse graph such that each row is stored with increasing values. Added in version 1.2.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'graph': {'type': 'array',\n",
       "          'description': 'sparse matrix of shape (n_samples, n_samples). Distance matrix to other samples, where only non-zero elements are\\nconsidered neighbors. Matrix is converted to CSR format if not already.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the graph is copied before sorting. If False, the sorting is\\nperformed inplace. If the graph is not of CSR format, copy must be\\nTrue to allow the conversion to CSR format, otherwise an error is\\nraised.\\n'},\n",
       "         'warn_when_not_sorted': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, a EfficiencyWarning is raised\\nwhen the input graph is not sorted by row values.\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.neighbors',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.neighbors.html'},\n",
       " 'sklearn.neural_network.html': {'functions': [{'defaults': [{'func_name': 'BernoulliRBM',\n",
       "      'func_desc': 'Bernoulli Restricted Boltzmann Machine (RBM).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html#sklearn.neural_network.BernoulliRBM',\n",
       "      'function_definitions': {'function_name': 'BernoulliRBM',\n",
       "       'full_function': 'class sklearn.neural_network.BernoulliRBM(n_components=256, *, learning_rate=0.1, batch_size=10, n_iter=10, verbose=0, random_state=None)',\n",
       "       'function_text': 'Bernoulli Restricted Boltzmann Machine (RBM). A Restricted Boltzmann Machine with binary visible units and\\nbinary hidden units. Parameters are estimated using Stochastic Maximum\\nLikelihood (SML), also known as Persistent Contrastive Divergence (PCD)\\n[2]. The time complexity of this implementation is O(d ** 2) assuming\\nd ~ n_features ~ n_components. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html#rbm',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to be transformed.\\n'}]},\n",
       "      'function_calling': {'name': 'BernoulliRBM',\n",
       "       'descriptions': 'Bernoulli Restricted Boltzmann Machine (RBM). A Restricted Boltzmann Machine with binary visible units and\\nbinary hidden units. Parameters are estimated using Stochastic Maximum\\nLikelihood (SML), also known as Persistent Contrastive Divergence (PCD)\\n[2]. The time complexity of this implementation is O(d ** 2) assuming\\nd ~ n_features ~ n_components. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data to be transformed.\\n'}},\n",
       "        'required': ['n_components=256']}}},\n",
       "     {'func_name': 'MLPRegressor',\n",
       "      'func_desc': 'Multi-layer Perceptron regressor.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor',\n",
       "      'function_definitions': {'function_name': 'MLPRegressor',\n",
       "       'full_function': \"class sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\",\n",
       "       'function_text': 'Multi-layer Perceptron regressor. This model optimizes the squared error using LBFGS or stochastic gradient\\ndescent. Added in version 0.18.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'MLPRegressor',\n",
       "       'descriptions': 'Multi-layer Perceptron regressor. This model optimizes the squared error using LBFGS or stochastic gradient\\ndescent. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['hidden_layer_sizes=(100', ')', \"activation='relu'\"]}}},\n",
       "     {'func_name': 'MLPClassifier',\n",
       "      'func_desc': 'Multi-layer Perceptron classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier',\n",
       "      'function_definitions': {'function_name': 'MLPClassifier',\n",
       "       'full_function': \"class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\",\n",
       "       'function_text': 'Multi-layer Perceptron classifier. This model optimizes the log-loss function using LBFGS or stochastic\\ngradient descent. Added in version 0.18.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'MLPClassifier',\n",
       "       'descriptions': 'Multi-layer Perceptron classifier. This model optimizes the log-loss function using LBFGS or stochastic\\ngradient descent. Added in version 0.18.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': ['hidden_layer_sizes=(100',\n",
       "         ')',\n",
       "         \"activation='relu'\"]}}}]}],\n",
       "  'name': 'sklearn.neural_network',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.neural_network.html'},\n",
       " 'sklearn.pipeline.html': {'functions': [{'defaults': [{'func_name': 'FeatureUnion',\n",
       "      'func_desc': 'Concatenates results of multiple transformer objects.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion',\n",
       "      'function_definitions': {'function_name': 'FeatureUnion',\n",
       "       'full_function': 'class sklearn.pipeline.FeatureUnion(transformer_list, *, n_jobs=None, transformer_weights=None, verbose=False, verbose_feature_names_out=True)',\n",
       "       'function_text': 'Concatenates results of multiple transformer objects. This estimator applies a list of transformer objects in parallel to the\\ninput data, then concatenates the results. This is useful to combine\\nseveral feature extraction mechanisms into a single transformer. Parameters of the transformers may be set using its name and the parameter\\nname separated by a ‘__’. A transformer may be replaced entirely by\\nsetting the parameter with its name to another transformer, removed by\\nsetting to ‘drop’ or disabled by setting to ‘passthrough’ (features are\\npassed without transformation). Read more in the User Guide. Added in version 0.13.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/compose.html#feature-union',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'iterable or array-like, depending on transformers',\n",
       "         'param_desc': 'Input data to be transformed.\\n'},\n",
       "        {'param_name': '**params',\n",
       "         'param_type': 'dict, default=None',\n",
       "         'param_desc': 'Parameters routed to the transform method of the sub-transformers via the\\nmetadata routing API. See Metadata Routing User Guide for more details.\\n\\nAdded in version 1.5.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'FeatureUnion',\n",
       "       'descriptions': 'Concatenates results of multiple transformer objects. This estimator applies a list of transformer objects in parallel to the\\ninput data, then concatenates the results. This is useful to combine\\nseveral feature extraction mechanisms into a single transformer. Parameters of the transformers may be set using its name and the parameter\\nname separated by a ‘__’. A transformer may be replaced entirely by\\nsetting the parameter with its name to another transformer, removed by\\nsetting to ‘drop’ or disabled by setting to ‘passthrough’ (features are\\npassed without transformation). Read more in the User Guide. Added in version 0.13.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'iterable or array-like, depending on transformers. Input data to be transformed.\\n'}},\n",
       "        'required': ['transformer_list']}}},\n",
       "     {'func_name': 'make_pipeline',\n",
       "      'func_desc': 'Construct a Pipeline from the given estimators.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline',\n",
       "      'function_definitions': {'function_name': 'make_pipeline',\n",
       "       'full_function': 'sklearn.pipeline.make_pipeline(*steps, memory=None, verbose=False)',\n",
       "       'function_text': 'Construct a Pipeline from the given estimators. This is a shorthand for the Pipeline constructor; it does not\\nrequire, and does not permit, naming the estimators. Instead, their names\\nwill be set to the lowercase of their types automatically.',\n",
       "       'func_text_user_guide': 'sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline',\n",
       "       'parameter_names_desc': [{'param_name': '*steps',\n",
       "         'param_type': 'list of Estimator objects',\n",
       "         'param_desc': 'List of the scikit-learn estimators that are chained together.\\n'},\n",
       "        {'param_name': 'memory',\n",
       "         'param_type': 'str or object with the joblib.Memory interface, default=None',\n",
       "         'param_desc': 'Used to cache the fitted transformers of the pipeline. The last step\\nwill never be cached, even if it is a transformer. By default, no\\ncaching is performed. If a string is given, it is the path to the\\ncaching directory. Enabling caching triggers a clone of the transformers\\nbefore fitting. Therefore, the transformer instance given to the\\npipeline cannot be inspected directly. Use the attribute named_steps\\nor steps to inspect estimators within the pipeline. Caching the\\ntransformers is advantageous when fitting is time consuming.\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the time elapsed while fitting each step will be printed as it\\nis completed.\\n'}]},\n",
       "      'function_calling': {'name': 'make_pipeline',\n",
       "       'descriptions': 'Construct a Pipeline from the given estimators. This is a shorthand for the Pipeline constructor; it does not\\nrequire, and does not permit, naming the estimators. Instead, their names\\nwill be set to the lowercase of their types automatically.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'*steps': {'type': 'object',\n",
       "          'description': 'list of Estimator objects. List of the scikit-learn estimators that are chained together.\\n'},\n",
       "         'memory': {'type': 'integer',\n",
       "          'description': 'str or object with the joblib.Memory interface, default=None. Used to cache the fitted transformers of the pipeline. The last step\\nwill never be cached, even if it is a transformer. By default, no\\ncaching is performed. If a string is given, it is the path to the\\ncaching directory. Enabling caching triggers a clone of the transformers\\nbefore fitting. Therefore, the transformer instance given to the\\npipeline cannot be inspected directly. Use the attribute named_steps\\nor steps to inspect estimators within the pipeline. Caching the\\ntransformers is advantageous when fitting is time consuming.\\n'},\n",
       "         'verbose': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the time elapsed while fitting each step will be printed as it\\nis completed.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'Pipeline',\n",
       "      'func_desc': 'A sequence of data transformers with an optional final predictor.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline',\n",
       "      'function_definitions': {'function_name': 'Pipeline',\n",
       "       'full_function': 'class sklearn.pipeline.Pipeline(steps, *, memory=None, verbose=False)',\n",
       "       'function_text': \"A sequence of data transformers with an optional final predictor. Pipeline allows you to sequentially apply a list of transformers to\\npreprocess the data and, if desired, conclude the sequence with a final\\npredictor for predictive modeling. Intermediate steps of the pipeline must be ‘transforms’, that is, they\\nmust implement fit and transform methods.\\nThe final estimator only needs to implement fit.\\nThe transformers in the pipeline can be cached using memory argument. The purpose of the pipeline is to assemble several steps that can be\\ncross-validated together while setting different parameters. For this, it\\nenables setting parameters of the various steps using their names and the\\nparameter name separated by a '__', as in the example below. A step’s\\nestimator may be replaced entirely by setting the parameter with its name\\nto another estimator, or a transformer removed by setting it to\\n'passthrough' or None. For an example use case of Pipeline combined with\\nGridSearchCV, refer to\\nSelecting dimensionality reduction with Pipeline and GridSearchCV. The\\nexample Pipelining: chaining a PCA and a logistic regression shows how\\nto grid search on a pipeline using '__' as a separator in the parameter names. Read more in the User Guide. Added in version 0.5.\",\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-predictor',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'iterable',\n",
       "         'param_desc': 'Data to transform. Must fulfill input requirements of first step\\nof the pipeline.\\n'},\n",
       "        {'param_name': '**params',\n",
       "         'param_type': 'dict of str -> object',\n",
       "         'param_desc': 'Parameters requested and accepted by steps. Each step must have\\nrequested certain metadata for these parameters to be forwarded to\\nthem.\\n\\nAdded in version 1.4: Only available if enable_metadata_routing=True. See\\nMetadata Routing User Guide for more\\ndetails.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'Pipeline',\n",
       "       'descriptions': \"A sequence of data transformers with an optional final predictor. Pipeline allows you to sequentially apply a list of transformers to\\npreprocess the data and, if desired, conclude the sequence with a final\\npredictor for predictive modeling. Intermediate steps of the pipeline must be ‘transforms’, that is, they\\nmust implement fit and transform methods.\\nThe final estimator only needs to implement fit.\\nThe transformers in the pipeline can be cached using memory argument. The purpose of the pipeline is to assemble several steps that can be\\ncross-validated together while setting different parameters. For this, it\\nenables setting parameters of the various steps using their names and the\\nparameter name separated by a '__', as in the example below. A step’s\\nestimator may be replaced entirely by setting the parameter with its name\\nto another estimator, or a transformer removed by setting it to\\n'passthrough' or None. For an example use case of Pipeline combined with\\nGridSearchCV, refer to\\nSelecting dimensionality reduction with Pipeline and GridSearchCV. The\\nexample Pipelining: chaining a PCA and a logistic regression shows how\\nto grid search on a pipeline using '__' as a separator in the parameter names. Read more in the User Guide. Added in version 0.5.\",\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'iterable. Data to transform. Must fulfill input requirements of first step\\nof the pipeline.\\n'}},\n",
       "        'required': ['steps']}}},\n",
       "     {'func_name': 'make_union',\n",
       "      'func_desc': 'Construct a FeatureUnion from the given transformers.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_union.html#sklearn.pipeline.make_union',\n",
       "      'function_definitions': {'function_name': 'make_union',\n",
       "       'full_function': 'sklearn.pipeline.make_union(*transformers, n_jobs=None, verbose=False)',\n",
       "       'function_text': 'Construct a FeatureUnion from the given transformers. This is a shorthand for the FeatureUnion constructor; it does not\\nrequire, and does not permit, naming the transformers. Instead, they will\\nbe given names automatically based on their types. It also does not allow\\nweighting.',\n",
       "       'func_text_user_guide': 'sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion',\n",
       "       'parameter_names_desc': [{'param_name': '*transformers',\n",
       "         'param_type': 'list of estimators',\n",
       "         'param_desc': 'One or more estimators.\\n'},\n",
       "        {'param_name': 'n_jobs',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of jobs to run in parallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n\\nChanged in version v0.20: n_jobs default changed from 1 to None.\\n\\n'},\n",
       "        {'param_name': 'verbose',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, the time elapsed while fitting each transformer will be\\nprinted as it is completed.\\n'}]},\n",
       "      'function_calling': {'name': 'make_union',\n",
       "       'descriptions': 'Construct a FeatureUnion from the given transformers. This is a shorthand for the FeatureUnion constructor; it does not\\nrequire, and does not permit, naming the transformers. Instead, they will\\nbe given names automatically based on their types. It also does not allow\\nweighting.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'*transformers': {'type': 'array',\n",
       "          'description': 'list of estimators. One or more estimators.\\n'},\n",
       "         'n_jobs': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of jobs to run in parallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details.\\n\\nChanged in version v0.20: n_jobs default changed from 1 to None.\\n\\n'},\n",
       "         'verbose': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, the time elapsed while fitting each transformer will be\\nprinted as it is completed.\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.pipeline',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.pipeline.html'},\n",
       " 'sklearn.preprocessing.html': {'functions': [{'defaults': [{'func_name': 'Binarizer',\n",
       "      'func_desc': 'Binarize data (set feature values to 0 or 1) according to a threshold.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html#sklearn.preprocessing.Binarizer',\n",
       "      'function_definitions': {'function_name': 'Binarizer',\n",
       "       'full_function': 'class sklearn.preprocessing.Binarizer(*, threshold=0.0, copy=True)',\n",
       "       'function_text': 'Binarize data (set feature values to 0 or 1) according to a threshold. Values greater than the threshold map to 1, while values less than\\nor equal to the threshold map to 0. With the default threshold of 0,\\nonly positive values map to 1. Binarization is a common operation on text count data where the\\nanalyst can decide to only consider the presence or absence of a\\nfeature rather than a quantified number of occurrences for instance. It can also be used as a pre-processing step for estimators that\\nconsider boolean random variables (e.g. modelled using the Bernoulli\\ndistribution in a Bayesian setting). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-binarization',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to binarize, element by element.\\nscipy.sparse matrices should be in CSR format to avoid an\\nun-necessary copy.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool',\n",
       "         'param_desc': 'Copy the input X or not.\\n'}]},\n",
       "      'function_calling': {'name': 'Binarizer',\n",
       "       'descriptions': 'Binarize data (set feature values to 0 or 1) according to a threshold. Values greater than the threshold map to 1, while values less than\\nor equal to the threshold map to 0. With the default threshold of 0,\\nonly positive values map to 1. Binarization is a common operation on text count data where the\\nanalyst can decide to only consider the presence or absence of a\\nfeature rather than a quantified number of occurrences for instance. It can also be used as a pre-processing step for estimators that\\nconsider boolean random variables (e.g. modelled using the Bernoulli\\ndistribution in a Bayesian setting). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data to binarize, element by element.\\nscipy.sparse matrices should be in CSR format to avoid an\\nun-necessary copy.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool. Copy the input X or not.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'KBinsDiscretizer',\n",
       "      'func_desc': 'Bin continuous data into intervals.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer',\n",
       "      'function_definitions': {'function_name': 'KBinsDiscretizer',\n",
       "       'full_function': \"class sklearn.preprocessing.KBinsDiscretizer(n_bins=5, *, encode='onehot', strategy='quantile', dtype=None, subsample=200000, random_state=None)\",\n",
       "       'function_text': 'Bin continuous data into intervals. Read more in the User Guide. Added in version 0.20.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-discretization',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Data to be discretized.\\n'}]},\n",
       "      'function_calling': {'name': 'KBinsDiscretizer',\n",
       "       'descriptions': 'Bin continuous data into intervals. Read more in the User Guide. Added in version 0.20.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Data to be discretized.\\n'}},\n",
       "        'required': ['n_bins=5']}}},\n",
       "     {'func_name': 'LabelBinarizer',\n",
       "      'func_desc': 'Binarize labels in a one-vs-all fashion.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html#sklearn.preprocessing.LabelBinarizer',\n",
       "      'function_definitions': {'function_name': 'LabelBinarizer',\n",
       "       'full_function': 'class sklearn.preprocessing.LabelBinarizer(*, neg_label=0, pos_label=1, sparse_output=False)',\n",
       "       'function_text': 'Binarize labels in a one-vs-all fashion. Several regression and binary classification algorithms are\\navailable in scikit-learn. A simple way to extend these algorithms\\nto the multi-class classification case is to use the so-called\\none-vs-all scheme. At learning time, this simply consists in learning one regressor\\nor binary classifier per class. In doing so, one needs to convert\\nmulti-class labels to binary labels (belong or does not belong\\nto the class). LabelBinarizer makes this process easy with the\\ntransform method. At prediction time, one assigns the class for which the corresponding\\nmodel gave the greatest confidence. LabelBinarizer makes this easy\\nwith the inverse_transform method. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#sklearn.preprocessing.LabelBinarizer.inverse_transform',\n",
       "       'parameter_names_desc': [{'param_name': 'y',\n",
       "         'param_type': '{array, sparse matrix} of shape (n_samples,) or                 (n_samples, n_classes)',\n",
       "         'param_desc': 'Target values. The 2-d matrix should only contain 0 and 1,\\nrepresents multilabel classification. Sparse matrix can be\\nCSR, CSC, COO, DOK, or LIL.\\n'}]},\n",
       "      'function_calling': {'name': 'LabelBinarizer',\n",
       "       'descriptions': 'Binarize labels in a one-vs-all fashion. Several regression and binary classification algorithms are\\navailable in scikit-learn. A simple way to extend these algorithms\\nto the multi-class classification case is to use the so-called\\none-vs-all scheme. At learning time, this simply consists in learning one regressor\\nor binary classifier per class. In doing so, one needs to convert\\nmulti-class labels to binary labels (belong or does not belong\\nto the class). LabelBinarizer makes this process easy with the\\ntransform method. At prediction time, one assigns the class for which the corresponding\\nmodel gave the greatest confidence. LabelBinarizer makes this easy\\nwith the inverse_transform method. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y': {'type': 'array',\n",
       "          'description': '{array, sparse matrix} of shape (n_samples,) or                 (n_samples, n_classes). Target values. The 2-d matrix should only contain 0 and 1,\\nrepresents multilabel classification. Sparse matrix can be\\nCSR, CSC, COO, DOK, or LIL.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'MaxAbsScaler',\n",
       "      'func_desc': 'Scale each feature by its maximum absolute value.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler',\n",
       "      'function_definitions': {'function_name': 'MaxAbsScaler',\n",
       "       'full_function': 'class sklearn.preprocessing.MaxAbsScaler(*, copy=True)',\n",
       "       'function_text': 'Scale each feature by its maximum absolute value. This estimator scales and translates each feature individually such\\nthat the maximal absolute value of each feature in the\\ntraining set will be 1.0. It does not shift/center the data, and\\nthus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices. MaxAbsScaler doesn’t reduce the effect of outliers; it only linearly\\nscales them down. For an example visualization, refer to Compare\\nMaxAbsScaler with other scalers. Added in version 0.17.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/preprocessing/plot_all_scaling.html#plot-all-scaling-max-abs-scaler-section',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data that should be scaled.\\n'}]},\n",
       "      'function_calling': {'name': 'MaxAbsScaler',\n",
       "       'descriptions': 'Scale each feature by its maximum absolute value. This estimator scales and translates each feature individually such\\nthat the maximal absolute value of each feature in the\\ntraining set will be 1.0. It does not shift/center the data, and\\nthus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices. MaxAbsScaler doesn’t reduce the effect of outliers; it only linearly\\nscales them down. For an example visualization, refer to Compare\\nMaxAbsScaler with other scalers. Added in version 0.17.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data that should be scaled.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'MultiLabelBinarizer',\n",
       "      'func_desc': 'Transform between iterable of iterables and a multilabel format.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html#sklearn.preprocessing.MultiLabelBinarizer',\n",
       "      'function_definitions': {'function_name': 'MultiLabelBinarizer',\n",
       "       'full_function': 'class sklearn.preprocessing.MultiLabelBinarizer(*, classes=None, sparse_output=False)',\n",
       "       'function_text': 'Transform between iterable of iterables and a multilabel format. Although a list of sets or tuples is a very intuitive format for multilabel\\ndata, it is unwieldy to process. This transformer converts between this\\nintuitive format and the supported multilabel format: a (samples x classes)\\nbinary matrix indicating the presence of a class label.',\n",
       "       'func_text_user_guide': 'sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder',\n",
       "       'parameter_names_desc': [{'param_name': 'y',\n",
       "         'param_type': 'iterable of iterables',\n",
       "         'param_desc': 'A set of labels (any orderable and hashable object) for each\\nsample. If the classes parameter is set, y will not be\\niterated.\\n'}]},\n",
       "      'function_calling': {'name': 'MultiLabelBinarizer',\n",
       "       'descriptions': 'Transform between iterable of iterables and a multilabel format. Although a list of sets or tuples is a very intuitive format for multilabel\\ndata, it is unwieldy to process. This transformer converts between this\\nintuitive format and the supported multilabel format: a (samples x classes)\\nbinary matrix indicating the presence of a class label.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y': {'type': 'array',\n",
       "          'description': 'iterable of iterables. A set of labels (any orderable and hashable object) for each\\nsample. If the classes parameter is set, y will not be\\niterated.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'OneHotEncoder',\n",
       "      'func_desc': 'Encode categorical features as a one-hot numeric array.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder',\n",
       "      'function_definitions': {'function_name': 'OneHotEncoder',\n",
       "       'full_function': \"class sklearn.preprocessing.OneHotEncoder(*, categories='auto', drop=None, sparse_output=True, dtype=<class 'numpy.float64'>, handle_unknown='error', min_frequency=None, max_categories=None, feature_name_combiner='concat')\",\n",
       "       'function_text': 'Encode categorical features as a one-hot numeric array. The input to this transformer should be an array-like of integers or\\nstrings, denoting the values taken on by categorical (discrete) features.\\nThe features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’)\\nencoding scheme. This creates a binary column for each category and\\nreturns a sparse matrix or dense array (depending on the sparse_output\\nparameter). By default, the encoder derives the categories based on the unique values\\nin each feature. Alternatively, you can also specify the categories\\nmanually. This encoding is needed for feeding categorical data to many scikit-learn\\nestimators, notably linear models and SVMs with the standard kernels. Note: a one-hot encoding of y labels should use a LabelBinarizer\\ninstead. Read more in the User Guide.\\nFor a comparison of different encoders, refer to:\\nComparing Target Encoder with Other Encoders.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to encode.\\n'}]},\n",
       "      'function_calling': {'name': 'OneHotEncoder',\n",
       "       'descriptions': 'Encode categorical features as a one-hot numeric array. The input to this transformer should be an array-like of integers or\\nstrings, denoting the values taken on by categorical (discrete) features.\\nThe features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’)\\nencoding scheme. This creates a binary column for each category and\\nreturns a sparse matrix or dense array (depending on the sparse_output\\nparameter). By default, the encoder derives the categories based on the unique values\\nin each feature. Alternatively, you can also specify the categories\\nmanually. This encoding is needed for feeding categorical data to many scikit-learn\\nestimators, notably linear models and SVMs with the standard kernels. Note: a one-hot encoding of y labels should use a LabelBinarizer\\ninstead. Read more in the User Guide.\\nFor a comparison of different encoders, refer to:\\nComparing Target Encoder with Other Encoders.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). The data to encode.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'PolynomialFeatures',\n",
       "      'func_desc': 'Generate polynomial and interaction features.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures',\n",
       "      'function_definitions': {'function_name': 'PolynomialFeatures',\n",
       "       'full_function': \"class sklearn.preprocessing.PolynomialFeatures(degree=2, *, interaction_only=False, include_bias=True, order='C')\",\n",
       "       'function_text': 'Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations\\nof the features with degree less than or equal to the specified degree.\\nFor example, if an input sample is two dimensional and of the form\\n[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing.html#polynomial-features',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to transform, row by row.\\nPrefer CSR over CSC for sparse input (for speed), but CSC is\\nrequired if the degree is 4 or higher. If the degree is less than\\n4 and the input format is CSC, it will be converted to CSR, have\\nits polynomial features generated, then converted back to CSC.\\nIf the degree is 2 or 3, the method described in “Leveraging\\nSparsity to Speed Up Polynomial Feature Expansions of CSR Matrices\\nUsing K-Simplex Numbers” by Andrew Nystrom and John Hughes is\\nused, which is much faster than the method used on CSC input. For\\nthis reason, a CSC input will be converted to CSR, and the output\\nwill be converted back to CSC prior to being returned, hence the\\npreference of CSR.\\n'}]},\n",
       "      'function_calling': {'name': 'PolynomialFeatures',\n",
       "       'descriptions': 'Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations\\nof the features with degree less than or equal to the specified degree.\\nFor example, if an input sample is two dimensional and of the form\\n[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data to transform, row by row.\\nPrefer CSR over CSC for sparse input (for speed), but CSC is\\nrequired if the degree is 4 or higher. If the degree is less than\\n4 and the input format is CSC, it will be converted to CSR, have\\nits polynomial features generated, then converted back to CSC.\\nIf the degree is 2 or 3, the method described in “Leveraging\\nSparsity to Speed Up Polynomial Feature Expansions of CSR Matrices\\nUsing K-Simplex Numbers” by Andrew Nystrom and John Hughes is\\nused, which is much faster than the method used on CSC input. For\\nthis reason, a CSC input will be converted to CSR, and the output\\nwill be converted back to CSC prior to being returned, hence the\\npreference of CSR.\\n'}},\n",
       "        'required': ['degree=2']}}},\n",
       "     {'func_name': 'QuantileTransformer',\n",
       "      'func_desc': 'Transform features using quantiles information.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer',\n",
       "      'function_definitions': {'function_name': 'QuantileTransformer',\n",
       "       'full_function': \"class sklearn.preprocessing.QuantileTransformer(*, n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=10000, random_state=None, copy=True)\",\n",
       "       'function_text': 'Transform features using quantiles information. This method transforms the features to follow a uniform or a normal\\ndistribution. Therefore, for a given feature, this transformation tends\\nto spread out the most frequent values. It also reduces the impact of\\n(marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. First an\\nestimate of the cumulative distribution function of a feature is\\nused to map the original values to a uniform distribution. The obtained\\nvalues are then mapped to the desired output distribution using the\\nassociated quantile function. Features values of new/unseen data that fall\\nbelow or above the fitted range will be mapped to the bounds of the output\\ndistribution. Note that this transform is non-linear. It may distort linear\\ncorrelations between variables measured at the same scale but renders\\nvariables measured at different scales more directly comparable. For example visualizations, refer to Compare QuantileTransformer with\\nother scalers. Read more in the User Guide. Added in version 0.19.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/preprocessing/plot_all_scaling.html#plot-all-scaling-quantile-transformer-section',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data used to scale along the features axis. If a sparse\\nmatrix is provided, it will be converted into a sparse\\ncsc_matrix. Additionally, the sparse matrix needs to be\\nnonnegative if ignore_implicit_zeros is False.\\n'}]},\n",
       "      'function_calling': {'name': 'QuantileTransformer',\n",
       "       'descriptions': 'Transform features using quantiles information. This method transforms the features to follow a uniform or a normal\\ndistribution. Therefore, for a given feature, this transformation tends\\nto spread out the most frequent values. It also reduces the impact of\\n(marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. First an\\nestimate of the cumulative distribution function of a feature is\\nused to map the original values to a uniform distribution. The obtained\\nvalues are then mapped to the desired output distribution using the\\nassociated quantile function. Features values of new/unseen data that fall\\nbelow or above the fitted range will be mapped to the bounds of the output\\ndistribution. Note that this transform is non-linear. It may distort linear\\ncorrelations between variables measured at the same scale but renders\\nvariables measured at different scales more directly comparable. For example visualizations, refer to Compare QuantileTransformer with\\nother scalers. Read more in the User Guide. Added in version 0.19.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data used to scale along the features axis. If a sparse\\nmatrix is provided, it will be converted into a sparse\\ncsc_matrix. Additionally, the sparse matrix needs to be\\nnonnegative if ignore_implicit_zeros is False.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'SplineTransformer',\n",
       "      'func_desc': 'Generate univariate B-spline bases for features.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.SplineTransformer.html#sklearn.preprocessing.SplineTransformer',\n",
       "      'function_definitions': {'function_name': 'SplineTransformer',\n",
       "       'full_function': \"class sklearn.preprocessing.SplineTransformer(n_knots=5, degree=3, *, knots='uniform', extrapolation='constant', include_bias=True, order='C', sparse_output=False)\",\n",
       "       'function_text': 'Generate univariate B-spline bases for features. Generate a new feature matrix consisting of\\nn_splines=n_knots + degree - 1 (n_knots - 1 for\\nextrapolation=\"periodic\") spline basis functions\\n(B-splines) of polynomial order=`degree` for each feature. In order to learn more about the SplineTransformer class go to:\\nTime-related feature engineering Read more in the User Guide. Added in version 1.0.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/applications/plot_cyclical_feature_engineering.html#sphx-glr-auto-examples-applications-plot-cyclical-feature-engineering-py',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to transform.\\n'}]},\n",
       "      'function_calling': {'name': 'SplineTransformer',\n",
       "       'descriptions': 'Generate univariate B-spline bases for features. Generate a new feature matrix consisting of\\nn_splines=n_knots + degree - 1 (n_knots - 1 for\\nextrapolation=\"periodic\") spline basis functions\\n(B-splines) of polynomial order=`degree` for each feature. In order to learn more about the SplineTransformer class go to:\\nTime-related feature engineering Read more in the User Guide. Added in version 1.0.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). The data to transform.\\n'}},\n",
       "        'required': ['n_knots=5', 'degree=3']}}},\n",
       "     {'func_name': 'TargetEncoder',\n",
       "      'func_desc': 'Target Encoder for regression and classification targets.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder',\n",
       "      'function_definitions': {'function_name': 'TargetEncoder',\n",
       "       'full_function': \"class sklearn.preprocessing.TargetEncoder(categories='auto', target_type='auto', smooth='auto', cv=5, shuffle=True, random_state=None)\",\n",
       "       'function_text': 'Target Encoder for regression and classification targets. Each category is encoded based on a shrunk estimate of the average target\\nvalues for observations belonging to the category. The encoding scheme mixes\\nthe global target mean with the target mean conditioned on the value of the\\ncategory (see [MIC]). When the target type is “multiclass”, encodings are based\\non the conditional probability estimate for each class. The target is first\\nbinarized using the “one-vs-all” scheme via\\nLabelBinarizer, then the average target\\nvalue for each class and each category is used for encoding, resulting in\\nn_features * n_classes encoded output features. TargetEncoder considers missing values, such as np.nan or None,\\nas another category and encodes them like any other category. Categories\\nthat are not seen during fit are encoded with the target mean, i.e.\\ntarget_mean_. For a demo on the importance of the TargetEncoder internal cross-fitting,\\nsee\\nTarget Encoder’s Internal Cross fitting.\\nFor a comparison of different encoders, refer to\\nComparing Target Encoder with Other Encoders. Read\\nmore in the User Guide. Note fit(X, y).transform(X) does not equal fit_transform(X, y) because a\\ncross fitting scheme is used in fit_transform for encoding.\\nSee the User Guide for details. Added in version 1.3.',\n",
       "       'func_text_user_guide': '#rf862141e5a0c-mic',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to determine the categories of each feature.\\n'}]},\n",
       "      'function_calling': {'name': 'TargetEncoder',\n",
       "       'descriptions': 'Target Encoder for regression and classification targets. Each category is encoded based on a shrunk estimate of the average target\\nvalues for observations belonging to the category. The encoding scheme mixes\\nthe global target mean with the target mean conditioned on the value of the\\ncategory (see [MIC]). When the target type is “multiclass”, encodings are based\\non the conditional probability estimate for each class. The target is first\\nbinarized using the “one-vs-all” scheme via\\nLabelBinarizer, then the average target\\nvalue for each class and each category is used for encoding, resulting in\\nn_features * n_classes encoded output features. TargetEncoder considers missing values, such as np.nan or None,\\nas another category and encodes them like any other category. Categories\\nthat are not seen during fit are encoded with the target mean, i.e.\\ntarget_mean_. For a demo on the importance of the TargetEncoder internal cross-fitting,\\nsee\\nTarget Encoder’s Internal Cross fitting.\\nFor a comparison of different encoders, refer to\\nComparing Target Encoder with Other Encoders. Read\\nmore in the User Guide. Note fit(X, y).transform(X) does not equal fit_transform(X, y) because a\\ncross fitting scheme is used in fit_transform for encoding.\\nSee the User Guide for details. Added in version 1.3.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). The data to determine the categories of each feature.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'binarize',\n",
       "      'func_desc': 'Boolean thresholding of array-like or scipy.sparse matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.binarize.html#sklearn.preprocessing.binarize',\n",
       "      'function_definitions': {'function_name': 'binarize',\n",
       "       'full_function': 'sklearn.preprocessing.binarize(X, *, threshold=0.0, copy=True)',\n",
       "       'function_text': 'Boolean thresholding of array-like or scipy.sparse matrix. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-binarization',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to binarize, element by element.\\nscipy.sparse matrices should be in CSR or CSC format to avoid an\\nun-necessary copy.\\n'},\n",
       "        {'param_name': 'threshold',\n",
       "         'param_type': 'float, default=0.0',\n",
       "         'param_desc': 'Feature values below or equal to this are replaced by 0, above it by 1.\\nThreshold may not be less than 0 for operations on sparse matrices.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, try to avoid a copy and binarize in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an object dtype, a copy will be returned even with\\ncopy=False.\\n'}]},\n",
       "      'function_calling': {'name': 'binarize',\n",
       "       'descriptions': 'Boolean thresholding of array-like or scipy.sparse matrix. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data to binarize, element by element.\\nscipy.sparse matrices should be in CSR or CSC format to avoid an\\nun-necessary copy.\\n'},\n",
       "         'threshold': {'type': 'number',\n",
       "          'description': 'float, default=0.0. Feature values below or equal to this are replaced by 0, above it by 1.\\nThreshold may not be less than 0 for operations on sparse matrices.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, try to avoid a copy and binarize in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an object dtype, a copy will be returned even with\\ncopy=False.\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'maxabs_scale',\n",
       "      'func_desc': 'Scale each feature to the [-1, 1] range without breaking the sparsity.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.maxabs_scale.html#sklearn.preprocessing.maxabs_scale',\n",
       "      'function_definitions': {'function_name': 'maxabs_scale',\n",
       "       'full_function': 'sklearn.preprocessing.maxabs_scale(X, *, axis=0, copy=True)',\n",
       "       'function_text': 'Scale each feature to the [-1, 1] range without breaking the sparsity. This estimator scales each feature individually such\\nthat the maximal absolute value of each feature in the\\ntraining set will be 1.0. This scaler can also be applied to sparse CSR or CSC matrices.',\n",
       "       'func_text_user_guide': '#sklearn.preprocessing.maxabs_scale',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data.\\n'},\n",
       "        {'param_name': 'axis',\n",
       "         'param_type': '{0, 1}, default=0',\n",
       "         'param_desc': 'Axis used to scale along. If 0, independently scale each feature,\\notherwise (if 1) scale each sample.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, try to avoid a copy and scale in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n'}]},\n",
       "      'function_calling': {'name': 'maxabs_scale',\n",
       "       'descriptions': 'Scale each feature to the [-1, 1] range without breaking the sparsity. This estimator scales each feature individually such\\nthat the maximal absolute value of each feature in the\\ntraining set will be 1.0. This scaler can also be applied to sparse CSR or CSC matrices.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data.\\n'},\n",
       "         'axis': {'type': 'string',\n",
       "          'enum': ['0', '1'],\n",
       "          'description': '{0, 1}, default=0. Axis used to scale along. If 0, independently scale each feature,\\notherwise (if 1) scale each sample.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, try to avoid a copy and scale in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'normalize',\n",
       "      'func_desc': 'Scale input vectors individually to unit norm (vector length).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize',\n",
       "      'function_definitions': {'function_name': 'normalize',\n",
       "       'full_function': \"sklearn.preprocessing.normalize(X, norm='l2', *, axis=1, copy=True, return_norm=False)\",\n",
       "       'function_text': 'Scale input vectors individually to unit norm (vector length). Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-normalization',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to normalize, element by element.\\nscipy.sparse matrices should be in CSR format to avoid an\\nun-necessary copy.\\n'},\n",
       "        {'param_name': 'norm',\n",
       "         'param_type': '{‘l1’, ‘l2’, ‘max’}, default=’l2’',\n",
       "         'param_desc': 'The norm to use to normalize each non zero sample (or each non-zero\\nfeature if axis is 0).\\n'},\n",
       "        {'param_name': 'axis',\n",
       "         'param_type': '{0, 1}, default=1',\n",
       "         'param_desc': 'Define axis used to normalize the data along. If 1, independently\\nnormalize each sample, otherwise (if 0) normalize each feature.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, try to avoid a copy and normalize in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n'},\n",
       "        {'param_name': 'return_norm',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to return the computed norms.\\n'}]},\n",
       "      'function_calling': {'name': 'normalize',\n",
       "       'descriptions': 'Scale input vectors individually to unit norm (vector length). Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data to normalize, element by element.\\nscipy.sparse matrices should be in CSR format to avoid an\\nun-necessary copy.\\n'},\n",
       "         'norm': {'type': 'string',\n",
       "          'enum': ['l1', 'l2', 'max'],\n",
       "          'description': '{‘l1’, ‘l2’, ‘max’}, default=’l2’. The norm to use to normalize each non zero sample (or each non-zero\\nfeature if axis is 0).\\n'},\n",
       "         'axis': {'type': 'string',\n",
       "          'enum': ['0', '1'],\n",
       "          'description': '{0, 1}, default=1. Define axis used to normalize the data along. If 1, independently\\nnormalize each sample, otherwise (if 0) normalize each feature.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, try to avoid a copy and normalize in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n'},\n",
       "         'return_norm': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to return the computed norms.\\n'}},\n",
       "        'required': ['X', \"norm='l2'\"]}}},\n",
       "     {'func_name': 'quantile_transform',\n",
       "      'func_desc': 'Transform features using quantiles information.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.quantile_transform.html#sklearn.preprocessing.quantile_transform',\n",
       "      'function_definitions': {'function_name': 'quantile_transform',\n",
       "       'full_function': \"sklearn.preprocessing.quantile_transform(X, *, axis=0, n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=100000, random_state=None, copy=True)\",\n",
       "       'function_text': 'Transform features using quantiles information. This method transforms the features to follow a uniform or a normal\\ndistribution. Therefore, for a given feature, this transformation tends\\nto spread out the most frequent values. It also reduces the impact of\\n(marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. First an\\nestimate of the cumulative distribution function of a feature is\\nused to map the original values to a uniform distribution. The obtained\\nvalues are then mapped to the desired output distribution using the\\nassociated quantile function. Features values of new/unseen data that fall\\nbelow or above the fitted range will be mapped to the bounds of the output\\ndistribution. Note that this transform is non-linear. It may distort linear\\ncorrelations between variables measured at the same scale but renders\\nvariables measured at different scales more directly comparable. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-transformer',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to transform.\\n'},\n",
       "        {'param_name': 'axis',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Axis used to compute the means and standard deviations along. If 0,\\ntransform each feature, otherwise (if 1) transform each sample.\\n'},\n",
       "        {'param_name': 'n_quantiles',\n",
       "         'param_type': 'int, default=1000 or n_samples',\n",
       "         'param_desc': 'Number of quantiles to be computed. It corresponds to the number\\nof landmarks used to discretize the cumulative distribution function.\\nIf n_quantiles is larger than the number of samples, n_quantiles is set\\nto the number of samples as a larger number of quantiles does not give\\na better approximation of the cumulative distribution function\\nestimator.\\n'},\n",
       "        {'param_name': 'output_distribution',\n",
       "         'param_type': '{‘uniform’, ‘normal’}, default=’uniform’',\n",
       "         'param_desc': 'Marginal distribution for the transformed data. The choices are\\n‘uniform’ (default) or ‘normal’.\\n'},\n",
       "        {'param_name': 'ignore_implicit_zeros',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Only applies to sparse matrices. If True, the sparse entries of the\\nmatrix are discarded to compute the quantile statistics. If False,\\nthese entries are treated as zeros.\\n'},\n",
       "        {'param_name': 'subsample',\n",
       "         'param_type': 'int or None, default=1e5',\n",
       "         'param_desc': 'Maximum number of samples used to estimate the quantiles for\\ncomputational efficiency. Note that the subsampling procedure may\\ndiffer for value-identical sparse and dense matrices.\\nDisable subsampling by setting subsample=None.\\n\\nAdded in version 1.5: The option None to disable subsampling was added.\\n\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for subsampling and smoothing\\nnoise.\\nPlease see subsample for more details.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, try to avoid a copy and transform in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n\\nChanged in version 0.23: The default value of copy changed from False to True in 0.23.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'quantile_transform',\n",
       "       'descriptions': 'Transform features using quantiles information. This method transforms the features to follow a uniform or a normal\\ndistribution. Therefore, for a given feature, this transformation tends\\nto spread out the most frequent values. It also reduces the impact of\\n(marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. First an\\nestimate of the cumulative distribution function of a feature is\\nused to map the original values to a uniform distribution. The obtained\\nvalues are then mapped to the desired output distribution using the\\nassociated quantile function. Features values of new/unseen data that fall\\nbelow or above the fitted range will be mapped to the bounds of the output\\ndistribution. Note that this transform is non-linear. It may distort linear\\ncorrelations between variables measured at the same scale but renders\\nvariables measured at different scales more directly comparable. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data to transform.\\n'},\n",
       "         'axis': {'type': 'integer',\n",
       "          'description': 'int, default=0. Axis used to compute the means and standard deviations along. If 0,\\ntransform each feature, otherwise (if 1) transform each sample.\\n'},\n",
       "         'n_quantiles': {'type': 'integer',\n",
       "          'description': 'int, default=1000 or n_samples. Number of quantiles to be computed. It corresponds to the number\\nof landmarks used to discretize the cumulative distribution function.\\nIf n_quantiles is larger than the number of samples, n_quantiles is set\\nto the number of samples as a larger number of quantiles does not give\\na better approximation of the cumulative distribution function\\nestimator.\\n'},\n",
       "         'output_distribution': {'type': 'string',\n",
       "          'enum': ['uniform', 'normal'],\n",
       "          'description': '{‘uniform’, ‘normal’}, default=’uniform’. Marginal distribution for the transformed data. The choices are\\n‘uniform’ (default) or ‘normal’.\\n'},\n",
       "         'ignore_implicit_zeros': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Only applies to sparse matrices. If True, the sparse entries of the\\nmatrix are discarded to compute the quantile statistics. If False,\\nthese entries are treated as zeros.\\n'},\n",
       "         'subsample': {'type': 'integer',\n",
       "          'description': 'int or None, default=1e5. Maximum number of samples used to estimate the quantiles for\\ncomputational efficiency. Note that the subsampling procedure may\\ndiffer for value-identical sparse and dense matrices.\\nDisable subsampling by setting subsample=None.\\n\\nAdded in version 1.5: The option None to disable subsampling was added.\\n\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for subsampling and smoothing\\nnoise.\\nPlease see subsample for more details.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, try to avoid a copy and transform in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n\\nChanged in version 0.23: The default value of copy changed from False to True in 0.23.\\n\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'scale',\n",
       "      'func_desc': 'Standardize a dataset along any axis.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale',\n",
       "      'function_definitions': {'function_name': 'scale',\n",
       "       'full_function': 'sklearn.preprocessing.scale(X, *, axis=0, with_mean=True, with_std=True, copy=True)',\n",
       "       'function_text': 'Standardize a dataset along any axis. Center to the mean and component wise scale to unit variance. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to center and scale.\\n'},\n",
       "        {'param_name': 'axis',\n",
       "         'param_type': '{0, 1}, default=0',\n",
       "         'param_desc': 'Axis used to compute the means and standard deviations along. If 0,\\nindependently standardize each feature, otherwise (if 1) standardize\\neach sample.\\n'},\n",
       "        {'param_name': 'with_mean',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, center the data before scaling.\\n'},\n",
       "        {'param_name': 'with_std',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, scale the data to unit variance (or equivalently,\\nunit standard deviation).\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, try to avoid a copy and scale in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n'}]},\n",
       "      'function_calling': {'name': 'scale',\n",
       "       'descriptions': 'Standardize a dataset along any axis. Center to the mean and component wise scale to unit variance. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data to center and scale.\\n'},\n",
       "         'axis': {'type': 'string',\n",
       "          'enum': ['0', '1'],\n",
       "          'description': '{0, 1}, default=0. Axis used to compute the means and standard deviations along. If 0,\\nindependently standardize each feature, otherwise (if 1) standardize\\neach sample.\\n'},\n",
       "         'with_mean': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, center the data before scaling.\\n'},\n",
       "         'with_std': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, scale the data to unit variance (or equivalently,\\nunit standard deviation).\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, try to avoid a copy and scale in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'FunctionTransformer',\n",
       "      'func_desc': 'Constructs a transformer from an arbitrary callable.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer',\n",
       "      'function_definitions': {'function_name': 'FunctionTransformer',\n",
       "       'full_function': 'class sklearn.preprocessing.FunctionTransformer(func=None, inverse_func=None, *, validate=False, accept_sparse=False, check_inverse=True, feature_names_out=None, kw_args=None, inv_kw_args=None)',\n",
       "       'function_text': 'Constructs a transformer from an arbitrary callable. A FunctionTransformer forwards its X (and optionally y) arguments to a\\nuser-defined function or function object and returns the result of this\\nfunction. This is useful for stateless transformations such as taking the\\nlog of frequencies, doing custom scaling, etc. Note: If a lambda is used as the function, then the resulting\\ntransformer will not be pickleable. Added in version 0.17. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing.html#function-transformer',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse-matrix} of shape (n_samples, n_features)                 if validate=True else any object that func can handle',\n",
       "         'param_desc': 'Input array.\\n'}]},\n",
       "      'function_calling': {'name': 'FunctionTransformer',\n",
       "       'descriptions': 'Constructs a transformer from an arbitrary callable. A FunctionTransformer forwards its X (and optionally y) arguments to a\\nuser-defined function or function object and returns the result of this\\nfunction. This is useful for stateless transformations such as taking the\\nlog of frequencies, doing custom scaling, etc. Note: If a lambda is used as the function, then the resulting\\ntransformer will not be pickleable. Added in version 0.17. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse-matrix} of shape (n_samples, n_features)                 if validate=True else any object that func can handle. Input array.\\n'}},\n",
       "        'required': ['func=None', 'inverse_func=None']}}},\n",
       "     {'func_name': 'KernelCenterer',\n",
       "      'func_desc': 'Center an arbitrary kernel matrix \\\\(K\\\\).',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KernelCenterer.html#sklearn.preprocessing.KernelCenterer',\n",
       "      'function_definitions': {'function_name': 'KernelCenterer',\n",
       "       'full_function': 'class sklearn.preprocessing.KernelCenterer',\n",
       "       'function_text': 'Center an arbitrary kernel matrix \\\\(K\\\\). Let define a kernel \\\\(K\\\\) such that: \\\\(\\\\phi(X)\\\\) is a function mapping of rows of \\\\(X\\\\) to a\\nHilbert space and \\\\(K\\\\) is of shape (n_samples, n_samples). This class allows to compute \\\\(\\\\tilde{K}(X, Y)\\\\) such that: \\\\(\\\\tilde{\\\\phi}(X)\\\\) is the centered mapped data in the Hilbert\\nspace. KernelCenterer centers the features without explicitly computing the\\nmapping \\\\(\\\\phi(\\\\cdot)\\\\). Working with centered kernels is sometime\\nexpected when dealing with algebra computation such as eigendecomposition\\nfor KernelPCA for instance. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'LabelEncoder',\n",
       "      'func_desc': 'Encode target labels with value between 0 and n_classes-1.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder',\n",
       "      'function_definitions': {'function_name': 'LabelEncoder',\n",
       "       'full_function': 'class sklearn.preprocessing.LabelEncoder',\n",
       "       'function_text': 'Encode target labels with value between 0 and n_classes-1. This transformer should be used to encode target values, i.e. y, and\\nnot the input X. Read more in the User Guide. Added in version 0.12.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targets',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'MinMaxScaler',\n",
       "      'func_desc': 'Transform features by scaling each feature to a given range.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler',\n",
       "      'function_definitions': {'function_name': 'MinMaxScaler',\n",
       "       'full_function': 'class sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), *, copy=True, clip=False)',\n",
       "       'function_text': 'Transform features by scaling each feature to a given range. This estimator scales and translates each feature individually such\\nthat it is in the given range on the training set, e.g. between\\nzero and one. The transformation is given by: where min, max = feature_range. This transformation is often used as an alternative to zero mean,\\nunit variance scaling. MinMaxScaler doesn’t reduce the effect of outliers, but it linearly\\nscales them down into a fixed range, where the largest occurring data point\\ncorresponds to the maximum value and the smallest one corresponds to the\\nminimum value. For an example visualization, refer to Compare\\nMinMaxScaler with other scalers. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/preprocessing/plot_all_scaling.html#plot-all-scaling-minmax-scaler-section',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Input data that will be transformed.\\n'}]},\n",
       "      'function_calling': {'name': 'MinMaxScaler',\n",
       "       'descriptions': 'Transform features by scaling each feature to a given range. This estimator scales and translates each feature individually such\\nthat it is in the given range on the training set, e.g. between\\nzero and one. The transformation is given by: where min, max = feature_range. This transformation is often used as an alternative to zero mean,\\nunit variance scaling. MinMaxScaler doesn’t reduce the effect of outliers, but it linearly\\nscales them down into a fixed range, where the largest occurring data point\\ncorresponds to the maximum value and the smallest one corresponds to the\\nminimum value. For an example visualization, refer to Compare\\nMinMaxScaler with other scalers. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). Input data that will be transformed.\\n'}},\n",
       "        'required': ['feature_range=(0', '1)']}}},\n",
       "     {'func_name': 'Normalizer',\n",
       "      'func_desc': 'Normalize samples individually to unit norm.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer',\n",
       "      'function_definitions': {'function_name': 'Normalizer',\n",
       "       'full_function': \"class sklearn.preprocessing.Normalizer(norm='l2', *, copy=True)\",\n",
       "       'function_text': 'Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one\\nnon zero component is rescaled independently of other samples so\\nthat its norm (l1, l2 or inf) equals one. This transformer is able to work both with dense numpy arrays and\\nscipy.sparse matrix (use CSR format if you want to avoid the burden of\\na copy / conversion). Scaling inputs to unit norms is a common operation for text\\nclassification or clustering for instance. For instance the dot\\nproduct of two l2-normalized TF-IDF vectors is the cosine similarity\\nof the vectors and is the base similarity metric for the Vector\\nSpace Model commonly used by the Information Retrieval community. For an example visualization, refer to Compare Normalizer with other\\nscalers. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/preprocessing/plot_all_scaling.html#plot-all-scaling-normalizer-section',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to normalize, row by row. scipy.sparse matrices should be\\nin CSR format to avoid an un-necessary copy.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'Copy the input X or not.\\n'}]},\n",
       "      'function_calling': {'name': 'Normalizer',\n",
       "       'descriptions': 'Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one\\nnon zero component is rescaled independently of other samples so\\nthat its norm (l1, l2 or inf) equals one. This transformer is able to work both with dense numpy arrays and\\nscipy.sparse matrix (use CSR format if you want to avoid the burden of\\na copy / conversion). Scaling inputs to unit norms is a common operation for text\\nclassification or clustering for instance. For instance the dot\\nproduct of two l2-normalized TF-IDF vectors is the cosine similarity\\nof the vectors and is the base similarity metric for the Vector\\nSpace Model commonly used by the Information Retrieval community. For an example visualization, refer to Compare Normalizer with other\\nscalers. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data to normalize, row by row. scipy.sparse matrices should be\\nin CSR format to avoid an un-necessary copy.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. Copy the input X or not.\\n'}},\n",
       "        'required': [\"norm='l2'\"]}}},\n",
       "     {'func_name': 'OrdinalEncoder',\n",
       "      'func_desc': 'Encode categorical features as an integer array.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder',\n",
       "      'function_definitions': {'function_name': 'OrdinalEncoder',\n",
       "       'full_function': \"class sklearn.preprocessing.OrdinalEncoder(*, categories='auto', dtype=<class 'numpy.float64'>, handle_unknown='error', unknown_value=None, encoded_missing_value=nan, min_frequency=None, max_categories=None)\",\n",
       "       'function_text': 'Encode categorical features as an integer array. The input to this transformer should be an array-like of integers or\\nstrings, denoting the values taken on by categorical (discrete) features.\\nThe features are converted to ordinal integers. This results in\\na single column of integers (0 to n_categories - 1) per feature. Read more in the User Guide.\\nFor a comparison of different encoders, refer to:\\nComparing Target Encoder with Other Encoders. Added in version 0.20.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to encode.\\n'}]},\n",
       "      'function_calling': {'name': 'OrdinalEncoder',\n",
       "       'descriptions': 'Encode categorical features as an integer array. The input to this transformer should be an array-like of integers or\\nstrings, denoting the values taken on by categorical (discrete) features.\\nThe features are converted to ordinal integers. This results in\\na single column of integers (0 to n_categories - 1) per feature. Read more in the User Guide.\\nFor a comparison of different encoders, refer to:\\nComparing Target Encoder with Other Encoders. Added in version 0.20.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). The data to encode.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'PowerTransformer',\n",
       "      'func_desc': 'Apply a power transform featurewise to make data more Gaussian-like.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer',\n",
       "      'function_definitions': {'function_name': 'PowerTransformer',\n",
       "       'full_function': \"class sklearn.preprocessing.PowerTransformer(method='yeo-johnson', *, standardize=True, copy=True)\",\n",
       "       'function_text': 'Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations\\nthat are applied to make data more Gaussian-like. This is useful for\\nmodeling issues related to heteroscedasticity (non-constant variance),\\nor other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the\\nYeo-Johnson transform. The optimal parameter for stabilizing variance and\\nminimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson\\nsupports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the\\ntransformed data. For an example visualization, refer to Compare PowerTransformer with\\nother scalers. To see the\\neffect of Box-Cox and Yeo-Johnson transformations on different\\ndistributions, see:\\nMap data to a normal distribution. Read more in the User Guide. Added in version 0.20.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/preprocessing/plot_all_scaling.html#plot-all-scaling-power-transformer-section',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to be transformed using a power transformation.\\n'}]},\n",
       "      'function_calling': {'name': 'PowerTransformer',\n",
       "       'descriptions': 'Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations\\nthat are applied to make data more Gaussian-like. This is useful for\\nmodeling issues related to heteroscedasticity (non-constant variance),\\nor other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the\\nYeo-Johnson transform. The optimal parameter for stabilizing variance and\\nminimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson\\nsupports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the\\ntransformed data. For an example visualization, refer to Compare PowerTransformer with\\nother scalers. To see the\\neffect of Box-Cox and Yeo-Johnson transformations on different\\ndistributions, see:\\nMap data to a normal distribution. Read more in the User Guide. Added in version 0.20.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). The data to be transformed using a power transformation.\\n'}},\n",
       "        'required': [\"method='yeo-johnson'\"]}}},\n",
       "     {'func_name': 'RobustScaler',\n",
       "      'func_desc': 'Scale features using statistics that are robust to outliers.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler',\n",
       "      'function_definitions': {'function_name': 'RobustScaler',\n",
       "       'full_function': 'class sklearn.preprocessing.RobustScaler(*, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True, unit_variance=False)',\n",
       "       'function_text': 'Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to\\nthe quantile range (defaults to IQR: Interquartile Range).\\nThe IQR is the range between the 1st quartile (25th quantile)\\nand the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by\\ncomputing the relevant statistics on the samples in the training\\nset. Median and interquartile range are then stored to be used on\\nlater data using the transform method. Standardization of a dataset is a common preprocessing for many machine\\nlearning estimators. Typically this is done by removing the mean and\\nscaling to unit variance. However, outliers can often influence the sample\\nmean / variance in a negative way. In such cases, using the median and the\\ninterquartile range often give better results. For an example visualization\\nand comparison to other scalers, refer to Compare RobustScaler with\\nother scalers. Added in version 0.17. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#sklearn.preprocessing.RobustScaler.transform',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data used to scale along the specified axis.\\n'}]},\n",
       "      'function_calling': {'name': 'RobustScaler',\n",
       "       'descriptions': 'Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to\\nthe quantile range (defaults to IQR: Interquartile Range).\\nThe IQR is the range between the 1st quartile (25th quantile)\\nand the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by\\ncomputing the relevant statistics on the samples in the training\\nset. Median and interquartile range are then stored to be used on\\nlater data using the transform method. Standardization of a dataset is a common preprocessing for many machine\\nlearning estimators. Typically this is done by removing the mean and\\nscaling to unit variance. However, outliers can often influence the sample\\nmean / variance in a negative way. In such cases, using the median and the\\ninterquartile range often give better results. For an example visualization\\nand comparison to other scalers, refer to Compare RobustScaler with\\nother scalers. Added in version 0.17. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data used to scale along the specified axis.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'StandardScaler',\n",
       "      'func_desc': 'Standardize features by removing the mean and scaling to unit variance.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler',\n",
       "      'function_definitions': {'function_name': 'StandardScaler',\n",
       "       'full_function': 'class sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)',\n",
       "       'function_text': 'Standardize features by removing the mean and scaling to unit variance. The standard score of a sample x is calculated as: z = (x - u) / s where u is the mean of the training samples or zero if with_mean=False,\\nand s is the standard deviation of the training samples or one if\\nwith_std=False. Centering and scaling happen independently on each feature by computing\\nthe relevant statistics on the samples in the training set. Mean and\\nstandard deviation are then stored to be used on later data using\\ntransform. Standardization of a dataset is a common requirement for many\\nmachine learning estimators: they might behave badly if the\\nindividual features do not more or less look like standard normally\\ndistributed data (e.g. Gaussian with 0 mean and unit variance). For instance many elements used in the objective function of\\na learning algorithm (such as the RBF kernel of Support Vector\\nMachines or the L1 and L2 regularizers of linear models) assume that\\nall features are centered around 0 and have variance in the same\\norder. If a feature has a variance that is orders of magnitude larger\\nthan others, it might dominate the objective function and make the\\nestimator unable to learn from other features correctly as expected. StandardScaler is sensitive to outliers, and the features may scale\\ndifferently from each other in the presence of outliers. For an example\\nvisualization, refer to Compare StandardScaler with other scalers. This scaler can also be applied to sparse CSR or CSC matrices by passing\\nwith_mean=False to avoid breaking the sparsity structure of the data. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#sklearn.preprocessing.StandardScaler.transform',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data used to scale along the features axis.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=None',\n",
       "         'param_desc': 'Copy the input X or not.\\n'}]},\n",
       "      'function_calling': {'name': 'StandardScaler',\n",
       "       'descriptions': 'Standardize features by removing the mean and scaling to unit variance. The standard score of a sample x is calculated as: z = (x - u) / s where u is the mean of the training samples or zero if with_mean=False,\\nand s is the standard deviation of the training samples or one if\\nwith_std=False. Centering and scaling happen independently on each feature by computing\\nthe relevant statistics on the samples in the training set. Mean and\\nstandard deviation are then stored to be used on later data using\\ntransform. Standardization of a dataset is a common requirement for many\\nmachine learning estimators: they might behave badly if the\\nindividual features do not more or less look like standard normally\\ndistributed data (e.g. Gaussian with 0 mean and unit variance). For instance many elements used in the objective function of\\na learning algorithm (such as the RBF kernel of Support Vector\\nMachines or the L1 and L2 regularizers of linear models) assume that\\nall features are centered around 0 and have variance in the same\\norder. If a feature has a variance that is orders of magnitude larger\\nthan others, it might dominate the objective function and make the\\nestimator unable to learn from other features correctly as expected. StandardScaler is sensitive to outliers, and the features may scale\\ndifferently from each other in the presence of outliers. For an example\\nvisualization, refer to Compare StandardScaler with other scalers. This scaler can also be applied to sparse CSR or CSC matrices by passing\\nwith_mean=False to avoid breaking the sparsity structure of the data. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix of shape (n_samples, n_features). The data used to scale along the features axis.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=None. Copy the input X or not.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'add_dummy_feature',\n",
       "      'func_desc': 'Augment dataset with an additional dummy feature.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.add_dummy_feature.html#sklearn.preprocessing.add_dummy_feature',\n",
       "      'function_definitions': {'function_name': 'add_dummy_feature',\n",
       "       'full_function': 'sklearn.preprocessing.add_dummy_feature(X, value=1.0)',\n",
       "       'function_text': 'Augment dataset with an additional dummy feature. This is useful for fitting an intercept term with implementations which\\ncannot otherwise fit it directly.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Data.\\n'},\n",
       "        {'param_name': 'value',\n",
       "         'param_type': 'float',\n",
       "         'param_desc': 'Value to use for the dummy feature.\\n'}]},\n",
       "      'function_calling': {'name': 'add_dummy_feature',\n",
       "       'descriptions': 'Augment dataset with an additional dummy feature. This is useful for fitting an intercept term with implementations which\\ncannot otherwise fit it directly.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Data.\\n'},\n",
       "         'value': {'type': 'number',\n",
       "          'description': 'float. Value to use for the dummy feature.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'label_binarize',\n",
       "      'func_desc': 'Binarize labels in a one-vs-all fashion.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.label_binarize.html#sklearn.preprocessing.label_binarize',\n",
       "      'function_definitions': {'function_name': 'label_binarize',\n",
       "       'full_function': 'sklearn.preprocessing.label_binarize(y, *, classes, neg_label=0, pos_label=1, sparse_output=False)',\n",
       "       'function_text': 'Binarize labels in a one-vs-all fashion. Several regression and binary classification algorithms are\\navailable in scikit-learn. A simple way to extend these algorithms\\nto the multi-class classification case is to use the so-called\\none-vs-all scheme. This function makes it possible to compute this transformation for a\\nfixed set of class labels known ahead of time.',\n",
       "       'func_text_user_guide': 'sklearn.preprocessing.LabelBinarizer.html#sklearn.preprocessing.LabelBinarizer',\n",
       "       'parameter_names_desc': [{'param_name': 'y',\n",
       "         'param_type': 'array-like or sparse matrix',\n",
       "         'param_desc': 'Sequence of integer labels or multilabel data to encode.\\n'},\n",
       "        {'param_name': 'classes',\n",
       "         'param_type': 'array-like of shape (n_classes,)',\n",
       "         'param_desc': 'Uniquely holds the label for each class.\\n'},\n",
       "        {'param_name': 'neg_label',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Value with which negative labels must be encoded.\\n'},\n",
       "        {'param_name': 'pos_label',\n",
       "         'param_type': 'int, default=1',\n",
       "         'param_desc': 'Value with which positive labels must be encoded.\\n'},\n",
       "        {'param_name': 'sparse_output',\n",
       "         'param_type': 'bool, default=False,',\n",
       "         'param_desc': 'Set to true if output binary array is desired in CSR sparse format.\\n'}]},\n",
       "      'function_calling': {'name': 'label_binarize',\n",
       "       'descriptions': 'Binarize labels in a one-vs-all fashion. Several regression and binary classification algorithms are\\navailable in scikit-learn. A simple way to extend these algorithms\\nto the multi-class classification case is to use the so-called\\none-vs-all scheme. This function makes it possible to compute this transformation for a\\nfixed set of class labels known ahead of time.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y': {'type': 'array',\n",
       "          'description': 'array-like or sparse matrix. Sequence of integer labels or multilabel data to encode.\\n'},\n",
       "         'classes': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_classes,). Uniquely holds the label for each class.\\n'},\n",
       "         'neg_label': {'type': 'integer',\n",
       "          'description': 'int, default=0. Value with which negative labels must be encoded.\\n'},\n",
       "         'pos_label': {'type': 'integer',\n",
       "          'description': 'int, default=1. Value with which positive labels must be encoded.\\n'},\n",
       "         'sparse_output': {'type': 'boolean',\n",
       "          'description': 'bool, default=False,. Set to true if output binary array is desired in CSR sparse format.\\n'}},\n",
       "        'required': ['y']}}},\n",
       "     {'func_name': 'minmax_scale',\n",
       "      'func_desc': 'Transform features by scaling each feature to a given range.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html#sklearn.preprocessing.minmax_scale',\n",
       "      'function_definitions': {'function_name': 'minmax_scale',\n",
       "       'full_function': 'sklearn.preprocessing.minmax_scale(X, feature_range=(0, 1), *, axis=0, copy=True)',\n",
       "       'function_text': 'Transform features by scaling each feature to a given range. This estimator scales and translates each feature individually such\\nthat it is in the given range on the training set, i.e. between\\nzero and one. The transformation is given by (when axis=0): where min, max = feature_range. The transformation is calculated as (when axis=0): This transformation is often used as an alternative to zero mean,\\nunit variance scaling. Read more in the User Guide. Added in version 0.17: minmax_scale function interface\\nto MinMaxScaler.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data.\\n'},\n",
       "        {'param_name': 'feature_range',\n",
       "         'param_type': 'tuple (min, max), default=(0, 1)',\n",
       "         'param_desc': 'Desired range of transformed data.\\n'},\n",
       "        {'param_name': 'axis',\n",
       "         'param_type': '{0, 1}, default=0',\n",
       "         'param_desc': 'Axis used to scale along. If 0, independently scale each feature,\\notherwise (if 1) scale each sample.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, try to avoid a copy and scale in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n'}]},\n",
       "      'function_calling': {'name': 'minmax_scale',\n",
       "       'descriptions': 'Transform features by scaling each feature to a given range. This estimator scales and translates each feature individually such\\nthat it is in the given range on the training set, i.e. between\\nzero and one. The transformation is given by (when axis=0): where min, max = feature_range. The transformation is calculated as (when axis=0): This transformation is often used as an alternative to zero mean,\\nunit variance scaling. Read more in the User Guide. Added in version 0.17: minmax_scale function interface\\nto MinMaxScaler.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). The data.\\n'},\n",
       "         'feature_range': {'type': 'array',\n",
       "          'description': 'tuple (min, max), default=(0, 1). Desired range of transformed data.\\n'},\n",
       "         'axis': {'type': 'string',\n",
       "          'enum': ['0', '1'],\n",
       "          'description': '{0, 1}, default=0. Axis used to scale along. If 0, independently scale each feature,\\notherwise (if 1) scale each sample.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, try to avoid a copy and scale in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n'}},\n",
       "        'required': ['X', 'feature_range=(0', '1)']}}},\n",
       "     {'func_name': 'power_transform',\n",
       "      'func_desc': 'Parametric, monotonic transformation to make data more Gaussian-like.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.power_transform.html#sklearn.preprocessing.power_transform',\n",
       "      'function_definitions': {'function_name': 'power_transform',\n",
       "       'full_function': \"sklearn.preprocessing.power_transform(X, method='yeo-johnson', *, standardize=True, copy=True)\",\n",
       "       'function_text': 'Parametric, monotonic transformation to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations\\nthat are applied to make data more Gaussian-like. This is useful for\\nmodeling issues related to heteroscedasticity (non-constant variance),\\nor other situations where normality is desired. Currently, power_transform supports the Box-Cox transform and the\\nYeo-Johnson transform. The optimal parameter for stabilizing variance and\\nminimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson\\nsupports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the\\ntransformed data. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-transformer',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The data to be transformed using a power transformation.\\n'},\n",
       "        {'param_name': 'method',\n",
       "         'param_type': '{‘yeo-johnson’, ‘box-cox’}, default=’yeo-johnson’',\n",
       "         'param_desc': 'The power transform method. Available methods are:\\n\\n‘yeo-johnson’ [1], works with positive and negative values\\n‘box-cox’ [2], only works with strictly positive values\\n\\n\\nChanged in version 0.23: The default value of the method parameter changed from\\n‘box-cox’ to ‘yeo-johnson’ in 0.23.\\n\\n'},\n",
       "        {'param_name': 'standardize',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Set to True to apply zero-mean, unit-variance normalization to the\\ntransformed output.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, try to avoid a copy and transform in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n'}]},\n",
       "      'function_calling': {'name': 'power_transform',\n",
       "       'descriptions': 'Parametric, monotonic transformation to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations\\nthat are applied to make data more Gaussian-like. This is useful for\\nmodeling issues related to heteroscedasticity (non-constant variance),\\nor other situations where normality is desired. Currently, power_transform supports the Box-Cox transform and the\\nYeo-Johnson transform. The optimal parameter for stabilizing variance and\\nminimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson\\nsupports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the\\ntransformed data. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples, n_features). The data to be transformed using a power transformation.\\n'},\n",
       "         'method': {'type': 'string',\n",
       "          'enum': ['yeo-johnson', 'box-cox'],\n",
       "          'description': '{‘yeo-johnson’, ‘box-cox’}, default=’yeo-johnson’. The power transform method. Available methods are:\\n\\n‘yeo-johnson’ [1], works with positive and negative values\\n‘box-cox’ [2], only works with strictly positive values\\n\\n\\nChanged in version 0.23: The default value of the method parameter changed from\\n‘box-cox’ to ‘yeo-johnson’ in 0.23.\\n\\n'},\n",
       "         'standardize': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Set to True to apply zero-mean, unit-variance normalization to the\\ntransformed output.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, try to avoid a copy and transform in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n'}},\n",
       "        'required': ['X', \"method='yeo-johnson'\"]}}},\n",
       "     {'func_name': 'robust_scale',\n",
       "      'func_desc': 'Standardize a dataset along any axis.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.robust_scale.html#sklearn.preprocessing.robust_scale',\n",
       "      'function_definitions': {'function_name': 'robust_scale',\n",
       "       'full_function': 'sklearn.preprocessing.robust_scale(X, *, axis=0, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True, unit_variance=False)',\n",
       "       'function_text': 'Standardize a dataset along any axis. Center to the median and component wise scale\\naccording to the interquartile range. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_sample, n_features)',\n",
       "         'param_desc': 'The data to center and scale.\\n'},\n",
       "        {'param_name': 'axis',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Axis used to compute the medians and IQR along. If 0,\\nindependently scale each feature, otherwise (if 1) scale\\neach sample.\\n'},\n",
       "        {'param_name': 'with_centering',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, center the data before scaling.\\n'},\n",
       "        {'param_name': 'with_scaling',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, scale the data to unit variance (or equivalently,\\nunit standard deviation).\\n'},\n",
       "        {'param_name': 'quantile_range',\n",
       "         'param_type': 'tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0,        default=(25.0, 75.0)',\n",
       "         'param_desc': 'Quantile range used to calculate scale_. By default this is equal to\\nthe IQR, i.e., q_min is the first quantile and q_max is the third\\nquantile.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If False, try to avoid a copy and scale in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n'},\n",
       "        {'param_name': 'unit_variance',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, scale data so that normally distributed features have a\\nvariance of 1. In general, if the difference between the x-values of\\nq_max and q_min for a standard normal distribution is greater\\nthan 1, the dataset will be scaled down. If less than 1, the dataset\\nwill be scaled up.\\n\\nAdded in version 0.24.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'robust_scale',\n",
       "       'descriptions': 'Standardize a dataset along any axis. Center to the median and component wise scale\\naccording to the interquartile range. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_sample, n_features). The data to center and scale.\\n'},\n",
       "         'axis': {'type': 'integer',\n",
       "          'description': 'int, default=0. Axis used to compute the medians and IQR along. If 0,\\nindependently scale each feature, otherwise (if 1) scale\\neach sample.\\n'},\n",
       "         'with_centering': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, center the data before scaling.\\n'},\n",
       "         'with_scaling': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, scale the data to unit variance (or equivalently,\\nunit standard deviation).\\n'},\n",
       "         'quantile_range': {'type': 'array',\n",
       "          'description': 'tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0,        default=(25.0, 75.0). Quantile range used to calculate scale_. By default this is equal to\\nthe IQR, i.e., q_min is the first quantile and q_max is the third\\nquantile.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If False, try to avoid a copy and scale in place.\\nThis is not guaranteed to always work in place; e.g. if the data is\\na numpy array with an int dtype, a copy will be returned even with\\ncopy=False.\\n'},\n",
       "         'unit_variance': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, scale data so that normally distributed features have a\\nvariance of 1. In general, if the difference between the x-values of\\nq_max and q_min for a standard normal distribution is greater\\nthan 1, the dataset will be scaled down. If less than 1, the dataset\\nwill be scaled up.\\n\\nAdded in version 0.24.\\n\\n'}},\n",
       "        'required': ['X']}}}]}],\n",
       "  'name': 'sklearn.preprocessing',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.preprocessing.html'},\n",
       " 'sklearn.random_projection.html': {'functions': [{'defaults': [{'func_name': 'GaussianRandomProjection',\n",
       "      'func_desc': 'Reduce dimensionality through Gaussian random projection.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.GaussianRandomProjection.html#sklearn.random_projection.GaussianRandomProjection',\n",
       "      'function_definitions': {'function_name': 'GaussianRandomProjection',\n",
       "       'full_function': \"class sklearn.random_projection.GaussianRandomProjection(n_components='auto', *, eps=0.1, compute_inverse_components=False, random_state=None)\",\n",
       "       'function_text': 'Reduce dimensionality through Gaussian random projection. The components of the random matrix are drawn from N(0, 1 / n_components). Read more in the User Guide. Added in version 0.13.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/random_projection.html#gaussian-random-matrix',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{ndarray, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The input data to project into a smaller dimensional space.\\n'}]},\n",
       "      'function_calling': {'name': 'GaussianRandomProjection',\n",
       "       'descriptions': 'Reduce dimensionality through Gaussian random projection. The components of the random matrix are drawn from N(0, 1 / n_components). Read more in the User Guide. Added in version 0.13.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{ndarray, sparse matrix} of shape (n_samples, n_features). The input data to project into a smaller dimensional space.\\n'}},\n",
       "        'required': [\"n_components='auto'\"]}}},\n",
       "     {'func_name': 'johnson_lindenstrauss_min_dim',\n",
       "      'func_desc': \"Find a 'safe' number of components to randomly project to.\",\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.johnson_lindenstrauss_min_dim.html#sklearn.random_projection.johnson_lindenstrauss_min_dim',\n",
       "      'function_definitions': {'function_name': 'johnson_lindenstrauss_min_dim',\n",
       "       'full_function': 'sklearn.random_projection.johnson_lindenstrauss_min_dim(n_samples, *, eps=0.1)',\n",
       "       'function_text': 'Find a ‘safe’ number of components to randomly project to. The distortion introduced by a random projection p only changes the\\ndistance between two points by a factor (1 +- eps) in a euclidean space\\nwith good probability. The projection p is an eps-embedding as defined\\nby: (1 - eps) ||u - v||^2 < ||p(u) - p(v)||^2 < (1 + eps) ||u - v||^2 Where u and v are any rows taken from a dataset of shape (n_samples,\\nn_features), eps is in ]0, 1[ and p is a projection by a random Gaussian\\nN(0, 1) matrix of shape (n_components, n_features) (or a sparse\\nAchlioptas matrix). The minimum number of components to guarantee the eps-embedding is\\ngiven by: n_components >= 4 log(n_samples) / (eps^2 / 2 - eps^3 / 3) Note that the number of dimensions is independent of the original\\nnumber of features but instead depends on the size of the dataset:\\nthe larger the dataset, the higher is the minimal dimensionality of\\nan eps-embedding. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/random_projection.html#johnson-lindenstrauss',\n",
       "       'parameter_names_desc': [{'param_name': 'n_samples',\n",
       "         'param_type': 'int or array-like of int',\n",
       "         'param_desc': 'Number of samples that should be an integer greater than 0. If an array\\nis given, it will compute a safe number of components array-wise.\\n'},\n",
       "        {'param_name': 'eps',\n",
       "         'param_type': 'float or array-like of shape (n_components,), dtype=float,             default=0.1',\n",
       "         'param_desc': 'Maximum distortion rate in the range (0, 1) as defined by the\\nJohnson-Lindenstrauss lemma. If an array is given, it will compute a\\nsafe number of components array-wise.\\n'}]},\n",
       "      'function_calling': {'name': 'johnson_lindenstrauss_min_dim',\n",
       "       'descriptions': 'Find a ‘safe’ number of components to randomly project to. The distortion introduced by a random projection p only changes the\\ndistance between two points by a factor (1 +- eps) in a euclidean space\\nwith good probability. The projection p is an eps-embedding as defined\\nby: (1 - eps) ||u - v||^2 < ||p(u) - p(v)||^2 < (1 + eps) ||u - v||^2 Where u and v are any rows taken from a dataset of shape (n_samples,\\nn_features), eps is in ]0, 1[ and p is a projection by a random Gaussian\\nN(0, 1) matrix of shape (n_components, n_features) (or a sparse\\nAchlioptas matrix). The minimum number of components to guarantee the eps-embedding is\\ngiven by: n_components >= 4 log(n_samples) / (eps^2 / 2 - eps^3 / 3) Note that the number of dimensions is independent of the original\\nnumber of features but instead depends on the size of the dataset:\\nthe larger the dataset, the higher is the minimal dimensionality of\\nan eps-embedding. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_samples': {'type': 'integer',\n",
       "          'description': 'int or array-like of int. Number of samples that should be an integer greater than 0. If an array\\nis given, it will compute a safe number of components array-wise.\\n'},\n",
       "         'eps': {'type': 'number',\n",
       "          'description': 'float or array-like of shape (n_components,), dtype=float,             default=0.1. Maximum distortion rate in the range (0, 1) as defined by the\\nJohnson-Lindenstrauss lemma. If an array is given, it will compute a\\nsafe number of components array-wise.\\n'}},\n",
       "        'required': ['n_samples']}}},\n",
       "     {'func_name': 'SparseRandomProjection',\n",
       "      'func_desc': 'Reduce dimensionality through sparse random projection.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.SparseRandomProjection.html#sklearn.random_projection.SparseRandomProjection',\n",
       "      'function_definitions': {'function_name': 'SparseRandomProjection',\n",
       "       'full_function': \"class sklearn.random_projection.SparseRandomProjection(n_components='auto', *, density='auto', eps=0.1, dense_output=False, compute_inverse_components=False, random_state=None)\",\n",
       "       'function_text': 'Reduce dimensionality through sparse random projection. Sparse random matrix is an alternative to dense random\\nprojection matrix that guarantees similar embedding quality while being\\nmuch more memory efficient and allowing faster computation of the\\nprojected data. If we note s = 1 / density the components of the random matrix are\\ndrawn from: -sqrt(s) / sqrt(n_components)   with probability 1 / 2s 0                              with probability 1 - 1 / s +sqrt(s) / sqrt(n_components)   with probability 1 / 2s Read more in the User Guide. Added in version 0.13.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/random_projection.html#sparse-random-matrix',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{ndarray, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'The input data to project into a smaller dimensional space.\\n'}]},\n",
       "      'function_calling': {'name': 'SparseRandomProjection',\n",
       "       'descriptions': 'Reduce dimensionality through sparse random projection. Sparse random matrix is an alternative to dense random\\nprojection matrix that guarantees similar embedding quality while being\\nmuch more memory efficient and allowing faster computation of the\\nprojected data. If we note s = 1 / density the components of the random matrix are\\ndrawn from: -sqrt(s) / sqrt(n_components)   with probability 1 / 2s 0                              with probability 1 - 1 / s +sqrt(s) / sqrt(n_components)   with probability 1 / 2s Read more in the User Guide. Added in version 0.13.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{ndarray, sparse matrix} of shape (n_samples, n_features). The input data to project into a smaller dimensional space.\\n'}},\n",
       "        'required': [\"n_components='auto'\"]}}}]}],\n",
       "  'name': 'sklearn.random_projection',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.random_projection.html'},\n",
       " 'sklearn.semi_supervised.html': {'functions': [{'defaults': [{'func_name': 'LabelPropagation',\n",
       "      'func_desc': 'Label Propagation classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation',\n",
       "      'function_definitions': {'function_name': 'LabelPropagation',\n",
       "       'full_function': \"class sklearn.semi_supervised.LabelPropagation(kernel='rbf', *, gamma=20, n_neighbors=7, max_iter=1000, tol=0.001, n_jobs=None)\",\n",
       "       'function_text': 'Label Propagation classifier. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/semi_supervised.html#label-propagation',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'LabelPropagation',\n",
       "       'descriptions': 'Label Propagation classifier. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': [\"kernel='rbf'\"]}}},\n",
       "     {'func_name': 'SelfTrainingClassifier',\n",
       "      'func_desc': 'Self-training classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.SelfTrainingClassifier.html#sklearn.semi_supervised.SelfTrainingClassifier',\n",
       "      'function_definitions': {'function_name': 'SelfTrainingClassifier',\n",
       "       'full_function': \"class sklearn.semi_supervised.SelfTrainingClassifier(base_estimator, threshold=0.75, criterion='threshold', k_best=10, max_iter=10, verbose=False)\",\n",
       "       'function_text': 'Self-training classifier. This metaestimator allows a given supervised classifier to function as a\\nsemi-supervised classifier, allowing it to learn from unlabeled data. It\\ndoes this by iteratively predicting pseudo-labels for the unlabeled data\\nand adding them to the training set. The classifier will continue iterating until either max_iter is reached, or\\nno pseudo-labels were added to the training set in the previous iteration. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-metaestimator',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'SelfTrainingClassifier',\n",
       "       'descriptions': 'Self-training classifier. This metaestimator allows a given supervised classifier to function as a\\nsemi-supervised classifier, allowing it to learn from unlabeled data. It\\ndoes this by iteratively predicting pseudo-labels for the unlabeled data\\nand adding them to the training set. The classifier will continue iterating until either max_iter is reached, or\\nno pseudo-labels were added to the training set in the previous iteration. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object', 'properties': {}, 'required': []}}},\n",
       "     {'func_name': 'LabelSpreading',\n",
       "      'func_desc': 'LabelSpreading model for semi-supervised learning.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading',\n",
       "      'function_definitions': {'function_name': 'LabelSpreading',\n",
       "       'full_function': \"class sklearn.semi_supervised.LabelSpreading(kernel='rbf', *, gamma=20, n_neighbors=7, alpha=0.2, max_iter=30, tol=0.001, n_jobs=None)\",\n",
       "       'function_text': 'LabelSpreading model for semi-supervised learning. This model is similar to the basic Label Propagation algorithm,\\nbut uses affinity matrix based on the normalized graph Laplacian\\nand soft clamping across the labels. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/semi_supervised.html#label-propagation',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'LabelSpreading',\n",
       "       'descriptions': 'LabelSpreading model for semi-supervised learning. This model is similar to the basic Label Propagation algorithm,\\nbut uses affinity matrix based on the normalized graph Laplacian\\nand soft clamping across the labels. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': [\"kernel='rbf'\"]}}}]}],\n",
       "  'name': 'sklearn.semi_supervised',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.semi_supervised.html'},\n",
       " 'sklearn.svm.html': {'functions': [{'defaults': [{'func_name': 'LinearSVC',\n",
       "      'func_desc': 'Linear Support Vector Classification.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC',\n",
       "      'function_definitions': {'function_name': 'LinearSVC',\n",
       "       'full_function': \"class sklearn.svm.LinearSVC(penalty='l2', loss='squared_hinge', *, dual='auto', tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\",\n",
       "       'function_text': 'Linear Support Vector Classification. Similar to SVC with parameter kernel=’linear’, but implemented in terms of\\nliblinear rather than libsvm, so it has more flexibility in the choice of\\npenalties and loss functions and should scale better to large numbers of\\nsamples. The main differences between LinearSVC and\\nSVC lie in the loss function used by default, and in\\nthe handling of intercept regularization between those two implementations. This class supports both dense and sparse input and the multiclass support\\nis handled according to a one-vs-the-rest scheme. Read more in the User Guide.',\n",
       "       'func_text_user_guide': '#sklearn.svm.LinearSVC',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'NuSVC',\n",
       "      'func_desc': 'Nu-Support Vector Classification.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC',\n",
       "      'function_definitions': {'function_name': 'NuSVC',\n",
       "       'full_function': \"class sklearn.svm.NuSVC(*, nu=0.5, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\",\n",
       "       'function_text': 'Nu-Support Vector Classification. Similar to SVC but uses a parameter to control the number of support\\nvectors. The implementation is based on libsvm. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/svm.html#svm-classification',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'NuSVC',\n",
       "       'descriptions': 'Nu-Support Vector Classification. Similar to SVC but uses a parameter to control the number of support\\nvectors. The implementation is based on libsvm. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'OneClassSVM',\n",
       "      'func_desc': 'Unsupervised Outlier Detection.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM',\n",
       "      'function_definitions': {'function_name': 'OneClassSVM',\n",
       "       'full_function': \"class sklearn.svm.OneClassSVM(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, nu=0.5, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\",\n",
       "       'function_text': 'Unsupervised Outlier Detection. Estimate the support of a high-dimensional distribution. The implementation is based on libsvm. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection',\n",
       "       'parameter_names_desc': [{'param_name': '**params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'Estimator parameters.\\n'}]},\n",
       "      'function_calling': {'name': 'OneClassSVM',\n",
       "       'descriptions': 'Unsupervised Outlier Detection. Estimate the support of a high-dimensional distribution. The implementation is based on libsvm. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object', 'properties': {}, 'required': []}}},\n",
       "     {'func_name': 'SVR',\n",
       "      'func_desc': 'Epsilon-Support Vector Regression.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR',\n",
       "      'function_definitions': {'function_name': 'SVR',\n",
       "       'full_function': \"class sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\",\n",
       "       'function_text': 'Epsilon-Support Vector Regression. The free parameters in the model are C and epsilon. The implementation is based on libsvm. The fit time complexity\\nis more than quadratic with the number of samples which makes it hard\\nto scale to datasets with more than a couple of 10000 samples. For large\\ndatasets consider using LinearSVR or\\nSGDRegressor instead, possibly after a\\nNystroem transformer or\\nother Kernel Approximation. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'SVR',\n",
       "       'descriptions': 'Epsilon-Support Vector Regression. The free parameters in the model are C and epsilon. The implementation is based on libsvm. The fit time complexity\\nis more than quadratic with the number of samples which makes it hard\\nto scale to datasets with more than a couple of 10000 samples. For large\\ndatasets consider using LinearSVR or\\nSGDRegressor instead, possibly after a\\nNystroem transformer or\\nother Kernel Approximation. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'LinearSVR',\n",
       "      'func_desc': 'Linear Support Vector Regression.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR',\n",
       "      'function_definitions': {'function_name': 'LinearSVR',\n",
       "       'full_function': \"class sklearn.svm.LinearSVR(*, epsilon=0.0, tol=0.0001, C=1.0, loss='epsilon_insensitive', fit_intercept=True, intercept_scaling=1.0, dual='auto', verbose=0, random_state=None, max_iter=1000)\",\n",
       "       'function_text': 'Linear Support Vector Regression. Similar to SVR with parameter kernel=’linear’, but implemented in terms of\\nliblinear rather than libsvm, so it has more flexibility in the choice of\\npenalties and loss functions and should scale better to large numbers of\\nsamples. The main differences between LinearSVR and\\nSVR lie in the loss function used by default, and in\\nthe handling of intercept regularization between those two implementations. This class supports both dense and sparse input. Read more in the User Guide. Added in version 0.16.',\n",
       "       'func_text_user_guide': '#sklearn.svm.LinearSVR',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'LinearSVR',\n",
       "       'descriptions': 'Linear Support Vector Regression. Similar to SVR with parameter kernel=’linear’, but implemented in terms of\\nliblinear rather than libsvm, so it has more flexibility in the choice of\\npenalties and loss functions and should scale better to large numbers of\\nsamples. The main differences between LinearSVR and\\nSVR lie in the loss function used by default, and in\\nthe handling of intercept regularization between those two implementations. This class supports both dense and sparse input. Read more in the User Guide. Added in version 0.16.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'NuSVR',\n",
       "      'func_desc': 'Nu Support Vector Regression.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR',\n",
       "      'function_definitions': {'function_name': 'NuSVR',\n",
       "       'full_function': \"class sklearn.svm.NuSVR(*, nu=0.5, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, tol=0.001, cache_size=200, verbose=False, max_iter=-1)\",\n",
       "       'function_text': 'Nu Support Vector Regression. Similar to NuSVC, for regression, uses a parameter nu to control\\nthe number of support vectors. However, unlike NuSVC, where nu\\nreplaces C, here nu replaces the parameter epsilon of epsilon-SVR. The implementation is based on libsvm. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/svm.html#svm-regression',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'NuSVR',\n",
       "       'descriptions': 'Nu Support Vector Regression. Similar to NuSVC, for regression, uses a parameter nu to control\\nthe number of support vectors. However, unlike NuSVC, where nu\\nreplaces C, here nu replaces the parameter epsilon of epsilon-SVR. The implementation is based on libsvm. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'SVC',\n",
       "      'func_desc': 'C-Support Vector Classification.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC',\n",
       "      'function_definitions': {'function_name': 'SVC',\n",
       "       'full_function': \"class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\",\n",
       "       'function_text': 'C-Support Vector Classification. The implementation is based on libsvm. The fit time scales at least\\nquadratically with the number of samples and may be impractical\\nbeyond tens of thousands of samples. For large datasets\\nconsider using LinearSVC or\\nSGDClassifier instead, possibly after a\\nNystroem transformer or\\nother Kernel Approximation. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided\\nkernel functions and how gamma, coef0 and degree affect each\\nother, see the corresponding section in the narrative documentation:\\nKernel functions. To learn how to tune SVC’s hyperparameters, see the following example:\\nNested versus non-nested cross-validation Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'SVC',\n",
       "       'descriptions': 'C-Support Vector Classification. The implementation is based on libsvm. The fit time scales at least\\nquadratically with the number of samples and may be impractical\\nbeyond tens of thousands of samples. For large datasets\\nconsider using LinearSVC or\\nSGDClassifier instead, possibly after a\\nNystroem transformer or\\nother Kernel Approximation. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided\\nkernel functions and how gamma, coef0 and degree affect each\\nother, see the corresponding section in the narrative documentation:\\nKernel functions. To learn how to tune SVC’s hyperparameters, see the following example:\\nNested versus non-nested cross-validation Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'l1_min_c',\n",
       "      'func_desc': 'Return the lowest bound for C.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.svm.l1_min_c.html#sklearn.svm.l1_min_c',\n",
       "      'function_definitions': {'function_name': 'l1_min_c',\n",
       "       'full_function': \"sklearn.svm.l1_min_c(X, y, *, loss='squared_hinge', fit_intercept=True, intercept_scaling=1.0)\",\n",
       "       'function_text': 'Return the lowest bound for C. The lower bound for C is computed such that for C in (l1_min_C, infinity)\\nthe model is guaranteed not to be empty. This applies to l1 penalized\\nclassifiers, such as LinearSVC with penalty=’l1’ and\\nlinear_model.LogisticRegression with penalty=’l1’. This value is valid if class_weight parameter in fit() is not set.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Training vector, where n_samples is the number of samples and\\nn_features is the number of features.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Target vector relative to X.\\n'},\n",
       "        {'param_name': 'loss',\n",
       "         'param_type': '{‘squared_hinge’, ‘log’}, default=’squared_hinge’',\n",
       "         'param_desc': 'Specifies the loss function.\\nWith ‘squared_hinge’ it is the squared hinge loss (a.k.a. L2 loss).\\nWith ‘log’ it is the loss of logistic regression models.\\n'},\n",
       "        {'param_name': 'fit_intercept',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Specifies if the intercept should be fitted by the model.\\nIt must match the fit() method parameter.\\n'},\n",
       "        {'param_name': 'intercept_scaling',\n",
       "         'param_type': 'float, default=1.0',\n",
       "         'param_desc': 'When fit_intercept is True, instance vector x becomes\\n[x, intercept_scaling],\\ni.e. a “synthetic” feature with constant value equals to\\nintercept_scaling is appended to the instance vector.\\nIt must match the fit() method parameter.\\n'}]},\n",
       "      'function_calling': {'name': 'l1_min_c',\n",
       "       'descriptions': 'Return the lowest bound for C. The lower bound for C is computed such that for C in (l1_min_C, infinity)\\nthe model is guaranteed not to be empty. This applies to l1 penalized\\nclassifiers, such as LinearSVC with penalty=’l1’ and\\nlinear_model.LogisticRegression with penalty=’l1’. This value is valid if class_weight parameter in fit() is not set.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples, n_features). Training vector, where n_samples is the number of samples and\\nn_features is the number of features.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Target vector relative to X.\\n'},\n",
       "         'loss': {'type': 'string',\n",
       "          'enum': ['squared_hinge', 'log'],\n",
       "          'description': '{‘squared_hinge’, ‘log’}, default=’squared_hinge’. Specifies the loss function.\\nWith ‘squared_hinge’ it is the squared hinge loss (a.k.a. L2 loss).\\nWith ‘log’ it is the loss of logistic regression models.\\n'},\n",
       "         'fit_intercept': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Specifies if the intercept should be fitted by the model.\\nIt must match the fit() method parameter.\\n'},\n",
       "         'intercept_scaling': {'type': 'number',\n",
       "          'description': 'float, default=1.0. When fit_intercept is True, instance vector x becomes\\n[x, intercept_scaling],\\ni.e. a “synthetic” feature with constant value equals to\\nintercept_scaling is appended to the instance vector.\\nIt must match the fit() method parameter.\\n'}},\n",
       "        'required': ['X', 'y']}}}]}],\n",
       "  'name': 'sklearn.svm',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.svm.html'},\n",
       " 'sklearn.tree.html': {'functions': [{'defaults': [{'func_name': 'DecisionTreeClassifier',\n",
       "      'func_desc': 'A decision tree classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier',\n",
       "      'function_definitions': {'function_name': 'DecisionTreeClassifier',\n",
       "       'full_function': \"class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0, monotonic_cst=None)\",\n",
       "       'function_text': 'A decision tree classifier. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/tree.html#tree',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'DecisionTreeClassifier',\n",
       "       'descriptions': 'A decision tree classifier. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'ExtraTreeClassifier',\n",
       "      'func_desc': 'An extremely randomized tree classifier.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier',\n",
       "      'function_definitions': {'function_name': 'ExtraTreeClassifier',\n",
       "       'full_function': \"class sklearn.tree.ExtraTreeClassifier(*, criterion='gini', splitter='random', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0, monotonic_cst=None)\",\n",
       "       'function_text': 'An extremely randomized tree classifier. Extra-trees differ from classic decision trees in the way they are built.\\nWhen looking for the best split to separate the samples of a node into two\\ngroups, random splits are drawn for each of the max_features randomly\\nselected features and the best split among those is chosen. When\\nmax_features is set 1, this amounts to building a totally random\\ndecision tree. Warning: Extra-trees should only be used within ensemble methods. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/tree.html#tree',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'ExtraTreeClassifier',\n",
       "       'descriptions': 'An extremely randomized tree classifier. Extra-trees differ from classic decision trees in the way they are built.\\nWhen looking for the best split to separate the samples of a node into two\\ngroups, random splits are drawn for each of the max_features randomly\\nselected features and the best split among those is chosen. When\\nmax_features is set 1, this amounts to building a totally random\\ndecision tree. Warning: Extra-trees should only be used within ensemble methods. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'DecisionTreeRegressor',\n",
       "      'func_desc': 'A decision tree regressor.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor',\n",
       "      'function_definitions': {'function_name': 'DecisionTreeRegressor',\n",
       "       'full_function': \"class sklearn.tree.DecisionTreeRegressor(*, criterion='squared_error', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0, monotonic_cst=None)\",\n",
       "       'function_text': 'A decision tree regressor. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/tree.html#tree',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'DecisionTreeRegressor',\n",
       "       'descriptions': 'A decision tree regressor. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'ExtraTreeRegressor',\n",
       "      'func_desc': 'An extremely randomized tree regressor.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeRegressor.html#sklearn.tree.ExtraTreeRegressor',\n",
       "      'function_definitions': {'function_name': 'ExtraTreeRegressor',\n",
       "       'full_function': \"class sklearn.tree.ExtraTreeRegressor(*, criterion='squared_error', splitter='random', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, random_state=None, min_impurity_decrease=0.0, max_leaf_nodes=None, ccp_alpha=0.0, monotonic_cst=None)\",\n",
       "       'function_text': 'An extremely randomized tree regressor. Extra-trees differ from classic decision trees in the way they are built.\\nWhen looking for the best split to separate the samples of a node into two\\ngroups, random splits are drawn for each of the max_features randomly\\nselected features and the best split among those is chosen. When\\nmax_features is set 1, this amounts to building a totally random\\ndecision tree. Warning: Extra-trees should only be used within ensemble methods. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/tree.html#tree',\n",
       "       'parameter_names_desc': [{'param_name': 'sample_weight',\n",
       "         'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED',\n",
       "         'param_desc': 'Metadata routing for sample_weight parameter in score.\\n'}]},\n",
       "      'function_calling': {'name': 'ExtraTreeRegressor',\n",
       "       'descriptions': 'An extremely randomized tree regressor. Extra-trees differ from classic decision trees in the way they are built.\\nWhen looking for the best split to separate the samples of a node into two\\ngroups, random splits are drawn for each of the max_features randomly\\nselected features and the best split among those is chosen. When\\nmax_features is set 1, this amounts to building a totally random\\ndecision tree. Warning: Extra-trees should only be used within ensemble methods. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'sample_weight': {'type': 'string',\n",
       "          'description': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Exporting': [{'func_name': 'export_graphviz',\n",
       "      'func_desc': 'Export a decision tree in DOT format.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz',\n",
       "      'function_definitions': {'function_name': 'export_graphviz',\n",
       "       'full_function': \"sklearn.tree.export_graphviz(decision_tree, out_file=None, *, max_depth=None, feature_names=None, class_names=None, label='all', filled=False, leaves_parallel=False, impurity=True, node_ids=False, proportion=False, rotate=False, rounded=False, special_characters=False, precision=3, fontname='helvetica')\",\n",
       "       'function_text': 'Export a decision tree in DOT format. This function generates a GraphViz representation of the decision tree,\\nwhich is then written into out_file. Once exported, graphical renderings\\ncan be generated using, for example: The sample counts that are shown are weighted with any sample_weights that\\nmight be present. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/tree.html#tree',\n",
       "       'parameter_names_desc': [{'param_name': 'decision_tree',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'The decision tree estimator to be exported to GraphViz.\\n'},\n",
       "        {'param_name': 'out_file',\n",
       "         'param_type': 'object or str, default=None',\n",
       "         'param_desc': 'Handle or name of the output file. If None, the result is\\nreturned as a string.\\n\\nChanged in version 0.20: Default of out_file changed from “tree.dot” to None.\\n\\n'},\n",
       "        {'param_name': 'max_depth',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The maximum depth of the representation. If None, the tree is fully\\ngenerated.\\n'},\n",
       "        {'param_name': 'feature_names',\n",
       "         'param_type': 'array-like of shape (n_features,), default=None',\n",
       "         'param_desc': 'An array containing the feature names.\\nIf None, generic names will be used (“x[0]”, “x[1]”, …).\\n'},\n",
       "        {'param_name': 'class_names',\n",
       "         'param_type': 'array-like of shape (n_classes,) or bool, default=None',\n",
       "         'param_desc': 'Names of each of the target classes in ascending numerical order.\\nOnly relevant for classification and not supported for multi-output.\\nIf True, shows a symbolic representation of the class name.\\n'},\n",
       "        {'param_name': 'label',\n",
       "         'param_type': '{‘all’, ‘root’, ‘none’}, default=’all’',\n",
       "         'param_desc': 'Whether to show informative labels for impurity, etc.\\nOptions include ‘all’ to show at every node, ‘root’ to show only at\\nthe top root node, or ‘none’ to not show at any node.\\n'},\n",
       "        {'param_name': 'filled',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When set to True, paint nodes to indicate majority class for\\nclassification, extremity of values for regression, or purity of node\\nfor multi-output.\\n'},\n",
       "        {'param_name': 'leaves_parallel',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When set to True, draw all leaf nodes at the bottom of the tree.\\n'},\n",
       "        {'param_name': 'impurity',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'When set to True, show the impurity at each node.\\n'},\n",
       "        {'param_name': 'node_ids',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When set to True, show the ID number on each node.\\n'},\n",
       "        {'param_name': 'proportion',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When set to True, change the display of ‘values’ and/or ‘samples’\\nto be proportions and percentages respectively.\\n'},\n",
       "        {'param_name': 'rotate',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When set to True, orient tree left to right rather than top-down.\\n'},\n",
       "        {'param_name': 'rounded',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When set to True, draw node boxes with rounded corners.\\n'},\n",
       "        {'param_name': 'special_characters',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When set to False, ignore special characters for PostScript\\ncompatibility.\\n'},\n",
       "        {'param_name': 'precision',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of digits of precision for floating point in the values of\\nimpurity, threshold and value attributes of each node.\\n'},\n",
       "        {'param_name': 'fontname',\n",
       "         'param_type': 'str, default=’helvetica’',\n",
       "         'param_desc': 'Name of font used to render text.\\n'}]},\n",
       "      'function_calling': {'name': 'export_graphviz',\n",
       "       'descriptions': 'Export a decision tree in DOT format. This function generates a GraphViz representation of the decision tree,\\nwhich is then written into out_file. Once exported, graphical renderings\\ncan be generated using, for example: The sample counts that are shown are weighted with any sample_weights that\\nmight be present. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'decision_tree': {'type': 'object',\n",
       "          'description': 'object. The decision tree estimator to be exported to GraphViz.\\n'},\n",
       "         'out_file': {'type': 'string',\n",
       "          'description': 'object or str, default=None. Handle or name of the output file. If None, the result is\\nreturned as a string.\\n\\nChanged in version 0.20: Default of out_file changed from “tree.dot” to None.\\n\\n'},\n",
       "         'max_depth': {'type': 'integer',\n",
       "          'description': 'int, default=None. The maximum depth of the representation. If None, the tree is fully\\ngenerated.\\n'},\n",
       "         'feature_names': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_features,), default=None. An array containing the feature names.\\nIf None, generic names will be used (“x[0]”, “x[1]”, …).\\n'},\n",
       "         'class_names': {'type': 'boolean',\n",
       "          'description': 'array-like of shape (n_classes,) or bool, default=None. Names of each of the target classes in ascending numerical order.\\nOnly relevant for classification and not supported for multi-output.\\nIf True, shows a symbolic representation of the class name.\\n'},\n",
       "         'label': {'type': 'string',\n",
       "          'enum': ['all', 'root', 'none'],\n",
       "          'description': '{‘all’, ‘root’, ‘none’}, default=’all’. Whether to show informative labels for impurity, etc.\\nOptions include ‘all’ to show at every node, ‘root’ to show only at\\nthe top root node, or ‘none’ to not show at any node.\\n'},\n",
       "         'filled': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When set to True, paint nodes to indicate majority class for\\nclassification, extremity of values for regression, or purity of node\\nfor multi-output.\\n'},\n",
       "         'leaves_parallel': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When set to True, draw all leaf nodes at the bottom of the tree.\\n'},\n",
       "         'impurity': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. When set to True, show the impurity at each node.\\n'},\n",
       "         'node_ids': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When set to True, show the ID number on each node.\\n'},\n",
       "         'proportion': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When set to True, change the display of ‘values’ and/or ‘samples’\\nto be proportions and percentages respectively.\\n'},\n",
       "         'rotate': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When set to True, orient tree left to right rather than top-down.\\n'},\n",
       "         'rounded': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When set to True, draw node boxes with rounded corners.\\n'},\n",
       "         'special_characters': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When set to False, ignore special characters for PostScript\\ncompatibility.\\n'},\n",
       "         'precision': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of digits of precision for floating point in the values of\\nimpurity, threshold and value attributes of each node.\\n'},\n",
       "         'fontname': {'type': 'string',\n",
       "          'description': 'str, default=’helvetica’. Name of font used to render text.\\n'}},\n",
       "        'required': ['decision_tree', 'out_file=None']}}},\n",
       "     {'func_name': 'export_text',\n",
       "      'func_desc': 'Build a text report showing the rules of a decision tree.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_text.html#sklearn.tree.export_text',\n",
       "      'function_definitions': {'function_name': 'export_text',\n",
       "       'full_function': 'sklearn.tree.export_text(decision_tree, *, feature_names=None, class_names=None, max_depth=10, spacing=3, decimals=2, show_weights=False)',\n",
       "       'function_text': 'Build a text report showing the rules of a decision tree. Note that backwards compatibility may not be supported.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'decision_tree',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'The decision tree estimator to be exported.\\nIt can be an instance of\\nDecisionTreeClassifier or DecisionTreeRegressor.\\n'},\n",
       "        {'param_name': 'feature_names',\n",
       "         'param_type': 'array-like of shape (n_features,), default=None',\n",
       "         'param_desc': 'An array containing the feature names.\\nIf None generic names will be used (“feature_0”, “feature_1”, …).\\n'},\n",
       "        {'param_name': 'class_names',\n",
       "         'param_type': 'array-like of shape (n_classes,), default=None',\n",
       "         'param_desc': 'Names of each of the target classes in ascending numerical order.\\nOnly relevant for classification and not supported for multi-output.\\n\\nif None, the class names are delegated to decision_tree.classes_;\\notherwise, class_names will be used as class names instead of\\ndecision_tree.classes_. The length of class_names must match\\nthe length of decision_tree.classes_.\\n\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "        {'param_name': 'max_depth',\n",
       "         'param_type': 'int, default=10',\n",
       "         'param_desc': 'Only the first max_depth levels of the tree are exported.\\nTruncated branches will be marked with “…”.\\n'},\n",
       "        {'param_name': 'spacing',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of spaces between edges. The higher it is, the wider the result.\\n'},\n",
       "        {'param_name': 'decimals',\n",
       "         'param_type': 'int, default=2',\n",
       "         'param_desc': 'Number of decimal digits to display.\\n'},\n",
       "        {'param_name': 'show_weights',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If true the classification weights will be exported on each leaf.\\nThe classification weights are the number of samples each class.\\n'}]},\n",
       "      'function_calling': {'name': 'export_text',\n",
       "       'descriptions': 'Build a text report showing the rules of a decision tree. Note that backwards compatibility may not be supported.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'decision_tree': {'type': 'object',\n",
       "          'description': 'object. The decision tree estimator to be exported.\\nIt can be an instance of\\nDecisionTreeClassifier or DecisionTreeRegressor.\\n'},\n",
       "         'feature_names': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_features,), default=None. An array containing the feature names.\\nIf None generic names will be used (“feature_0”, “feature_1”, …).\\n'},\n",
       "         'class_names': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_classes,), default=None. Names of each of the target classes in ascending numerical order.\\nOnly relevant for classification and not supported for multi-output.\\n\\nif None, the class names are delegated to decision_tree.classes_;\\notherwise, class_names will be used as class names instead of\\ndecision_tree.classes_. The length of class_names must match\\nthe length of decision_tree.classes_.\\n\\n\\nAdded in version 1.3.\\n\\n'},\n",
       "         'max_depth': {'type': 'integer',\n",
       "          'description': 'int, default=10. Only the first max_depth levels of the tree are exported.\\nTruncated branches will be marked with “…”.\\n'},\n",
       "         'spacing': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of spaces between edges. The higher it is, the wider the result.\\n'},\n",
       "         'decimals': {'type': 'integer',\n",
       "          'description': 'int, default=2. Number of decimal digits to display.\\n'},\n",
       "         'show_weights': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If true the classification weights will be exported on each leaf.\\nThe classification weights are the number of samples each class.\\n'}},\n",
       "        'required': ['decision_tree']}}}]},\n",
       "   {'Plotting': [{'func_name': 'plot_tree',\n",
       "      'func_desc': 'Plot a decision tree.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree',\n",
       "      'function_definitions': {'function_name': 'plot_tree',\n",
       "       'full_function': \"sklearn.tree.plot_tree(decision_tree, *, max_depth=None, feature_names=None, class_names=None, label='all', filled=False, impurity=True, node_ids=False, proportion=False, rounded=False, precision=3, ax=None, fontsize=None)\",\n",
       "       'function_text': 'Plot a decision tree. The sample counts that are shown are weighted with any sample_weights that\\nmight be present. The visualization is fit automatically to the size of the axis.\\nUse the figsize or dpi arguments of plt.figure  to control\\nthe size of the rendering. Read more in the User Guide. Added in version 0.21.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/tree.html#tree',\n",
       "       'parameter_names_desc': [{'param_name': 'decision_tree',\n",
       "         'param_type': 'decision tree regressor or classifier',\n",
       "         'param_desc': 'The decision tree to be plotted.\\n'},\n",
       "        {'param_name': 'max_depth',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'The maximum depth of the representation. If None, the tree is fully\\ngenerated.\\n'},\n",
       "        {'param_name': 'feature_names',\n",
       "         'param_type': 'array-like of str, default=None',\n",
       "         'param_desc': 'Names of each of the features.\\nIf None, generic names will be used (“x[0]”, “x[1]”, …).\\n'},\n",
       "        {'param_name': 'class_names',\n",
       "         'param_type': 'array-like of str or True, default=None',\n",
       "         'param_desc': 'Names of each of the target classes in ascending numerical order.\\nOnly relevant for classification and not supported for multi-output.\\nIf True, shows a symbolic representation of the class name.\\n'},\n",
       "        {'param_name': 'label',\n",
       "         'param_type': '{‘all’, ‘root’, ‘none’}, default=’all’',\n",
       "         'param_desc': 'Whether to show informative labels for impurity, etc.\\nOptions include ‘all’ to show at every node, ‘root’ to show only at\\nthe top root node, or ‘none’ to not show at any node.\\n'},\n",
       "        {'param_name': 'filled',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When set to True, paint nodes to indicate majority class for\\nclassification, extremity of values for regression, or purity of node\\nfor multi-output.\\n'},\n",
       "        {'param_name': 'impurity',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'When set to True, show the impurity at each node.\\n'},\n",
       "        {'param_name': 'node_ids',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When set to True, show the ID number on each node.\\n'},\n",
       "        {'param_name': 'proportion',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When set to True, change the display of ‘values’ and/or ‘samples’\\nto be proportions and percentages respectively.\\n'},\n",
       "        {'param_name': 'rounded',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When set to True, draw node boxes with rounded corners and use\\nHelvetica fonts instead of Times-Roman.\\n'},\n",
       "        {'param_name': 'precision',\n",
       "         'param_type': 'int, default=3',\n",
       "         'param_desc': 'Number of digits of precision for floating point in the values of\\nimpurity, threshold and value attributes of each node.\\n'},\n",
       "        {'param_name': 'ax',\n",
       "         'param_type': 'matplotlib axis, default=None',\n",
       "         'param_desc': 'Axes to plot to. If None, use current axis. Any previous content\\nis cleared.\\n'},\n",
       "        {'param_name': 'fontsize',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Size of text font. If None, determined automatically to fit figure.\\n'}]},\n",
       "      'function_calling': {'name': 'plot_tree',\n",
       "       'descriptions': 'Plot a decision tree. The sample counts that are shown are weighted with any sample_weights that\\nmight be present. The visualization is fit automatically to the size of the axis.\\nUse the figsize or dpi arguments of plt.figure  to control\\nthe size of the rendering. Read more in the User Guide. Added in version 0.21.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'decision_tree': {'type': 'object',\n",
       "          'description': 'decision tree regressor or classifier. The decision tree to be plotted.\\n'},\n",
       "         'max_depth': {'type': 'integer',\n",
       "          'description': 'int, default=None. The maximum depth of the representation. If None, the tree is fully\\ngenerated.\\n'},\n",
       "         'feature_names': {'type': 'string',\n",
       "          'description': 'array-like of str, default=None. Names of each of the features.\\nIf None, generic names will be used (“x[0]”, “x[1]”, …).\\n'},\n",
       "         'class_names': {'type': 'string',\n",
       "          'description': 'array-like of str or True, default=None. Names of each of the target classes in ascending numerical order.\\nOnly relevant for classification and not supported for multi-output.\\nIf True, shows a symbolic representation of the class name.\\n'},\n",
       "         'label': {'type': 'string',\n",
       "          'enum': ['all', 'root', 'none'],\n",
       "          'description': '{‘all’, ‘root’, ‘none’}, default=’all’. Whether to show informative labels for impurity, etc.\\nOptions include ‘all’ to show at every node, ‘root’ to show only at\\nthe top root node, or ‘none’ to not show at any node.\\n'},\n",
       "         'filled': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When set to True, paint nodes to indicate majority class for\\nclassification, extremity of values for regression, or purity of node\\nfor multi-output.\\n'},\n",
       "         'impurity': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. When set to True, show the impurity at each node.\\n'},\n",
       "         'node_ids': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When set to True, show the ID number on each node.\\n'},\n",
       "         'proportion': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When set to True, change the display of ‘values’ and/or ‘samples’\\nto be proportions and percentages respectively.\\n'},\n",
       "         'rounded': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When set to True, draw node boxes with rounded corners and use\\nHelvetica fonts instead of Times-Roman.\\n'},\n",
       "         'precision': {'type': 'integer',\n",
       "          'description': 'int, default=3. Number of digits of precision for floating point in the values of\\nimpurity, threshold and value attributes of each node.\\n'},\n",
       "         'ax': {'type': 'object',\n",
       "          'description': 'matplotlib axis, default=None. Axes to plot to. If None, use current axis. Any previous content\\nis cleared.\\n'},\n",
       "         'fontsize': {'type': 'integer',\n",
       "          'description': 'int, default=None. Size of text font. If None, determined automatically to fit figure.\\n'}},\n",
       "        'required': ['decision_tree']}}}]}],\n",
       "  'name': 'sklearn.tree',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.tree.html'},\n",
       " 'sklearn.utils.html': {'functions': [{'defaults': [{'func_name': 'Bunch',\n",
       "      'func_desc': 'Container object exposing keys as attributes.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html#sklearn.utils.Bunch',\n",
       "      'function_definitions': {'function_name': 'Bunch',\n",
       "       'full_function': 'class sklearn.utils.Bunch(**kwargs)',\n",
       "       'function_text': 'Container object exposing keys as attributes. Bunch objects are sometimes used as an output for functions and methods.\\nThey extend dictionaries by enabling values to be accessed by key,\\nbunch[\"value_key\"], or by an attribute, bunch.value_key. Examples Create a new dictionary with keys from iterable and values set to value. Return the value for key if key is in the dictionary, else default. If key is not found, default is returned if given, otherwise KeyError is raised Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order.\\nRaises KeyError if the dict is empty. Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\\nIf E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\\nIn either case, this is followed by: for k in F:  D[k] = F[k]',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'as_float_array',\n",
       "      'func_desc': 'Convert an array-like to an array of floats.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.as_float_array.html#sklearn.utils.as_float_array',\n",
       "      'function_definitions': {'function_name': 'as_float_array',\n",
       "       'full_function': 'sklearn.utils.as_float_array(X, *, copy=True, force_all_finite=True)',\n",
       "       'function_text': 'Convert an array-like to an array of floats. The new dtype will be np.float32 or np.float64, depending on the original\\ntype. The function can create a copy or modify the argument depending\\non the argument copy.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix}',\n",
       "         'param_desc': 'The input data.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True, a copy of X will be created. If False, a copy may still be\\nreturned if X’s dtype is not a floating point type.\\n'},\n",
       "        {'param_name': 'force_all_finite',\n",
       "         'param_type': 'bool or ‘allow-nan’, default=True',\n",
       "         'param_desc': \"Whether to raise an error on np.inf, np.nan, pd.NA in X. The\\npossibilities are:\\n\\nTrue: Force all values of X to be finite.\\nFalse: accepts np.inf, np.nan, pd.NA in X.\\n‘allow-nan’: accepts only np.nan and pd.NA values in X. Values cannot\\nbe infinite.\\n\\n\\nAdded in version 0.20: force_all_finite accepts the string 'allow-nan'.\\n\\n\\nChanged in version 0.23: Accepts pd.NA and converts it into np.nan\\n\\n\"}]},\n",
       "      'function_calling': {'name': 'as_float_array',\n",
       "       'descriptions': 'Convert an array-like to an array of floats. The new dtype will be np.float32 or np.float64, depending on the original\\ntype. The function can create a copy or modify the argument depending\\non the argument copy.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix}. The input data.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True, a copy of X will be created. If False, a copy may still be\\nreturned if X’s dtype is not a floating point type.\\n'},\n",
       "         'force_all_finite': {'type': 'boolean',\n",
       "          'description': \"bool or ‘allow-nan’, default=True. Whether to raise an error on np.inf, np.nan, pd.NA in X. The\\npossibilities are:\\n\\nTrue: Force all values of X to be finite.\\nFalse: accepts np.inf, np.nan, pd.NA in X.\\n‘allow-nan’: accepts only np.nan and pd.NA values in X. Values cannot\\nbe infinite.\\n\\n\\nAdded in version 0.20: force_all_finite accepts the string 'allow-nan'.\\n\\n\\nChanged in version 0.23: Accepts pd.NA and converts it into np.nan\\n\\n\"}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'deprecated',\n",
       "      'func_desc': 'Decorator to mark a function or class as deprecated.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.deprecated.html#sklearn.utils.deprecated',\n",
       "      'function_definitions': {'function_name': 'deprecated',\n",
       "       'full_function': \"class sklearn.utils.deprecated(extra='')\",\n",
       "       'function_text': 'Decorator to mark a function or class as deprecated. Issue a warning when the function is called/the class is instantiated and\\nadds a warning to the docstring. The optional extra argument will be appended to the deprecation message\\nand the docstring. Note: to use this with the default value for extra, put\\nin an empty of parentheses:',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'obj',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': ''}]},\n",
       "      'function_calling': {'name': 'deprecated',\n",
       "       'descriptions': 'Decorator to mark a function or class as deprecated. Issue a warning when the function is called/the class is instantiated and\\nadds a warning to the docstring. The optional extra argument will be appended to the deprecation message\\nand the docstring. Note: to use this with the default value for extra, put\\nin an empty of parentheses:',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'obj': {'type': 'object', 'description': 'object. '}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'gen_batches',\n",
       "      'func_desc': 'Generator to create slices containing batch_size elements from 0 to n.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.gen_batches.html#sklearn.utils.gen_batches',\n",
       "      'function_definitions': {'function_name': 'gen_batches',\n",
       "       'full_function': 'sklearn.utils.gen_batches(n, batch_size, *, min_batch_size=0)',\n",
       "       'function_text': 'Generator to create slices containing batch_size elements from 0 to n. The last slice may contain less than batch_size elements, when\\nbatch_size does not divide n.',\n",
       "       'func_text_user_guide': 'sklearn.utils.gen_even_slices.html#sklearn.utils.gen_even_slices',\n",
       "       'parameter_names_desc': [{'param_name': 'n',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Size of the sequence.\\n'},\n",
       "        {'param_name': 'batch_size',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Number of elements in each batch.\\n'},\n",
       "        {'param_name': 'min_batch_size',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Minimum number of elements in each batch.\\n'}]},\n",
       "      'function_calling': {'name': 'gen_batches',\n",
       "       'descriptions': 'Generator to create slices containing batch_size elements from 0 to n. The last slice may contain less than batch_size elements, when\\nbatch_size does not divide n.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n': {'type': 'integer',\n",
       "          'description': 'int. Size of the sequence.\\n'},\n",
       "         'batch_size': {'type': 'integer',\n",
       "          'description': 'int. Number of elements in each batch.\\n'},\n",
       "         'min_batch_size': {'type': 'integer',\n",
       "          'description': 'int, default=0. Minimum number of elements in each batch.\\n'}},\n",
       "        'required': ['n', 'batch_size']}}},\n",
       "     {'func_name': 'indexable',\n",
       "      'func_desc': 'Make arrays indexable for cross-validation.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.indexable.html#sklearn.utils.indexable',\n",
       "      'function_definitions': {'function_name': 'indexable',\n",
       "       'full_function': 'sklearn.utils.indexable(*iterables)',\n",
       "       'function_text': 'Make arrays indexable for cross-validation. Checks consistent length, passes through None, and ensures that everything\\ncan be indexed by converting sparse matrices to csr and converting\\nnon-interable objects to arrays.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': '*iterables',\n",
       "         'param_type': '{lists, dataframes, ndarrays, sparse matrices}',\n",
       "         'param_desc': 'List of objects to ensure sliceability.\\n'}]},\n",
       "      'function_calling': {'name': 'indexable',\n",
       "       'descriptions': 'Make arrays indexable for cross-validation. Checks consistent length, passes through None, and ensures that everything\\ncan be indexed by converting sparse matrices to csr and converting\\nnon-interable objects to arrays.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'*iterables': {'type': 'array',\n",
       "          'description': '{lists, dataframes, ndarrays, sparse matrices}. List of objects to ensure sliceability.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'resample',\n",
       "      'func_desc': 'Resample arrays or sparse matrices in a consistent way.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html#sklearn.utils.resample',\n",
       "      'function_definitions': {'function_name': 'resample',\n",
       "       'full_function': 'sklearn.utils.resample(*arrays, replace=True, n_samples=None, random_state=None, stratify=None)',\n",
       "       'function_text': 'Resample arrays or sparse matrices in a consistent way. The default strategy implements one step of the bootstrapping\\nprocedure.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-random_state',\n",
       "       'parameter_names_desc': [{'param_name': '*arrays',\n",
       "         'param_type': 'sequence of array-like of shape (n_samples,) or             (n_samples, n_outputs)',\n",
       "         'param_desc': 'Indexable data-structures can be arrays, lists, dataframes or scipy\\nsparse matrices with consistent first dimension.\\n'},\n",
       "        {'param_name': 'replace',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Implements resampling with replacement. If False, this will implement\\n(sliced) random permutations.\\n'},\n",
       "        {'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of samples to generate. If left to None this is\\nautomatically set to the first dimension of the arrays.\\nIf replace is False it should not be larger than the length of\\narrays.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for shuffling\\nthe data.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'stratify',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples,) or             (n_samples, n_outputs), default=None',\n",
       "         'param_desc': 'If not None, data is split in a stratified fashion, using this as\\nthe class labels.\\n'}]},\n",
       "      'function_calling': {'name': 'resample',\n",
       "       'descriptions': 'Resample arrays or sparse matrices in a consistent way. The default strategy implements one step of the bootstrapping\\nprocedure.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'*arrays': {'type': 'array',\n",
       "          'description': 'sequence of array-like of shape (n_samples,) or             (n_samples, n_outputs). Indexable data-structures can be arrays, lists, dataframes or scipy\\nsparse matrices with consistent first dimension.\\n'},\n",
       "         'replace': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Implements resampling with replacement. If False, this will implement\\n(sliced) random permutations.\\n'},\n",
       "         'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of samples to generate. If left to None this is\\nautomatically set to the first dimension of the arrays.\\nIf replace is False it should not be larger than the length of\\narrays.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for shuffling\\nthe data.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'stratify': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples,) or             (n_samples, n_outputs), default=None. If not None, data is split in a stratified fashion, using this as\\nthe class labels.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'safe_sqr',\n",
       "      'func_desc': 'Element wise squaring of array-likes and sparse matrices.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.safe_sqr.html#sklearn.utils.safe_sqr',\n",
       "      'function_definitions': {'function_name': 'safe_sqr',\n",
       "       'full_function': 'sklearn.utils.safe_sqr(X, *, copy=True)',\n",
       "       'function_text': 'Element wise squaring of array-likes and sparse matrices.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, ndarray, sparse matrix}',\n",
       "         'param_desc': ''},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to create a copy of X and operate on it or to perform\\ninplace computation (default behaviour).\\n'}]},\n",
       "      'function_calling': {'name': 'safe_sqr',\n",
       "       'descriptions': 'Element wise squaring of array-likes and sparse matrices.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, ndarray, sparse matrix}. '},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to create a copy of X and operate on it or to perform\\ninplace computation (default behaviour).\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': '_safe_indexing',\n",
       "      'func_desc': 'Return rows, items or columns of X using indices.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils._safe_indexing.html#sklearn.utils._safe_indexing',\n",
       "      'function_definitions': {'function_name': '_safe_indexing',\n",
       "       'full_function': 'sklearn.utils._safe_indexing(X, indices, *, axis=0)',\n",
       "       'function_text': 'Return rows, items or columns of X using indices. Warning This utility is documented, but private. This means that\\nbackward compatibility might be broken without any deprecation\\ncycle.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'array-like, sparse-matrix, list, pandas.DataFrame, pandas.Series',\n",
       "         'param_desc': 'Data from which to sample rows, items or columns. list are only\\nsupported when axis=0.\\n'},\n",
       "        {'param_name': 'indices',\n",
       "         'param_type': 'bool, int, str, slice, array-like',\n",
       "         'param_desc': '\\nIf axis=0, boolean and integer array-like, integer slice,\\nand scalar integer are supported.\\n\\nIf axis=1:\\nto select a single column, indices can be of int type for\\nall X types and str only for dataframe. The selected subset\\nwill be 1D, unless X is a sparse matrix in which case it will\\nbe 2D.\\nto select multiples columns, indices can be one of the\\nfollowing: list, array, slice. The type used in\\nthese containers can be one of the following: int, ‘bool’ and\\nstr. However, str is only supported when X is a dataframe.\\nThe selected subset will be 2D.\\n\\n\\n\\n\\n\\n'},\n",
       "        {'param_name': 'axis',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'The axis along which X will be subsampled. axis=0 will select\\nrows while axis=1 will select columns.\\n'}]},\n",
       "      'function_calling': {'name': '_safe_indexing',\n",
       "       'descriptions': 'Return rows, items or columns of X using indices. Warning This utility is documented, but private. This means that\\nbackward compatibility might be broken without any deprecation\\ncycle.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'array-like, sparse-matrix, list, pandas.DataFrame, pandas.Series. Data from which to sample rows, items or columns. list are only\\nsupported when axis=0.\\n'},\n",
       "         'indices': {'type': 'integer',\n",
       "          'description': 'bool, int, str, slice, array-like. \\nIf axis=0, boolean and integer array-like, integer slice,\\nand scalar integer are supported.\\n\\nIf axis=1:\\nto select a single column, indices can be of int type for\\nall X types and str only for dataframe. The selected subset\\nwill be 1D, unless X is a sparse matrix in which case it will\\nbe 2D.\\nto select multiples columns, indices can be one of the\\nfollowing: list, array, slice. The type used in\\nthese containers can be one of the following: int, ‘bool’ and\\nstr. However, str is only supported when X is a dataframe.\\nThe selected subset will be 2D.\\n\\n\\n\\n\\n\\n'},\n",
       "         'axis': {'type': 'integer',\n",
       "          'description': 'int, default=0. The axis along which X will be subsampled. axis=0 will select\\nrows while axis=1 will select columns.\\n'}},\n",
       "        'required': ['X', 'indices']}}},\n",
       "     {'func_name': 'assert_all_finite',\n",
       "      'func_desc': 'Throw a ValueError if X contains NaN or infinity.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.assert_all_finite.html#sklearn.utils.assert_all_finite',\n",
       "      'function_definitions': {'function_name': 'assert_all_finite',\n",
       "       'full_function': \"sklearn.utils.assert_all_finite(X, *, allow_nan=False, estimator_name=None, input_name='')\",\n",
       "       'function_text': 'Throw a ValueError if X contains NaN or infinity.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{ndarray, sparse matrix}',\n",
       "         'param_desc': 'The input data.\\n'},\n",
       "        {'param_name': 'allow_nan',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, do not throw error when X contains NaN.\\n'},\n",
       "        {'param_name': 'estimator_name',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'The estimator name, used to construct the error message.\\n'},\n",
       "        {'param_name': 'input_name',\n",
       "         'param_type': 'str, default=””',\n",
       "         'param_desc': 'The data name used to construct the error message. In particular\\nif input_name is “X” and the data has NaN values and\\nallow_nan is False, the error message will link to the imputer\\ndocumentation.\\n'}]},\n",
       "      'function_calling': {'name': 'assert_all_finite',\n",
       "       'descriptions': 'Throw a ValueError if X contains NaN or infinity.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{ndarray, sparse matrix}. The input data.\\n'},\n",
       "         'allow_nan': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, do not throw error when X contains NaN.\\n'},\n",
       "         'estimator_name': {'type': 'string',\n",
       "          'description': 'str, default=None. The estimator name, used to construct the error message.\\n'},\n",
       "         'input_name': {'type': 'string',\n",
       "          'description': 'str, default=””. The data name used to construct the error message. In particular\\nif input_name is “X” and the data has NaN values and\\nallow_nan is False, the error message will link to the imputer\\ndocumentation.\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'estimator_html_repr',\n",
       "      'func_desc': 'Build a HTML representation of an estimator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.estimator_html_repr.html#sklearn.utils.estimator_html_repr',\n",
       "      'function_definitions': {'function_name': 'estimator_html_repr',\n",
       "       'full_function': 'sklearn.utils.estimator_html_repr(estimator)',\n",
       "       'function_text': 'Build a HTML representation of an estimator. Read more in the User Guide.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/modules/compose.html#visualizing-composite-estimators',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'estimator object',\n",
       "         'param_desc': 'The estimator to visualize.\\n'}]},\n",
       "      'function_calling': {'name': 'estimator_html_repr',\n",
       "       'descriptions': 'Build a HTML representation of an estimator. Read more in the User Guide.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'estimator object. The estimator to visualize.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'gen_even_slices',\n",
       "      'func_desc': 'Generator to create n_packs evenly spaced slices going up to n.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.gen_even_slices.html#sklearn.utils.gen_even_slices',\n",
       "      'function_definitions': {'function_name': 'gen_even_slices',\n",
       "       'full_function': 'sklearn.utils.gen_even_slices(n, n_packs, *, n_samples=None)',\n",
       "       'function_text': 'Generator to create n_packs evenly spaced slices going up to n. If n_packs does not divide n, except for the first n % n_packs\\nslices, remaining slices may contain fewer elements.',\n",
       "       'func_text_user_guide': 'sklearn.utils.gen_batches.html#sklearn.utils.gen_batches',\n",
       "       'parameter_names_desc': [{'param_name': 'n',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Size of the sequence.\\n'},\n",
       "        {'param_name': 'n_packs',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Number of slices to generate.\\n'},\n",
       "        {'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of samples. Pass n_samples when the slices are to be used for\\nsparse matrix indexing; slicing off-the-end raises an exception, while\\nit works for NumPy arrays.\\n'}]},\n",
       "      'function_calling': {'name': 'gen_even_slices',\n",
       "       'descriptions': 'Generator to create n_packs evenly spaced slices going up to n. If n_packs does not divide n, except for the first n % n_packs\\nslices, remaining slices may contain fewer elements.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n': {'type': 'integer',\n",
       "          'description': 'int. Size of the sequence.\\n'},\n",
       "         'n_packs': {'type': 'integer',\n",
       "          'description': 'int. Number of slices to generate.\\n'},\n",
       "         'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of samples. Pass n_samples when the slices are to be used for\\nsparse matrix indexing; slicing off-the-end raises an exception, while\\nit works for NumPy arrays.\\n'}},\n",
       "        'required': ['n', 'n_packs']}}},\n",
       "     {'func_name': 'murmurhash3_32',\n",
       "      'func_desc': 'Compute the 32bit murmurhash3 of key at seed.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.murmurhash3_32.html#sklearn.utils.murmurhash3_32',\n",
       "      'function_definitions': {'function_name': 'murmurhash3_32',\n",
       "       'full_function': 'sklearn.utils.murmurhash3_32(key, seed=0, positive=False)#',\n",
       "       'function_text': 'Compute the 32bit murmurhash3 of key at seed. The underlying implementation is MurmurHash3_x86_32 generating low\\nlatency 32bits hash suitable for implementing lookup tables, Bloom\\nfilters, count min sketch or feature hashing.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'key',\n",
       "         'param_type': 'np.int32, bytes, unicode or ndarray of dtype=np.int32',\n",
       "         'param_desc': 'The physical object to hash.\\n'},\n",
       "        {'param_name': 'seed',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Integer seed for the hashing algorithm.\\n'},\n",
       "        {'param_name': 'positive',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': '\\nTrue: the results is casted to an unsigned intfrom 0 to 2 ** 32 - 1\\n\\nFalse: the results is casted to a signed intfrom -(2 ** 31) to 2 ** 31 - 1\\n\\n\\n'}]},\n",
       "      'function_calling': {'name': 'murmurhash3_32',\n",
       "       'descriptions': 'Compute the 32bit murmurhash3 of key at seed. The underlying implementation is MurmurHash3_x86_32 generating low\\nlatency 32bits hash suitable for implementing lookup tables, Bloom\\nfilters, count min sketch or feature hashing.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'key': {'type': 'integer',\n",
       "          'description': 'np.int32, bytes, unicode or ndarray of dtype=np.int32. The physical object to hash.\\n'},\n",
       "         'seed': {'type': 'integer',\n",
       "          'description': 'int, default=0. Integer seed for the hashing algorithm.\\n'},\n",
       "         'positive': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. \\nTrue: the results is casted to an unsigned intfrom 0 to 2 ** 32 - 1\\n\\nFalse: the results is casted to a signed intfrom -(2 ** 31) to 2 ** 31 - 1\\n\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'safe_mask',\n",
       "      'func_desc': 'Return a mask which is safe to use on X.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.safe_mask.html#sklearn.utils.safe_mask',\n",
       "      'function_definitions': {'function_name': 'safe_mask',\n",
       "       'full_function': 'sklearn.utils.safe_mask(X, mask)',\n",
       "       'function_text': 'Return a mask which is safe to use on X.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{array-like, sparse matrix}',\n",
       "         'param_desc': 'Data on which to apply mask.\\n'},\n",
       "        {'param_name': 'mask',\n",
       "         'param_type': 'array-like',\n",
       "         'param_desc': 'Mask to be used on X.\\n'}]},\n",
       "      'function_calling': {'name': 'safe_mask',\n",
       "       'descriptions': 'Return a mask which is safe to use on X.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix}. Data on which to apply mask.\\n'},\n",
       "         'mask': {'type': 'array',\n",
       "          'description': 'array-like. Mask to be used on X.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'shuffle',\n",
       "      'func_desc': 'Shuffle arrays or sparse matrices in a consistent way.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html#sklearn.utils.shuffle',\n",
       "      'function_definitions': {'function_name': 'shuffle',\n",
       "       'full_function': 'sklearn.utils.shuffle(*arrays, random_state=None, n_samples=None)',\n",
       "       'function_text': 'Shuffle arrays or sparse matrices in a consistent way. This is a convenience alias to resample(*arrays, replace=False) to do\\nrandom permutations of the collections.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-random_state',\n",
       "       'parameter_names_desc': [{'param_name': '*arrays',\n",
       "         'param_type': 'sequence of indexable data-structures',\n",
       "         'param_desc': 'Indexable data-structures can be arrays, lists, dataframes or scipy\\nsparse matrices with consistent first dimension.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'Determines random number generation for shuffling\\nthe data.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "        {'param_name': 'n_samples',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Number of samples to generate. If left to None this is\\nautomatically set to the first dimension of the arrays.  It should\\nnot be larger than the length of arrays.\\n'}]},\n",
       "      'function_calling': {'name': 'shuffle',\n",
       "       'descriptions': 'Shuffle arrays or sparse matrices in a consistent way. This is a convenience alias to resample(*arrays, replace=False) to do\\nrandom permutations of the collections.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'*arrays': {'type': 'string',\n",
       "          'description': 'sequence of indexable data-structures. Indexable data-structures can be arrays, lists, dataframes or scipy\\nsparse matrices with consistent first dimension.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. Determines random number generation for shuffling\\nthe data.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'},\n",
       "         'n_samples': {'type': 'integer',\n",
       "          'description': 'int, default=None. Number of samples to generate. If left to None this is\\nautomatically set to the first dimension of the arrays.  It should\\nnot be larger than the length of arrays.\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Input and parameter validation': [{'func_name': 'check_X_y',\n",
       "      'func_desc': 'Input validation for standard estimators.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_X_y.html#sklearn.utils.check_X_y',\n",
       "      'function_definitions': {'function_name': 'check_X_y',\n",
       "       'full_function': \"sklearn.utils.check_X_y(X, y, accept_sparse=False, *, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, estimator=None)\",\n",
       "       'function_text': 'Input validation for standard estimators. Checks X and y for consistent length, enforces X to be 2D and y 1D. By\\ndefault, X is checked to be non-empty and containing only finite values.\\nStandard input checks are also applied to y, such as checking that y\\ndoes not have np.nan or np.inf targets. For multi-label y, set\\nmulti_output=True to allow 2D and sparse y. If the dtype of X is\\nobject, attempt converting to float, raising on failure.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': '{ndarray, list, sparse matrix}',\n",
       "         'param_desc': 'Input data.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': '{ndarray, list, sparse matrix}',\n",
       "         'param_desc': 'Labels.\\n'},\n",
       "        {'param_name': 'accept_sparse',\n",
       "         'param_type': 'str, bool or list of str, default=False',\n",
       "         'param_desc': 'String[s] representing allowed sparse matrix formats, such as ‘csc’,\\n‘csr’, etc. If the input is sparse but not in the allowed format,\\nit will be converted to the first listed format. True allows the input\\nto be any format. False means that a sparse matrix input will\\nraise an error.\\n'},\n",
       "        {'param_name': 'accept_large_sparse',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\\naccept_sparse, accept_large_sparse will cause it to be accepted only\\nif its indices are stored with a 32-bit dtype.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'dtype',\n",
       "         'param_type': '‘numeric’, type, list of type or None, default=’numeric’',\n",
       "         'param_desc': 'Data type of result. If None, the dtype of the input is preserved.\\nIf “numeric”, dtype is preserved unless array.dtype is object.\\nIf dtype is a list of types, conversion on the first type is only\\nperformed if the dtype of the input is not in the list.\\n'},\n",
       "        {'param_name': 'order',\n",
       "         'param_type': '{‘F’, ‘C’}, default=None',\n",
       "         'param_desc': 'Whether an array will be forced to be fortran or c-style. If\\nNone, then the input data’s order is preserved when possible.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether a forced copy will be triggered. If copy=False, a copy might\\nbe triggered by a conversion.\\n'},\n",
       "        {'param_name': 'force_all_finite',\n",
       "         'param_type': 'bool or ‘allow-nan’, default=True',\n",
       "         'param_desc': \"Whether to raise an error on np.inf, np.nan, pd.NA in X. This parameter\\ndoes not influence whether y can have np.inf, np.nan, pd.NA values.\\nThe possibilities are:\\n\\nTrue: Force all values of X to be finite.\\nFalse: accepts np.inf, np.nan, pd.NA in X.\\n‘allow-nan’: accepts only np.nan or pd.NA values in X. Values cannot\\nbe infinite.\\n\\n\\nAdded in version 0.20: force_all_finite accepts the string 'allow-nan'.\\n\\n\\nChanged in version 0.23: Accepts pd.NA and converts it into np.nan\\n\\n\"},\n",
       "        {'param_name': 'ensure_2d',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to raise a value error if X is not 2D.\\n'},\n",
       "        {'param_name': 'allow_nd',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to allow X.ndim > 2.\\n'},\n",
       "        {'param_name': 'multi_output',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to allow 2D y (array or sparse matrix). If false, y will be\\nvalidated as a vector. y cannot have np.nan or np.inf values if\\nmulti_output=True.\\n'},\n",
       "        {'param_name': 'ensure_min_samples',\n",
       "         'param_type': 'int, default=1',\n",
       "         'param_desc': 'Make sure that X has a minimum number of samples in its first\\naxis (rows for a 2D array).\\n'},\n",
       "        {'param_name': 'ensure_min_features',\n",
       "         'param_type': 'int, default=1',\n",
       "         'param_desc': 'Make sure that the 2D array has some minimum number of features\\n(columns). The default value of 1 rejects empty datasets.\\nThis check is only enforced when X has effectively 2 dimensions or\\nis originally 1D and ensure_2d is True. Setting to 0 disables\\nthis check.\\n'},\n",
       "        {'param_name': 'y_numeric',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to ensure that y has a numeric type. If dtype of y is object,\\nit is converted to float64. Should only be used for regression\\nalgorithms.\\n'},\n",
       "        {'param_name': 'estimator',\n",
       "         'param_type': 'str or estimator instance, default=None',\n",
       "         'param_desc': 'If passed, include the name of the estimator in warning messages.\\n'}]},\n",
       "      'function_calling': {'name': 'check_X_y',\n",
       "       'descriptions': 'Input validation for standard estimators. Checks X and y for consistent length, enforces X to be 2D and y 1D. By\\ndefault, X is checked to be non-empty and containing only finite values.\\nStandard input checks are also applied to y, such as checking that y\\ndoes not have np.nan or np.inf targets. For multi-label y, set\\nmulti_output=True to allow 2D and sparse y. If the dtype of X is\\nobject, attempt converting to float, raising on failure.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': '{ndarray, list, sparse matrix}. Input data.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': '{ndarray, list, sparse matrix}. Labels.\\n'},\n",
       "         'accept_sparse': {'type': 'string',\n",
       "          'description': 'str, bool or list of str, default=False. String[s] representing allowed sparse matrix formats, such as ‘csc’,\\n‘csr’, etc. If the input is sparse but not in the allowed format,\\nit will be converted to the first listed format. True allows the input\\nto be any format. False means that a sparse matrix input will\\nraise an error.\\n'},\n",
       "         'accept_large_sparse': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\\naccept_sparse, accept_large_sparse will cause it to be accepted only\\nif its indices are stored with a 32-bit dtype.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'dtype': {'type': 'array',\n",
       "          'description': '‘numeric’, type, list of type or None, default=’numeric’. Data type of result. If None, the dtype of the input is preserved.\\nIf “numeric”, dtype is preserved unless array.dtype is object.\\nIf dtype is a list of types, conversion on the first type is only\\nperformed if the dtype of the input is not in the list.\\n'},\n",
       "         'order': {'type': 'string',\n",
       "          'enum': ['F', 'C'],\n",
       "          'description': '{‘F’, ‘C’}, default=None. Whether an array will be forced to be fortran or c-style. If\\nNone, then the input data’s order is preserved when possible.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether a forced copy will be triggered. If copy=False, a copy might\\nbe triggered by a conversion.\\n'},\n",
       "         'force_all_finite': {'type': 'boolean',\n",
       "          'description': \"bool or ‘allow-nan’, default=True. Whether to raise an error on np.inf, np.nan, pd.NA in X. This parameter\\ndoes not influence whether y can have np.inf, np.nan, pd.NA values.\\nThe possibilities are:\\n\\nTrue: Force all values of X to be finite.\\nFalse: accepts np.inf, np.nan, pd.NA in X.\\n‘allow-nan’: accepts only np.nan or pd.NA values in X. Values cannot\\nbe infinite.\\n\\n\\nAdded in version 0.20: force_all_finite accepts the string 'allow-nan'.\\n\\n\\nChanged in version 0.23: Accepts pd.NA and converts it into np.nan\\n\\n\"},\n",
       "         'ensure_2d': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to raise a value error if X is not 2D.\\n'},\n",
       "         'allow_nd': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to allow X.ndim > 2.\\n'},\n",
       "         'multi_output': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to allow 2D y (array or sparse matrix). If false, y will be\\nvalidated as a vector. y cannot have np.nan or np.inf values if\\nmulti_output=True.\\n'},\n",
       "         'ensure_min_samples': {'type': 'integer',\n",
       "          'description': 'int, default=1. Make sure that X has a minimum number of samples in its first\\naxis (rows for a 2D array).\\n'},\n",
       "         'ensure_min_features': {'type': 'integer',\n",
       "          'description': 'int, default=1. Make sure that the 2D array has some minimum number of features\\n(columns). The default value of 1 rejects empty datasets.\\nThis check is only enforced when X has effectively 2 dimensions or\\nis originally 1D and ensure_2d is True. Setting to 0 disables\\nthis check.\\n'},\n",
       "         'y_numeric': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to ensure that y has a numeric type. If dtype of y is object,\\nit is converted to float64. Should only be used for regression\\nalgorithms.\\n'},\n",
       "         'estimator': {'type': 'string',\n",
       "          'description': 'str or estimator instance, default=None. If passed, include the name of the estimator in warning messages.\\n'}},\n",
       "        'required': ['X', 'y', 'accept_sparse=False']}}},\n",
       "     {'func_name': 'check_consistent_length',\n",
       "      'func_desc': 'Check that all arrays have consistent first dimensions.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_consistent_length.html#sklearn.utils.check_consistent_length',\n",
       "      'function_definitions': {'function_name': 'check_consistent_length',\n",
       "       'full_function': 'sklearn.utils.check_consistent_length(*arrays)',\n",
       "       'function_text': 'Check that all arrays have consistent first dimensions. Checks whether all objects in arrays have the same shape or length.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': '*arrays',\n",
       "         'param_type': 'list or tuple of input objects.',\n",
       "         'param_desc': 'Objects that will be checked for consistent length.\\n'}]},\n",
       "      'function_calling': {'name': 'check_consistent_length',\n",
       "       'descriptions': 'Check that all arrays have consistent first dimensions. Checks whether all objects in arrays have the same shape or length.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'*arrays': {'type': 'array',\n",
       "          'description': 'list or tuple of input objects.. Objects that will be checked for consistent length.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'check_scalar',\n",
       "      'func_desc': 'Validate scalar parameters type and value.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_scalar.html#sklearn.utils.check_scalar',\n",
       "      'function_definitions': {'function_name': 'check_scalar',\n",
       "       'full_function': \"sklearn.utils.check_scalar(x, name, target_type, *, min_val=None, max_val=None, include_boundaries='both')\",\n",
       "       'function_text': 'Validate scalar parameters type and value.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'validation.check_memory',\n",
       "      'func_desc': 'Check that memory is joblib.Memory-like.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.validation.check_memory.html#sklearn.utils.validation.check_memory',\n",
       "      'function_definitions': {'function_name': 'check_memory',\n",
       "       'full_function': 'sklearn.utils.validation.check_memory(memory)',\n",
       "       'function_text': 'Check that memory is joblib.Memory-like. joblib.Memory-like means that memory can be converted into a\\njoblib.Memory instance (typically a str denoting the location)\\nor has the same interface (has a cache method).',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'validation.column_or_1d',\n",
       "      'func_desc': 'Ravel column or 1d numpy array, else raises an error.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.validation.column_or_1d.html#sklearn.utils.validation.column_or_1d',\n",
       "      'function_definitions': {'function_name': 'column_or_1d',\n",
       "       'full_function': 'sklearn.utils.validation.column_or_1d(y, *, dtype=None, warn=False)',\n",
       "       'function_text': 'Ravel column or 1d numpy array, else raises an error.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}},\n",
       "     {'func_name': 'check_array',\n",
       "      'func_desc': 'Input validation on an array, list, sparse matrix or similar.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_array.html#sklearn.utils.check_array',\n",
       "      'function_definitions': {'function_name': 'check_array',\n",
       "       'full_function': \"sklearn.utils.check_array(array, accept_sparse=False, *, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, estimator=None, input_name='')\",\n",
       "       'function_text': 'Input validation on an array, list, sparse matrix or similar. By default, the input is checked to be a non-empty 2D array containing\\nonly finite values. If the dtype of the array is object, attempt\\nconverting to float, raising on failure.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'array',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'Input object to check / convert.\\n'},\n",
       "        {'param_name': 'accept_sparse',\n",
       "         'param_type': 'str, bool or list/tuple of str, default=False',\n",
       "         'param_desc': 'String[s] representing allowed sparse matrix formats, such as ‘csc’,\\n‘csr’, etc. If the input is sparse but not in the allowed format,\\nit will be converted to the first listed format. True allows the input\\nto be any format. False means that a sparse matrix input will\\nraise an error.\\n'},\n",
       "        {'param_name': 'accept_large_sparse',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\\naccept_sparse, accept_large_sparse=False will cause it to be accepted\\nonly if its indices are stored with a 32-bit dtype.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "        {'param_name': 'dtype',\n",
       "         'param_type': '‘numeric’, type, list of type or None, default=’numeric’',\n",
       "         'param_desc': 'Data type of result. If None, the dtype of the input is preserved.\\nIf “numeric”, dtype is preserved unless array.dtype is object.\\nIf dtype is a list of types, conversion on the first type is only\\nperformed if the dtype of the input is not in the list.\\n'},\n",
       "        {'param_name': 'order',\n",
       "         'param_type': '{‘F’, ‘C’} or None, default=None',\n",
       "         'param_desc': 'Whether an array will be forced to be fortran or c-style.\\nWhen order is None (default), then if copy=False, nothing is ensured\\nabout the memory layout of the output array; otherwise (copy=True)\\nthe memory layout of the returned array is kept as close as possible\\nto the original array.\\n'},\n",
       "        {'param_name': 'copy',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether a forced copy will be triggered. If copy=False, a copy might\\nbe triggered by a conversion.\\n'},\n",
       "        {'param_name': 'force_all_finite',\n",
       "         'param_type': 'bool or ‘allow-nan’, default=True',\n",
       "         'param_desc': \"Whether to raise an error on np.inf, np.nan, pd.NA in array. The\\npossibilities are:\\n\\nTrue: Force all values of array to be finite.\\nFalse: accepts np.inf, np.nan, pd.NA in array.\\n‘allow-nan’: accepts only np.nan and pd.NA values in array. Values\\ncannot be infinite.\\n\\n\\nAdded in version 0.20: force_all_finite accepts the string 'allow-nan'.\\n\\n\\nChanged in version 0.23: Accepts pd.NA and converts it into np.nan\\n\\n\"},\n",
       "        {'param_name': 'ensure_2d',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'Whether to raise a value error if array is not 2D.\\n'},\n",
       "        {'param_name': 'allow_nd',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'Whether to allow array.ndim > 2.\\n'},\n",
       "        {'param_name': 'ensure_min_samples',\n",
       "         'param_type': 'int, default=1',\n",
       "         'param_desc': 'Make sure that the array has a minimum number of samples in its first\\naxis (rows for a 2D array). Setting to 0 disables this check.\\n'},\n",
       "        {'param_name': 'ensure_min_features',\n",
       "         'param_type': 'int, default=1',\n",
       "         'param_desc': 'Make sure that the 2D array has some minimum number of features\\n(columns). The default value of 1 rejects empty datasets.\\nThis check is only enforced when the input data has effectively 2\\ndimensions or is originally 1D and ensure_2d is True. Setting to 0\\ndisables this check.\\n'},\n",
       "        {'param_name': 'estimator',\n",
       "         'param_type': 'str or estimator instance, default=None',\n",
       "         'param_desc': 'If passed, include the name of the estimator in warning messages.\\n'},\n",
       "        {'param_name': 'input_name',\n",
       "         'param_type': 'str, default=””',\n",
       "         'param_desc': 'The data name used to construct the error message. In particular\\nif input_name is “X” and the data has NaN values and\\nallow_nan is False, the error message will link to the imputer\\ndocumentation.\\n\\nAdded in version 1.1.0.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'check_array',\n",
       "       'descriptions': 'Input validation on an array, list, sparse matrix or similar. By default, the input is checked to be a non-empty 2D array containing\\nonly finite values. If the dtype of the array is object, attempt\\nconverting to float, raising on failure.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'array': {'type': 'object',\n",
       "          'description': 'object. Input object to check / convert.\\n'},\n",
       "         'accept_sparse': {'type': 'string',\n",
       "          'description': 'str, bool or list/tuple of str, default=False. String[s] representing allowed sparse matrix formats, such as ‘csc’,\\n‘csr’, etc. If the input is sparse but not in the allowed format,\\nit will be converted to the first listed format. True allows the input\\nto be any format. False means that a sparse matrix input will\\nraise an error.\\n'},\n",
       "         'accept_large_sparse': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\\naccept_sparse, accept_large_sparse=False will cause it to be accepted\\nonly if its indices are stored with a 32-bit dtype.\\n\\nAdded in version 0.20.\\n\\n'},\n",
       "         'dtype': {'type': 'array',\n",
       "          'description': '‘numeric’, type, list of type or None, default=’numeric’. Data type of result. If None, the dtype of the input is preserved.\\nIf “numeric”, dtype is preserved unless array.dtype is object.\\nIf dtype is a list of types, conversion on the first type is only\\nperformed if the dtype of the input is not in the list.\\n'},\n",
       "         'order': {'type': 'string',\n",
       "          'enum': ['F', 'C'],\n",
       "          'description': '{‘F’, ‘C’} or None, default=None. Whether an array will be forced to be fortran or c-style.\\nWhen order is None (default), then if copy=False, nothing is ensured\\nabout the memory layout of the output array; otherwise (copy=True)\\nthe memory layout of the returned array is kept as close as possible\\nto the original array.\\n'},\n",
       "         'copy': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether a forced copy will be triggered. If copy=False, a copy might\\nbe triggered by a conversion.\\n'},\n",
       "         'force_all_finite': {'type': 'boolean',\n",
       "          'description': \"bool or ‘allow-nan’, default=True. Whether to raise an error on np.inf, np.nan, pd.NA in array. The\\npossibilities are:\\n\\nTrue: Force all values of array to be finite.\\nFalse: accepts np.inf, np.nan, pd.NA in array.\\n‘allow-nan’: accepts only np.nan and pd.NA values in array. Values\\ncannot be infinite.\\n\\n\\nAdded in version 0.20: force_all_finite accepts the string 'allow-nan'.\\n\\n\\nChanged in version 0.23: Accepts pd.NA and converts it into np.nan\\n\\n\"},\n",
       "         'ensure_2d': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. Whether to raise a value error if array is not 2D.\\n'},\n",
       "         'allow_nd': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. Whether to allow array.ndim > 2.\\n'},\n",
       "         'ensure_min_samples': {'type': 'integer',\n",
       "          'description': 'int, default=1. Make sure that the array has a minimum number of samples in its first\\naxis (rows for a 2D array). Setting to 0 disables this check.\\n'},\n",
       "         'ensure_min_features': {'type': 'integer',\n",
       "          'description': 'int, default=1. Make sure that the 2D array has some minimum number of features\\n(columns). The default value of 1 rejects empty datasets.\\nThis check is only enforced when the input data has effectively 2\\ndimensions or is originally 1D and ensure_2d is True. Setting to 0\\ndisables this check.\\n'},\n",
       "         'estimator': {'type': 'string',\n",
       "          'description': 'str or estimator instance, default=None. If passed, include the name of the estimator in warning messages.\\n'},\n",
       "         'input_name': {'type': 'string',\n",
       "          'description': 'str, default=””. The data name used to construct the error message. In particular\\nif input_name is “X” and the data has NaN values and\\nallow_nan is False, the error message will link to the imputer\\ndocumentation.\\n\\nAdded in version 1.1.0.\\n\\n'}},\n",
       "        'required': ['array', 'accept_sparse=False']}}},\n",
       "     {'func_name': 'check_random_state',\n",
       "      'func_desc': 'Turn seed into a np.random.RandomState instance.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_random_state.html#sklearn.utils.check_random_state',\n",
       "      'function_definitions': {'function_name': 'check_random_state',\n",
       "       'full_function': 'sklearn.utils.check_random_state(seed)',\n",
       "       'function_text': 'Turn seed into a np.random.RandomState instance.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'seed',\n",
       "         'param_type': 'None, int or instance of RandomState',\n",
       "         'param_desc': 'If seed is None, return the RandomState singleton used by np.random.\\nIf seed is an int, return a new RandomState instance seeded with seed.\\nIf seed is already a RandomState instance, return it.\\nOtherwise raise ValueError.\\n'}]},\n",
       "      'function_calling': {'name': 'check_random_state',\n",
       "       'descriptions': 'Turn seed into a np.random.RandomState instance.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'seed': {'type': 'integer',\n",
       "          'description': 'None, int or instance of RandomState. If seed is None, return the RandomState singleton used by np.random.\\nIf seed is an int, return a new RandomState instance seeded with seed.\\nIf seed is already a RandomState instance, return it.\\nOtherwise raise ValueError.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'validation.check_is_fitted',\n",
       "      'func_desc': 'Perform is_fitted validation for estimator.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.validation.check_is_fitted.html#sklearn.utils.validation.check_is_fitted',\n",
       "      'function_definitions': {'function_name': 'check_is_fitted',\n",
       "       'full_function': 'sklearn.utils.validation.check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=<built-in function all>)',\n",
       "       'function_text': 'Perform is_fitted validation for estimator. Checks if the estimator is fitted by verifying the presence of\\nfitted attributes (ending with a trailing underscore) and otherwise\\nraises a NotFittedError with the given message. If an estimator does not set any attributes with a trailing underscore, it\\ncan define a __sklearn_is_fitted__ method returning a boolean to\\nspecify if the estimator is fitted or not. See\\n__sklearn_is_fitted__ as Developer API\\nfor an example on how to use the API.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/auto_examples/developing_estimators/sklearn_is_fitted.html#sphx-glr-auto-examples-developing-estimators-sklearn-is-fitted-py',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'estimator instance',\n",
       "         'param_desc': 'Estimator instance for which the check is performed.\\n'},\n",
       "        {'param_name': 'attributes',\n",
       "         'param_type': 'str, list or tuple of str, default=None',\n",
       "         'param_desc': 'Attribute name(s) given as string or a list/tuple of strings\\nEg.: [\"coef_\", \"estimator_\", ...], \"coef_\"\\nIf None, estimator is considered fitted if there exist an\\nattribute that ends with a underscore and does not start with double\\nunderscore.\\n'},\n",
       "        {'param_name': 'msg',\n",
       "         'param_type': 'str, default=None',\n",
       "         'param_desc': 'The default error message is, “This %(name)s instance is not fitted\\nyet. Call ‘fit’ with appropriate arguments before using this\\nestimator.”\\nFor custom messages if “%(name)s” is present in the message string,\\nit is substituted for the estimator name.\\nEg. : “Estimator, %(name)s, must be fitted before sparsifying”.\\n'},\n",
       "        {'param_name': 'all_or_any',\n",
       "         'param_type': 'callable, {all, any}, default=all',\n",
       "         'param_desc': 'Specify whether all or any of the given attributes must exist.\\n'}]},\n",
       "      'function_calling': {'name': 'check_is_fitted',\n",
       "       'descriptions': 'Perform is_fitted validation for estimator. Checks if the estimator is fitted by verifying the presence of\\nfitted attributes (ending with a trailing underscore) and otherwise\\nraises a NotFittedError with the given message. If an estimator does not set any attributes with a trailing underscore, it\\ncan define a __sklearn_is_fitted__ method returning a boolean to\\nspecify if the estimator is fitted or not. See\\n__sklearn_is_fitted__ as Developer API\\nfor an example on how to use the API.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'estimator instance. Estimator instance for which the check is performed.\\n'},\n",
       "         'attributes': {'type': 'string',\n",
       "          'description': 'str, list or tuple of str, default=None. Attribute name(s) given as string or a list/tuple of strings\\nEg.: [\"coef_\", \"estimator_\", ...], \"coef_\"\\nIf None, estimator is considered fitted if there exist an\\nattribute that ends with a underscore and does not start with double\\nunderscore.\\n'},\n",
       "         'msg': {'type': 'string',\n",
       "          'description': 'str, default=None. The default error message is, “This %(name)s instance is not fitted\\nyet. Call ‘fit’ with appropriate arguments before using this\\nestimator.”\\nFor custom messages if “%(name)s” is present in the message string,\\nit is substituted for the estimator name.\\nEg. : “Estimator, %(name)s, must be fitted before sparsifying”.\\n'},\n",
       "         'all_or_any': {'type': 'object',\n",
       "          'description': 'callable, {all, any}, default=all. Specify whether all or any of the given attributes must exist.\\n'}},\n",
       "        'required': ['estimator', 'attributes=None']}}},\n",
       "     {'func_name': 'validation.check_symmetric',\n",
       "      'func_desc': 'Make sure that array is 2D, square and symmetric.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.validation.check_symmetric.html#sklearn.utils.validation.check_symmetric',\n",
       "      'function_definitions': {'function_name': 'check_symmetric',\n",
       "       'full_function': 'sklearn.utils.validation.check_symmetric(array, *, tol=1e-10, raise_warning=True, raise_exception=False)',\n",
       "       'function_text': 'Make sure that array is 2D, square and symmetric. If the array is not symmetric, then a symmetrized version is returned.\\nOptionally, a warning or exception is raised if the matrix is not\\nsymmetric.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'array',\n",
       "         'param_type': '{ndarray, sparse matrix}',\n",
       "         'param_desc': 'Input object to check / convert. Must be two-dimensional and square,\\notherwise a ValueError will be raised.\\n'},\n",
       "        {'param_name': 'tol',\n",
       "         'param_type': 'float, default=1e-10',\n",
       "         'param_desc': 'Absolute tolerance for equivalence of arrays. Default = 1E-10.\\n'},\n",
       "        {'param_name': 'raise_warning',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'If True then raise a warning if conversion is required.\\n'},\n",
       "        {'param_name': 'raise_exception',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True then raise an exception if array is not symmetric.\\n'}]},\n",
       "      'function_calling': {'name': 'check_symmetric',\n",
       "       'descriptions': 'Make sure that array is 2D, square and symmetric. If the array is not symmetric, then a symmetrized version is returned.\\nOptionally, a warning or exception is raised if the matrix is not\\nsymmetric.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'array': {'type': 'array',\n",
       "          'description': '{ndarray, sparse matrix}. Input object to check / convert. Must be two-dimensional and square,\\notherwise a ValueError will be raised.\\n'},\n",
       "         'tol': {'type': 'number',\n",
       "          'description': 'float, default=1e-10. Absolute tolerance for equivalence of arrays. Default = 1E-10.\\n'},\n",
       "         'raise_warning': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. If True then raise a warning if conversion is required.\\n'},\n",
       "         'raise_exception': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True then raise an exception if array is not symmetric.\\n'}},\n",
       "        'required': ['array']}}},\n",
       "     {'func_name': 'validation.has_fit_parameter',\n",
       "      'func_desc': \"Check whether the estimator's fit method supports the given parameter.\",\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.validation.has_fit_parameter.html#sklearn.utils.validation.has_fit_parameter',\n",
       "      'function_definitions': {'function_name': 'has_fit_parameter',\n",
       "       'full_function': 'sklearn.utils.validation.has_fit_parameter(estimator, parameter)',\n",
       "       'function_text': 'Check whether the estimator’s fit method supports the given parameter.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'An estimator to inspect.\\n'},\n",
       "        {'param_name': 'parameter',\n",
       "         'param_type': 'str',\n",
       "         'param_desc': 'The searched parameter.\\n'}]},\n",
       "      'function_calling': {'name': 'has_fit_parameter',\n",
       "       'descriptions': 'Check whether the estimator’s fit method supports the given parameter.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'object. An estimator to inspect.\\n'},\n",
       "         'parameter': {'type': 'string',\n",
       "          'description': 'str. The searched parameter.\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Meta-estimators': [{'func_name': 'metaestimators.available_if',\n",
       "      'func_desc': 'An attribute that is available only if check returns a truthy value.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.metaestimators.available_if.html#sklearn.utils.metaestimators.available_if',\n",
       "      'function_definitions': {'function_name': 'available_if',\n",
       "       'full_function': 'sklearn.utils.metaestimators.available_if(check)',\n",
       "       'function_text': 'An attribute that is available only if check returns a truthy value.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'check',\n",
       "         'param_type': 'callable',\n",
       "         'param_desc': 'When passed the object with the decorated method, this should return\\na truthy value if the attribute is available, and either return False\\nor raise an AttributeError if not available.\\n'}]},\n",
       "      'function_calling': {'name': 'available_if',\n",
       "       'descriptions': 'An attribute that is available only if check returns a truthy value.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'check': {'type': 'object',\n",
       "          'description': 'callable. When passed the object with the decorated method, this should return\\na truthy value if the attribute is available, and either return False\\nor raise an AttributeError if not available.\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Weight handling based on class labels': [{'func_name': 'class_weight.compute_class_weight',\n",
       "      'func_desc': 'Estimate class weights for unbalanced datasets.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html#sklearn.utils.class_weight.compute_class_weight',\n",
       "      'function_definitions': {'function_name': 'compute_class_weight',\n",
       "       'full_function': 'sklearn.utils.class_weight.compute_class_weight(class_weight, *, classes, y)',\n",
       "       'function_text': 'Estimate class weights for unbalanced datasets.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'class_weight',\n",
       "         'param_type': 'dict, “balanced” or None',\n",
       "         'param_desc': 'If “balanced”, class weights will be given by\\nn_samples / (n_classes * np.bincount(y)).\\nIf a dictionary is given, keys are classes and values are corresponding class\\nweights.\\nIf None is given, the class weights will be uniform.\\n'},\n",
       "        {'param_name': 'classes',\n",
       "         'param_type': 'ndarray',\n",
       "         'param_desc': 'Array of the classes occurring in the data, as given by\\nnp.unique(y_org) with y_org the original class labels.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Array of original class labels per sample.\\n'}]},\n",
       "      'function_calling': {'name': 'compute_class_weight',\n",
       "       'descriptions': 'Estimate class weights for unbalanced datasets.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'class_weight': {'type': 'array',\n",
       "          'description': 'dict, “balanced” or None. If “balanced”, class weights will be given by\\nn_samples / (n_classes * np.bincount(y)).\\nIf a dictionary is given, keys are classes and values are corresponding class\\nweights.\\nIf None is given, the class weights will be uniform.\\n'},\n",
       "         'classes': {'type': 'array',\n",
       "          'description': 'ndarray. Array of the classes occurring in the data, as given by\\nnp.unique(y_org) with y_org the original class labels.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Array of original class labels per sample.\\n'}},\n",
       "        'required': ['class_weight']}}},\n",
       "     {'func_name': 'class_weight.compute_sample_weight',\n",
       "      'func_desc': 'Estimate sample weights by class for unbalanced datasets.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_sample_weight.html#sklearn.utils.class_weight.compute_sample_weight',\n",
       "      'function_definitions': {'function_name': 'compute_sample_weight',\n",
       "       'full_function': 'sklearn.utils.class_weight.compute_sample_weight(class_weight, y, *, indices=None)',\n",
       "       'function_text': 'Estimate sample weights by class for unbalanced datasets.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'class_weight',\n",
       "         'param_type': 'dict, list of dicts, “balanced”, or None',\n",
       "         'param_desc': 'Weights associated with classes in the form {class_label: weight}.\\nIf not given, all classes are supposed to have weight one. For\\nmulti-output problems, a list of dicts can be provided in the same\\norder as the columns of y.\\nNote that for multioutput (including multilabel) weights should be\\ndefined for each class of every column in its own dict. For example,\\nfor four-class multilabel classification weights should be\\n[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\\n[{1:1}, {2:5}, {3:1}, {4:1}].\\nThe \"balanced\" mode uses the values of y to automatically adjust\\nweights inversely proportional to class frequencies in the input data:\\nn_samples / (n_classes * np.bincount(y)).\\nFor multi-output, the weights of each column of y will be multiplied.\\n'},\n",
       "        {'param_name': 'y',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_samples,) or (n_samples, n_outputs)',\n",
       "         'param_desc': 'Array of original class labels per sample.\\n'},\n",
       "        {'param_name': 'indices',\n",
       "         'param_type': 'array-like of shape (n_subsample,), default=None',\n",
       "         'param_desc': 'Array of indices to be used in a subsample. Can be of length less than\\nn_samples in the case of a subsample, or equal to n_samples in the\\ncase of a bootstrap subsample with repeated indices. If None, the\\nsample weight will be calculated over the full sample. Only \"balanced\"\\nis supported for class_weight if this is provided.\\n'}]},\n",
       "      'function_calling': {'name': 'compute_sample_weight',\n",
       "       'descriptions': 'Estimate sample weights by class for unbalanced datasets.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'class_weight': {'type': 'array',\n",
       "          'description': 'dict, list of dicts, “balanced”, or None. Weights associated with classes in the form {class_label: weight}.\\nIf not given, all classes are supposed to have weight one. For\\nmulti-output problems, a list of dicts can be provided in the same\\norder as the columns of y.\\nNote that for multioutput (including multilabel) weights should be\\ndefined for each class of every column in its own dict. For example,\\nfor four-class multilabel classification weights should be\\n[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\\n[{1:1}, {2:5}, {3:1}, {4:1}].\\nThe \"balanced\" mode uses the values of y to automatically adjust\\nweights inversely proportional to class frequencies in the input data:\\nn_samples / (n_classes * np.bincount(y)).\\nFor multi-output, the weights of each column of y will be multiplied.\\n'},\n",
       "         'y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_samples,) or (n_samples, n_outputs). Array of original class labels per sample.\\n'},\n",
       "         'indices': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_subsample,), default=None. Array of indices to be used in a subsample. Can be of length less than\\nn_samples in the case of a subsample, or equal to n_samples in the\\ncase of a bootstrap subsample with repeated indices. If None, the\\nsample weight will be calculated over the full sample. Only \"balanced\"\\nis supported for class_weight if this is provided.\\n'}},\n",
       "        'required': ['class_weight', 'y']}}}]},\n",
       "   {'Dealing with multiclass target in classifiers': [{'func_name': 'multiclass.is_multilabel',\n",
       "      'func_desc': 'Check if y is in a multilabel format.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.multiclass.is_multilabel.html#sklearn.utils.multiclass.is_multilabel',\n",
       "      'function_definitions': {'function_name': 'is_multilabel',\n",
       "       'full_function': 'sklearn.utils.multiclass.is_multilabel(y)',\n",
       "       'function_text': 'Check if y is in a multilabel format.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'y',\n",
       "         'param_type': 'ndarray of shape (n_samples,)',\n",
       "         'param_desc': 'Target values.\\n'}]},\n",
       "      'function_calling': {'name': 'is_multilabel',\n",
       "       'descriptions': 'Check if y is in a multilabel format.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples,). Target values.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'multiclass.unique_labels',\n",
       "      'func_desc': 'Extract an ordered array of unique labels.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.multiclass.unique_labels.html#sklearn.utils.multiclass.unique_labels',\n",
       "      'function_definitions': {'function_name': 'unique_labels',\n",
       "       'full_function': 'sklearn.utils.multiclass.unique_labels(*ys)',\n",
       "       'function_text': 'Extract an ordered array of unique labels. mix of multilabel and multiclass (single label) targets mix of label indicator matrix and anything else,\\nbecause there are no explicit labels) mix of label indicator matrices of different sizes mix of string and integer labels At the moment, we also don’t allow “multiclass-multioutput” input type.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': '*ys',\n",
       "         'param_type': 'array-likes',\n",
       "         'param_desc': 'Label values.\\n'}]},\n",
       "      'function_calling': {'name': 'unique_labels',\n",
       "       'descriptions': 'Extract an ordered array of unique labels. mix of multilabel and multiclass (single label) targets mix of label indicator matrix and anything else,\\nbecause there are no explicit labels) mix of label indicator matrices of different sizes mix of string and integer labels At the moment, we also don’t allow “multiclass-multioutput” input type.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'*ys': {'type': 'array',\n",
       "          'description': 'array-likes. Label values.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'multiclass.type_of_target',\n",
       "      'func_desc': 'Determine the type of data indicated by the target.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.multiclass.type_of_target.html#sklearn.utils.multiclass.type_of_target',\n",
       "      'function_definitions': {'function_name': 'type_of_target',\n",
       "       'full_function': \"sklearn.utils.multiclass.type_of_target(y, input_name='')\",\n",
       "       'function_text': 'Determine the type of data indicated by the target. Note that this type is the most specific type that can be inferred.\\nFor example: binary is more specific but compatible with multiclass. multiclass of integers is more specific but compatible with\\ncontinuous. multilabel-indicator is more specific but compatible with\\nmulticlass-multioutput.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'y',\n",
       "         'param_type': '{array-like, sparse matrix}',\n",
       "         'param_desc': 'Target values. If a sparse matrix, y is expected to be a\\nCSR/CSC matrix.\\n'},\n",
       "        {'param_name': 'input_name',\n",
       "         'param_type': 'str, default=””',\n",
       "         'param_desc': 'The data name used to construct the error message.\\n\\nAdded in version 1.1.0.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'type_of_target',\n",
       "       'descriptions': 'Determine the type of data indicated by the target. Note that this type is the most specific type that can be inferred.\\nFor example: binary is more specific but compatible with multiclass. multiclass of integers is more specific but compatible with\\ncontinuous. multilabel-indicator is more specific but compatible with\\nmulticlass-multioutput.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'y': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix}. Target values. If a sparse matrix, y is expected to be a\\nCSR/CSC matrix.\\n'},\n",
       "         'input_name': {'type': 'string',\n",
       "          'description': 'str, default=””. The data name used to construct the error message.\\n\\nAdded in version 1.1.0.\\n\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Optimal mathematical operations': [{'func_name': 'extmath.density',\n",
       "      'func_desc': 'Compute density of a sparse vector.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.extmath.density.html#sklearn.utils.extmath.density',\n",
       "      'function_definitions': {'function_name': 'density',\n",
       "       'full_function': 'sklearn.utils.extmath.density(w)',\n",
       "       'function_text': 'Compute density of a sparse vector.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'w',\n",
       "         'param_type': '{ndarray, sparse matrix}',\n",
       "         'param_desc': 'The input data can be numpy ndarray or a sparse matrix.\\n'}]},\n",
       "      'function_calling': {'name': 'density',\n",
       "       'descriptions': 'Compute density of a sparse vector.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'w': {'type': 'array',\n",
       "          'description': '{ndarray, sparse matrix}. The input data can be numpy ndarray or a sparse matrix.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'extmath.randomized_range_finder',\n",
       "      'func_desc': 'Compute an orthonormal matrix whose range approximates the range of A.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.extmath.randomized_range_finder.html#sklearn.utils.extmath.randomized_range_finder',\n",
       "      'function_definitions': {'function_name': 'randomized_range_finder',\n",
       "       'full_function': \"sklearn.utils.extmath.randomized_range_finder(A, *, size, n_iter, power_iteration_normalizer='auto', random_state=None)\",\n",
       "       'function_text': 'Compute an orthonormal matrix whose range approximates the range of A.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-random_state',\n",
       "       'parameter_names_desc': [{'param_name': 'A',\n",
       "         'param_type': '2D array',\n",
       "         'param_desc': 'The input data matrix.\\n'},\n",
       "        {'param_name': 'size',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Size of the return array.\\n'},\n",
       "        {'param_name': 'n_iter',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Number of power iterations used to stabilize the result.\\n'},\n",
       "        {'param_name': 'power_iteration_normalizer',\n",
       "         'param_type': '{‘auto’, ‘QR’, ‘LU’, ‘none’}, default=’auto’',\n",
       "         'param_desc': 'Whether the power iterations are normalized with step-by-step\\nQR factorization (the slowest but most accurate), ‘none’\\n(the fastest but numerically unstable when n_iter is large, e.g.\\ntypically 5 or larger), or ‘LU’ factorization (numerically stable\\nbut can lose slightly in accuracy). The ‘auto’ mode applies no\\nnormalization if n_iter <= 2 and switches to LU otherwise.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'The seed of the pseudo random number generator to use when shuffling\\nthe data, i.e. getting the random vectors to initialize the algorithm.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'}]},\n",
       "      'function_calling': {'name': 'randomized_range_finder',\n",
       "       'descriptions': 'Compute an orthonormal matrix whose range approximates the range of A.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'A': {'type': 'array',\n",
       "          'description': '2D array. The input data matrix.\\n'},\n",
       "         'size': {'type': 'integer',\n",
       "          'description': 'int. Size of the return array.\\n'},\n",
       "         'n_iter': {'type': 'integer',\n",
       "          'description': 'int. Number of power iterations used to stabilize the result.\\n'},\n",
       "         'power_iteration_normalizer': {'type': 'string',\n",
       "          'enum': ['auto', 'QR', 'LU', 'none'],\n",
       "          'description': '{‘auto’, ‘QR’, ‘LU’, ‘none’}, default=’auto’. Whether the power iterations are normalized with step-by-step\\nQR factorization (the slowest but most accurate), ‘none’\\n(the fastest but numerically unstable when n_iter is large, e.g.\\ntypically 5 or larger), or ‘LU’ factorization (numerically stable\\nbut can lose slightly in accuracy). The ‘auto’ mode applies no\\nnormalization if n_iter <= 2 and switches to LU otherwise.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. The seed of the pseudo random number generator to use when shuffling\\nthe data, i.e. getting the random vectors to initialize the algorithm.\\nPass an int for reproducible results across multiple function calls.\\nSee Glossary.\\n'}},\n",
       "        'required': ['A']}}},\n",
       "     {'func_name': 'extmath.safe_sparse_dot',\n",
       "      'func_desc': 'Dot product that handle the sparse matrix case correctly.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.extmath.safe_sparse_dot.html#sklearn.utils.extmath.safe_sparse_dot',\n",
       "      'function_definitions': {'function_name': 'safe_sparse_dot',\n",
       "       'full_function': 'sklearn.utils.extmath.safe_sparse_dot(a, b, *, dense_output=False)',\n",
       "       'function_text': 'Dot product that handle the sparse matrix case correctly.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'a',\n",
       "         'param_type': '{ndarray, sparse matrix}',\n",
       "         'param_desc': ''},\n",
       "        {'param_name': 'b',\n",
       "         'param_type': '{ndarray, sparse matrix}',\n",
       "         'param_desc': ''},\n",
       "        {'param_name': 'dense_output',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When False, a and b both being sparse will yield sparse output.\\nWhen True, output will always be a dense array.\\n'}]},\n",
       "      'function_calling': {'name': 'safe_sparse_dot',\n",
       "       'descriptions': 'Dot product that handle the sparse matrix case correctly.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'a': {'type': 'array',\n",
       "          'description': '{ndarray, sparse matrix}. '},\n",
       "         'b': {'type': 'array', 'description': '{ndarray, sparse matrix}. '},\n",
       "         'dense_output': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When False, a and b both being sparse will yield sparse output.\\nWhen True, output will always be a dense array.\\n'}},\n",
       "        'required': ['a', 'b']}}},\n",
       "     {'func_name': 'extmath.fast_logdet',\n",
       "      'func_desc': 'Compute logarithm of determinant of a square matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.extmath.fast_logdet.html#sklearn.utils.extmath.fast_logdet',\n",
       "      'function_definitions': {'function_name': 'fast_logdet',\n",
       "       'full_function': 'sklearn.utils.extmath.fast_logdet(A)',\n",
       "       'function_text': 'Compute logarithm of determinant of a square matrix. The (natural) logarithm of the determinant of a square matrix\\nis returned if det(A) is non-negative and well defined.\\nIf the determinant is zero or negative returns -Inf. Equivalent to : np.log(np.det(A)) but more robust.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'A',\n",
       "         'param_type': 'array_like of shape (n, n)',\n",
       "         'param_desc': 'The square matrix.\\n'}]},\n",
       "      'function_calling': {'name': 'fast_logdet',\n",
       "       'descriptions': 'Compute logarithm of determinant of a square matrix. The (natural) logarithm of the determinant of a square matrix\\nis returned if det(A) is non-negative and well defined.\\nIf the determinant is zero or negative returns -Inf. Equivalent to : np.log(np.det(A)) but more robust.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'A': {'type': 'array',\n",
       "          'description': 'array_like of shape (n, n). The square matrix.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'extmath.randomized_svd',\n",
       "      'func_desc': 'Compute a truncated randomized SVD.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.extmath.randomized_svd.html#sklearn.utils.extmath.randomized_svd',\n",
       "      'function_definitions': {'function_name': 'randomized_svd',\n",
       "       'full_function': \"sklearn.utils.extmath.randomized_svd(M, n_components, *, n_oversamples=10, n_iter='auto', power_iteration_normalizer='auto', transpose='auto', flip_sign=True, random_state=None, svd_lapack_driver='gesdd')\",\n",
       "       'function_text': 'Compute a truncated randomized SVD. This method solves the fixed-rank approximation problem described in [1]\\n(problem (1.5), p5).',\n",
       "       'func_text_user_guide': '#rf38c2b656ebc-1',\n",
       "       'parameter_names_desc': [{'param_name': 'M',\n",
       "         'param_type': '{ndarray, sparse matrix}',\n",
       "         'param_desc': 'Matrix to decompose.\\n'},\n",
       "        {'param_name': 'n_components',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Number of singular values and vectors to extract.\\n'},\n",
       "        {'param_name': 'n_oversamples',\n",
       "         'param_type': 'int, default=10',\n",
       "         'param_desc': 'Additional number of random vectors to sample the range of M so as\\nto ensure proper conditioning. The total number of random vectors\\nused to find the range of M is n_components + n_oversamples. Smaller\\nnumber can improve speed but can negatively impact the quality of\\napproximation of singular vectors and singular values. Users might wish\\nto increase this parameter up to 2*k - n_components where k is the\\neffective rank, for large matrices, noisy problems, matrices with\\nslowly decaying spectrums, or to increase precision accuracy. See [1]\\n(pages 5, 23 and 26).\\n'},\n",
       "        {'param_name': 'n_iter',\n",
       "         'param_type': 'int or ‘auto’, default=’auto’',\n",
       "         'param_desc': 'Number of power iterations. It can be used to deal with very noisy\\nproblems. When ‘auto’, it is set to 4, unless n_components is small\\n(< .1 * min(X.shape)) in which case n_iter is set to 7.\\nThis improves precision with few components. Note that in general\\nusers should rather increase n_oversamples before increasing n_iter\\nas the principle of the randomized method is to avoid usage of these\\nmore costly power iterations steps. When n_components is equal\\nor greater to the effective matrix rank and the spectrum does not\\npresent a slow decay, n_iter=0 or 1 should even work fine in theory\\n(see [1] page 9).\\n\\nChanged in version 0.18.\\n\\n'},\n",
       "        {'param_name': 'power_iteration_normalizer',\n",
       "         'param_type': '{‘auto’, ‘QR’, ‘LU’, ‘none’}, default=’auto’',\n",
       "         'param_desc': 'Whether the power iterations are normalized with step-by-step\\nQR factorization (the slowest but most accurate), ‘none’\\n(the fastest but numerically unstable when n_iter is large, e.g.\\ntypically 5 or larger), or ‘LU’ factorization (numerically stable\\nbut can lose slightly in accuracy). The ‘auto’ mode applies no\\nnormalization if n_iter <= 2 and switches to LU otherwise.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "        {'param_name': 'transpose',\n",
       "         'param_type': 'bool or ‘auto’, default=’auto’',\n",
       "         'param_desc': 'Whether the algorithm should be applied to M.T instead of M. The\\nresult should approximately be the same. The ‘auto’ mode will\\ntrigger the transposition if M.shape[1] > M.shape[0] since this\\nimplementation of randomized SVD tend to be a little faster in that\\ncase.\\n\\nChanged in version 0.18.\\n\\n'},\n",
       "        {'param_name': 'flip_sign',\n",
       "         'param_type': 'bool, default=True',\n",
       "         'param_desc': 'The output of a singular value decomposition is only unique up to a\\npermutation of the signs of the singular vectors. If flip_sign is\\nset to True, the sign ambiguity is resolved by making the largest\\nloadings for each component in the left singular vectors positive.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=’warn’',\n",
       "         'param_desc': 'The seed of the pseudo random number generator to use when\\nshuffling the data, i.e. getting the random vectors to initialize\\nthe algorithm. Pass an int for reproducible results across multiple\\nfunction calls. See Glossary.\\n\\nChanged in version 1.2: The default value changed from 0 to None.\\n\\n'},\n",
       "        {'param_name': 'svd_lapack_driver',\n",
       "         'param_type': '{“gesdd”, “gesvd”}, default=”gesdd”',\n",
       "         'param_desc': 'Whether to use the more efficient divide-and-conquer approach\\n(\"gesdd\") or more general rectangular approach (\"gesvd\") to compute\\nthe SVD of the matrix B, which is the projection of M into a low\\ndimensional subspace, as described in [1].\\n\\nAdded in version 1.2.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'randomized_svd',\n",
       "       'descriptions': 'Compute a truncated randomized SVD. This method solves the fixed-rank approximation problem described in [1]\\n(problem (1.5), p5).',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'M': {'type': 'array',\n",
       "          'description': '{ndarray, sparse matrix}. Matrix to decompose.\\n'},\n",
       "         'n_components': {'type': 'integer',\n",
       "          'description': 'int. Number of singular values and vectors to extract.\\n'},\n",
       "         'n_oversamples': {'type': 'integer',\n",
       "          'description': 'int, default=10. Additional number of random vectors to sample the range of M so as\\nto ensure proper conditioning. The total number of random vectors\\nused to find the range of M is n_components + n_oversamples. Smaller\\nnumber can improve speed but can negatively impact the quality of\\napproximation of singular vectors and singular values. Users might wish\\nto increase this parameter up to 2*k - n_components where k is the\\neffective rank, for large matrices, noisy problems, matrices with\\nslowly decaying spectrums, or to increase precision accuracy. See [1]\\n(pages 5, 23 and 26).\\n'},\n",
       "         'n_iter': {'type': 'integer',\n",
       "          'description': 'int or ‘auto’, default=’auto’. Number of power iterations. It can be used to deal with very noisy\\nproblems. When ‘auto’, it is set to 4, unless n_components is small\\n(< .1 * min(X.shape)) in which case n_iter is set to 7.\\nThis improves precision with few components. Note that in general\\nusers should rather increase n_oversamples before increasing n_iter\\nas the principle of the randomized method is to avoid usage of these\\nmore costly power iterations steps. When n_components is equal\\nor greater to the effective matrix rank and the spectrum does not\\npresent a slow decay, n_iter=0 or 1 should even work fine in theory\\n(see [1] page 9).\\n\\nChanged in version 0.18.\\n\\n'},\n",
       "         'power_iteration_normalizer': {'type': 'string',\n",
       "          'enum': ['auto', 'QR', 'LU', 'none'],\n",
       "          'description': '{‘auto’, ‘QR’, ‘LU’, ‘none’}, default=’auto’. Whether the power iterations are normalized with step-by-step\\nQR factorization (the slowest but most accurate), ‘none’\\n(the fastest but numerically unstable when n_iter is large, e.g.\\ntypically 5 or larger), or ‘LU’ factorization (numerically stable\\nbut can lose slightly in accuracy). The ‘auto’ mode applies no\\nnormalization if n_iter <= 2 and switches to LU otherwise.\\n\\nAdded in version 0.18.\\n\\n'},\n",
       "         'transpose': {'type': 'boolean',\n",
       "          'description': 'bool or ‘auto’, default=’auto’. Whether the algorithm should be applied to M.T instead of M. The\\nresult should approximately be the same. The ‘auto’ mode will\\ntrigger the transposition if M.shape[1] > M.shape[0] since this\\nimplementation of randomized SVD tend to be a little faster in that\\ncase.\\n\\nChanged in version 0.18.\\n\\n'},\n",
       "         'flip_sign': {'type': 'boolean',\n",
       "          'description': 'bool, default=True. The output of a singular value decomposition is only unique up to a\\npermutation of the signs of the singular vectors. If flip_sign is\\nset to True, the sign ambiguity is resolved by making the largest\\nloadings for each component in the left singular vectors positive.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=’warn’. The seed of the pseudo random number generator to use when\\nshuffling the data, i.e. getting the random vectors to initialize\\nthe algorithm. Pass an int for reproducible results across multiple\\nfunction calls. See Glossary.\\n\\nChanged in version 1.2: The default value changed from 0 to None.\\n\\n'},\n",
       "         'svd_lapack_driver': {'type': 'string',\n",
       "          'enum': ['gesdd', 'gesvd'],\n",
       "          'description': '{“gesdd”, “gesvd”}, default=”gesdd”. Whether to use the more efficient divide-and-conquer approach\\n(\"gesdd\") or more general rectangular approach (\"gesvd\") to compute\\nthe SVD of the matrix B, which is the projection of M into a low\\ndimensional subspace, as described in [1].\\n\\nAdded in version 1.2.\\n\\n'}},\n",
       "        'required': ['M', 'n_components']}}},\n",
       "     {'func_name': 'extmath.weighted_mode',\n",
       "      'func_desc': 'Return an array of the weighted modal (most common) value in the passed array.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.extmath.weighted_mode.html#sklearn.utils.extmath.weighted_mode',\n",
       "      'function_definitions': {'function_name': 'weighted_mode',\n",
       "       'full_function': 'sklearn.utils.extmath.weighted_mode(a, w, *, axis=0)',\n",
       "       'function_text': 'Return an array of the weighted modal (most common) value in the passed array. If there is more than one such value, only the first is returned.\\nThe bin-count for the modal bins is also returned. This is an extension of the algorithm in scipy.stats.mode.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'a',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Array of which values to find mode(s).\\n'},\n",
       "        {'param_name': 'w',\n",
       "         'param_type': 'array-like of shape (n_samples,)',\n",
       "         'param_desc': 'Array of weights for each value.\\n'},\n",
       "        {'param_name': 'axis',\n",
       "         'param_type': 'int, default=0',\n",
       "         'param_desc': 'Axis along which to operate. Default is 0, i.e. the first axis.\\n'}]},\n",
       "      'function_calling': {'name': 'weighted_mode',\n",
       "       'descriptions': 'Return an array of the weighted modal (most common) value in the passed array. If there is more than one such value, only the first is returned.\\nThe bin-count for the modal bins is also returned. This is an extension of the algorithm in scipy.stats.mode.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'a': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Array of which values to find mode(s).\\n'},\n",
       "         'w': {'type': 'array',\n",
       "          'description': 'array-like of shape (n_samples,). Array of weights for each value.\\n'},\n",
       "         'axis': {'type': 'integer',\n",
       "          'description': 'int, default=0. Axis along which to operate. Default is 0, i.e. the first axis.\\n'}},\n",
       "        'required': ['a', 'w']}}}]},\n",
       "   {'Working with sparse matrices and arrays': [{'func_name': 'sparsefuncs.incr_mean_variance_axis',\n",
       "      'func_desc': 'Compute incremental mean and variance along an axis on a CSR or CSC matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.sparsefuncs.incr_mean_variance_axis.html#sklearn.utils.sparsefuncs.incr_mean_variance_axis',\n",
       "      'function_definitions': {'function_name': 'incr_mean_variance_axis',\n",
       "       'full_function': 'sklearn.utils.sparsefuncs.incr_mean_variance_axis(X, *, axis, last_mean, last_var, last_n, weights=None)',\n",
       "       'function_text': 'Compute incremental mean and variance along an axis on a CSR or CSC matrix. last_mean, last_var are the statistics computed at the last step by this\\nfunction. Both must be initialized to 0-arrays of the proper size, i.e.\\nthe number of features in X. last_n is the number of samples encountered\\nuntil now.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'CSR or CSC sparse matrix of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Input data.\\n'},\n",
       "        {'param_name': 'axis',\n",
       "         'param_type': '{0, 1}',\n",
       "         'param_desc': 'Axis along which the axis should be computed.\\n'},\n",
       "        {'param_name': 'last_mean',\n",
       "         'param_type': 'ndarray of shape (n_features,) or (n_samples,), dtype=floating',\n",
       "         'param_desc': 'Array of means to update with the new data X.\\nShould be of shape (n_features,) if axis=0 or (n_samples,) if axis=1.\\n'},\n",
       "        {'param_name': 'last_var',\n",
       "         'param_type': 'ndarray of shape (n_features,) or (n_samples,), dtype=floating',\n",
       "         'param_desc': 'Array of variances to update with the new data X.\\nShould be of shape (n_features,) if axis=0 or (n_samples,) if axis=1.\\n'},\n",
       "        {'param_name': 'last_n',\n",
       "         'param_type': 'float or ndarray of shape (n_features,) or (n_samples,),             dtype=floating',\n",
       "         'param_desc': 'Sum of the weights seen so far, excluding the current weights\\nIf not float, it should be of shape (n_features,) if\\naxis=0 or (n_samples,) if axis=1. If float it corresponds to\\nhaving same weights for all samples (or features).\\n'},\n",
       "        {'param_name': 'weights',\n",
       "         'param_type': 'ndarray of shape (n_samples,) or (n_features,), default=None',\n",
       "         'param_desc': 'If axis is set to 0 shape is (n_samples,) or\\nif axis is set to 1 shape is (n_features,).\\nIf it is set to None, then samples are equally weighted.\\n\\nAdded in version 0.24.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'incr_mean_variance_axis',\n",
       "       'descriptions': 'Compute incremental mean and variance along an axis on a CSR or CSC matrix. last_mean, last_var are the statistics computed at the last step by this\\nfunction. Both must be initialized to 0-arrays of the proper size, i.e.\\nthe number of features in X. last_n is the number of samples encountered\\nuntil now.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'CSR or CSC sparse matrix of shape (n_samples, n_features). Input data.\\n'},\n",
       "         'axis': {'type': 'string',\n",
       "          'enum': ['0', '1'],\n",
       "          'description': '{0, 1}. Axis along which the axis should be computed.\\n'},\n",
       "         'last_mean': {'type': 'number',\n",
       "          'description': 'ndarray of shape (n_features,) or (n_samples,), dtype=floating. Array of means to update with the new data X.\\nShould be of shape (n_features,) if axis=0 or (n_samples,) if axis=1.\\n'},\n",
       "         'last_var': {'type': 'number',\n",
       "          'description': 'ndarray of shape (n_features,) or (n_samples,), dtype=floating. Array of variances to update with the new data X.\\nShould be of shape (n_features,) if axis=0 or (n_samples,) if axis=1.\\n'},\n",
       "         'last_n': {'type': 'number',\n",
       "          'description': 'float or ndarray of shape (n_features,) or (n_samples,),             dtype=floating. Sum of the weights seen so far, excluding the current weights\\nIf not float, it should be of shape (n_features,) if\\naxis=0 or (n_samples,) if axis=1. If float it corresponds to\\nhaving same weights for all samples (or features).\\n'},\n",
       "         'weights': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples,) or (n_features,), default=None. If axis is set to 0 shape is (n_samples,) or\\nif axis is set to 1 shape is (n_features,).\\nIf it is set to None, then samples are equally weighted.\\n\\nAdded in version 0.24.\\n\\n'}},\n",
       "        'required': ['X']}}},\n",
       "     {'func_name': 'sparsefuncs.inplace_csr_column_scale',\n",
       "      'func_desc': 'Inplace column scaling of a CSR matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.sparsefuncs.inplace_csr_column_scale.html#sklearn.utils.sparsefuncs.inplace_csr_column_scale',\n",
       "      'function_definitions': {'function_name': 'inplace_csr_column_scale',\n",
       "       'full_function': 'sklearn.utils.sparsefuncs.inplace_csr_column_scale(X, scale)',\n",
       "       'function_text': 'Inplace column scaling of a CSR matrix. Scale each feature of the data matrix by multiplying with specific scale\\nprovided by the caller assuming a (n_samples, n_features) shape.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'sparse matrix of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Matrix to normalize using the variance of the features.\\nIt should be of CSR format.\\n'},\n",
       "        {'param_name': 'scale',\n",
       "         'param_type': 'ndarray of shape (n_features,), dtype={np.float32, np.float64}',\n",
       "         'param_desc': 'Array of precomputed feature-wise values to use for scaling.\\n'}]},\n",
       "      'function_calling': {'name': 'inplace_csr_column_scale',\n",
       "       'descriptions': 'Inplace column scaling of a CSR matrix. Scale each feature of the data matrix by multiplying with specific scale\\nprovided by the caller assuming a (n_samples, n_features) shape.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'sparse matrix of shape (n_samples, n_features). Matrix to normalize using the variance of the features.\\nIt should be of CSR format.\\n'},\n",
       "         'scale': {'type': 'number',\n",
       "          'description': 'ndarray of shape (n_features,), dtype={np.float32, np.float64}. Array of precomputed feature-wise values to use for scaling.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'sparsefuncs.inplace_swap_column',\n",
       "      'func_desc': 'Swap two columns of a CSC/CSR matrix in-place.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.sparsefuncs.inplace_swap_column.html#sklearn.utils.sparsefuncs.inplace_swap_column',\n",
       "      'function_definitions': {'function_name': 'inplace_swap_column',\n",
       "       'full_function': 'sklearn.utils.sparsefuncs.inplace_swap_column(X, m, n)',\n",
       "       'function_text': 'Swap two columns of a CSC/CSR matrix in-place.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'sparse matrix of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Matrix whose two columns are to be swapped. It should be of\\nCSR or CSC format.\\n'},\n",
       "        {'param_name': 'm',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Index of the column of X to be swapped.\\n'},\n",
       "        {'param_name': 'n',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Index of the column of X to be swapped.\\n'}]},\n",
       "      'function_calling': {'name': 'inplace_swap_column',\n",
       "       'descriptions': 'Swap two columns of a CSC/CSR matrix in-place.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'sparse matrix of shape (n_samples, n_features). Matrix whose two columns are to be swapped. It should be of\\nCSR or CSC format.\\n'},\n",
       "         'm': {'type': 'integer',\n",
       "          'description': 'int. Index of the column of X to be swapped.\\n'},\n",
       "         'n': {'type': 'integer',\n",
       "          'description': 'int. Index of the column of X to be swapped.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'sparsefuncs.mean_variance_axis',\n",
       "      'func_desc': 'Compute mean and variance along an axis on a CSR or CSC matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.sparsefuncs.mean_variance_axis.html#sklearn.utils.sparsefuncs.mean_variance_axis',\n",
       "      'function_definitions': {'function_name': 'mean_variance_axis',\n",
       "       'full_function': 'sklearn.utils.sparsefuncs.mean_variance_axis(X, axis, weights=None, return_sum_weights=False)',\n",
       "       'function_text': 'Compute mean and variance along an axis on a CSR or CSC matrix.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'sparse matrix of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Input data. It can be of CSR or CSC format.\\n'},\n",
       "        {'param_name': 'axis',\n",
       "         'param_type': '{0, 1}',\n",
       "         'param_desc': 'Axis along which the axis should be computed.\\n'},\n",
       "        {'param_name': 'weights',\n",
       "         'param_type': 'ndarray of shape (n_samples,) or (n_features,), default=None',\n",
       "         'param_desc': 'If axis is set to 0 shape is (n_samples,) or\\nif axis is set to 1 shape is (n_features,).\\nIf it is set to None, then samples are equally weighted.\\n\\nAdded in version 0.24.\\n\\n'},\n",
       "        {'param_name': 'return_sum_weights',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'If True, returns the sum of weights seen for each feature\\nif axis=0 or each sample if axis=1.\\n\\nAdded in version 0.24.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'mean_variance_axis',\n",
       "       'descriptions': 'Compute mean and variance along an axis on a CSR or CSC matrix.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'sparse matrix of shape (n_samples, n_features). Input data. It can be of CSR or CSC format.\\n'},\n",
       "         'axis': {'type': 'string',\n",
       "          'enum': ['0', '1'],\n",
       "          'description': '{0, 1}. Axis along which the axis should be computed.\\n'},\n",
       "         'weights': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n_samples,) or (n_features,), default=None. If axis is set to 0 shape is (n_samples,) or\\nif axis is set to 1 shape is (n_features,).\\nIf it is set to None, then samples are equally weighted.\\n\\nAdded in version 0.24.\\n\\n'},\n",
       "         'return_sum_weights': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. If True, returns the sum of weights seen for each feature\\nif axis=0 or each sample if axis=1.\\n\\nAdded in version 0.24.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'sparsefuncs_fast.inplace_csr_row_normalize_l1',\n",
       "      'func_desc': 'Normalize inplace the rows of a CSR matrix or array by their L1 norm.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l1.html#sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l1',\n",
       "      'function_definitions': {'function_name': 'inplace_csr_row_normalize_l1',\n",
       "       'full_function': 'sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l1(X)#',\n",
       "       'function_text': 'Normalize inplace the rows of a CSR matrix or array by their L1 norm.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'scipy.sparse.csr_matrix and scipy.sparse.csr_array,             shape=(n_samples, n_features)',\n",
       "         'param_desc': 'The input matrix or array to be modified inplace.\\n'}]},\n",
       "      'function_calling': {'name': 'inplace_csr_row_normalize_l1',\n",
       "       'descriptions': 'Normalize inplace the rows of a CSR matrix or array by their L1 norm.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'scipy.sparse.csr_matrix and scipy.sparse.csr_array,             shape=(n_samples, n_features). The input matrix or array to be modified inplace.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'sparsefuncs.inplace_column_scale',\n",
       "      'func_desc': 'Inplace column scaling of a CSC/CSR matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.sparsefuncs.inplace_column_scale.html#sklearn.utils.sparsefuncs.inplace_column_scale',\n",
       "      'function_definitions': {'function_name': 'inplace_column_scale',\n",
       "       'full_function': 'sklearn.utils.sparsefuncs.inplace_column_scale(X, scale)',\n",
       "       'function_text': 'Inplace column scaling of a CSC/CSR matrix. Scale each feature of the data matrix by multiplying with specific scale\\nprovided by the caller assuming a (n_samples, n_features) shape.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'sparse matrix of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Matrix to normalize using the variance of the features. It should be\\nof CSC or CSR format.\\n'},\n",
       "        {'param_name': 'scale',\n",
       "         'param_type': 'ndarray of shape (n_features,), dtype={np.float32, np.float64}',\n",
       "         'param_desc': 'Array of precomputed feature-wise values to use for scaling.\\n'}]},\n",
       "      'function_calling': {'name': 'inplace_column_scale',\n",
       "       'descriptions': 'Inplace column scaling of a CSC/CSR matrix. Scale each feature of the data matrix by multiplying with specific scale\\nprovided by the caller assuming a (n_samples, n_features) shape.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'sparse matrix of shape (n_samples, n_features). Matrix to normalize using the variance of the features. It should be\\nof CSC or CSR format.\\n'},\n",
       "         'scale': {'type': 'number',\n",
       "          'description': 'ndarray of shape (n_features,), dtype={np.float32, np.float64}. Array of precomputed feature-wise values to use for scaling.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'sparsefuncs.inplace_row_scale',\n",
       "      'func_desc': 'Inplace row scaling of a CSR or CSC matrix.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.sparsefuncs.inplace_row_scale.html#sklearn.utils.sparsefuncs.inplace_row_scale',\n",
       "      'function_definitions': {'function_name': 'inplace_row_scale',\n",
       "       'full_function': 'sklearn.utils.sparsefuncs.inplace_row_scale(X, scale)',\n",
       "       'function_text': 'Inplace row scaling of a CSR or CSC matrix. Scale each row of the data matrix by multiplying with specific scale\\nprovided by the caller assuming a (n_samples, n_features) shape.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'sparse matrix of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Matrix to be scaled. It should be of CSR or CSC format.\\n'},\n",
       "        {'param_name': 'scale',\n",
       "         'param_type': 'ndarray of shape (n_features,), dtype={np.float32, np.float64}',\n",
       "         'param_desc': 'Array of precomputed sample-wise values to use for scaling.\\n'}]},\n",
       "      'function_calling': {'name': 'inplace_row_scale',\n",
       "       'descriptions': 'Inplace row scaling of a CSR or CSC matrix. Scale each row of the data matrix by multiplying with specific scale\\nprovided by the caller assuming a (n_samples, n_features) shape.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'sparse matrix of shape (n_samples, n_features). Matrix to be scaled. It should be of CSR or CSC format.\\n'},\n",
       "         'scale': {'type': 'number',\n",
       "          'description': 'ndarray of shape (n_features,), dtype={np.float32, np.float64}. Array of precomputed sample-wise values to use for scaling.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'sparsefuncs.inplace_swap_row',\n",
       "      'func_desc': 'Swap two rows of a CSC/CSR matrix in-place.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.sparsefuncs.inplace_swap_row.html#sklearn.utils.sparsefuncs.inplace_swap_row',\n",
       "      'function_definitions': {'function_name': 'inplace_swap_row',\n",
       "       'full_function': 'sklearn.utils.sparsefuncs.inplace_swap_row(X, m, n)',\n",
       "       'function_text': 'Swap two rows of a CSC/CSR matrix in-place.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'sparse matrix of shape (n_samples, n_features)',\n",
       "         'param_desc': 'Matrix whose two rows are to be swapped. It should be of CSR or\\nCSC format.\\n'},\n",
       "        {'param_name': 'm',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Index of the row of X to be swapped.\\n'},\n",
       "        {'param_name': 'n',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Index of the row of X to be swapped.\\n'}]},\n",
       "      'function_calling': {'name': 'inplace_swap_row',\n",
       "       'descriptions': 'Swap two rows of a CSC/CSR matrix in-place.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'sparse matrix of shape (n_samples, n_features). Matrix whose two rows are to be swapped. It should be of CSR or\\nCSC format.\\n'},\n",
       "         'm': {'type': 'integer',\n",
       "          'description': 'int. Index of the row of X to be swapped.\\n'},\n",
       "         'n': {'type': 'integer',\n",
       "          'description': 'int. Index of the row of X to be swapped.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'sparsefuncs_fast.inplace_csr_row_normalize_l2',\n",
       "      'func_desc': 'Normalize inplace the rows of a CSR matrix or array by their L2 norm.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l2.html#sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l2',\n",
       "      'function_definitions': {'function_name': 'inplace_csr_row_normalize_l2',\n",
       "       'full_function': 'sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l2(X)#',\n",
       "       'function_text': 'Normalize inplace the rows of a CSR matrix or array by their L2 norm.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'scipy.sparse.csr_matrix, shape=(n_samples, n_features)',\n",
       "         'param_desc': 'The input matrix or array to be modified inplace.\\n'}]},\n",
       "      'function_calling': {'name': 'inplace_csr_row_normalize_l2',\n",
       "       'descriptions': 'Normalize inplace the rows of a CSR matrix or array by their L2 norm.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'scipy.sparse.csr_matrix, shape=(n_samples, n_features). The input matrix or array to be modified inplace.\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Working with graphs': [{'func_name': 'graph.single_source_shortest_path_length',\n",
       "      'func_desc': 'Return the length of the shortest path from source to all reachable nodes.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.graph.single_source_shortest_path_length.html#sklearn.utils.graph.single_source_shortest_path_length',\n",
       "      'function_definitions': {'function_name': 'single_source_shortest_path_length',\n",
       "       'full_function': 'sklearn.utils.graph.single_source_shortest_path_length(graph, source, *, cutoff=None)',\n",
       "       'function_text': 'Return the length of the shortest path from source to all reachable nodes.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'graph',\n",
       "         'param_type': '{array-like, sparse matrix} of shape (n_nodes, n_nodes)',\n",
       "         'param_desc': 'Adjacency matrix of the graph. Sparse matrix of format LIL is\\npreferred.\\n'},\n",
       "        {'param_name': 'source',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'Start node for path.\\n'},\n",
       "        {'param_name': 'cutoff',\n",
       "         'param_type': 'int, default=None',\n",
       "         'param_desc': 'Depth to stop the search - only paths of length <= cutoff are returned.\\n'}]},\n",
       "      'function_calling': {'name': 'single_source_shortest_path_length',\n",
       "       'descriptions': 'Return the length of the shortest path from source to all reachable nodes.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'graph': {'type': 'array',\n",
       "          'description': '{array-like, sparse matrix} of shape (n_nodes, n_nodes). Adjacency matrix of the graph. Sparse matrix of format LIL is\\npreferred.\\n'},\n",
       "         'source': {'type': 'integer',\n",
       "          'description': 'int. Start node for path.\\n'},\n",
       "         'cutoff': {'type': 'integer',\n",
       "          'description': 'int, default=None. Depth to stop the search - only paths of length <= cutoff are returned.\\n'}},\n",
       "        'required': ['graph', 'source']}}}]},\n",
       "   {'Random sampling': [{'func_name': 'random.sample_without_replacement',\n",
       "      'func_desc': 'Sample integers without replacement.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.random.sample_without_replacement.html#sklearn.utils.random.sample_without_replacement',\n",
       "      'function_definitions': {'function_name': 'sample_without_replacement',\n",
       "       'full_function': \"sklearn.utils.random.sample_without_replacement(n_population, n_samples, method='auto', random_state=None)#\",\n",
       "       'function_text': 'Sample integers without replacement. Select n_samples integers from the set [0, n_population) without\\nreplacement.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'n_population',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'The size of the set to sample from.\\n'},\n",
       "        {'param_name': 'n_samples',\n",
       "         'param_type': 'int',\n",
       "         'param_desc': 'The number of integer to sample.\\n'},\n",
       "        {'param_name': 'random_state',\n",
       "         'param_type': 'int, RandomState instance or None, default=None',\n",
       "         'param_desc': 'If int, random_state is the seed used by the random number generator;\\nIf RandomState instance, random_state is the random number generator;\\nIf None, the random number generator is the RandomState instance used\\nby np.random.\\n'},\n",
       "        {'param_name': 'method',\n",
       "         'param_type': '{“auto”, “tracking_selection”, “reservoir_sampling”, “pool”},             default=’auto’',\n",
       "         'param_desc': 'If method == “auto”, the ratio of n_samples / n_population is used\\nto determine which algorithm to use:\\nIf ratio is between 0 and 0.01, tracking selection is used.\\nIf ratio is between 0.01 and 0.99, numpy.random.permutation is used.\\nIf ratio is greater than 0.99, reservoir sampling is used.\\nThe order of the selected integers is undefined. If a random order is\\ndesired, the selected subset should be shuffled.\\nIf method ==”tracking_selection”, a set based implementation is used\\nwhich is suitable for n_samples <<< n_population.\\nIf method == “reservoir_sampling”, a reservoir sampling algorithm is\\nused which is suitable for high memory constraint or when\\nO(n_samples) ~ O(n_population).\\nThe order of the selected integers is undefined. If a random order is\\ndesired, the selected subset should be shuffled.\\nIf method == “pool”, a pool based algorithm is particularly fast, even\\nfaster than the tracking selection method. However, a vector containing\\nthe entire population has to be initialized.\\nIf n_samples ~ n_population, the reservoir sampling method is faster.\\n'}]},\n",
       "      'function_calling': {'name': 'sample_without_replacement',\n",
       "       'descriptions': 'Sample integers without replacement. Select n_samples integers from the set [0, n_population) without\\nreplacement.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'n_population': {'type': 'integer',\n",
       "          'description': 'int. The size of the set to sample from.\\n'},\n",
       "         'n_samples': {'type': 'integer',\n",
       "          'description': 'int. The number of integer to sample.\\n'},\n",
       "         'random_state': {'type': 'integer',\n",
       "          'description': 'int, RandomState instance or None, default=None. If int, random_state is the seed used by the random number generator;\\nIf RandomState instance, random_state is the random number generator;\\nIf None, the random number generator is the RandomState instance used\\nby np.random.\\n'},\n",
       "         'method': {'type': 'string',\n",
       "          'enum': ['auto', 'tracking_selection', 'reservoir_sampling', 'pool'],\n",
       "          'description': '{“auto”, “tracking_selection”, “reservoir_sampling”, “pool”},             default=’auto’. If method == “auto”, the ratio of n_samples / n_population is used\\nto determine which algorithm to use:\\nIf ratio is between 0 and 0.01, tracking selection is used.\\nIf ratio is between 0.01 and 0.99, numpy.random.permutation is used.\\nIf ratio is greater than 0.99, reservoir sampling is used.\\nThe order of the selected integers is undefined. If a random order is\\ndesired, the selected subset should be shuffled.\\nIf method ==”tracking_selection”, a set based implementation is used\\nwhich is suitable for n_samples <<< n_population.\\nIf method == “reservoir_sampling”, a reservoir sampling algorithm is\\nused which is suitable for high memory constraint or when\\nO(n_samples) ~ O(n_population).\\nThe order of the selected integers is undefined. If a random order is\\ndesired, the selected subset should be shuffled.\\nIf method == “pool”, a pool based algorithm is particularly fast, even\\nfaster than the tracking selection method. However, a vector containing\\nthe entire population has to be initialized.\\nIf n_samples ~ n_population, the reservoir sampling method is faster.\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Auxiliary functions that operate on arrays': [{'func_name': 'arrayfuncs.min_pos',\n",
       "      'func_desc': 'Find the minimum value of an array over positive values.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.arrayfuncs.min_pos.html#sklearn.utils.arrayfuncs.min_pos',\n",
       "      'function_definitions': {'function_name': 'min_pos',\n",
       "       'full_function': 'sklearn.utils.arrayfuncs.min_pos(X)#',\n",
       "       'function_text': 'Find the minimum value of an array over positive values. Returns the maximum representable value of the input dtype if none of the\\nvalues are positive.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'X',\n",
       "         'param_type': 'ndarray of shape (n,)',\n",
       "         'param_desc': 'Input array.\\n'}]},\n",
       "      'function_calling': {'name': 'min_pos',\n",
       "       'descriptions': 'Find the minimum value of an array over positive values. Returns the maximum representable value of the input dtype if none of the\\nvalues are positive.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'X': {'type': 'array',\n",
       "          'description': 'ndarray of shape (n,). Input array.\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Metadata routing': [{'func_name': 'metadata_routing.MetadataRequest',\n",
       "      'func_desc': 'Contains the metadata request info of a consumer.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.metadata_routing.MetadataRequest.html#sklearn.utils.metadata_routing.MetadataRequest',\n",
       "      'function_definitions': {'function_name': 'MetadataRequest',\n",
       "       'full_function': 'class sklearn.utils.metadata_routing.MetadataRequest(owner)',\n",
       "       'function_text': 'Contains the metadata request info of a consumer. Instances of MethodMetadataRequest are used in this class for each\\navailable method under metadatarequest.{method}. Consumer-only classes such as simple estimators return a serialized\\nversion of this class as the output of get_metadata_routing(). Added in version 1.3.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'method',\n",
       "         'param_type': 'str',\n",
       "         'param_desc': 'The name of the method to check.\\n'},\n",
       "        {'param_name': 'params',\n",
       "         'param_type': 'iterable of str',\n",
       "         'param_desc': 'An iterable of parameters to check.\\n'}]},\n",
       "      'function_calling': {'name': 'MetadataRequest',\n",
       "       'descriptions': 'Contains the metadata request info of a consumer. Instances of MethodMetadataRequest are used in this class for each\\navailable method under metadatarequest.{method}. Consumer-only classes such as simple estimators return a serialized\\nversion of this class as the output of get_metadata_routing(). Added in version 1.3.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'method': {'type': 'string',\n",
       "          'description': 'str. The name of the method to check.\\n'},\n",
       "         'params': {'type': 'string',\n",
       "          'description': 'iterable of str. An iterable of parameters to check.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'metadata_routing.MethodMapping',\n",
       "      'func_desc': 'Stores the mapping between caller and callee methods for a router.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.metadata_routing.MethodMapping.html#sklearn.utils.metadata_routing.MethodMapping',\n",
       "      'function_definitions': {'function_name': 'MethodMapping',\n",
       "       'full_function': 'class sklearn.utils.metadata_routing.MethodMapping',\n",
       "       'function_text': 'Stores the mapping between caller and callee methods for a router. This class is primarily used in a get_metadata_routing() of a router\\nobject when defining the mapping between a sub-object (a sub-estimator or a\\nscorer) to the router’s methods. It stores a collection of namedtuples. Iterating through an instance of this class will yield named\\nMethodPair(caller, callee) tuples. Added in version 1.3. Add a method mapping.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'caller',\n",
       "         'param_type': 'str',\n",
       "         'param_desc': 'Parent estimator’s method name in which the callee is called.\\n'},\n",
       "        {'param_name': 'callee',\n",
       "         'param_type': 'str',\n",
       "         'param_desc': 'Child object’s method name. This method is called in caller.\\n'}]},\n",
       "      'function_calling': {'name': 'MethodMapping',\n",
       "       'descriptions': 'Stores the mapping between caller and callee methods for a router. This class is primarily used in a get_metadata_routing() of a router\\nobject when defining the mapping between a sub-object (a sub-estimator or a\\nscorer) to the router’s methods. It stores a collection of namedtuples. Iterating through an instance of this class will yield named\\nMethodPair(caller, callee) tuples. Added in version 1.3. Add a method mapping.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'caller': {'type': 'string',\n",
       "          'description': 'str. Parent estimator’s method name in which the callee is called.\\n'},\n",
       "         'callee': {'type': 'string',\n",
       "          'description': 'str. Child object’s method name. This method is called in caller.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'metadata_routing.process_routing',\n",
       "      'func_desc': 'Validate and route input parameters.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.metadata_routing.process_routing.html#sklearn.utils.metadata_routing.process_routing',\n",
       "      'function_definitions': {'function_name': 'process_routing',\n",
       "       'full_function': 'sklearn.utils.metadata_routing.process_routing(_obj, _method, /, **kwargs)',\n",
       "       'function_text': 'Validate and route input parameters. This function is used inside a router’s method, e.g. fit,\\nto validate the metadata and handle the routing. Assuming this signature of a router’s fit method:\\nfit(self, X, y, sample_weight=None, **fit_params),\\na call to this function would be:\\nprocess_routing(self, \"fit\", sample_weight=sample_weight, **fit_params). Note that if routing is not enabled and kwargs is empty, then it\\nreturns an empty routing where process_routing(...).ANYTHING.ANY_METHOD\\nis always an empty dictionary. Added in version 1.3.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/glossary.html#term-fit',\n",
       "       'parameter_names_desc': [{'param_name': '_obj',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': 'An object implementing get_metadata_routing. Typically a\\nmeta-estimator.\\n'},\n",
       "        {'param_name': '_method',\n",
       "         'param_type': 'str',\n",
       "         'param_desc': 'The name of the router’s method in which this function is called.\\n'}]},\n",
       "      'function_calling': {'name': 'process_routing',\n",
       "       'descriptions': 'Validate and route input parameters. This function is used inside a router’s method, e.g. fit,\\nto validate the metadata and handle the routing. Assuming this signature of a router’s fit method:\\nfit(self, X, y, sample_weight=None, **fit_params),\\na call to this function would be:\\nprocess_routing(self, \"fit\", sample_weight=sample_weight, **fit_params). Note that if routing is not enabled and kwargs is empty, then it\\nreturns an empty routing where process_routing(...).ANYTHING.ANY_METHOD\\nis always an empty dictionary. Added in version 1.3.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'_obj': {'type': 'object',\n",
       "          'description': 'object. An object implementing get_metadata_routing. Typically a\\nmeta-estimator.\\n'},\n",
       "         '_method': {'type': 'string',\n",
       "          'description': 'str. The name of the router’s method in which this function is called.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'metadata_routing.MetadataRouter',\n",
       "      'func_desc': 'Stores and handles metadata routing for a router object.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.metadata_routing.MetadataRouter.html#sklearn.utils.metadata_routing.MetadataRouter',\n",
       "      'function_definitions': {'function_name': 'MetadataRouter',\n",
       "       'full_function': 'class sklearn.utils.metadata_routing.MetadataRouter(owner)',\n",
       "       'function_text': 'Stores and handles metadata routing for a router object. This class is used by router objects to store and handle metadata routing.\\nRouting information is stored as a dictionary of the form {\"object_name\":\\nRouteMappingPair(method_mapping, routing_info)}, where method_mapping\\nis an instance of MethodMapping and\\nrouting_info is either a\\nMetadataRequest or a\\nMetadataRouter instance. Added in version 1.3.',\n",
       "       'func_text_user_guide': 'sklearn.utils.metadata_routing.MethodMapping.html#sklearn.utils.metadata_routing.MethodMapping',\n",
       "       'parameter_names_desc': [{'param_name': 'method',\n",
       "         'param_type': 'str',\n",
       "         'param_desc': 'The name of the method for which the parameters are requested and\\nrouted. If called inside the fit method of a router, it\\nwould be \"fit\".\\n'},\n",
       "        {'param_name': 'params',\n",
       "         'param_type': 'dict',\n",
       "         'param_desc': 'A dictionary of provided metadata.\\n'}]},\n",
       "      'function_calling': {'name': 'MetadataRouter',\n",
       "       'descriptions': 'Stores and handles metadata routing for a router object. This class is used by router objects to store and handle metadata routing.\\nRouting information is stored as a dictionary of the form {\"object_name\":\\nRouteMappingPair(method_mapping, routing_info)}, where method_mapping\\nis an instance of MethodMapping and\\nrouting_info is either a\\nMetadataRequest or a\\nMetadataRouter instance. Added in version 1.3.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'method': {'type': 'string',\n",
       "          'description': 'str. The name of the method for which the parameters are requested and\\nrouted. If called inside the fit method of a router, it\\nwould be \"fit\".\\n'},\n",
       "         'params': {'type': 'array',\n",
       "          'description': 'dict. A dictionary of provided metadata.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'metadata_routing.get_routing_for_object',\n",
       "      'func_desc': 'Get a Metadata{Router, Request} instance from the given object.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.metadata_routing.get_routing_for_object.html#sklearn.utils.metadata_routing.get_routing_for_object',\n",
       "      'function_definitions': {'function_name': 'get_routing_for_object',\n",
       "       'full_function': 'sklearn.utils.metadata_routing.get_routing_for_object(obj=None)',\n",
       "       'function_text': 'Get a Metadata{Router, Request} instance from the given object. This function returns a\\nMetadataRouter or a\\nMetadataRequest from the given input. This function always returns a copy or an instance constructed from the\\ninput, such that changing the output of this function will not change the\\noriginal object. Added in version 1.3.',\n",
       "       'func_text_user_guide': 'sklearn.utils.metadata_routing.MetadataRouter.html#sklearn.utils.metadata_routing.MetadataRouter',\n",
       "       'parameter_names_desc': [{'param_name': 'obj',\n",
       "         'param_type': 'object',\n",
       "         'param_desc': '\\n\\nIf the object provides a get_metadata_routing method, return a copyof the output of that method.\\n\\n\\n\\n\\nIf the object is already aMetadataRequest or a\\nMetadataRouter, return a copy\\nof that.\\n\\n\\n\\n\\nReturns an empty MetadataRequestotherwise.\\n\\n\\n\\n\\n'}]},\n",
       "      'function_calling': {'name': 'get_routing_for_object',\n",
       "       'descriptions': 'Get a Metadata{Router, Request} instance from the given object. This function returns a\\nMetadataRouter or a\\nMetadataRequest from the given input. This function always returns a copy or an instance constructed from the\\ninput, such that changing the output of this function will not change the\\noriginal object. Added in version 1.3.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'obj': {'type': 'object',\n",
       "          'description': 'object. \\n\\nIf the object provides a get_metadata_routing method, return a copyof the output of that method.\\n\\n\\n\\n\\nIf the object is already aMetadataRequest or a\\nMetadataRouter, return a copy\\nof that.\\n\\n\\n\\n\\nReturns an empty MetadataRequestotherwise.\\n\\n\\n\\n\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Discovering scikit-learn objects': [{'func_name': 'discovery.all_displays',\n",
       "      'func_desc': 'Get a list of all displays from sklearn.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.discovery.all_displays.html#sklearn.utils.discovery.all_displays',\n",
       "      'function_definitions': {'function_name': 'all_displays',\n",
       "       'full_function': 'sklearn.utils.discovery.all_displays()',\n",
       "       'function_text': 'Get a list of all displays from sklearn.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}}]},\n",
       "   {'API compatibility checkers': [{'func_name': 'estimator_checks.check_estimator',\n",
       "      'func_desc': 'Check if estimator adheres to scikit-learn conventions.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.estimator_checks.check_estimator.html#sklearn.utils.estimator_checks.check_estimator',\n",
       "      'function_definitions': {'function_name': 'check_estimator',\n",
       "       'full_function': 'sklearn.utils.estimator_checks.check_estimator(estimator=None, generate_only=False)',\n",
       "       'function_text': 'Check if estimator adheres to scikit-learn conventions. This function will run an extensive test-suite for input validation,\\nshapes, etc, making sure that the estimator complies with scikit-learn\\nconventions as detailed in Rolling your own estimator.\\nAdditional tests for classifiers, regressors, clustering or transformers\\nwill be run if the Estimator class inherits from the corresponding mixin\\nfrom sklearn.base. Setting generate_only=True returns a generator that yields (estimator,\\ncheck) tuples where the check can be called independently from each\\nother, i.e. check(estimator). This allows all checks to be run\\nindependently and report the checks that are failing. scikit-learn provides a pytest specific decorator,\\nparametrize_with_checks, making it\\neasier to test multiple estimators.',\n",
       "       'func_text_user_guide': 'https://scikit-learn.org/stable/developers/developers/develop.html#rolling-your-own-estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'estimator',\n",
       "         'param_type': 'estimator object',\n",
       "         'param_desc': 'Estimator instance to check.\\n\\nAdded in version 1.1: Passing a class was deprecated in version 0.23, and support for\\nclasses was removed in 0.24.\\n\\n'},\n",
       "        {'param_name': 'generate_only',\n",
       "         'param_type': 'bool, default=False',\n",
       "         'param_desc': 'When False, checks are evaluated when check_estimator is called.\\nWhen True, check_estimator returns a generator that yields\\n(estimator, check) tuples. The check is run by calling\\ncheck(estimator).\\n\\nAdded in version 0.22.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'check_estimator',\n",
       "       'descriptions': 'Check if estimator adheres to scikit-learn conventions. This function will run an extensive test-suite for input validation,\\nshapes, etc, making sure that the estimator complies with scikit-learn\\nconventions as detailed in Rolling your own estimator.\\nAdditional tests for classifiers, regressors, clustering or transformers\\nwill be run if the Estimator class inherits from the corresponding mixin\\nfrom sklearn.base. Setting generate_only=True returns a generator that yields (estimator,\\ncheck) tuples where the check can be called independently from each\\nother, i.e. check(estimator). This allows all checks to be run\\nindependently and report the checks that are failing. scikit-learn provides a pytest specific decorator,\\nparametrize_with_checks, making it\\neasier to test multiple estimators.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimator': {'type': 'object',\n",
       "          'description': 'estimator object. Estimator instance to check.\\n\\nAdded in version 1.1: Passing a class was deprecated in version 0.23, and support for\\nclasses was removed in 0.24.\\n\\n'},\n",
       "         'generate_only': {'type': 'boolean',\n",
       "          'description': 'bool, default=False. When False, checks are evaluated when check_estimator is called.\\nWhen True, check_estimator returns a generator that yields\\n(estimator, check) tuples. The check is run by calling\\ncheck(estimator).\\n\\nAdded in version 0.22.\\n\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'estimator_checks.parametrize_with_checks',\n",
       "      'func_desc': 'Pytest specific decorator for parametrizing estimator checks.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.estimator_checks.parametrize_with_checks.html#sklearn.utils.estimator_checks.parametrize_with_checks',\n",
       "      'function_definitions': {'function_name': 'parametrize_with_checks',\n",
       "       'full_function': 'sklearn.utils.estimator_checks.parametrize_with_checks(estimators)',\n",
       "       'function_text': 'Pytest specific decorator for parametrizing estimator checks. The id of each check is set to be a pprint version of the estimator\\nand the name of the check with its keyword arguments.\\nThis allows to use pytest -k to specify which tests to run:',\n",
       "       'func_text_user_guide': 'sklearn.utils.estimator_checks.check_estimator.html#sklearn.utils.estimator_checks.check_estimator',\n",
       "       'parameter_names_desc': [{'param_name': 'estimators',\n",
       "         'param_type': 'list of estimators instances',\n",
       "         'param_desc': 'Estimators to generated checks for.\\n\\nChanged in version 0.24: Passing a class was deprecated in version 0.23, and support for\\nclasses was removed in 0.24. Pass an instance instead.\\n\\n\\nAdded in version 0.24.\\n\\n'}]},\n",
       "      'function_calling': {'name': 'parametrize_with_checks',\n",
       "       'descriptions': 'Pytest specific decorator for parametrizing estimator checks. The id of each check is set to be a pprint version of the estimator\\nand the name of the check with its keyword arguments.\\nThis allows to use pytest -k to specify which tests to run:',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'estimators': {'type': 'object',\n",
       "          'description': 'list of estimators instances. Estimators to generated checks for.\\n\\nChanged in version 0.24: Passing a class was deprecated in version 0.23, and support for\\nclasses was removed in 0.24. Pass an instance instead.\\n\\n\\nAdded in version 0.24.\\n\\n'}},\n",
       "        'required': []}}}]},\n",
       "   {'Parallel computing': [{'func_name': 'parallel.Parallel',\n",
       "      'func_desc': 'Tweak of joblib.Parallel that propagates the scikit-learn configuration.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.parallel.Parallel.html#sklearn.utils.parallel.Parallel',\n",
       "      'function_definitions': {'function_name': 'Parallel',\n",
       "       'full_function': \"class sklearn.utils.parallel.Parallel(n_jobs=default(None), backend=default(None), return_as='list', verbose=default(0), timeout=None, pre_dispatch='2 * n_jobs', batch_size='auto', temp_folder=default(None), max_nbytes=default('1M'), mmap_mode=default('r'), prefer=default(None), require=default(None))\",\n",
       "       'function_text': 'Tweak of joblib.Parallel that propagates the scikit-learn configuration. This subclass of joblib.Parallel ensures that the active configuration\\n(thread-local) of scikit-learn is propagated to the parallel workers for the\\nduration of the execution of the parallel tasks. The API does not change and you can refer to joblib.Parallel\\ndocumentation for more details. Added in version 1.3. Dispatch the tasks and return the results.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'iterable',\n",
       "         'param_type': 'iterable',\n",
       "         'param_desc': 'Iterable containing tuples of (delayed_function, args, kwargs) that should\\nbe consumed.\\n'}]},\n",
       "      'function_calling': {'name': 'Parallel',\n",
       "       'descriptions': 'Tweak of joblib.Parallel that propagates the scikit-learn configuration. This subclass of joblib.Parallel ensures that the active configuration\\n(thread-local) of scikit-learn is propagated to the parallel workers for the\\nduration of the execution of the parallel tasks. The API does not change and you can refer to joblib.Parallel\\ndocumentation for more details. Added in version 1.3. Dispatch the tasks and return the results.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'iterable': {'type': 'array',\n",
       "          'description': 'iterable. Iterable containing tuples of (delayed_function, args, kwargs) that should\\nbe consumed.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'parallel.delayed',\n",
       "      'func_desc': 'Decorator used to capture the arguments of a function.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.parallel.delayed.html#sklearn.utils.parallel.delayed',\n",
       "      'function_definitions': {'function_name': 'delayed',\n",
       "       'full_function': 'sklearn.utils.parallel.delayed(function)',\n",
       "       'function_text': 'Decorator used to capture the arguments of a function. This alternative to joblib.delayed is meant to be used in conjunction\\nwith sklearn.utils.parallel.Parallel. The latter captures the scikit-\\nlearn configuration by calling sklearn.get_config() in the current\\nthread, prior to dispatching the first task. The captured configuration is\\nthen propagated and enabled for the duration of the execution of the\\ndelayed function in the joblib workers. Changed in version 1.3: delayed was moved from sklearn.utils.fixes to sklearn.utils.parallel\\nin scikit-learn 1.3.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'function',\n",
       "         'param_type': 'callable',\n",
       "         'param_desc': 'The function to be delayed.\\n'}]},\n",
       "      'function_calling': {'name': 'delayed',\n",
       "       'descriptions': 'Decorator used to capture the arguments of a function. This alternative to joblib.delayed is meant to be used in conjunction\\nwith sklearn.utils.parallel.Parallel. The latter captures the scikit-\\nlearn configuration by calling sklearn.get_config() in the current\\nthread, prior to dispatching the first task. The captured configuration is\\nthen propagated and enabled for the duration of the execution of the\\ndelayed function in the joblib workers. Changed in version 1.3: delayed was moved from sklearn.utils.fixes to sklearn.utils.parallel\\nin scikit-learn 1.3.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'function': {'type': 'object',\n",
       "          'description': 'callable. The function to be delayed.\\n'}},\n",
       "        'required': []}}}]}],\n",
       "  'name': 'sklearn.utils',\n",
       "  'url': 'https://scikit-learn.org/stable/api/sklearn.utils.html'},\n",
       " 'deprecated.html': {'functions': [{'defaults': [{'func_name': 'utils.discovery.all_estimators',\n",
       "      'func_desc': 'Get a list of all estimators from sklearn.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.discovery.all_estimators.html#sklearn.utils.discovery.all_estimators',\n",
       "      'function_definitions': {'function_name': 'all_estimators',\n",
       "       'full_function': 'sklearn.utils.discovery.all_estimators(type_filter=None)',\n",
       "       'function_text': 'Get a list of all estimators from sklearn. This function crawls the module and gets all classes that inherit\\nfrom BaseEstimator. Classes that are defined in test-modules are not\\nincluded.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': [{'param_name': 'type_filter',\n",
       "         'param_type': '{“classifier”, “regressor”, “cluster”, “transformer”}             or list of such str, default=None',\n",
       "         'param_desc': 'Which kind of estimators should be returned. If None, no filter is\\napplied and all estimators are returned.  Possible values are\\n‘classifier’, ‘regressor’, ‘cluster’ and ‘transformer’ to get\\nestimators only of these specific types, or a list of these to\\nget the estimators that fit at least one of the types.\\n'}]},\n",
       "      'function_calling': {'name': 'all_estimators',\n",
       "       'descriptions': 'Get a list of all estimators from sklearn. This function crawls the module and gets all classes that inherit\\nfrom BaseEstimator. Classes that are defined in test-modules are not\\nincluded.',\n",
       "       'parameters': {'type': 'object',\n",
       "        'properties': {'type_filter': {'type': 'string',\n",
       "          'description': '{“classifier”, “regressor”, “cluster”, “transformer”}             or list of such str, default=None. Which kind of estimators should be returned. If None, no filter is\\napplied and all estimators are returned.  Possible values are\\n‘classifier’, ‘regressor’, ‘cluster’ and ‘transformer’ to get\\nestimators only of these specific types, or a list of these to\\nget the estimators that fit at least one of the types.\\n'}},\n",
       "        'required': []}}},\n",
       "     {'func_name': 'utils.discovery.all_functions',\n",
       "      'func_desc': 'Get a list of all functions from sklearn.',\n",
       "      'func_url': 'https://scikit-learn.org/stable/modules/generated/sklearn.utils.discovery.all_functions.html#sklearn.utils.discovery.all_functions',\n",
       "      'function_definitions': {'function_name': 'all_functions',\n",
       "       'full_function': 'sklearn.utils.discovery.all_functions()',\n",
       "       'function_text': 'Get a list of all functions from sklearn.',\n",
       "       'func_text_user_guide': '',\n",
       "       'parameter_names_desc': []},\n",
       "      'function_calling': {}}]}],\n",
       "  'name': 'Recently Deprecated',\n",
       "  'url': 'https://scikit-learn.org/stable/api/deprecated.html'}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_agent import scrape_sklearn_website\n",
    "\n",
    "scrape_sklearn_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_agent import load_database as sklearn_load_database\n",
    "from pandas_agent import load_database as pandas_load_database\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pandas_database = pandas_load_database(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_database = sklearn_load_database(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_agent import SklearnAgentChroma\n",
    "from pandas_agent import PandasAgentChroma\n",
    "\n",
    "sklearn_chroma = SklearnAgentChroma(sklearn_database)\n",
    "pandas_chroma = PandasAgentChroma(pandas_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.isotonic: Section Navigation Determine whether y is monotonically correlated with x. y is found increasing or decreasing with respect to x based on a Spearman correlation test.\n",
      "\n",
      "sklearn.cross_decomposition: Section Navigation Canonical Correlation Analysis, also known as “Mode B” PLS. For a comparison between other cross decomposition algorithms, see Compare cross decomposition methods. Read more in the User Guide.\n",
      "\n",
      "sklearn.feature_selection: Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure. The cross correlation between each regressor and the target is computed as: For more on usage see the User Guide. Added in version 1.0.\n",
      "\n",
      " sklearn.feature_selection\n",
      "sklearn.feature_selection#defaults: Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure. The cross correlation between each regressor and the target is computed as: For more on usage see the User Guide. Added in version 1.0.\n",
      "\n",
      "sklearn.feature_selection#defaults: Section Navigation Univariate linear regression tests returning F-statistic and p-values. Quick linear model for testing the effect of a single regressor, sequentially for many regressors. This is done in 2 steps: The cross correlation between each regressor and the target is computed using r_regression as: It is converted to an F score and then to a p-value. f_regression is derived from r_regression and will rank features in the same order if all the features are positively correlated with the target. Note however that contrary to f_regression, r_regression values lie in [-1, 1] and can thus be negative. f_regression is therefore recommended as a feature selection criterion to identify potentially predictive feature for a downstream classifier, irrespective of the sign of the association with the target variable. Furthermore f_regression returns p-values while r_regression does not. Read more in the User Guide.\n",
      "\n",
      "sklearn.feature_selection#defaults: Section Navigation Compute the ANOVA F-value for the provided sample. Read more in the User Guide.\n",
      "\n",
      "\n",
      "sklearn.feature_selection#defaults\n",
      "{'function_trail': {'$eq': 'sklearn.feature_selection-->defaults'}}\n"
     ]
    }
   ],
   "source": [
    "funcs = sklearn_chroma(\"How to do correlation analysis?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sklearn.feature_selection.r_regression(X, y, *, center=True, force_finite=True)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs[0]['full_function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'name': 'r_regression', 'descriptions': 'Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors.\\\\nThis is a scoring function to be used in a feature selection procedure, not\\\\na free standing feature selection procedure. The cross correlation between each regressor and the target is computed\\\\nas: For more on usage see the User Guide. Added in version 1.0.', 'parameters': {'type': 'object', 'properties': {'X': {'type': 'array', 'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data matrix.\\\\n'}, 'y': {'type': 'array', 'description': 'array-like of shape (n_samples,). The target vector.\\\\n'}, 'center': {'type': 'boolean', 'description': 'bool, default=True. Whether or not to center the data matrix X and the target vector y.\\\\nBy default, X and y will be centered.\\\\n'}, 'force_finite': {'type': 'boolean', 'description': 'bool, default=True. Whether or not to force the Pearson’s R correlation to be finite.\\\\nIn the particular case where some features in X or the target y\\\\nare constant, the Pearson’s R correlation is not defined. When\\\\nforce_finite=False, a correlation of np.nan is returned to\\\\nacknowledge this case. When force_finite=True, this value will be\\\\nforced to a minimal correlation of 0.0.\\\\n\\\\nAdded in version 1.1.\\\\n\\\\n'}}, 'required': ['X', 'y']}}\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs[0]['function_calling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit_code=0 output='Hello, World!\\n' code_file='/home/athekunal/Function Calling/hierarchical-function-calling-agent/coding/tmp_code_07da107bb575cc4e02b0e1d6d99cc204.py'\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from autogen.coding import CodeBlock, LocalCommandLineCodeExecutor\n",
    "\n",
    "work_dir = Path(\"coding\")\n",
    "work_dir.mkdir(exist_ok=True)\n",
    "\n",
    "executor = LocalCommandLineCodeExecutor(work_dir=work_dir)\n",
    "print(\n",
    "    executor.execute_code_blocks(\n",
    "        code_blocks=[\n",
    "            CodeBlock(language=\"python\", code=\"print('Hello, World!')\"),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import Tuple\n",
    "def sklearn_tool(query:str)->Tuple[str,dict]:\n",
    "    \"\"\"The sklearn tool will fetch the most relevant function definition that can answer the question that can be answered by the sklearn library. \n",
    "        It returns the full dummy function and function definition in JSON schema\n",
    "\n",
    "    Args:\n",
    "        query (str): query to the sklearn tool to fetch the most relevant function to answer the question\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str,dict]: The full dummy function and function definition in JSON schema\n",
    "    \"\"\"\n",
    "    sklearn_functions = sklearn_chroma(query)\n",
    "    if len(sklearn_functions) > 0:\n",
    "        return sklearn_functions[0]['full_function'],ast.literal_eval(sklearn_functions[0]['function_calling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_tool(query:str)->Tuple[str,dict]:\n",
    "    \"\"\"The pandas tool will fetch the most relevant function definition that can answer the question that can be answered by the pandas library. \n",
    "        It returns the full dummy function and function definition in JSON schema\n",
    "\n",
    "    Args:\n",
    "        query (str): query to the pandas tool to fetch the most relevant function to answer the question\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str,dict]: The full dummy function and function definition in JSON schema\n",
    "    \"\"\"\n",
    "    pandas_functions = pandas_chroma(query)\n",
    "    print(pandas_functions)\n",
    "    if len(pandas_functions) > 0:\n",
    "        return pandas_functions[0]['full_function'],ast.literal_eval(pandas_functions[0]['function_calling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_function,func_tool = pandas_tool(\"How to do correlation analysis of a dataframe?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn_assistant.llm_config['tools'] = func_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOGEN AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "sklearn_assistant = ConversableAgent(\n",
    "    name=\"SklearnAssistant\",\n",
    "    system_message=\"You are a helpful AI assistant that can write code. \"\n",
    "    \"You can help with writing code using the sklearn library. You will also be given a dummy full function that you need to refer when building the function\"\n",
    "    \"Return 'TERMINATE' when the task is done.\",\n",
    "    description=\"This agent can answer questions related to the sklearn library.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    ")\n",
    "pandas_assistant = ConversableAgent(\n",
    "    name=\"PandasAssistant\",\n",
    "    system_message=\"You are a helpful AI assistant that can write code. \"\n",
    "    \"You can help with writing code using the pandas library. You will also be given a dummy full function that you need to refer when building the function\"\n",
    "    \"Return 'TERMINATE' when the task is done.\",\n",
    "    description=\"This agent can answer questions related to the pandas library.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    ")\n",
    "\n",
    "execution_proxy = ConversableAgent(\n",
    "    name=\"CodeExecutor\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\"executor\": executor}\n",
    ")\n",
    "\n",
    "user_proxy = ConversableAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import GroupChat, GroupChatManager\n",
    "\n",
    "admin_proxy = ConversableAgent(\n",
    "    name=\"CodeAdmin\",\n",
    "    system_message=\"You are a helpful AI assistnat that will decide whom to allocate the task between sklearn assistant and pandas assistant. \"\n",
    "    \"Come up with a reasoning to decide between these two tools. The scikit-learn (sklearn assistant) library in Python is a powerful tool for machine learning. \"\n",
    "    \" It provides simple and efficient tools for data mining and data analysis, making it easy to implement various machine learning algorithms. \"\n",
    "    \"Key features include classification, regression, clustering, dimensionality, machine learning and preprocessing. The pandas library in Python is essential for data manipulation and analysis. \"\n",
    "    \" It provides data structures like Series (one-dimensional) and DataFrame (two-dimensional) to handle labeled data.\" \n",
    "    \" With pandas assistant, you can easily read and write data from various file formats (CSV, Excel, SQL, etc.), perform data cleaning, filtering, and transformation, handle missing data, and conduct complex data analysis.\"\n",
    "    \" Answer in one word only, either sklearn or pandas\",\n",
    "    llm_config={\"config_list\":[{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_transitions = {\n",
    "    sklearn_assistant: [execution_proxy],\n",
    "    pandas_assistant: [execution_proxy],\n",
    "    execution_proxy: [sklearn_assistant, pandas_assistant],\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_chat = GroupChat(\n",
    "    agents = [sklearn_assistant, pandas_assistant, execution_proxy],\n",
    "    messages=[],\n",
    "    max_round=3,\n",
    "    send_introductions=True,\n",
    "    allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
    "    speaker_transitions_type=\"allowed\"\n",
    ")\n",
    "\n",
    "group_chat_manager = GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    system_message=\"You are a helpful AI assistnat that will decide whom to allocate the task between sklearn assistant and pandas assistant. \"\n",
    "    \"Come up with a reasoning to decide between these two tools. The scikit-learn (sklearn assistant) library in Python is a powerful tool for machine learning. \"\n",
    "    \" It provides simple and efficient tools for data mining and data analysis, making it easy to implement various machine learning algorithms. \"\n",
    "    \"Key features include classification, regression, clustering, dimensionality, machine learning and preprocessing. The pandas library in Python is essential for data manipulation and analysis. \"\n",
    "    \" It provides data structures like Series (one-dimensional) and DataFrame (two-dimensional) to handle labeled data.\" \n",
    "    \" With pandas assistant, you can easily read and write data from various file formats (CSV, Excel, SQL, etc.), perform data cleaning, filtering, and transformation, handle missing data, and conduct complex data analysis.\"\n",
    "    \" Answer in one word only, either sklearn or pandas\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_result = user_proxy.initiate_chat(\n",
    "#     group_chat_manager,\n",
    "#     message=\"How to clustering?\",\n",
    "#     max_turns=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "sklearn\n"
     ]
    }
   ],
   "source": [
    "reply = admin_proxy.generate_reply(messages=[{\"content\": \"How to do clustering?\", \"role\": \"user\"}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "sklearn.cluster: Section Navigation Perform K-means clustering algorithm. Read more in the User Guide.\n",
      "\n",
      "sklearn.cluster: Section Navigation K-Means clustering. Read more in the User Guide.\n",
      "\n",
      "sklearn.cluster: Section Navigation Mini-Batch K-Means clustering. Read more in the User Guide.\n",
      "\n",
      " sklearn.cluster\n",
      "sklearn.cluster#defaults: Section Navigation Perform K-means clustering algorithm. Read more in the User Guide.\n",
      "\n",
      "sklearn.cluster#defaults: Section Navigation K-Means clustering. Read more in the User Guide.\n",
      "\n",
      "sklearn.cluster#defaults: Section Navigation Mini-Batch K-Means clustering. Read more in the User Guide.\n",
      "\n",
      "\n",
      "sklearn.cluster#defaults\n",
      "{'function_trail': {'$eq': 'sklearn.cluster-->defaults'}}\n"
     ]
    }
   ],
   "source": [
    "question = \"How to do clustering with KMeans?\"\n",
    "# while True:\n",
    "reply = admin_proxy.generate_reply(messages=[{\"content\": question , \"role\": \"user\"}])\n",
    "\n",
    "if reply not in ['sklearn', 'pandas']:\n",
    "    question_with_error = f\"Please output sklearn or pandas based on the question. Don't output {reply}\\n Question: {question}\"\n",
    "    reply = admin_proxy.generate_reply(messages=[{\"content\": question_with_error , \"role\": \"user\"}])\n",
    "\n",
    "if reply == \"sklearn\":\n",
    "    sklearn_full_function,sklearn_function = sklearn_tool(question)\n",
    "elif reply == \"pandas\":\n",
    "    pandas_full_function,pandas_function = pandas_tool(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_function_tool = [\n",
    "    {'type':'function',\n",
    "     'function':sklearn_function\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'k_means',\n",
       "   'descriptions': 'Section Navigation Perform K-means clustering algorithm. Read more in the User Guide.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'X': {'type': 'array',\n",
       "      'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The observations to cluster. It must be noted that the data\\nwill be converted to C ordering, which will cause a memory copy\\nif the given data is not C-contiguous.\\n'},\n",
       "     'n_clusters': {'type': 'integer',\n",
       "      'description': 'int. The number of clusters to form as well as the number of\\ncentroids to generate.\\n'},\n",
       "     'sample_weight': {'type': 'array',\n",
       "      'description': 'array-like of shape (n_samples,), default=None. The weights for each observation in X. If None, all observations\\nare assigned equal weight. sample_weight is not used during\\ninitialization if init is a callable or a user provided array.\\n'},\n",
       "     'init': {'type': 'object',\n",
       "      'description': \"{‘k-means++’, ‘random’}, callable or array-like of shape             (n_clusters, n_features), default=’k-means++’. Method for initialization:\\n\\n'k-means++' : selects initial cluster centers for k-mean\\nclustering in a smart way to speed up convergence. See section\\nNotes in k_init for more details.\\n'random': choose n_clusters observations (rows) at random from data\\nfor the initial centroids.\\nIf an array is passed, it should be of shape (n_clusters, n_features)\\nand gives the initial centers.\\nIf a callable is passed, it should take arguments X, n_clusters and a\\nrandom state and return an initialization.\\n\\n\"},\n",
       "     'n_init': {'type': 'integer',\n",
       "      'description': \"‘auto’ or int, default=”auto”. Number of time the k-means algorithm will be run with different\\ncentroid seeds. The final results will be the best output of\\nn_init consecutive runs in terms of inertia.\\nWhen n_init='auto', the number of runs depends on the value of init:\\n10 if using init='random' or init is a callable;\\n1 if using init='k-means++' or init is an array-like.\\n\\nAdded in version 1.2: Added ‘auto’ option for n_init.\\n\\n\\nChanged in version 1.4: Default value for n_init changed to 'auto'.\\n\\n\"},\n",
       "     'max_iter': {'type': 'integer',\n",
       "      'description': 'int, default=300. Maximum number of iterations of the k-means algorithm to run.\\n'},\n",
       "     'verbose': {'type': 'boolean',\n",
       "      'description': 'bool, default=False. Verbosity mode.\\n'},\n",
       "     'tol': {'type': 'float',\n",
       "      'description': 'float, default=1e-4. Relative tolerance with regards to Frobenius norm of the difference\\nin the cluster centers of two consecutive iterations to declare\\nconvergence.\\n'},\n",
       "     'random_state': {'type': 'integer',\n",
       "      'description': 'int, RandomState instance or None, default=None. Determines random number generation for centroid initialization. Use\\nan int to make the randomness deterministic.\\nSee Glossary.\\n'},\n",
       "     'copy_x': {'type': 'boolean',\n",
       "      'description': 'bool, default=True. When pre-computing distances it is more numerically accurate to center\\nthe data first. If copy_x is True (default), then the original data is\\nnot modified. If False, the original data is modified, and put back\\nbefore the function returns, but small numerical differences may be\\nintroduced by subtracting and then adding the data mean. Note that if\\nthe original data is not C-contiguous, a copy will be made even if\\ncopy_x is False. If the original data is sparse, but not in CSR format,\\na copy will be made even if copy_x is False.\\n'},\n",
       "     'algorithm': {'type': '{“lloyd”, “elkan”}, default=”lloyd”',\n",
       "      'description': '{“lloyd”, “elkan”}, default=”lloyd”. K-means algorithm to use. The classical EM-style algorithm is \"lloyd\".\\nThe \"elkan\" variation can be more efficient on some datasets with\\nwell-defined clusters, by using the triangle inequality. However it’s\\nmore memory intensive due to the allocation of an extra array of shape\\n(n_samples, n_clusters).\\n\\nChanged in version 0.18: Added Elkan algorithm\\n\\n\\nChanged in version 1.1: Renamed “full” to “lloyd”, and deprecated “auto” and “full”.\\nChanged “auto” to use “lloyd” instead of “elkan”.\\n\\n'},\n",
       "     'return_n_iter': {'type': 'boolean',\n",
       "      'description': 'bool, default=False. Whether or not to return the number of iterations.\\n'}},\n",
       "    'required': ['X', 'n_clusters']}}}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_function_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_assistant.llm_config['tools'] = sklearn_function_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'k_means',\n",
       "   'descriptions': 'Section Navigation Perform K-means clustering algorithm. Read more in the User Guide.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'X': {'type': 'array',\n",
       "      'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The observations to cluster. It must be noted that the data\\nwill be converted to C ordering, which will cause a memory copy\\nif the given data is not C-contiguous.\\n'},\n",
       "     'n_clusters': {'type': 'integer',\n",
       "      'description': 'int. The number of clusters to form as well as the number of\\ncentroids to generate.\\n'},\n",
       "     'sample_weight': {'type': 'array',\n",
       "      'description': 'array-like of shape (n_samples,), default=None. The weights for each observation in X. If None, all observations\\nare assigned equal weight. sample_weight is not used during\\ninitialization if init is a callable or a user provided array.\\n'},\n",
       "     'init': {'type': 'object',\n",
       "      'description': \"{‘k-means++’, ‘random’}, callable or array-like of shape             (n_clusters, n_features), default=’k-means++’. Method for initialization:\\n\\n'k-means++' : selects initial cluster centers for k-mean\\nclustering in a smart way to speed up convergence. See section\\nNotes in k_init for more details.\\n'random': choose n_clusters observations (rows) at random from data\\nfor the initial centroids.\\nIf an array is passed, it should be of shape (n_clusters, n_features)\\nand gives the initial centers.\\nIf a callable is passed, it should take arguments X, n_clusters and a\\nrandom state and return an initialization.\\n\\n\"},\n",
       "     'n_init': {'type': 'integer',\n",
       "      'description': \"‘auto’ or int, default=”auto”. Number of time the k-means algorithm will be run with different\\ncentroid seeds. The final results will be the best output of\\nn_init consecutive runs in terms of inertia.\\nWhen n_init='auto', the number of runs depends on the value of init:\\n10 if using init='random' or init is a callable;\\n1 if using init='k-means++' or init is an array-like.\\n\\nAdded in version 1.2: Added ‘auto’ option for n_init.\\n\\n\\nChanged in version 1.4: Default value for n_init changed to 'auto'.\\n\\n\"},\n",
       "     'max_iter': {'type': 'integer',\n",
       "      'description': 'int, default=300. Maximum number of iterations of the k-means algorithm to run.\\n'},\n",
       "     'verbose': {'type': 'boolean',\n",
       "      'description': 'bool, default=False. Verbosity mode.\\n'},\n",
       "     'tol': {'type': 'float',\n",
       "      'description': 'float, default=1e-4. Relative tolerance with regards to Frobenius norm of the difference\\nin the cluster centers of two consecutive iterations to declare\\nconvergence.\\n'},\n",
       "     'random_state': {'type': 'integer',\n",
       "      'description': 'int, RandomState instance or None, default=None. Determines random number generation for centroid initialization. Use\\nan int to make the randomness deterministic.\\nSee Glossary.\\n'},\n",
       "     'copy_x': {'type': 'boolean',\n",
       "      'description': 'bool, default=True. When pre-computing distances it is more numerically accurate to center\\nthe data first. If copy_x is True (default), then the original data is\\nnot modified. If False, the original data is modified, and put back\\nbefore the function returns, but small numerical differences may be\\nintroduced by subtracting and then adding the data mean. Note that if\\nthe original data is not C-contiguous, a copy will be made even if\\ncopy_x is False. If the original data is sparse, but not in CSR format,\\na copy will be made even if copy_x is False.\\n'},\n",
       "     'algorithm': {'type': '{“lloyd”, “elkan”}, default=”lloyd”',\n",
       "      'description': '{“lloyd”, “elkan”}, default=”lloyd”. K-means algorithm to use. The classical EM-style algorithm is \"lloyd\".\\nThe \"elkan\" variation can be more efficient on some datasets with\\nwell-defined clusters, by using the triangle inequality. However it’s\\nmore memory intensive due to the allocation of an extra array of shape\\n(n_samples, n_clusters).\\n\\nChanged in version 0.18: Added Elkan algorithm\\n\\n\\nChanged in version 1.1: Renamed “full” to “lloyd”, and deprecated “auto” and “full”.\\nChanged “auto” to use “lloyd” instead of “elkan”.\\n\\n'},\n",
       "     'return_n_iter': {'type': 'boolean',\n",
       "      'description': 'bool, default=False. Whether or not to return the number of iterations.\\n'}},\n",
       "    'required': ['X', 'n_clusters']}}}]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_assistant.llm_config['tools']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "How to do clustering with KMeans?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "To perform clustering with KMeans using the sklearn library in Python, you can follow the general steps below:\n",
      "\n",
      "1. Import the necessary libraries:\n",
      "```python\n",
      "from sklearn.cluster import KMeans\n",
      "```\n",
      "\n",
      "2. Create a KMeans object with the desired number of clusters (n_clusters) and other parameters if needed:\n",
      "```python\n",
      "kmeans = KMeans(n_clusters=3, random_state=0)\n",
      "```\n",
      "\n",
      "3. Fit the KMeans model to your data:\n",
      "```python\n",
      "kmeans.fit(data)\n",
      "```\n",
      "\n",
      "4. Get the predicted clusters for your data points:\n",
      "```python\n",
      "predicted_clusters = kmeans.predict(data)\n",
      "```\n",
      "\n",
      "5. Get the cluster centers:\n",
      "```python\n",
      "cluster_centers = kmeans.cluster_centers_\n",
      "```\n",
      "\n",
      "6. You can also assign new data points to the existing clusters using the `predict` method:\n",
      "```python\n",
      "new_data = [[1, 2], [3, 4]]\n",
      "predicted_new_data_clusters = kmeans.predict(new_data)\n",
      "```\n",
      "\n",
      "7. Evaluate the quality of clusters using metrics like inertia or silhouette score:\n",
      "```python\n",
      "inertia = kmeans.inertia_\n",
      "```\n",
      "\n",
      "This is a high-level overview of how to perform clustering with KMeans in sklearn. Let me know if you need help with a specific part of the code or have any other questions.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "I hope the explanation provided was helpful! If you have any specific questions or need further assistance with implementing KMeans clustering using the sklearn library, feel free to ask.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_proxy = ConversableAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "chat_result = user_proxy.initiate_chat(sklearn_assistant, message=question,max_turns=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
