{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_agent import scrape_pandas_website\n",
    "# scrape_pandas_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_agent import load_database as sklearn_load_database\n",
    "from pandas_agent import load_database as pandas_load_database\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pandas_database = pandas_load_database(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_database = sklearn_load_database(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_agent import SklearnAgentChroma\n",
    "from pandas_agent import PandasAgentChroma\n",
    "\n",
    "sklearn_chroma = SklearnAgentChroma(sklearn_database)\n",
    "pandas_chroma = PandasAgentChroma(pandas_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.isotonic: Section Navigation Determine whether y is monotonically correlated with x. y is found increasing or decreasing with respect to x based on a Spearman correlation test.\n",
      "\n",
      "sklearn.cross_decomposition: Section Navigation Canonical Correlation Analysis, also known as “Mode B” PLS. For a comparison between other cross decomposition algorithms, see Compare cross decomposition methods. Read more in the User Guide.\n",
      "\n",
      "sklearn.feature_selection: Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure. The cross correlation between each regressor and the target is computed as: For more on usage see the User Guide. Added in version 1.0.\n",
      "\n",
      " sklearn.feature_selection\n",
      "sklearn.feature_selection#defaults: Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure. The cross correlation between each regressor and the target is computed as: For more on usage see the User Guide. Added in version 1.0.\n",
      "\n",
      "sklearn.feature_selection#defaults: Section Navigation Univariate linear regression tests returning F-statistic and p-values. Quick linear model for testing the effect of a single regressor, sequentially for many regressors. This is done in 2 steps: The cross correlation between each regressor and the target is computed using r_regression as: It is converted to an F score and then to a p-value. f_regression is derived from r_regression and will rank features in the same order if all the features are positively correlated with the target. Note however that contrary to f_regression, r_regression values lie in [-1, 1] and can thus be negative. f_regression is therefore recommended as a feature selection criterion to identify potentially predictive feature for a downstream classifier, irrespective of the sign of the association with the target variable. Furthermore f_regression returns p-values while r_regression does not. Read more in the User Guide.\n",
      "\n",
      "sklearn.feature_selection#defaults: Section Navigation Compute the ANOVA F-value for the provided sample. Read more in the User Guide.\n",
      "\n",
      "\n",
      "sklearn.feature_selection#defaults\n",
      "{'function_trail': {'$eq': 'sklearn.feature_selection-->defaults'}}\n"
     ]
    }
   ],
   "source": [
    "funcs = sklearn_chroma(\"How to do correlation analysis?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sklearn.feature_selection.r_regression(X, y, *, center=True, force_finite=True)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs[0]['full_function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'name': 'r_regression', 'descriptions': 'Section Navigation Compute Pearson’s r for each features and the target. Pearson’s r is also known as the Pearson correlation coefficient. Linear model for testing the individual effect of each of many regressors.\\\\nThis is a scoring function to be used in a feature selection procedure, not\\\\na free standing feature selection procedure. The cross correlation between each regressor and the target is computed\\\\nas: For more on usage see the User Guide. Added in version 1.0.', 'parameters': {'type': 'object', 'properties': {'X': {'type': 'array', 'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The data matrix.\\\\n'}, 'y': {'type': 'array', 'description': 'array-like of shape (n_samples,). The target vector.\\\\n'}, 'center': {'type': 'boolean', 'description': 'bool, default=True. Whether or not to center the data matrix X and the target vector y.\\\\nBy default, X and y will be centered.\\\\n'}, 'force_finite': {'type': 'boolean', 'description': 'bool, default=True. Whether or not to force the Pearson’s R correlation to be finite.\\\\nIn the particular case where some features in X or the target y\\\\nare constant, the Pearson’s R correlation is not defined. When\\\\nforce_finite=False, a correlation of np.nan is returned to\\\\nacknowledge this case. When force_finite=True, this value will be\\\\nforced to a minimal correlation of 0.0.\\\\n\\\\nAdded in version 1.1.\\\\n\\\\n'}}, 'required': ['X', 'y']}}\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs[0]['function_calling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit_code=0 output='Hello, World!\\n' code_file='/home/athekunal/Function Calling/hierarchical-function-calling-agent/coding/tmp_code_07da107bb575cc4e02b0e1d6d99cc204.py'\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from autogen.coding import CodeBlock, LocalCommandLineCodeExecutor\n",
    "\n",
    "work_dir = Path(\"coding\")\n",
    "work_dir.mkdir(exist_ok=True)\n",
    "\n",
    "executor = LocalCommandLineCodeExecutor(work_dir=work_dir)\n",
    "print(\n",
    "    executor.execute_code_blocks(\n",
    "        code_blocks=[\n",
    "            CodeBlock(language=\"python\", code=\"print('Hello, World!')\"),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import Tuple\n",
    "def sklearn_tool(query:str)->Tuple[str,dict]:\n",
    "    \"\"\"The sklearn tool will fetch the most relevant function definition that can answer the question that can be answered by the sklearn library. \n",
    "        It returns the full dummy function and function definition in JSON schema\n",
    "\n",
    "    Args:\n",
    "        query (str): query to the sklearn tool to fetch the most relevant function to answer the question\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str,dict]: The full dummy function and function definition in JSON schema\n",
    "    \"\"\"\n",
    "    sklearn_functions = sklearn_chroma(query)\n",
    "    if len(sklearn_functions) > 0:\n",
    "        return sklearn_functions[0]['full_function'],ast.literal_eval(sklearn_functions[0]['function_calling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_tool(query:str)->Tuple[str,dict]:\n",
    "    \"\"\"The pandas tool will fetch the most relevant function definition that can answer the question that can be answered by the pandas library. \n",
    "        It returns the full dummy function and function definition in JSON schema\n",
    "\n",
    "    Args:\n",
    "        query (str): query to the pandas tool to fetch the most relevant function to answer the question\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str,dict]: The full dummy function and function definition in JSON schema\n",
    "    \"\"\"\n",
    "    pandas_functions = pandas_chroma(query)\n",
    "    print(pandas_functions)\n",
    "    if len(pandas_functions) > 0:\n",
    "        return pandas_functions[0]['full_function'],ast.literal_eval(pandas_functions[0]['function_calling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_function,func_tool = pandas_tool(\"How to do correlation analysis of a dataframe?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn_assistant.llm_config['tools'] = func_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOGEN AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "sklearn_assistant = ConversableAgent(\n",
    "    name=\"SklearnAssistant\",\n",
    "    system_message=\"You are a helpful AI assistant that can write code. \"\n",
    "    \"You can help with writing code using the sklearn library. You will also be given a dummy full function that you need to refer when building the function\"\n",
    "    \"Return 'TERMINATE' when the task is done.\",\n",
    "    description=\"This agent can answer questions related to the sklearn library.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    ")\n",
    "pandas_assistant = ConversableAgent(\n",
    "    name=\"PandasAssistant\",\n",
    "    system_message=\"You are a helpful AI assistant that can write code. \"\n",
    "    \"You can help with writing code using the pandas library. You will also be given a dummy full function that you need to refer when building the function\"\n",
    "    \"Return 'TERMINATE' when the task is done.\",\n",
    "    description=\"This agent can answer questions related to the pandas library.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    ")\n",
    "\n",
    "execution_proxy = ConversableAgent(\n",
    "    name=\"CodeExecutor\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\"executor\": executor}\n",
    ")\n",
    "\n",
    "user_proxy = ConversableAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import GroupChat, GroupChatManager\n",
    "\n",
    "admin_proxy = ConversableAgent(\n",
    "    name=\"CodeAdmin\",\n",
    "    system_message=\"You are a helpful AI assistnat that will decide whom to allocate the task between sklearn assistant and pandas assistant. \"\n",
    "    \"Come up with a reasoning to decide between these two tools. The scikit-learn (sklearn assistant) library in Python is a powerful tool for machine learning. \"\n",
    "    \" It provides simple and efficient tools for data mining and data analysis, making it easy to implement various machine learning algorithms. \"\n",
    "    \"Key features include classification, regression, clustering, dimensionality, machine learning and preprocessing. The pandas library in Python is essential for data manipulation and analysis. \"\n",
    "    \" It provides data structures like Series (one-dimensional) and DataFrame (two-dimensional) to handle labeled data.\" \n",
    "    \" With pandas assistant, you can easily read and write data from various file formats (CSV, Excel, SQL, etc.), perform data cleaning, filtering, and transformation, handle missing data, and conduct complex data analysis.\"\n",
    "    \" Answer in one word only, either sklearn or pandas\",\n",
    "    llm_config={\"config_list\":[{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_transitions = {\n",
    "    sklearn_assistant: [execution_proxy],\n",
    "    pandas_assistant: [execution_proxy],\n",
    "    execution_proxy: [sklearn_assistant, pandas_assistant],\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_chat = GroupChat(\n",
    "    agents = [sklearn_assistant, pandas_assistant, execution_proxy],\n",
    "    messages=[],\n",
    "    max_round=3,\n",
    "    send_introductions=True,\n",
    "    allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
    "    speaker_transitions_type=\"allowed\"\n",
    ")\n",
    "\n",
    "group_chat_manager = GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    system_message=\"You are a helpful AI assistnat that will decide whom to allocate the task between sklearn assistant and pandas assistant. \"\n",
    "    \"Come up with a reasoning to decide between these two tools. The scikit-learn (sklearn assistant) library in Python is a powerful tool for machine learning. \"\n",
    "    \" It provides simple and efficient tools for data mining and data analysis, making it easy to implement various machine learning algorithms. \"\n",
    "    \"Key features include classification, regression, clustering, dimensionality, machine learning and preprocessing. The pandas library in Python is essential for data manipulation and analysis. \"\n",
    "    \" It provides data structures like Series (one-dimensional) and DataFrame (two-dimensional) to handle labeled data.\" \n",
    "    \" With pandas assistant, you can easily read and write data from various file formats (CSV, Excel, SQL, etc.), perform data cleaning, filtering, and transformation, handle missing data, and conduct complex data analysis.\"\n",
    "    \" Answer in one word only, either sklearn or pandas\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_result = user_proxy.initiate_chat(\n",
    "#     group_chat_manager,\n",
    "#     message=\"How to clustering?\",\n",
    "#     max_turns=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "sklearn\n"
     ]
    }
   ],
   "source": [
    "reply = admin_proxy.generate_reply(messages=[{\"content\": \"How to do clustering?\", \"role\": \"user\"}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "sklearn.cluster: Section Navigation Perform K-means clustering algorithm. Read more in the User Guide.\n",
      "\n",
      "sklearn.cluster: Section Navigation K-Means clustering. Read more in the User Guide.\n",
      "\n",
      "sklearn.cluster: Section Navigation Mini-Batch K-Means clustering. Read more in the User Guide.\n",
      "\n",
      " sklearn.cluster\n",
      "sklearn.cluster#defaults: Section Navigation Perform K-means clustering algorithm. Read more in the User Guide.\n",
      "\n",
      "sklearn.cluster#defaults: Section Navigation K-Means clustering. Read more in the User Guide.\n",
      "\n",
      "sklearn.cluster#defaults: Section Navigation Mini-Batch K-Means clustering. Read more in the User Guide.\n",
      "\n",
      "\n",
      "sklearn.cluster#defaults\n",
      "{'function_trail': {'$eq': 'sklearn.cluster-->defaults'}}\n"
     ]
    }
   ],
   "source": [
    "question = \"How to do clustering with KMeans?\"\n",
    "# while True:\n",
    "reply = admin_proxy.generate_reply(messages=[{\"content\": question , \"role\": \"user\"}])\n",
    "\n",
    "if reply not in ['sklearn', 'pandas']:\n",
    "    question_with_error = f\"Please output sklearn or pandas based on the question. Don't output {reply}\\n Question: {question}\"\n",
    "    reply = admin_proxy.generate_reply(messages=[{\"content\": question_with_error , \"role\": \"user\"}])\n",
    "\n",
    "if reply == \"sklearn\":\n",
    "    sklearn_full_function,sklearn_function = sklearn_tool(question)\n",
    "elif reply == \"pandas\":\n",
    "    pandas_full_function,pandas_function = pandas_tool(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_function_tool = [\n",
    "    {'type':'function',\n",
    "     'function':sklearn_function\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'k_means',\n",
       "   'descriptions': 'Section Navigation Perform K-means clustering algorithm. Read more in the User Guide.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'X': {'type': 'array',\n",
       "      'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The observations to cluster. It must be noted that the data\\nwill be converted to C ordering, which will cause a memory copy\\nif the given data is not C-contiguous.\\n'},\n",
       "     'n_clusters': {'type': 'integer',\n",
       "      'description': 'int. The number of clusters to form as well as the number of\\ncentroids to generate.\\n'},\n",
       "     'sample_weight': {'type': 'array',\n",
       "      'description': 'array-like of shape (n_samples,), default=None. The weights for each observation in X. If None, all observations\\nare assigned equal weight. sample_weight is not used during\\ninitialization if init is a callable or a user provided array.\\n'},\n",
       "     'init': {'type': 'object',\n",
       "      'description': \"{‘k-means++’, ‘random’}, callable or array-like of shape             (n_clusters, n_features), default=’k-means++’. Method for initialization:\\n\\n'k-means++' : selects initial cluster centers for k-mean\\nclustering in a smart way to speed up convergence. See section\\nNotes in k_init for more details.\\n'random': choose n_clusters observations (rows) at random from data\\nfor the initial centroids.\\nIf an array is passed, it should be of shape (n_clusters, n_features)\\nand gives the initial centers.\\nIf a callable is passed, it should take arguments X, n_clusters and a\\nrandom state and return an initialization.\\n\\n\"},\n",
       "     'n_init': {'type': 'integer',\n",
       "      'description': \"‘auto’ or int, default=”auto”. Number of time the k-means algorithm will be run with different\\ncentroid seeds. The final results will be the best output of\\nn_init consecutive runs in terms of inertia.\\nWhen n_init='auto', the number of runs depends on the value of init:\\n10 if using init='random' or init is a callable;\\n1 if using init='k-means++' or init is an array-like.\\n\\nAdded in version 1.2: Added ‘auto’ option for n_init.\\n\\n\\nChanged in version 1.4: Default value for n_init changed to 'auto'.\\n\\n\"},\n",
       "     'max_iter': {'type': 'integer',\n",
       "      'description': 'int, default=300. Maximum number of iterations of the k-means algorithm to run.\\n'},\n",
       "     'verbose': {'type': 'boolean',\n",
       "      'description': 'bool, default=False. Verbosity mode.\\n'},\n",
       "     'tol': {'type': 'float',\n",
       "      'description': 'float, default=1e-4. Relative tolerance with regards to Frobenius norm of the difference\\nin the cluster centers of two consecutive iterations to declare\\nconvergence.\\n'},\n",
       "     'random_state': {'type': 'integer',\n",
       "      'description': 'int, RandomState instance or None, default=None. Determines random number generation for centroid initialization. Use\\nan int to make the randomness deterministic.\\nSee Glossary.\\n'},\n",
       "     'copy_x': {'type': 'boolean',\n",
       "      'description': 'bool, default=True. When pre-computing distances it is more numerically accurate to center\\nthe data first. If copy_x is True (default), then the original data is\\nnot modified. If False, the original data is modified, and put back\\nbefore the function returns, but small numerical differences may be\\nintroduced by subtracting and then adding the data mean. Note that if\\nthe original data is not C-contiguous, a copy will be made even if\\ncopy_x is False. If the original data is sparse, but not in CSR format,\\na copy will be made even if copy_x is False.\\n'},\n",
       "     'algorithm': {'type': '{“lloyd”, “elkan”}, default=”lloyd”',\n",
       "      'description': '{“lloyd”, “elkan”}, default=”lloyd”. K-means algorithm to use. The classical EM-style algorithm is \"lloyd\".\\nThe \"elkan\" variation can be more efficient on some datasets with\\nwell-defined clusters, by using the triangle inequality. However it’s\\nmore memory intensive due to the allocation of an extra array of shape\\n(n_samples, n_clusters).\\n\\nChanged in version 0.18: Added Elkan algorithm\\n\\n\\nChanged in version 1.1: Renamed “full” to “lloyd”, and deprecated “auto” and “full”.\\nChanged “auto” to use “lloyd” instead of “elkan”.\\n\\n'},\n",
       "     'return_n_iter': {'type': 'boolean',\n",
       "      'description': 'bool, default=False. Whether or not to return the number of iterations.\\n'}},\n",
       "    'required': ['X', 'n_clusters']}}}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_function_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_assistant.llm_config['tools'] = sklearn_function_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'k_means',\n",
       "   'descriptions': 'Section Navigation Perform K-means clustering algorithm. Read more in the User Guide.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'X': {'type': 'array',\n",
       "      'description': '{array-like, sparse matrix} of shape (n_samples, n_features). The observations to cluster. It must be noted that the data\\nwill be converted to C ordering, which will cause a memory copy\\nif the given data is not C-contiguous.\\n'},\n",
       "     'n_clusters': {'type': 'integer',\n",
       "      'description': 'int. The number of clusters to form as well as the number of\\ncentroids to generate.\\n'},\n",
       "     'sample_weight': {'type': 'array',\n",
       "      'description': 'array-like of shape (n_samples,), default=None. The weights for each observation in X. If None, all observations\\nare assigned equal weight. sample_weight is not used during\\ninitialization if init is a callable or a user provided array.\\n'},\n",
       "     'init': {'type': 'object',\n",
       "      'description': \"{‘k-means++’, ‘random’}, callable or array-like of shape             (n_clusters, n_features), default=’k-means++’. Method for initialization:\\n\\n'k-means++' : selects initial cluster centers for k-mean\\nclustering in a smart way to speed up convergence. See section\\nNotes in k_init for more details.\\n'random': choose n_clusters observations (rows) at random from data\\nfor the initial centroids.\\nIf an array is passed, it should be of shape (n_clusters, n_features)\\nand gives the initial centers.\\nIf a callable is passed, it should take arguments X, n_clusters and a\\nrandom state and return an initialization.\\n\\n\"},\n",
       "     'n_init': {'type': 'integer',\n",
       "      'description': \"‘auto’ or int, default=”auto”. Number of time the k-means algorithm will be run with different\\ncentroid seeds. The final results will be the best output of\\nn_init consecutive runs in terms of inertia.\\nWhen n_init='auto', the number of runs depends on the value of init:\\n10 if using init='random' or init is a callable;\\n1 if using init='k-means++' or init is an array-like.\\n\\nAdded in version 1.2: Added ‘auto’ option for n_init.\\n\\n\\nChanged in version 1.4: Default value for n_init changed to 'auto'.\\n\\n\"},\n",
       "     'max_iter': {'type': 'integer',\n",
       "      'description': 'int, default=300. Maximum number of iterations of the k-means algorithm to run.\\n'},\n",
       "     'verbose': {'type': 'boolean',\n",
       "      'description': 'bool, default=False. Verbosity mode.\\n'},\n",
       "     'tol': {'type': 'float',\n",
       "      'description': 'float, default=1e-4. Relative tolerance with regards to Frobenius norm of the difference\\nin the cluster centers of two consecutive iterations to declare\\nconvergence.\\n'},\n",
       "     'random_state': {'type': 'integer',\n",
       "      'description': 'int, RandomState instance or None, default=None. Determines random number generation for centroid initialization. Use\\nan int to make the randomness deterministic.\\nSee Glossary.\\n'},\n",
       "     'copy_x': {'type': 'boolean',\n",
       "      'description': 'bool, default=True. When pre-computing distances it is more numerically accurate to center\\nthe data first. If copy_x is True (default), then the original data is\\nnot modified. If False, the original data is modified, and put back\\nbefore the function returns, but small numerical differences may be\\nintroduced by subtracting and then adding the data mean. Note that if\\nthe original data is not C-contiguous, a copy will be made even if\\ncopy_x is False. If the original data is sparse, but not in CSR format,\\na copy will be made even if copy_x is False.\\n'},\n",
       "     'algorithm': {'type': '{“lloyd”, “elkan”}, default=”lloyd”',\n",
       "      'description': '{“lloyd”, “elkan”}, default=”lloyd”. K-means algorithm to use. The classical EM-style algorithm is \"lloyd\".\\nThe \"elkan\" variation can be more efficient on some datasets with\\nwell-defined clusters, by using the triangle inequality. However it’s\\nmore memory intensive due to the allocation of an extra array of shape\\n(n_samples, n_clusters).\\n\\nChanged in version 0.18: Added Elkan algorithm\\n\\n\\nChanged in version 1.1: Renamed “full” to “lloyd”, and deprecated “auto” and “full”.\\nChanged “auto” to use “lloyd” instead of “elkan”.\\n\\n'},\n",
       "     'return_n_iter': {'type': 'boolean',\n",
       "      'description': 'bool, default=False. Whether or not to return the number of iterations.\\n'}},\n",
       "    'required': ['X', 'n_clusters']}}}]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_assistant.llm_config['tools']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "How to do clustering with KMeans?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "To perform clustering with KMeans using the sklearn library in Python, you can follow the general steps below:\n",
      "\n",
      "1. Import the necessary libraries:\n",
      "```python\n",
      "from sklearn.cluster import KMeans\n",
      "```\n",
      "\n",
      "2. Create a KMeans object with the desired number of clusters (n_clusters) and other parameters if needed:\n",
      "```python\n",
      "kmeans = KMeans(n_clusters=3, random_state=0)\n",
      "```\n",
      "\n",
      "3. Fit the KMeans model to your data:\n",
      "```python\n",
      "kmeans.fit(data)\n",
      "```\n",
      "\n",
      "4. Get the predicted clusters for your data points:\n",
      "```python\n",
      "predicted_clusters = kmeans.predict(data)\n",
      "```\n",
      "\n",
      "5. Get the cluster centers:\n",
      "```python\n",
      "cluster_centers = kmeans.cluster_centers_\n",
      "```\n",
      "\n",
      "6. You can also assign new data points to the existing clusters using the `predict` method:\n",
      "```python\n",
      "new_data = [[1, 2], [3, 4]]\n",
      "predicted_new_data_clusters = kmeans.predict(new_data)\n",
      "```\n",
      "\n",
      "7. Evaluate the quality of clusters using metrics like inertia or silhouette score:\n",
      "```python\n",
      "inertia = kmeans.inertia_\n",
      "```\n",
      "\n",
      "This is a high-level overview of how to perform clustering with KMeans in sklearn. Let me know if you need help with a specific part of the code or have any other questions.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to SklearnAssistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSklearnAssistant\u001b[0m (to User):\n",
      "\n",
      "I hope the explanation provided was helpful! If you have any specific questions or need further assistance with implementing KMeans clustering using the sklearn library, feel free to ask.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_proxy = ConversableAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "chat_result = user_proxy.initiate_chat(sklearn_assistant, message=question,max_turns=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
