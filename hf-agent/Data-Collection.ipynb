{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGENTS PAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_xpath = '//*[@id=\"hf-doc-container\"]/div[1]/div/div/nav/div[17]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "api_page = requests.get(\"https://huggingface.co/docs/transformers/index\")\n",
    "index_page =  BeautifulSoup(api_page.content, 'lxml',from_encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# elems = index_page.find_all(attrs={\"class\":\"inline-block space-x-1 leading-5\"})\n",
    "# for idx,elem in enumerate(elems):\n",
    "#     if elem.text.strip() == \"API\":\n",
    "#         print(elem.text)\n",
    "#         break\n",
    "# req_elems = elems[idx+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤— Transformers\n",
      "Quick tour\n",
      "Installation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run inference with pipelines\n",
      "Write portable code with AutoClass\n",
      "Preprocess data\n",
      "Fine-tune a pretrained model\n",
      "Train with a script\n",
      "Set up distributed training with ðŸ¤— Accelerate\n",
      "Load and train adapters with ðŸ¤— PEFT\n",
      "Share your model\n",
      "Agents\n",
      "Generation with LLMs\n",
      "Chatting with Transformers\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Natural Language Processing\n",
      "\n",
      "\n",
      "Audio\n",
      "\n",
      "\n",
      "Computer Vision\n",
      "\n",
      "\n",
      "Multimodal\n",
      "\n",
      "\n",
      "Generation\n",
      "\n",
      "\n",
      "Prompting\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Use fast tokenizers from ðŸ¤— Tokenizers\n",
      "Run inference with multilingual models\n",
      "Use model-specific APIs\n",
      "Share a custom model\n",
      "Templates for chat models\n",
      "Trainer\n",
      "Run training on Amazon SageMaker\n",
      "Export to ONNX\n",
      "Export to TFLite\n",
      "Export to TorchScript\n",
      "Benchmarks\n",
      "Notebooks with examples\n",
      "Community resources\n",
      "Troubleshoot\n",
      "Contribute new quantization method\n",
      "Interoperability with GGUF files\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Overview\n",
      "LLM inference optimization\n",
      "Quantization\n",
      "\n",
      "Efficient training techniques\n",
      "\n",
      "\n",
      "Methods and tools for efficient training on a single GPU\n",
      "Multiple GPUs and parallelism\n",
      "Fully Sharded Data Parallel\n",
      "DeepSpeed\n",
      "Efficient training on CPU\n",
      "Distributed CPU training\n",
      "Training on TPU with TensorFlow\n",
      "PyTorch training on Apple silicon\n",
      "Custom hardware for training\n",
      "Hyperparameter Search using Trainer API\n",
      "\n",
      "\n",
      "Optimizing inference\n",
      "\n",
      "\n",
      "CPU inference\n",
      "GPU inference\n",
      "\n",
      "Instantiate a big model\n",
      "Debugging\n",
      "XLA Integration for TensorFlow Models\n",
      "Optimize inference using `torch.compile()`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Methods and tools for efficient training on a single GPU\n",
      "Multiple GPUs and parallelism\n",
      "Fully Sharded Data Parallel\n",
      "DeepSpeed\n",
      "Efficient training on CPU\n",
      "Distributed CPU training\n",
      "Training on TPU with TensorFlow\n",
      "PyTorch training on Apple silicon\n",
      "Custom hardware for training\n",
      "Hyperparameter Search using Trainer API\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU inference\n",
      "GPU inference\n",
      "----------------------------------------------------------------------------------------------------\n",
      "How to contribute to ðŸ¤— Transformers?\n",
      "How to add a model to ðŸ¤— Transformers?\n",
      "How to add a pipeline to ðŸ¤— Transformers?\n",
      "Testing\n",
      "Checks on a Pull Request\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Philosophy\n",
      "Glossary\n",
      "What ðŸ¤— Transformers can do\n",
      "How ðŸ¤— Transformers solve tasks\n",
      "The Transformer model family\n",
      "Summary of the tokenizers\n",
      "Attention mechanisms\n",
      "Padding and truncation\n",
      "BERTology\n",
      "Perplexity of fixed-length models\n",
      "Pipelines for webserver inference\n",
      "Model training anatomy\n",
      "Getting the most out of LLMs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Main Classes\n",
      "\n",
      "\n",
      "Agents and Tools\n",
      "Auto Classes\n",
      "Backbones\n",
      "Callbacks\n",
      "Configuration\n",
      "Data Collator\n",
      "Keras callbacks\n",
      "Logging\n",
      "Models\n",
      "Text Generation\n",
      "ONNX\n",
      "Optimization\n",
      "Model outputs\n",
      "Pipelines\n",
      "Processors\n",
      "Quantization\n",
      "Tokenizer\n",
      "Trainer\n",
      "DeepSpeed\n",
      "Feature Extractor\n",
      "Image Processor\n",
      "\n",
      "\n",
      "Models\n",
      "\n",
      "\n",
      "\n",
      "Text models\n",
      "\n",
      "\n",
      "Vision models\n",
      "\n",
      "\n",
      "Audio models\n",
      "\n",
      "\n",
      "Video models\n",
      "\n",
      "\n",
      "Multimodal models\n",
      "\n",
      "\n",
      "Reinforcement learning models\n",
      "\n",
      "\n",
      "Time series models\n",
      "\n",
      "\n",
      "Graph models\n",
      "\n",
      "\n",
      "\n",
      "Internal Helpers\n",
      "\n",
      "\n",
      "Custom Layers and Utilities\n",
      "Utilities for pipelines\n",
      "Utilities for Tokenizers\n",
      "Utilities for Trainer\n",
      "Utilities for Generation\n",
      "Utilities for Image Processors\n",
      "Utilities for Audio processing\n",
      "General Utilities\n",
      "Utilities for Time Series\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Agents and Tools\n",
      "Auto Classes\n",
      "Backbones\n",
      "Callbacks\n",
      "Configuration\n",
      "Data Collator\n",
      "Keras callbacks\n",
      "Logging\n",
      "Models\n",
      "Text Generation\n",
      "ONNX\n",
      "Optimization\n",
      "Model outputs\n",
      "Pipelines\n",
      "Processors\n",
      "Quantization\n",
      "Tokenizer\n",
      "Trainer\n",
      "DeepSpeed\n",
      "Feature Extractor\n",
      "Image Processor\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text models\n",
      "\n",
      "\n",
      "Vision models\n",
      "\n",
      "\n",
      "Audio models\n",
      "\n",
      "\n",
      "Video models\n",
      "\n",
      "\n",
      "Multimodal models\n",
      "\n",
      "\n",
      "Reinforcement learning models\n",
      "\n",
      "\n",
      "Time series models\n",
      "\n",
      "\n",
      "Graph models\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Custom Layers and Utilities\n",
      "Utilities for pipelines\n",
      "Utilities for Tokenizers\n",
      "Utilities for Trainer\n",
      "Utilities for Generation\n",
      "Utilities for Image Processors\n",
      "Utilities for Audio processing\n",
      "General Utilities\n",
      "Utilities for Time Series\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "elems = index_page.find_all(attrs={\"class\":\"flex flex-col\"})\n",
    "\n",
    "\n",
    "for idx,elem in enumerate(elems):\n",
    "    print(elem.text.strip())\n",
    "    print('-'*100)\n",
    "    # if elem.text.strip() == \"Main Classes\":\n",
    "    #     print(elem.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*[@id=\"hf-doc-container\"]/div[1]/div/div/nav/div[17]/div[2]\n",
    "# //*[@id=\"hf-doc-container\"]/div[1]/div/div/nav/div[17]/div[4]\n",
    "# //*[@id=\"hf-doc-container\"]/div[1]/div/div/nav/div[17]/div[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elems = index_page.find_all(attrs={\"xpath\":'//*[@id=\"hf-doc-container\"]/div[1]/div/div/nav/div[17]/div[2]'})\n",
    "elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "tree = html.fromstring(api_page.content)\n",
    "buyers = tree.xpath('//*[@id=\"hf-doc-container\"]/div[1]/div/div/nav/div[17]/div[2]')\n",
    "buyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome(service=Service())\n",
    "driver.get(\"https://huggingface.co/docs/transformers/index\")\n",
    "driver.maximize_window()\n",
    "driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "\n",
    "for i in range(1,16):\n",
    "    model = driver.find_element(\n",
    "            By.XPATH, f'//*[@id=\"hf-doc-container\"]/div[1]/div/div/nav/div[16]/div[4]/div[{i}]'\n",
    "        )\n",
    "    model.click()\n",
    "element = driver.find_element(By.XPATH, '//*[@id=\"hf-doc-container\"]/div[1]/div/div/nav/div[16]')\n",
    "element_html = element.get_attribute('outerHTML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_bs4 = BeautifulSoup(element_html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text = elem_bs4.find(class_=\"flex flex-col\").text\n",
    "model_types = [\"Text models\",\"Vision models\",\"Audio models\",\"Video models\",\"Multimodal models\",\"Reinforcement learning models\",'Time series models',\"Graph models\",\"Internal Helpers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "model_dict = {}\n",
    "for i in range(len(model_types)-1):\n",
    "    start_pattern = re.escape(model_types[i])\n",
    "    end_pattern = re.escape(model_types[i+1])\n",
    "    match = re.search(f'{start_pattern}(.*?){end_pattern}', model_text, re.DOTALL)\n",
    "    if match:\n",
    "        extracted_text = match.group(1).strip()\n",
    "        model_dict.update({model_types[i]:extracted_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text models': 'ALBERT BART BARThez BARTpho BERT BertGeneration BertJapanese Bertweet BigBird BigBirdPegasus BioGpt Blenderbot Blenderbot Small BLOOM BORT ByT5 CamemBERT CANINE CodeGen CodeLlama Cohere ConvBERT CPM CPMANT CTRL DBRX DeBERTa DeBERTa-v2 DialoGPT DistilBERT DPR ELECTRA Encoder Decoder Models ERNIE ErnieM ESM Falcon FastSpeech2Conformer FLAN-T5 FLAN-UL2 FlauBERT FNet FSMT Funnel Transformer Fuyu Gemma GPT GPT Neo GPT NeoX GPT NeoX Japanese GPT-J GPT2 GPTBigCode GPTSAN Japanese GPTSw3 HerBERT I-BERT Jamba JetMoe Jukebox LED LLaMA Llama2 Llama3 Longformer LongT5 LUKE M2M100 MADLAD-400 Mamba MarianMT MarkupLM MBart and MBart-50 MEGA MegatronBERT MegatronGPT2 Mistral Mixtral mLUKE MobileBERT MPNet MPT MRA MT5 MVP NEZHA NLLB NLLB-MoE NystrÃ¶mformer OLMo Open-Llama OPT Pegasus PEGASUS-X Persimmon Phi Phi-3 PhoBERT PLBart ProphetNet QDQBert Qwen2 Qwen2MoE RAG REALM RecurrentGemma Reformer RemBERT RetriBERT RoBERTa RoBERTa-PreLayerNorm RoCBert RoFormer RWKV Splinter SqueezeBERT StableLm Starcoder2 SwitchTransformers T5 T5v1.1 TAPEX Transformer XL UL2 UMT5 X-MOD XGLM XLM XLM-ProphetNet XLM-RoBERTa XLM-RoBERTa-XL XLM-V XLNet YOSO',\n",
       " 'Vision models': 'BEiT BiT Conditional DETR ConvNeXT ConvNeXTV2 CvT Deformable DETR DeiT Depth Anything DETA DETR DiNAT DINOV2 DiT DPT EfficientFormer EfficientNet FocalNet GLPN ImageGPT LeViT Mask2Former MaskFormer MobileNetV1 MobileNetV2 MobileViT MobileViTV2 NAT PoolFormer Pyramid Vision Transformer (PVT) Pyramid Vision Transformer v2 (PVTv2) RegNet ResNet SegFormer SegGpt SuperPoint SwiftFormer Swin Transformer Swin Transformer V2 Swin2SR Table Transformer UperNet VAN Vision Transformer (ViT) ViT Hybrid ViTDet ViTMAE ViTMatte ViTMSN YOLOS',\n",
       " 'Audio models': 'Audio Spectrogram Transformer Bark CLAP EnCodec Hubert MCTCT MMS MusicGen MusicGen Melody Pop2Piano Seamless-M4T SeamlessM4T-v2 SEW SEW-D Speech2Text Speech2Text2 SpeechT5 UniSpeech UniSpeech-SAT UnivNet VITS Wav2Vec2 Wav2Vec2-BERT Wav2Vec2-Conformer Wav2Vec2Phoneme WavLM Whisper XLS-R XLSR-Wav2Vec2',\n",
       " 'Video models': 'TimeSformer VideoMAE ViViT',\n",
       " 'Multimodal models': 'ALIGN AltCLIP BLIP BLIP-2 BridgeTower BROS Chinese-CLIP CLIP CLIPSeg CLVP Data2Vec DePlot Donut FLAVA GIT Grounding DINO GroupViT IDEFICS Idefics2 InstructBLIP KOSMOS-2 LayoutLM LayoutLMV2 LayoutLMV3 LayoutXLM LiLT Llava LLaVA-NeXT LXMERT MatCha MGP-STR Nougat OneFormer OWL-ViT OWLv2 PaliGemma Perceiver Pix2Struct Segment Anything SigLIP Speech Encoder Decoder Models TAPAS TrOCR TVLT TVP UDOP VideoLlava ViLT VipLlava Vision Encoder Decoder Models Vision Text Dual Encoder VisualBERT X-CLIP',\n",
       " 'Reinforcement learning models': 'Decision Transformer Trajectory Transformer',\n",
       " 'Time series models': 'Autoformer Informer PatchTSMixer PatchTST Time Series Transformer',\n",
       " 'Graph models': 'Graphormer'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_dict = {\"Main Classes\":[],\"Models\":{k:[] for k in model_dict},\"Internal Helpers\":[]}\n",
    "\n",
    "model_dict_vals = list(model_dict.values())\n",
    "base_url = \"https://huggingface.co\"\n",
    "for elems in elem_bs4.find_all('a'):\n",
    "    id_ = elems['id']\n",
    "    if \"main_classes\" in id_ or id_ == \"model_doc/auto\":\n",
    "        links_dict[\"Main Classes\"].append(base_url+elems['href'])\n",
    "    elif \"model_doc\" in id_:   \n",
    "        text_val = elems.text.strip()\n",
    "        for model_type,model_examples in model_dict.items():\n",
    "            if text_val in model_examples:\n",
    "                links_dict[\"Models\"][model_type].append(base_url+elems['href'])\n",
    "                break\n",
    "    elif \"internal\" in id_:\n",
    "        links_dict[\"Internal Helpers\"].append(base_url+elems['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
