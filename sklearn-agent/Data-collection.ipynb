{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "api_page = requests.get(\"https://scikit-learn.org/stable/modules/classes.html\")\n",
    "soup = BeautifulSoup(api_page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_elements = soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = []\n",
    "for i in range(len(h2_elements) - 1):\n",
    "    section = []\n",
    "    for sibling in h2_elements[i].next_siblings:\n",
    "        if sibling == h2_elements[i + 1]:\n",
    "            break\n",
    "        section.append(str(sibling))\n",
    "    section_text = ''.join(section)\n",
    "    sections.append(BeautifulSoup(section_text, 'lxml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(\"\\xa0\",\"\",text)\n",
    "    text = re.sub(\"¶\",\"\",text)\n",
    "    text = re.sub(\"\\n\",\"\",text)\n",
    "    return text.strip()\n",
    "def get_links(*,sub_section_elem,class_name,base_func_url,title):\n",
    "    curr_urls = []\n",
    "    try:\n",
    "        func_urls = sub_section_elem.find_all(attrs={\"class\": class_name})\n",
    "        for curr_url in func_urls:\n",
    "            try:\n",
    "                func_url = curr_url.find(\"a\")[\"href\"]\n",
    "                func_text = curr_url.text\n",
    "                if func_text.find(\")\") == -1:\n",
    "                    separator = \"\\n\"\n",
    "                else:\n",
    "                    separator = \")\"\n",
    "                func_name,func_desc = func_text[:func_text.find(separator)+1].strip(\"\\n\"),func_text[func_text.find(separator)+1:].strip(\"\\n\")\n",
    "                # curr_urls.append(base_func_url + func_url)\n",
    "                curr_urls.append({\"name\":func_name,\"description\":func_desc, \"url\":base_func_url + func_url})\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(title.text)\n",
    "        return curr_urls\n",
    "    except Exception as e:\n",
    "        curr_url = sub_section_elem.find(attrs={\"class\": class_name})\n",
    "        func_url = curr_url.find('a')[\"href\"]\n",
    "        func_text = curr_url.text\n",
    "        if func_text.find(\")\") == -1:\n",
    "            separator = \"\\n\"\n",
    "        else:\n",
    "            separator = \")\"\n",
    "        func_name,func_desc = func_text[:func_text.find(separator)+1].strip(\"\\n\"),func_text[func_text.find(separator)+1:].strip(\"\\n\")\n",
    "        curr_urls.append({\"name\":clean_text(func_name),\"description\":func_desc, \"url\":base_func_url + func_url})\n",
    "        return curr_urls\n",
    "    finally:\n",
    "        return curr_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_level_dict(h3_titles_list,base_sklearn_url):\n",
    "    sub_level_dict = {}\n",
    "    for idx,h3_title in enumerate(h3_titles_list):\n",
    "        h3_title_text = clean_text(h3_title.text)\n",
    "        title_siblings = []\n",
    "        # If it is not the last element\n",
    "        if idx != len(h3_titles_list)-1:\n",
    "            for title_functions_siblings in h3_title.next_siblings:\n",
    "                if title_functions_siblings == h3_titles_list[idx+1]: break\n",
    "                title_siblings.append(str(title_functions_siblings))\n",
    "            title_siblings_text = ''.join(title_siblings)\n",
    "            title_siblings_soup = BeautifulSoup(title_siblings_text, 'lxml')\n",
    "        else:\n",
    "            for title_functions_siblings in h3_title.next_siblings:\n",
    "                title_siblings.append(str(title_functions_siblings))\n",
    "            title_siblings_text = ''.join(title_siblings)\n",
    "            title_siblings_soup = BeautifulSoup(title_siblings_text, 'lxml')\n",
    "        odd_urls = get_links(sub_section_elem=title_siblings_soup,class_name=\"row-odd\",base_func_url=base_sklearn_url,title=h3_title)\n",
    "        even_urls = get_links(sub_section_elem=title_siblings_soup,class_name=\"row-even\",base_func_url=base_sklearn_url,title=h3_title)\n",
    "        all_urls = odd_urls + even_urls\n",
    "        sub_level_dict.update({h3_title_text:{'functions':all_urls}})\n",
    "    return sub_level_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# parent_dict = {}\n",
    "# pbar = tqdm(total=len(h2_elements[:-1]),desc=\"Scraping Sklearn\")\n",
    "# for sub_section_h2,sub_section in zip(h2_elements[:-1],sections):\n",
    "#     base_sklearn_url = \"https://scikit-learn.org/stable/modules/\"\n",
    "#     base_parent_url = \"https://scikit-learn.org/stable/modules/classes.html\"\n",
    "#     # sub_section = sections[1]\n",
    "#     # sub_section_h2 = h2_elements[1]\n",
    "#     parent_class_href = sub_section_h2.find('a')['href']\n",
    "#     parent_name = clean_text(sub_section_h2.text)\n",
    "#     parent_function = parent_name.split(\":\")[0]\n",
    "#     parent_name = parent_name.split(\":\")[1].strip()\n",
    "#     parent_text = \" \".join([para.text for para in sub_section.find_all('p')])\n",
    "#     if \"h3\" in str(sub_section):\n",
    "#         try:\n",
    "#             h3_titles_list = sub_section.find_all('h3')\n",
    "#             sub_level_dict = get_sub_level_dict(h3_titles_list,base_sklearn_url)\n",
    "#         except:\n",
    "#             h3_titles_list = [sub_section.find('h3')]\n",
    "#         sub_level_dict = get_sub_level_dict(h3_titles_list,base_sklearn_url)\n",
    "#         for sub_level,vals in sub_level_dict.items():\n",
    "#             if vals['functions'] == []:\n",
    "#                 odd_urls = get_links(sub_section_elem=sub_section,class_name=\"row-odd\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "#                 even_urls = get_links(sub_section_elem=sub_section,class_name=\"row-even\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "#                 all_urls = odd_urls + even_urls\n",
    "#                 sub_level_dict = {parent_name:{'functions':all_urls}}\n",
    "#     elif \"h3\" not in str(sub_section):\n",
    "#         odd_urls = get_links(sub_section_elem=sub_section,class_name=\"row-odd\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "#         even_urls = get_links(sub_section_elem=sub_section,class_name=\"row-even\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "#         all_urls = odd_urls + even_urls\n",
    "#         sub_level_dict = {parent_name:{'functions':all_urls}}\n",
    "#     parent_dict.update({parent_name:{\"functions\":parent_function,\"url\":base_parent_url+parent_class_href,\"sub_level_dict\":sub_level_dict,\"parent_text\":parent_text}})\n",
    "#     pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Sklearn:  59%|█████▉    | 23/39 [00:00<00:00, 190.78it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "parent_dict = {}\n",
    "pbar = tqdm(total=len(h2_elements[:-1]),desc=\"Scraping Sklearn\")\n",
    "for sub_section_h2,sub_section in zip(h2_elements[:-1],sections):\n",
    "    base_sklearn_url = \"https://scikit-learn.org/stable/modules/\"\n",
    "    base_parent_url = \"https://scikit-learn.org/stable/modules/classes.html\"\n",
    "    # sub_section = sections[1]\n",
    "    # sub_section_h2 = h2_elements[1]\n",
    "    parent_class_href = sub_section_h2.find('a')['href']\n",
    "    parent_name = clean_text(sub_section_h2.text)\n",
    "    parent_function = clean_text(parent_name.split(\":\")[0])\n",
    "    parent_name = parent_name.split(\":\")[1].strip()\n",
    "    parent_text = \" \".join([para.text for para in sub_section.find_all('p')])\n",
    "    try:\n",
    "        default_funcs = []\n",
    "        default_vals_list = []\n",
    "        defaul_vals = sub_section.find_all(class_=\"autosummary longtable docutils align-default\")\n",
    "        for df in defaul_vals:\n",
    "            ourl = get_links(sub_section_elem=sub_section,class_name=\"row-odd\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "            eurl = get_links(sub_section_elem=sub_section,class_name=\"row-even\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "            default_funcs.extend(ourl + eurl)\n",
    "        parent_dict.update({parent_name:{\"base_function\":parent_function,\"url\":base_parent_url+parent_class_href,\"functions\":default_funcs,\"parent_text\":parent_text}})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if \"h3\" in str(sub_section):\n",
    "        try:\n",
    "            h3_titles_list = sub_section.find_all('h3')\n",
    "        except:\n",
    "            h3_titles_list = [sub_section.find('h3')]\n",
    "        sub_level_dict = get_sub_level_dict(h3_titles_list,base_sklearn_url)\n",
    "        parent_dict[parent_name].update({\"sub_level_dict\":sub_level_dict})\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utils.as_float_array(X,*[,copy,...])'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('utils.as_float_array(X,\\xa0*[,\\xa0copy,\\xa0...])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdownify import MarkdownConverter\n",
    "\n",
    "# Create shorthand method for conversion\n",
    "def md(soup, **options):\n",
    "    return MarkdownConverter(**options).convert_soup(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Sklearn: 100%|██████████| 39/39 [00:20<00:00, 190.78it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m parent_vals[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     42\u001b[0m     func_url \u001b[38;5;241m=\u001b[39m func[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 43\u001b[0m     web_page_res \u001b[38;5;241m=\u001b[39m \u001b[43mget_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m web_page_res\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     45\u001b[0m         func\u001b[38;5;241m.\u001b[39mupdate({k:v})\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mget_py_obj\u001b[0;34m(base_func_url)\u001b[0m\n\u001b[1;32m     14\u001b[0m func_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(func_url\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m func_name \u001b[38;5;241m=\u001b[39m clean_text(func_soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m---> 16\u001b[0m func_signature \u001b[38;5;241m=\u001b[39m clean_text(\u001b[43mfunc_soup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msig sig-object py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m)\n\u001b[1;32m     17\u001b[0m all_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func_soup\u001b[38;5;241m.\u001b[39mfind(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpy class\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize_newlines(paragraph):\n",
    "    normalized_paragraph = re.sub(r'\\n+', '\\n\\n', paragraph)\n",
    "    return normalized_paragraph\n",
    "def remove_links(soup):\n",
    "        links = soup.find_all('a')\n",
    "        for link in links:\n",
    "            link.decompose()\n",
    "        \n",
    "        return soup\n",
    "def get_py_obj(base_func_url):\n",
    "    func_url = requests.get(base_func_url)\n",
    "    func_soup = BeautifulSoup(func_url.content, 'lxml')\n",
    "    func_name = clean_text(func_soup.find('h1').text)\n",
    "    func_signature = clean_text(func_soup.find(class_=\"sig sig-object py\").text)\n",
    "    all_urls = []\n",
    "    if func_soup.find(class_=\"py class\"):\n",
    "        class_or_fn = \"py class\"\n",
    "        type = \"class\"\n",
    "    elif func_soup.find(class_=\"py function\"):\n",
    "        class_or_fn = \"py function\"\n",
    "        type = \"function\"\n",
    "    py_soup = func_soup.find(class_=class_or_fn)\n",
    "    for url in py_soup.find_all('a'):\n",
    "        url = url['href']\n",
    "        if url is None: continue\n",
    "        elif url.startswith(\"#\"):\n",
    "            all_urls.append(base_func_url + url)\n",
    "        elif url.startswith(\"..\"):\n",
    "            continue\n",
    "        elif url.startswith(\"http\"):\n",
    "            all_urls.append(url)\n",
    "    py_soup = remove_links(py_soup)\n",
    "    py_md = normalize_newlines(md(py_soup))\n",
    "    return {\"func_name\":func_name,\"func_signature\":func_signature,\"func_md\":py_md,\"type\":type}\n",
    "\n",
    "pbar_ = tqdm(total=len(list(parent_dict.keys())))\n",
    "for parent_name,parent_vals in parent_dict.items():\n",
    "    if 'functions' in parent_vals:\n",
    "        for func in parent_vals['functions']:\n",
    "            func_url = func['url']\n",
    "            web_page_res = get_py_obj(func_url)\n",
    "            for k,v in web_page_res.items():\n",
    "                func.update({k:v})\n",
    "    if 'sub_level_dict' in parent_vals:\n",
    "        for sub_level_name,sub_level_vals in parent_vals['sub_level_dict'].items():\n",
    "            if 'functions' in sub_level_vals:\n",
    "                for func in sub_level_vals['functions']:\n",
    "                    func_url = func['url']\n",
    "                    web_page_res = get_py_obj(func_url)\n",
    "                    for k,v in web_page_res.items():\n",
    "                        func.update({k:v})\n",
    "    pbar_.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('sklearn.json', 'w') as f:\n",
    "    json.dump(parent_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "None.get('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
