{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "api_page = requests.get(\"https://scikit-learn.org/1.4/modules/classes.html\")\n",
    "soup = BeautifulSoup(api_page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_elements = soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = []\n",
    "for i in range(len(h2_elements) - 1):\n",
    "    section = []\n",
    "    for sibling in h2_elements[i].next_siblings:\n",
    "        if sibling == h2_elements[i + 1]:\n",
    "            break\n",
    "        section.append(str(sibling))\n",
    "    section_text = ''.join(section)\n",
    "    sections.append(BeautifulSoup(section_text, 'lxml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(\"\\xa0\",\"\",text)\n",
    "    text = re.sub(\"¶\",\"\",text)\n",
    "    text = re.sub(\"\\n\",\"\",text)\n",
    "    return text.strip()\n",
    "def get_links(*,sub_section_elem,class_name,base_func_url,title):\n",
    "    curr_urls = []\n",
    "    try:\n",
    "        func_urls = sub_section_elem.find_all(attrs={\"class\": class_name})\n",
    "        for curr_url in func_urls:\n",
    "            try:\n",
    "                func_url = curr_url.find(\"a\")[\"href\"]\n",
    "                func_text = curr_url.text\n",
    "                if func_text.find(\")\") == -1:\n",
    "                    separator = \"\\n\"\n",
    "                else:\n",
    "                    separator = \")\"\n",
    "                func_name,func_desc = func_text[:func_text.find(separator)+1].strip(\"\\n\"),func_text[func_text.find(separator)+1:].strip(\"\\n\")\n",
    "                # curr_urls.append(base_func_url + func_url)\n",
    "                curr_urls.append({\"name\":func_name,\"description\":func_desc, \"url\":base_func_url + func_url})\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(title.text)\n",
    "        return curr_urls\n",
    "    except Exception as e:\n",
    "        curr_url = sub_section_elem.find(attrs={\"class\": class_name})\n",
    "        func_url = curr_url.find('a')[\"href\"]\n",
    "        func_text = curr_url.text\n",
    "        if func_text.find(\")\") == -1:\n",
    "            separator = \"\\n\"\n",
    "        else:\n",
    "            separator = \")\"\n",
    "        func_name,func_desc = func_text[:func_text.find(separator)+1].strip(\"\\n\"),func_text[func_text.find(separator)+1:].strip(\"\\n\")\n",
    "        curr_urls.append({\"name\":clean_text(func_name),\"description\":func_desc, \"url\":base_func_url + func_url})\n",
    "        return curr_urls\n",
    "    finally:\n",
    "        return curr_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_level_dict(h3_titles_list,base_sklearn_url):\n",
    "    sub_level_dict = {}\n",
    "    for idx,h3_title in enumerate(h3_titles_list):\n",
    "        h3_title_text = clean_text(h3_title.text)\n",
    "        title_siblings = []\n",
    "        # If it is not the last element\n",
    "        if idx != len(h3_titles_list)-1:\n",
    "            for title_functions_siblings in h3_title.next_siblings:\n",
    "                if title_functions_siblings == h3_titles_list[idx+1]: break\n",
    "                title_siblings.append(str(title_functions_siblings))\n",
    "            title_siblings_text = ''.join(title_siblings)\n",
    "            title_siblings_soup = BeautifulSoup(title_siblings_text, 'lxml')\n",
    "        else:\n",
    "            for title_functions_siblings in h3_title.next_siblings:\n",
    "                title_siblings.append(str(title_functions_siblings))\n",
    "            title_siblings_text = ''.join(title_siblings)\n",
    "            title_siblings_soup = BeautifulSoup(title_siblings_text, 'lxml')\n",
    "        odd_urls = get_links(sub_section_elem=title_siblings_soup,class_name=\"row-odd\",base_func_url=base_sklearn_url,title=h3_title)\n",
    "        even_urls = get_links(sub_section_elem=title_siblings_soup,class_name=\"row-even\",base_func_url=base_sklearn_url,title=h3_title)\n",
    "        all_urls = odd_urls + even_urls\n",
    "        sub_level_dict.update({h3_title_text:{'functions':all_urls}})\n",
    "    return sub_level_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# parent_dict = {}\n",
    "# pbar = tqdm(total=len(h2_elements[:-1]),desc=\"Scraping Sklearn\")\n",
    "# for sub_section_h2,sub_section in zip(h2_elements[:-1],sections):\n",
    "#     base_sklearn_url = \"https://scikit-learn.org/stable/modules/\"\n",
    "#     base_parent_url = \"https://scikit-learn.org/stable/modules/classes.html\"\n",
    "#     # sub_section = sections[1]\n",
    "#     # sub_section_h2 = h2_elements[1]\n",
    "#     parent_class_href = sub_section_h2.find('a')['href']\n",
    "#     parent_name = clean_text(sub_section_h2.text)\n",
    "#     parent_function = parent_name.split(\":\")[0]\n",
    "#     parent_name = parent_name.split(\":\")[1].strip()\n",
    "#     parent_text = \" \".join([para.text for para in sub_section.find_all('p')])\n",
    "#     if \"h3\" in str(sub_section):\n",
    "#         try:\n",
    "#             h3_titles_list = sub_section.find_all('h3')\n",
    "#             sub_level_dict = get_sub_level_dict(h3_titles_list,base_sklearn_url)\n",
    "#         except:\n",
    "#             h3_titles_list = [sub_section.find('h3')]\n",
    "#         sub_level_dict = get_sub_level_dict(h3_titles_list,base_sklearn_url)\n",
    "#         for sub_level,vals in sub_level_dict.items():\n",
    "#             if vals['functions'] == []:\n",
    "#                 odd_urls = get_links(sub_section_elem=sub_section,class_name=\"row-odd\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "#                 even_urls = get_links(sub_section_elem=sub_section,class_name=\"row-even\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "#                 all_urls = odd_urls + even_urls\n",
    "#                 sub_level_dict = {parent_name:{'functions':all_urls}}\n",
    "#     elif \"h3\" not in str(sub_section):\n",
    "#         odd_urls = get_links(sub_section_elem=sub_section,class_name=\"row-odd\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "#         even_urls = get_links(sub_section_elem=sub_section,class_name=\"row-even\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "#         all_urls = odd_urls + even_urls\n",
    "#         sub_level_dict = {parent_name:{'functions':all_urls}}\n",
    "#     parent_dict.update({parent_name:{\"functions\":parent_function,\"url\":base_parent_url+parent_class_href,\"sub_level_dict\":sub_level_dict,\"parent_text\":parent_text}})\n",
    "#     pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Sklearn:   0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Sklearn:  59%|█████▉    | 23/39 [00:00<00:00, 177.24it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "parent_dict = {}\n",
    "pbar = tqdm(total=len(h2_elements[:-1]),desc=\"Scraping Sklearn\")\n",
    "for sub_section_h2,sub_section in zip(h2_elements[:-1],sections):\n",
    "    base_sklearn_url = \"https://scikit-learn.org/stable/modules/\"\n",
    "    base_parent_url = \"https://scikit-learn.org/stable/modules/classes.html\"\n",
    "    # sub_section = sections[1]\n",
    "    # sub_section_h2 = h2_elements[1]\n",
    "    parent_class_href = sub_section_h2.find('a')['href']\n",
    "    parent_name = clean_text(sub_section_h2.text)\n",
    "    parent_function = clean_text(parent_name.split(\":\")[0])\n",
    "    parent_name = parent_name.split(\":\")[1].strip()\n",
    "    parent_text = \" \".join([para.text for para in sub_section.find_all('p')])\n",
    "    try:\n",
    "        default_funcs = []\n",
    "        default_vals_list = []\n",
    "        defaul_vals = sub_section.find_all(class_=\"autosummary longtable docutils align-default\")\n",
    "        for df in defaul_vals:\n",
    "            ourl = get_links(sub_section_elem=sub_section,class_name=\"row-odd\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "            eurl = get_links(sub_section_elem=sub_section,class_name=\"row-even\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "            default_funcs.extend(ourl + eurl)\n",
    "        parent_dict.update({parent_name:{\"base_function\":parent_function,\"url\":base_parent_url+parent_class_href,\"functions\":default_funcs,\"parent_text\":parent_text}})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if \"h3\" in str(sub_section):\n",
    "        try:\n",
    "            h3_titles_list = sub_section.find_all('h3')\n",
    "        except:\n",
    "            h3_titles_list = [sub_section.find('h3')]\n",
    "        sub_level_dict = get_sub_level_dict(h3_titles_list,base_sklearn_url)\n",
    "        parent_dict[parent_name].update({\"sub_level_dict\":sub_level_dict})\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Sklearn: 100%|██████████| 39/39 [00:19<00:00, 177.24it/s]"
     ]
    }
   ],
   "source": [
    "from markdownify import MarkdownConverter\n",
    "\n",
    "# Create shorthand method for conversion\n",
    "def md(soup, **options):\n",
    "    return MarkdownConverter(**options).convert_soup(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'previous_siblings' Settings and information tools\n",
      "'NoneType' object has no attribute 'previous_siblings' Base classes and utility functions\n",
      "'NoneType' object has no attribute 'previous_siblings' Base classes and utility functions\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "def normalize_newlines(paragraph):\n",
    "    normalized_paragraph = re.sub(r'\\n+', '\\n\\n', paragraph)\n",
    "    return normalized_paragraph\n",
    "def remove_links(soup):\n",
    "        links = soup.find_all('a')\n",
    "        for link in links:\n",
    "            link.decompose()\n",
    "        \n",
    "        return soup\n",
    "def get_py_obj(base_func_url,parent_name):\n",
    "    func_url = requests.get(base_func_url)\n",
    "    func_soup = BeautifulSoup(func_url.content, 'lxml')\n",
    "    func_name = clean_text(func_soup.find('h1').text)\n",
    "    func_signature_elem = func_soup.find(class_=\"sig sig-object py\")\n",
    "    if func_signature_elem is not None:\n",
    "        func_signature = clean_text(func_signature_elem.text)\n",
    "    else:\n",
    "        func_signature = \"\"\n",
    "    all_urls = []\n",
    "    page_dict = {\"func_name\":func_name,\"func_signature\":func_signature}\n",
    "    try:\n",
    "        if func_soup.find(class_=\"py class\"):\n",
    "            class_or_fn = \"py class\"\n",
    "            type = \"class\"\n",
    "        elif func_soup.find(class_=\"py function\"):\n",
    "            class_or_fn = \"py function\"\n",
    "            type = \"function\"\n",
    "        py_soup = func_soup.find(class_=class_or_fn)\n",
    "        func_text_list = []\n",
    "        dd = py_soup.find('dd')\n",
    "        field_list = dd.find(class_=\"field-list\")\n",
    "\n",
    "        for i in field_list.previous_siblings:\n",
    "            func_text_list.append(i.text)\n",
    "        func_text = \"\".join(func_text_list[::-1]).replace(\"\\n\",\" \").strip()\n",
    "        page_dict.update({\"func_text\":func_text})\n",
    "        for url in py_soup.find_all('a'):\n",
    "            url = url['href']\n",
    "            if url is None: continue\n",
    "            elif url.startswith(\"#\"):\n",
    "                all_urls.append(base_func_url + url)\n",
    "            elif url.startswith(\"..\"):\n",
    "                continue\n",
    "            elif url.startswith(\"http\"):\n",
    "                all_urls.append(url)\n",
    "        py_soup = remove_links(py_soup)\n",
    "        py_md = normalize_newlines(md(py_soup))\n",
    "        page_dict.update({\"func_md\":py_md,\"type\":type})\n",
    "        fodd = field_list.find_all(class_=\"field-odd\")\n",
    "        if fodd[0].text == \"Parameters\":\n",
    "            dts = fodd[1].find_all('dt')\n",
    "            paremter_names_desc = {}\n",
    "\n",
    "            for idx,dt in enumerate(dts):\n",
    "                param_name = dt.find('strong').text\n",
    "                param_type = dt.find(class_=\"classifier\").text\n",
    "                param_desc = \"\"\n",
    "                for next_sib in dt.next_siblings:\n",
    "                    if idx == len(dts)-1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        if next_sib == dts[idx+1]:\n",
    "                            break\n",
    "                    next_sib = str(next_sib)\n",
    "                    param_desc += next_sib[next_sib.find(\"<p>\"):next_sib.find(\"</p>\")]\n",
    "                paremter_names_desc.update({param_name:{\"param_type\":param_type,\"params_desc\":param_desc}})\n",
    "            page_dict.update({\"paremter_names_desc\":paremter_names_desc})\n",
    "    except Exception as e:\n",
    "        print(e,parent_name)\n",
    "        return page_dict\n",
    "\n",
    "pbar_ = tqdm(total=len(list(parent_dict.keys())))\n",
    "for parent_name,parent_vals in parent_dict.items():\n",
    "    if 'functions' in parent_vals:\n",
    "        for func in parent_vals['functions']:\n",
    "            func_url = func['url']\n",
    "            web_page_res = get_py_obj(func_url,parent_name)\n",
    "            if web_page_res is not None:\n",
    "                for k,v in web_page_res.items():\n",
    "                    func.update({k:v})\n",
    "    if 'sub_level_dict' in parent_vals:\n",
    "        for sub_level_name,sub_level_vals in parent_vals['sub_level_dict'].items():\n",
    "            if 'functions' in sub_level_vals:\n",
    "                for func in sub_level_vals['functions']:\n",
    "                    func_url = func['url']\n",
    "                    web_page_res = get_py_obj(func_url,parent_name)\n",
    "                    if web_page_res is not None:\n",
    "                        for k,v in web_page_res.items():\n",
    "                            func.update({k:v})\n",
    "    pbar_.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('sklearn.json', 'w') as f:\n",
    "    json.dump(parent_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "# from copy import deepcopy\n",
    "\n",
    "# parent_dict_copy = deepcopy(parent_dict)\n",
    "# def main_scraper(parent_name):\n",
    "#     parent_vals = parent_dict_copy[parent_name]\n",
    "#     if 'functions' in parent_vals:\n",
    "#         for func in parent_vals['functions']:\n",
    "#             func_url = func['url']\n",
    "#             web_page_res = get_py_obj(func_url,parent_name)\n",
    "#             if web_page_res is not None:\n",
    "#                 for k,v in web_page_res.items():\n",
    "#                     func.update({k:v})\n",
    "#     if 'sub_level_dict' in parent_vals:\n",
    "#         for sub_level_name,sub_level_vals in parent_vals['sub_level_dict'].items():\n",
    "#             if 'functions' in sub_level_vals:\n",
    "#                 for func in sub_level_vals['functions']:\n",
    "#                     func_url = func['url']\n",
    "#                     web_page_res = get_py_obj(func_url)\n",
    "#                     if web_page_res is not None:\n",
    "#                         for k,v in web_page_res.items():\n",
    "#                             func.update({k:v})\n",
    "#     print(f\"Done for {parent_name}\")\n",
    "# if __name__ == \"__main__\":\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "#         parent_names_list = list(parent_dict.keys())\n",
    "#         results = executor.map(main_scraper, parent_names_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_func_url = \"https://scikit-learn.org/stable/modules/generated/sklearn.utils.as_float_array.html#sklearn.utils.as_float_array\"\n",
    "# func_url = requests.get(base_func_url)\n",
    "# func_soup = BeautifulSoup(func_url.content, 'lxml')\n",
    "# func_name = clean_text(func_soup.find('h1').text)\n",
    "# func_signature_elem = func_soup.find(class_=\"sig sig-object py\")\n",
    "# if func_signature_elem is not None:\n",
    "#     func_signature = clean_text(func_signature_elem.text)\n",
    "# else:\n",
    "#     func_signature = \"\"\n",
    "# all_urls = []\n",
    "\n",
    "# if func_soup.find(class_=\"py class\"):\n",
    "#     class_or_fn = \"py class\"\n",
    "#     type = \"class\"\n",
    "# elif func_soup.find(class_=\"py function\"):\n",
    "#     class_or_fn = \"py function\"\n",
    "#     type = \"function\"\n",
    "# py_soup = func_soup.find(class_=class_or_fn)\n",
    "# func_text_list = []\n",
    "# dd = py_soup.find('dd')\n",
    "# field_list = dd.find(class_=\"field-list\")\n",
    "\n",
    "# for i in field_list.previous_siblings:\n",
    "#     func_text_list.append(i.text)\n",
    "# func_text = \"\".join(func_text_list[::-1]).replace(\"\\n\",\" \").strip()\n",
    "# for url in py_soup.find_all('a'):\n",
    "#     url = url['href']\n",
    "#     if url is None: continue\n",
    "#     elif url.startswith(\"#\"):\n",
    "#         all_urls.append(base_func_url + url)\n",
    "#     elif url.startswith(\"..\"):\n",
    "#         continue\n",
    "#     elif url.startswith(\"http\"):\n",
    "#         all_urls.append(url)\n",
    "# fodd = field_list.find_all(class_=\"field-odd\")\n",
    "# dts = fodd[1].find_all('dt')\n",
    "# paremter_names_desc = {}\n",
    "\n",
    "# for idx,dt in enumerate(dts):\n",
    "#     param_name = dt.find('strong').text\n",
    "#     param_type = dt.find(class_=\"classifier\").text\n",
    "#     param_desc = \"\"\n",
    "#     for next_sib in dt.next_siblings:\n",
    "#         if idx == len(dts)-1:\n",
    "#             pass\n",
    "#         else:\n",
    "#             if next_sib == dts[idx+1]:\n",
    "#                 break\n",
    "#         next_sib = str(next_sib)\n",
    "#         param_desc += next_sib[next_sib.find(\"<p>\"):next_sib.find(\"</p>\")]\n",
    "#     paremter_names_desc.update({param_name:{\"param_type\":param_type,\"params_desc\":param_desc}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
