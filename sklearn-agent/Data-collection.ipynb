{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "api_page = requests.get(\"https://scikit-learn.org/1.4/modules/classes.html\")\n",
    "soup = BeautifulSoup(api_page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_elements = soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = []\n",
    "for i in range(len(h2_elements) - 1):\n",
    "    section = []\n",
    "    for sibling in h2_elements[i].next_siblings:\n",
    "        if sibling == h2_elements[i + 1]:\n",
    "            break\n",
    "        section.append(str(sibling))\n",
    "    section_text = ''.join(section)\n",
    "    sections.append(BeautifulSoup(section_text, 'lxml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(\"\\xa0\",\"\",text)\n",
    "    text = re.sub(\"¶\",\"\",text)\n",
    "    text = re.sub(\"\\n\",\"\",text)\n",
    "    return text.strip()\n",
    "def get_links(*,sub_section_elem,class_name,base_func_url,title):\n",
    "    curr_urls = []\n",
    "    try:\n",
    "        func_urls = sub_section_elem.find_all(attrs={\"class\": class_name})\n",
    "        for curr_url in func_urls:\n",
    "            try:\n",
    "                func_url = curr_url.find(\"a\")[\"href\"]\n",
    "                func_text = curr_url.text\n",
    "                if func_text.find(\")\") == -1:\n",
    "                    separator = \"\\n\"\n",
    "                else:\n",
    "                    separator = \")\"\n",
    "                func_name,func_desc = func_text[:func_text.find(separator)+1].strip(\"\\n\"),func_text[func_text.find(separator)+1:].strip(\"\\n\")\n",
    "                # curr_urls.append(base_func_url + func_url)\n",
    "                curr_urls.append({\"name\":func_name,\"description\":func_desc, \"url\":base_func_url + func_url})\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(title.text)\n",
    "        return curr_urls\n",
    "    except Exception as e:\n",
    "        curr_url = sub_section_elem.find(attrs={\"class\": class_name})\n",
    "        func_url = curr_url.find('a')[\"href\"]\n",
    "        func_text = curr_url.text\n",
    "        if func_text.find(\")\") == -1:\n",
    "            separator = \"\\n\"\n",
    "        else:\n",
    "            separator = \")\"\n",
    "        func_name,func_desc = func_text[:func_text.find(separator)+1].strip(\"\\n\"),func_text[func_text.find(separator)+1:].strip(\"\\n\")\n",
    "        curr_urls.append({\"name\":clean_text(func_name),\"description\":func_desc, \"url\":base_func_url + func_url})\n",
    "        return curr_urls\n",
    "    finally:\n",
    "        return curr_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_level_dict(h3_titles_list,base_sklearn_url):\n",
    "    sub_level_dict = {}\n",
    "    for idx,h3_title in enumerate(h3_titles_list):\n",
    "        h3_title_text = clean_text(h3_title.text)\n",
    "        title_siblings = []\n",
    "        # If it is not the last element\n",
    "        if idx != len(h3_titles_list)-1:\n",
    "            for title_functions_siblings in h3_title.next_siblings:\n",
    "                if title_functions_siblings == h3_titles_list[idx+1]: break\n",
    "                title_siblings.append(str(title_functions_siblings))\n",
    "            title_siblings_text = ''.join(title_siblings)\n",
    "            title_siblings_soup = BeautifulSoup(title_siblings_text, 'lxml')\n",
    "        else:\n",
    "            for title_functions_siblings in h3_title.next_siblings:\n",
    "                title_siblings.append(str(title_functions_siblings))\n",
    "            title_siblings_text = ''.join(title_siblings)\n",
    "            title_siblings_soup = BeautifulSoup(title_siblings_text, 'lxml')\n",
    "        odd_urls = get_links(sub_section_elem=title_siblings_soup,class_name=\"row-odd\",base_func_url=base_sklearn_url,title=h3_title)\n",
    "        even_urls = get_links(sub_section_elem=title_siblings_soup,class_name=\"row-even\",base_func_url=base_sklearn_url,title=h3_title)\n",
    "        all_urls = odd_urls + even_urls\n",
    "        sub_level_dict.update({h3_title_text:{'functions':all_urls}})\n",
    "    return sub_level_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# parent_dict = {}\n",
    "# pbar = tqdm(total=len(h2_elements[:-1]),desc=\"Scraping Sklearn\")\n",
    "# for sub_section_h2,sub_section in zip(h2_elements[:-1],sections):\n",
    "#     base_sklearn_url = \"https://scikit-learn.org/stable/modules/\"\n",
    "#     base_parent_url = \"https://scikit-learn.org/stable/modules/classes.html\"\n",
    "#     # sub_section = sections[1]\n",
    "#     # sub_section_h2 = h2_elements[1]\n",
    "#     parent_class_href = sub_section_h2.find('a')['href']\n",
    "#     parent_name = clean_text(sub_section_h2.text)\n",
    "#     parent_function = parent_name.split(\":\")[0]\n",
    "#     parent_name = parent_name.split(\":\")[1].strip()\n",
    "#     parent_text = \" \".join([para.text for para in sub_section.find_all('p')])\n",
    "#     if \"h3\" in str(sub_section):\n",
    "#         try:\n",
    "#             h3_titles_list = sub_section.find_all('h3')\n",
    "#             sub_level_dict = get_sub_level_dict(h3_titles_list,base_sklearn_url)\n",
    "#         except:\n",
    "#             h3_titles_list = [sub_section.find('h3')]\n",
    "#         sub_level_dict = get_sub_level_dict(h3_titles_list,base_sklearn_url)\n",
    "#         for sub_level,vals in sub_level_dict.items():\n",
    "#             if vals['functions'] == []:\n",
    "#                 odd_urls = get_links(sub_section_elem=sub_section,class_name=\"row-odd\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "#                 even_urls = get_links(sub_section_elem=sub_section,class_name=\"row-even\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "#                 all_urls = odd_urls + even_urls\n",
    "#                 sub_level_dict = {parent_name:{'functions':all_urls}}\n",
    "#     elif \"h3\" not in str(sub_section):\n",
    "#         odd_urls = get_links(sub_section_elem=sub_section,class_name=\"row-odd\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "#         even_urls = get_links(sub_section_elem=sub_section,class_name=\"row-even\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "#         all_urls = odd_urls + even_urls\n",
    "#         sub_level_dict = {parent_name:{'functions':all_urls}}\n",
    "#     parent_dict.update({parent_name:{\"functions\":parent_function,\"url\":base_parent_url+parent_class_href,\"sub_level_dict\":sub_level_dict,\"parent_text\":parent_text}})\n",
    "#     pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Sklearn:   0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Sklearn:  59%|█████▉    | 23/39 [00:00<00:00, 179.54it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "parent_dict = {}\n",
    "pbar = tqdm(total=len(h2_elements[:-1]),desc=\"Scraping Sklearn\")\n",
    "for sub_section_h2,sub_section in zip(h2_elements[:-1],sections):\n",
    "    base_sklearn_url = \"https://scikit-learn.org/stable/modules/\"\n",
    "    base_parent_url = \"https://scikit-learn.org/stable/modules/classes.html\"\n",
    "    # sub_section = sections[1]\n",
    "    # sub_section_h2 = h2_elements[1]\n",
    "    parent_class_href = sub_section_h2.find('a')['href']\n",
    "    parent_name = clean_text(sub_section_h2.text)\n",
    "    parent_function = clean_text(parent_name.split(\":\")[0])\n",
    "    parent_name = parent_name.split(\":\")[1].strip()\n",
    "    parent_text = \" \".join([para.text for para in sub_section.find_all('p')])\n",
    "    try:\n",
    "        default_funcs = []\n",
    "        default_vals_list = []\n",
    "        defaul_vals = sub_section.find_all(class_=\"autosummary longtable docutils align-default\")\n",
    "        for df in defaul_vals:\n",
    "            ourl = get_links(sub_section_elem=sub_section,class_name=\"row-odd\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "            eurl = get_links(sub_section_elem=sub_section,class_name=\"row-even\",base_func_url=base_sklearn_url,title=sub_section_h2)\n",
    "            default_funcs.extend(ourl + eurl)\n",
    "        parent_dict.update({parent_name:{\"base_function\":parent_function,\"url\":base_parent_url+parent_class_href,\"functions\":default_funcs,\"parent_text\":parent_text}})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if \"h3\" in str(sub_section):\n",
    "        try:\n",
    "            h3_titles_list = sub_section.find_all('h3')\n",
    "        except:\n",
    "            h3_titles_list = [sub_section.find('h3')]\n",
    "        sub_level_dict = get_sub_level_dict(h3_titles_list,base_sklearn_url)\n",
    "        parent_dict[parent_name].update({\"sub_level_dict\":sub_level_dict})\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdownify import MarkdownConverter\n",
    "\n",
    "# Create shorthand method for conversion\n",
    "def md(soup, **options):\n",
    "    return MarkdownConverter(**options).convert_soup(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'previous_siblings' Settings and information tools\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m parent_vals[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     79\u001b[0m     func_url \u001b[38;5;241m=\u001b[39m func[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 80\u001b[0m     web_page_res \u001b[38;5;241m=\u001b[39m \u001b[43mget_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparent_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m web_page_res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m web_page_res\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m, in \u001b[0;36mget_py_obj\u001b[0;34m(base_func_url, parent_name)\u001b[0m\n\u001b[1;32m     48\u001b[0m         all_urls\u001b[38;5;241m.\u001b[39mappend(url)\n\u001b[1;32m     49\u001b[0m py_soup \u001b[38;5;241m=\u001b[39m remove_links(py_soup)\n\u001b[0;32m---> 50\u001b[0m py_md \u001b[38;5;241m=\u001b[39m normalize_newlines(\u001b[43mmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpy_soup\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     51\u001b[0m page_dict\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc_md\u001b[39m\u001b[38;5;124m\"\u001b[39m:py_md,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mtype\u001b[39m})\n\u001b[1;32m     52\u001b[0m fodd \u001b[38;5;241m=\u001b[39m field_list\u001b[38;5;241m.\u001b[39mfind_all(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfield-odd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m, in \u001b[0;36mmd\u001b[0;34m(soup, **options)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmd\u001b[39m(soup, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMarkdownConverter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_soup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/markdownify/__init__.py:100\u001b[0m, in \u001b[0;36mMarkdownConverter.convert_soup\u001b[0;34m(self, soup)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_soup\u001b[39m(\u001b[38;5;28mself\u001b[39m, soup):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_as_inline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchildren_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/markdownify/__init__.py:143\u001b[0m, in \u001b[0;36mMarkdownConverter.process_tag\u001b[0;34m(self, node, convert_as_inline, children_only)\u001b[0m\n\u001b[1;32m    141\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_text(el)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_children_as_inline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m children_only:\n\u001b[1;32m    146\u001b[0m     convert_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvert_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m node\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/markdownify/__init__.py:143\u001b[0m, in \u001b[0;36mMarkdownConverter.process_tag\u001b[0;34m(self, node, convert_as_inline, children_only)\u001b[0m\n\u001b[1;32m    141\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_text(el)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_children_as_inline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m children_only:\n\u001b[1;32m    146\u001b[0m     convert_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvert_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m node\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/markdownify/__init__.py:141\u001b[0m, in \u001b[0;36mMarkdownConverter.process_tag\u001b[0;34m(self, node, convert_as_inline, children_only)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(el, NavigableString):\n\u001b[0;32m--> 141\u001b[0m     text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_tag(el, convert_children_as_inline)\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/markdownify/__init__.py:160\u001b[0m, in \u001b[0;36mMarkdownConverter.process_text\u001b[0;34m(self, el)\u001b[0m\n\u001b[1;32m    157\u001b[0m     text \u001b[38;5;241m=\u001b[39m whitespace_re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# escape special characters if we're not inside a preformatted or code element\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpre\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkbd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    161\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mescape(text)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# remove trailing whitespaces if any of the following condition is true:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# - current text node is the last node in li\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# - current text node is followed by an embedded list\u001b[39;00m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/bs4/element.py:739\u001b[0m, in \u001b[0;36mPageElement.find_parent\u001b[0;34m(self, name, attrs, **kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# NOTE: We can't use _find_one because findParents takes a different\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# set of arguments.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 739\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_parents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l:\n\u001b[1;32m    741\u001b[0m     r \u001b[38;5;241m=\u001b[39m l[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/bs4/element.py:760\u001b[0m, in \u001b[0;36mPageElement.find_parents\u001b[0;34m(self, name, attrs, limit, **kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find all parents of this PageElement that match the given criteria.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03mAll find_* methods take a common set of arguments. See the online\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;124;03m:rtype: bs4.element.Tag | bs4.element.NavigableString\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    759\u001b[0m _stacklevel \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_stacklevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 760\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/bs4/element.py:841\u001b[0m, in \u001b[0;36mPageElement._find_all\u001b[0;34m(self, name, attrs, string, limit, generator, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i:\n\u001b[0;32m--> 841\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[43mstrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[1;32m    843\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(found)\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/bs4/element.py:2325\u001b[0m, in \u001b[0;36mSoupStrainer.search\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(markup, Tag):\n\u001b[1;32m   2324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[0;32m-> 2325\u001b[0m         found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;66;03m# If it's text, make sure the text matches.\u001b[39;00m\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(markup, NavigableString) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m   2328\u001b[0m          \u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/bs4/element.py:2267\u001b[0m, in \u001b[0;36mSoupStrainer.search_tag\u001b[0;34m(self, markup_name, markup_attrs)\u001b[0m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markup \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m markup\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m markup\u001b[38;5;241m.\u001b[39mname:\n\u001b[1;32m   2264\u001b[0m          \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m call_function_with_tag_data \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 2267\u001b[0m     \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCallable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2268\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(markup_name, Tag))\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   2271\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m call_function_with_tag_data\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (markup \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_matches(markup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   2273\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m markup \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_matches(markup_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))):\n\u001b[1;32m   2274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m call_function_with_tag_data:\n",
      "File \u001b[0;32m/usr/lib/python3.10/abc.py:117\u001b[0m, in \u001b[0;36mABCMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Register a virtual subclass of an ABC.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    Returns the subclass, to allow usage as a class decorator.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _abc_register(\u001b[38;5;28mcls\u001b[39m, subclass)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, instance):\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _abc_instancecheck(\u001b[38;5;28mcls\u001b[39m, instance)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1758\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:9\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      6\u001b[0m _temp \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_stopped\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Python 3.x has this\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_thread_alive\u001b[39m(t):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Thread__stopped\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Python 2.x has this\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "def normalize_newlines(paragraph):\n",
    "    normalized_paragraph = re.sub(r'\\n+', '\\n\\n', paragraph)\n",
    "    return normalized_paragraph\n",
    "def remove_links(soup):\n",
    "        links = soup.find_all('a')\n",
    "        for link in links:\n",
    "            link.decompose()\n",
    "        \n",
    "        return soup\n",
    "def get_py_obj(base_func_url,parent_name):\n",
    "    func_url = requests.get(base_func_url)\n",
    "    func_soup = BeautifulSoup(func_url.content, 'lxml')\n",
    "    func_name = clean_text(func_soup.find('h1').text)\n",
    "    func_signature_elem = func_soup.find(class_=\"sig sig-object py\")\n",
    "    if func_signature_elem is not None:\n",
    "        func_signature = clean_text(func_signature_elem.text)\n",
    "    else:\n",
    "        func_signature = \"\"\n",
    "    all_urls = []\n",
    "    page_dict = {\"func_name\":func_name,\"func_signature\":func_signature}\n",
    "    try:\n",
    "        if func_soup.find(class_=\"py class\"):\n",
    "            class_or_fn = \"py class\"\n",
    "            type = \"class\"\n",
    "        elif func_soup.find(class_=\"py function\"):\n",
    "            class_or_fn = \"py function\"\n",
    "            type = \"function\"\n",
    "        py_soup = func_soup.find(class_=class_or_fn)\n",
    "        func_text_list = []\n",
    "        dd = py_soup.find('dd')\n",
    "        field_list = dd.find(class_=\"field-list\")\n",
    "\n",
    "        for i in field_list.previous_siblings:\n",
    "            func_text_list.append(i.text)\n",
    "        func_text = \"\".join(func_text_list[::-1]).replace(\"\\n\",\" \").strip()\n",
    "        page_dict.update({\"func_text\":func_text})\n",
    "        for url in py_soup.find_all('a'):\n",
    "            url = url['href']\n",
    "            if url is None: continue\n",
    "            elif url.startswith(\"#\"):\n",
    "                all_urls.append(base_func_url + url)\n",
    "            elif url.startswith(\"..\"):\n",
    "                continue\n",
    "            elif url.startswith(\"http\"):\n",
    "                all_urls.append(url)\n",
    "        py_soup = remove_links(py_soup)\n",
    "        py_md = normalize_newlines(md(py_soup))\n",
    "        page_dict.update({\"func_md\":py_md,\"type\":type})\n",
    "        fodd = field_list.find_all(class_=\"field-odd\")\n",
    "        if fodd[0].text == \"Parameters\":\n",
    "            dts = fodd[1].find_all('dt')\n",
    "            paremter_names_desc = {}\n",
    "\n",
    "            for idx,dt in enumerate(dts):\n",
    "                param_name = dt.find('strong').text\n",
    "                param_type = dt.find(class_=\"classifier\").text\n",
    "                param_desc = \"\"\n",
    "                for next_sib in dt.next_siblings:\n",
    "                    if idx == len(dts)-1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        if next_sib == dts[idx+1]:\n",
    "                            break\n",
    "                    next_sib = str(next_sib)\n",
    "                    param_desc += next_sib[next_sib.find(\"<p>\"):next_sib.find(\"</p>\")]\n",
    "                paremter_names_desc.update({param_name:{\"param_type\":param_type,\"params_desc\":param_desc}})\n",
    "            page_dict.update({\"paremter_names_desc\":paremter_names_desc})\n",
    "    except Exception as e:\n",
    "        print(e,parent_name)\n",
    "        return page_dict\n",
    "\n",
    "pbar_ = tqdm(total=len(list(parent_dict.keys())))\n",
    "for parent_name,parent_vals in parent_dict.items():\n",
    "    if 'functions' in parent_vals:\n",
    "        for func in parent_vals['functions']:\n",
    "            func_url = func['url']\n",
    "            web_page_res = get_py_obj(func_url,parent_name)\n",
    "            if web_page_res is not None:\n",
    "                for k,v in web_page_res.items():\n",
    "                    func.update({k:v})\n",
    "    if 'sub_level_dict' in parent_vals:\n",
    "        for sub_level_name,sub_level_vals in parent_vals['sub_level_dict'].items():\n",
    "            if 'functions' in sub_level_vals:\n",
    "                for func in sub_level_vals['functions']:\n",
    "                    func_url = func['url']\n",
    "                    web_page_res = get_py_obj(func_url,parent_name)\n",
    "                    if web_page_res is not None:\n",
    "                        for k,v in web_page_res.items():\n",
    "                            func.update({k:v})\n",
    "    pbar_.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('sklearn.json', 'w') as f:\n",
    "    json.dump(parent_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "# from copy import deepcopy\n",
    "\n",
    "# parent_dict_copy = deepcopy(parent_dict)\n",
    "# def main_scraper(parent_name):\n",
    "#     parent_vals = parent_dict_copy[parent_name]\n",
    "#     if 'functions' in parent_vals:\n",
    "#         for func in parent_vals['functions']:\n",
    "#             func_url = func['url']\n",
    "#             web_page_res = get_py_obj(func_url,parent_name)\n",
    "#             if web_page_res is not None:\n",
    "#                 for k,v in web_page_res.items():\n",
    "#                     func.update({k:v})\n",
    "#     if 'sub_level_dict' in parent_vals:\n",
    "#         for sub_level_name,sub_level_vals in parent_vals['sub_level_dict'].items():\n",
    "#             if 'functions' in sub_level_vals:\n",
    "#                 for func in sub_level_vals['functions']:\n",
    "#                     func_url = func['url']\n",
    "#                     web_page_res = get_py_obj(func_url)\n",
    "#                     if web_page_res is not None:\n",
    "#                         for k,v in web_page_res.items():\n",
    "#                             func.update({k:v})\n",
    "#     print(f\"Done for {parent_name}\")\n",
    "# if __name__ == \"__main__\":\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "#         parent_names_list = list(parent_dict.keys())\n",
    "#         results = executor.map(main_scraper, parent_names_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_func_url = \"https://scikit-learn.org/stable/modules/generated/sklearn.utils.as_float_array.html#sklearn.utils.as_float_array\"\n",
    "# func_url = requests.get(base_func_url)\n",
    "# func_soup = BeautifulSoup(func_url.content, 'lxml')\n",
    "# func_name = clean_text(func_soup.find('h1').text)\n",
    "# func_signature_elem = func_soup.find(class_=\"sig sig-object py\")\n",
    "# if func_signature_elem is not None:\n",
    "#     func_signature = clean_text(func_signature_elem.text)\n",
    "# else:\n",
    "#     func_signature = \"\"\n",
    "# all_urls = []\n",
    "\n",
    "# if func_soup.find(class_=\"py class\"):\n",
    "#     class_or_fn = \"py class\"\n",
    "#     type = \"class\"\n",
    "# elif func_soup.find(class_=\"py function\"):\n",
    "#     class_or_fn = \"py function\"\n",
    "#     type = \"function\"\n",
    "# py_soup = func_soup.find(class_=class_or_fn)\n",
    "# func_text_list = []\n",
    "# dd = py_soup.find('dd')\n",
    "# field_list = dd.find(class_=\"field-list\")\n",
    "\n",
    "# for i in field_list.previous_siblings:\n",
    "#     func_text_list.append(i.text)\n",
    "# func_text = \"\".join(func_text_list[::-1]).replace(\"\\n\",\" \").strip()\n",
    "# for url in py_soup.find_all('a'):\n",
    "#     url = url['href']\n",
    "#     if url is None: continue\n",
    "#     elif url.startswith(\"#\"):\n",
    "#         all_urls.append(base_func_url + url)\n",
    "#     elif url.startswith(\"..\"):\n",
    "#         continue\n",
    "#     elif url.startswith(\"http\"):\n",
    "#         all_urls.append(url)\n",
    "# fodd = field_list.find_all(class_=\"field-odd\")\n",
    "# dts = fodd[1].find_all('dt')\n",
    "# paremter_names_desc = {}\n",
    "\n",
    "# for idx,dt in enumerate(dts):\n",
    "#     param_name = dt.find('strong').text\n",
    "#     param_type = dt.find(class_=\"classifier\").text\n",
    "#     param_desc = \"\"\n",
    "#     for next_sib in dt.next_siblings:\n",
    "#         if idx == len(dts)-1:\n",
    "#             pass\n",
    "#         else:\n",
    "#             if next_sib == dts[idx+1]:\n",
    "#                 break\n",
    "#         next_sib = str(next_sib)\n",
    "#         param_desc += next_sib[next_sib.find(\"<p>\"):next_sib.find(\"</p>\")]\n",
    "#     paremter_names_desc.update({param_name:{\"param_type\":param_type,\"params_desc\":param_desc}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FROM PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://scikit-learn.org/stable/api/index.html\"\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "l1_elems = soup.find_all(class_=\"toctree-l1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../modules/generated/sklearn.config_context.html'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_elems[0].find_all('a')[1]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://scikit-learn.org/stable/api/index.html\"\n",
    "base_parent_url = \"https://scikit-learn.org/stable/\"\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "base_func_url = \"https://pandas.pydata.org/docs/reference/\"\n",
    "l1_elems = soup.find_all(class_=\"toctree-l1\")\n",
    "\n",
    "first_level = {}\n",
    "curr_parent = \"\"\n",
    "for parent_functions in l1_elems:\n",
    "    for func in parent_functions.find_all(\"a\"):\n",
    "        href = func[\"href\"]\n",
    "        if \"..\" not in href:\n",
    "            first_level.update(\n",
    "                {\n",
    "                    href: {\n",
    "                        \"functions\": [],\n",
    "                        \"name\": func.text,\n",
    "                        \"url\": base_parent_url +\"api/\" + href,\n",
    "                    }\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'functions': [],\n",
       " 'name': 'sklearn.feature_extraction',\n",
       " 'url': 'https://scikit-learn.org/stable/api/sklearn.feature_extraction.html'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_level[\"sklearn.feature_extraction.html\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_page = requests.get(first_level[\"sklearn.feature_extraction.html\"]['url'])\n",
    "parent_soup = BeautifulSoup(parent_page.content, 'lxml',from_encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (535049815.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[73], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    for next_sib in paren\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "func_text = parent_soup.find('h1').text.replace(\"#\",\"\")\n",
    "if \"h2\" in str(parent_soup):\n",
    "    h2_elements = parent_soup.find_all(\"h2\")\n",
    "    base_section = []\n",
    "    for next_sib in paren\n",
    "    # for idx,h2_elem in enumerate(h2_elements):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = parent_soup.find_all(class_=\"autosummary longtable table autosummary\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_elements = parent_soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "From images#\n",
      "Utilities to extract features from images.\n",
      "\n",
      "\n",
      "image.PatchExtractor\n",
      "Extracts patches from a collection of images.\n",
      "\n",
      "image.extract_patches_2d\n",
      "Reshape a 2D image into a collection of patches.\n",
      "\n",
      "image.grid_to_graph\n",
      "Graph of the pixel-to-pixel connections.\n",
      "\n",
      "image.img_to_graph\n",
      "Graph of the pixel-to-pixel gradient connections.\n",
      "\n",
      "image.reconstruct_from_patches_2d\n",
      "Reconstruct the image from all of its patches.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "From text#\n",
      "Utilities to build feature vectors from text documents.\n",
      "\n",
      "\n",
      "text.CountVectorizer\n",
      "Convert a collection of text documents to a matrix of token counts.\n",
      "\n",
      "text.HashingVectorizer\n",
      "Convert a collection of text documents to a matrix of token occurrences.\n",
      "\n",
      "text.TfidfTransformer\n",
      "Transform a count matrix to a normalized tf or tf-idf representation.\n",
      "\n",
      "text.TfidfVectorizer\n",
      "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tables.next_siblings:\n",
    "    if i == h2_elements[0]:\n",
    "        break\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<p>Feature extraction from raw data.</p>\n"
     ]
    }
   ],
   "source": [
    "h2_elements = parent_soup.find_all(\"h2\")\n",
    "for i in parent_soup.find('h1').next_siblings:\n",
    "    # print(i)\n",
    "    if i.find(' id')!=-1:\n",
    "        print(i)\n",
    "        break\n",
    "    print(i)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_soup.find_all(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
