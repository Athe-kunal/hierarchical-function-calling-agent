{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.scrape import scrape_sklearn_website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn_data = scrape_sklearn_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/sklearn_function_openai.json\",\"r\") as jsonfile:\n",
    "    sklearn_data = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.utils import build_no_summary_graph,get_parents_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_graph = build_no_summary_graph(sklearn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_dict = get_parents_dict(sklearn_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL TO SUMMARIZE THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = summary_chain.invoke({\"descriptions\":\"\\n\\n\".join(parents_dict['sklearn']['sklearn#defaults'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = await summary_chain.abatch([{\"descriptions\":\"\\n\\n\".join(parents_dict['sklearn']['sklearn#defaults'])}],config={\"max_concurrency\": 5})\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def split_description(parent_text:List[str],MAX_WORDS:int=500):\n",
    "        split_s = []\n",
    "        running_num_words = 0\n",
    "        curr_func_string = \"\"\n",
    "        for txt in parent_text:\n",
    "            num_words = len(txt.split(\" \"))\n",
    "            running_num_words += num_words\n",
    "            if running_num_words > MAX_WORDS:\n",
    "                running_num_words = num_words\n",
    "                split_s.append(curr_func_string)\n",
    "                curr_func_string = txt\n",
    "            else:\n",
    "                curr_func_string += txt + \"\\n\"\n",
    "        if split_s == [] or split_s==['']:\n",
    "            split_s.append(curr_func_string)\n",
    "        split_s = [s for s in split_s if s!=\"\"]\n",
    "        return split_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_summary_dict = {pn:{} for pn in parents_dict}\n",
    "batch_list = []\n",
    "description_list = []\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\",\"\")\n",
    "    text = text.replace(\"\\n\\n\",\"\")\n",
    "    return text\n",
    "\n",
    "for parent_name,child_nodes in parents_dict.items():\n",
    "    for child_name,child_texts in child_nodes.items():\n",
    "        child_split_list = split_description(child_texts,1000)\n",
    "        for ctext in child_texts:\n",
    "            clean_ctext = clean_text(ctext)\n",
    "            batch_list.append({f\"{parent_name}-->{child_name}\":clean_ctext})\n",
    "            description_list.append({\"descriptions\":clean_ctext})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'descriptions': 'Validate and route input parameters. This function is used inside a router’s method, e.g. fit,to validate the metadata and handle the routing. Assuming this signature of a router’s fit method:fit(self, X, y, sample_weight=None, **fit_params),a call to this function would be:process_routing(self, \"fit\", sample_weight=sample_weight, **fit_params). Note that if routing is not enabled and kwargs is empty, then itreturns an empty routing where process_routing(...).ANYTHING.ANY_METHODis always an empty dictionary. Added in version 1.3. An object implementing get_metadata_routing. Typically ameta-estimator. The name of the router’s method in which this function is called. Metadata to be routed. A Bunch of the form {\"object_name\": {\"method_name\":{params: value}}} which can be used to pass the required metadata toA Bunch of the form {\"object_name\": {\"method_name\":{params: value}}} which can be used to pass the required metadata tocorresponding methods or corresponding child objects. The object namesare those defined in obj.get_metadata_routing().'},\n",
       " {'sklearn.utils-->sklearn.utils#Metadata routing': 'Validate and route input parameters. This function is used inside a router’s method, e.g. fit,to validate the metadata and handle the routing. Assuming this signature of a router’s fit method:fit(self, X, y, sample_weight=None, **fit_params),a call to this function would be:process_routing(self, \"fit\", sample_weight=sample_weight, **fit_params). Note that if routing is not enabled and kwargs is empty, then itreturns an empty routing where process_routing(...).ANYTHING.ANY_METHODis always an empty dictionary. Added in version 1.3. An object implementing get_metadata_routing. Typically ameta-estimator. The name of the router’s method in which this function is called. Metadata to be routed. A Bunch of the form {\"object_name\": {\"method_name\":{params: value}}} which can be used to pass the required metadata toA Bunch of the form {\"object_name\": {\"method_name\":{params: value}}} which can be used to pass the required metadata tocorresponding methods or corresponding child objects. The object namesare those defined in obj.get_metadata_routing().'})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require_summary_list = []\n",
    "for bl,dl in zip(batch_list,description_list):\n",
    "    text = dl['descriptions']\n",
    "    num_words = len(text.split(\" \"))\n",
    "    if num_words > 50:\n",
    "        require_summary_list.append(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557, 557)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description_list),len(batch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding rows:  17%|█▋        | 2/12 [03:12<16:04, 96.43s/it]\n"
     ]
    },
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x7fd95974da50 state=finished raised RateLimitError>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[60], line 42\u001b[0m, in \u001b[0;36mget_summaries\u001b[0;34m(summary_chain, curr_description_list)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@retry\u001b[39m(wait\u001b[38;5;241m=\u001b[39mwait_random_exponential(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m),stop\u001b[38;5;241m=\u001b[39mstop_after_attempt(\u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_summaries\u001b[39m(summary_chain,curr_description_list):\n\u001b[0;32m---> 42\u001b[0m     curr_summary_list \u001b[38;5;241m=\u001b[39m \u001b[43msummary_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_description_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m curr_summary_list\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/runnables/base.py:2643\u001b[0m, in \u001b[0;36mRunnableSequence.batch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m   2642\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2643\u001b[0m             inputs \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# each step a child run of the corresponding root run\u001b[39;49;00m\n\u001b[1;32m   2647\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[38;5;66;03m# finish the root runs\u001b[39;00m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/runnables/base.py:633\u001b[0m, in \u001b[0;36mRunnable.batch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(configs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[Output], \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/runnables/config.py:466\u001b[0m, in \u001b[0;36mContextThreadPoolExecutor.map.<locals>._wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontexts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/runnables/base.py:626\u001b[0m, in \u001b[0;36mRunnable.batch.<locals>.invoke\u001b[0;34m(input, config)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:158\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    157\u001b[0m     ChatGeneration,\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    168\u001b[0m )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:560\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:421\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    422\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    423\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    425\u001b[0m ]\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:411\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 411\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m            \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:632\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:451\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    450\u001b[0m }\n\u001b[0;32m--> 451\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/resources/chat/completions.py:590\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1237\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m )\n\u001b[0;32m-> 1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: SyncAPIClient._request at line 1005 (2 times), SyncAPIClient._retry_request at line 1053 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1028\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-ClxZe8eHsxIqfKYK6gebyZ1H on tokens per min (TPM): Limit 60000, Used 59271, Requested 2231. Please try again in 1.502s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start\u001b[38;5;241m+\u001b[39mBATCH_SIZE,\u001b[38;5;28mlen\u001b[39m(description_list))\n\u001b[1;32m     48\u001b[0m curr_description_list \u001b[38;5;241m=\u001b[39m description_list[start:end]\n\u001b[0;32m---> 49\u001b[0m curr_summary_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_summaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcurr_description_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m summaries_list\u001b[38;5;241m.\u001b[39mextend(curr_summary_list)\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/tenacity/__init__.py:326\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n\u001b[1;32m    329\u001b[0m     sleep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(retry_state)\n",
      "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x7fd95974da50 state=finished raised RateLimitError>]"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from tqdm import tqdm\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    "    wait_fixed\n",
    ")\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are given a function description.\n",
    "Your task is to summarize all the text into coherent and detailed summary that covers all the functions descriptions.\n",
    "Be very diligent and make sure that no function description is left out of the final summary. \n",
    "\n",
    "Follow the following format\n",
    "\n",
    "List of function descriptions: list of function descriptions to be summarized\n",
    "Summary: summary of all the function descriptions\n",
    "\n",
    "-----\n",
    "\n",
    "List of functions descriptions: {descriptions}\n",
    "\n",
    "Summary: \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "summaries_list = []\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",api_key=os.environ[f'OPENAI_API_KEY'],max_retries=5)\n",
    "summary_chain = summary_prompt | model | StrOutputParser()\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1,max=60),stop=stop_after_attempt(6))\n",
    "def get_summaries(summary_chain,curr_description_list):\n",
    "    curr_summary_list = summary_chain.batch(curr_description_list)\n",
    "    return curr_summary_list\n",
    "\n",
    "for start in tqdm(range(0,len(description_list),BATCH_SIZE),desc=\"Embedding rows\"):\n",
    "    end = min(start+BATCH_SIZE,len(description_list))\n",
    "    \n",
    "    curr_description_list = description_list[start:end]\n",
    "    curr_summary_list = get_summaries(summary_chain,curr_description_list)\n",
    "    summaries_list.extend(curr_summary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('summary_file.txt', 'w') as f:\n",
    "    for line in summaries_list:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = summary_chain.batch_as_completed(description_list,config={\"max_concurrency\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, \"The global scikit-learn configuration offers various options for customization. \\n- The configuration includes settings for validation of finiteness, size of temporary arrays, printing of non-default parameters, display of estimators, row vectors per chunk, accelerated pairwise-distances reduction backend, Array API dispatching, output format of transform and fit_transform, metadata routing, validation of hyper-parameters' types and values, and retrieval of current configuration values.\\n- Users can choose to skip validation for finiteness for faster processing, set a limit for temporary array size, control the printing of non-default parameters, display estimators as diagrams or text, adjust the number of row vectors per chunk, use the accelerated pairwise-distances reduction backend, enable Array API dispatching, configure output formats, enable metadata routing, and disable validation of hyper-parameters' types and values.\\n- These configurations can be customized globally and the values can be retrieved as needed. \\n- It is important to note that changing configurations can impact the performance and behavior of scikit-learn, and caution should be exercised when making modifications.\")\n",
      "(3, \"The ProbabilityCalibration class in scikit-learn offers the functionality of calibrating classifiers using isotonic regression or logistic regression. It uses cross-validation to estimate classifier parameters and then calibrate the classifier. By default, it fits a copy of the base estimator to a training subset and calibrates it using the testing subset. Predicted probabilities are averaged across individual calibrated classifiers. The calibration method can be 'sigmoid' (Platt's method) or 'isotonic', with the latter not recommended for fewer than 1000 calibration samples due to overfitting tendencies. The class allows for various cross-validation splitting strategies, including default 5-fold cross-validation or custom strategies. The number of jobs to run in parallel can be specified, with the option to use all processors. The calibrator can be fitted using training data and calibrated using testing data for each cross-validation fold, resulting in an ensemble of fitted classifier and calibrator pairs. Alternatively, cross-validation can be used to compute unbiased predictions for calibration. The class also provides methods for fitting the calibrated model, predicting class labels and probabilities, and evaluating accuracy. Metadata routing is available for passing metadata to the fit and score methods. Additionally, the class supports setting and getting parameters for the estimator.\")\n",
      "(2, 'The Mixin class for all transformers in scikit-learn provides functionality such as a fit_transform method, a set_output method for specifying output container type, and automatic wrapping of transform and fit_transform methods. OneToOneFeatureMixin and ClassNamePrefixFeaturesOutMixin are useful for defining get_feature_names_out. The Fit method fits transformer to X and y with optional fit parameters and returns a transformed version of X. The set_output API allows configuring output format for transform and fit_transform, with options like \"default\", \"pandas\", \"polars\", or None. The is_classifier method determines if an estimator is a classifier. \\n\\nThe Mixin class for all bicluster estimators in scikit-learn offers properties like biclusters_, methods to get indices, shape, and submatrix of a bicluster. It provides a convenient way to access row and column indicators together, as well as specific information about biclusters like indices, shape, and submatrix.\\n\\nThe Mixin class for all classifiers defines attributes like _estimator_type and methods like score for calculating mean accuracy on test data. It enforces that fit requires passing y through the requires_y tag. \\n\\nThe Mixin class for all density estimators sets _estimator_type to \"DensityEstimator\" and provides a score method for calculating the model\\'s score on test samples. This class maintains API consistency by including a score method even though it defaults to a no-op.')\n",
      "(1, 'The function descriptions cover various mixins and base classes for estimators in scikit-learn. \\n\\nThe base class provides default implementations for setting and getting parameters, textual and HTML representation, serialization, and data validation. Estimators should specify all parameters in their __init__ method as explicit keyword arguments.\\n\\nThe MetadataMixin class provides metadata routing and encapsulates routing information for an object. ParameterMixin class allows getting and setting parameters for an estimator, including nested objects like Pipelines.\\n\\nThe PrefixMixin class is used for transformers that generate their own feature names, prefixing them with the lowercased class name. ClusterMixin class is for cluster estimators, defining _estimator_type as \"clusterer\" and a fit_predict method for clustering and returning cluster labels.\\n\\nThe MetaEstimatorMixin defines mandatory estimator parameters, while the OutlierMixin class is for outlier detection estimators, with _estimator_type as \"outlier_detector\" and a fit_predict method that returns -1 for outliers and 1 for inliers.')\n"
     ]
    }
   ],
   "source": [
    "for i in ans:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",api_key=os.environ['OPENAI_API_KEY'],max_retries=5,model_kwargs={\"batch_size\":20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_graph(sklearn_graph):\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import ast\n",
    "embed_docs = []\n",
    "embed_metadata = []\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \"\\u200b\",  # Zero-width space\n",
    "        \"\\uff0c\",  # Fullwidth comma\n",
    "        \"\\u3001\",  # Ideographic comma\n",
    "        \"\\uff0e\",  # Fullwidth full stop\n",
    "        \"\\u3002\",  # Ideographic full stop\n",
    "        \"\",\n",
    "    ],\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=400)\n",
    "with open(\"parent_summary.json\",\"r\") as jsonfile:\n",
    "    parent_summary_dict = json.load(jsonfile)\n",
    "\n",
    "splits = text_splitter.split_text(parent_summary_dict['sklearn.linear_model'])\n",
    "with open(\"parent_summary.json\",\"r\") as jsonfile:\n",
    "    parent_summary_dict = json.load(jsonfile)\n",
    "\n",
    "def clean_text(s:str):\n",
    "    s = re.sub(\"\\n\",\" \",s)\n",
    "    s = json.dumps(s)\n",
    "    return ast.literal_eval(s)\n",
    "for node,attr in sklearn_graph.nodes(data=True):\n",
    "    type = attr.get('type')\n",
    "    \n",
    "    if type == 'function_node':\n",
    "        ftext = attr['function_text']\n",
    "        if ftext == \"\": continue\n",
    "        embed_docs.append(clean_text(attr['function_text']))\n",
    "        for k,v in attr.items():\n",
    "            if not isinstance(v,str):\n",
    "                attr[k] = str(v)\n",
    "        embed_metadata.append(attr)\n",
    "    elif type == 'parent_node':\n",
    "        parent_summary = parent_summary_dict[node]\n",
    "        parent_summary_text_split = text_splitter.split_text(parent_summary)\n",
    "        for k,v in attr.items():\n",
    "            if not isinstance(v,str):\n",
    "                attr[k] = str(v)\n",
    "        attr.update({\"name\":node})\n",
    "        for parent_splits in parent_summary_text_split:\n",
    "            embed_docs.append(clean_text(parent_splits))\n",
    "            embed_metadata.append(attr)\n",
    "    elif type == 'sub_level_node':\n",
    "        for ctext in attr['child_texts']:\n",
    "            embed_docs.append(clean_text(ctext))\n",
    "            sub_level_attr = deepcopy(attr)\n",
    "            del sub_level_attr['child_texts']\n",
    "            for k,v in sub_level_attr.items():\n",
    "                if not isinstance(v,str):\n",
    "                    sub_level_attr[k] = str(v)\n",
    "            sub_level_attr.update({\"name\":node})\n",
    "            embed_metadata.append(sub_level_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1303, 1303)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed_docs),len(embed_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'sub_level_node',\n",
       " 'trail': 'sklearn.exceptions',\n",
       " 'name': 'sklearn.exceptions#defaults'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_metadata[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "from chromadb.utils.batch_utils import create_batches\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    "    wait_fixed\n",
    ")\n",
    "@retry(wait=wait_random_exponential(min=1,max=60),stop=stop_after_attempt(6))\n",
    "def build_database(offset:int,docs, metadata, api_key):\n",
    "    database_path = \"SKLEARN_DB\"\n",
    "    collection_name = \"sklearn\"\n",
    "    load_dotenv(find_dotenv(), override=True)\n",
    "    emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=api_key, model_name=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    client = chromadb.PersistentClient(path=database_path)\n",
    "    sklearn_collection = client.get_or_create_collection(\n",
    "        name=collection_name, embedding_function=emb_fn\n",
    "    )\n",
    "\n",
    "    sklearn_ids = [f\"id{offset+i}\" for i in range(len(docs))]\n",
    "    all_ids = sklearn_collection.get()['ids']\n",
    "    if all(skids in all_ids for skids in sklearn_ids):\n",
    "        print(f\"All ids already present in database {offset}\")\n",
    "        return \n",
    "    batches = create_batches(\n",
    "        api=client, ids=sklearn_ids, documents=docs, metadatas=metadata\n",
    "    )\n",
    "    for batch in batches:\n",
    "        sklearn_collection.add(ids=batch[0], documents=batch[3], metadatas=batch[2])\n",
    "    return sklearn_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for 0-200/1303\n",
      "Done for 200-400/1303\n",
      "Done for 400-600/1303\n",
      "Done for 600-800/1303\n",
      "Done for 800-1000/1303\n",
      "Done for 1000-1200/1303\n",
      "Done for 1200-1303/1303\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "def create_embeddings(embed_docs,embed_metadata):\n",
    "    BATCH_SIZE = 200\n",
    "    # pbar = tqdm(total=len(embed_docs)//BATCH_SIZE,)\n",
    "    for start in range(0,len(embed_docs),BATCH_SIZE):\n",
    "        end = min(start+BATCH_SIZE,len(embed_docs))\n",
    "        # sklearn_ids = [f\"id{start+i}\" for i in range()]\n",
    "        curr_docs = embed_docs[start:end]\n",
    "        curr_metadata = embed_metadata[start:end]\n",
    "        build_database(start,curr_docs[:len(curr_docs)//2],curr_metadata[:len(curr_metadata)//2],os.environ['OPENAI_API_KEY1'])\n",
    "        build_database(start+BATCH_SIZE//2,curr_docs[len(curr_docs)//2:],curr_metadata[len(curr_metadata)//2:],os.environ['OPENAI_API_KEY1'])\n",
    "        # print(\"Sleeping...\")\n",
    "        # time.sleep(60)\n",
    "        print(f\"Done for {start}-{end}/{len(embed_docs)}\")\n",
    "\n",
    "create_embeddings(embed_docs,embed_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = \"SKLEARN_DB\"\n",
    "emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=os.environ[\"OPENAI_API_KEY0\"], model_name=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "client = chromadb.PersistentClient(path=database_path)\n",
    "sklearn_collection = client.get_collection(\n",
    "        name=\"sklearn\", embedding_function=emb_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = sklearn_collection.get()['ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sklearn.model_selection#Model validation',\n",
       " 'trail': 'sklearn.model_selection',\n",
       " 'type': 'sub_level_node'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "d = sklearn_collection.get(\n",
    "    where = {\"type\":\"sub_level_node\"},\n",
    ")\n",
    "d['metadatas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.covariance: computing Mahalanobis distances, log-likelihood of test data under a Gaussian model, setting parameters of the estimator, and requesting metadata routing. These functions provide a comprehensive set of tools for covariance estimation and analysis. The GraphicalLassoCV function is used for sparse inverse covariance estimation with a cross-validated choice of the l1 penalty. It allows for the selection of the penalization parameter and provides various outputs such as the estimated mean, covariance matrix, precision matrix, and penalization parameter selected. The function uses a grid search approach to find the optimal penalization parameter and can handle ill-conditioned systems. Additionally, it provides options for setting the tolerance for convergence, the solver type, number of iterations, and parallel processing. The function also offers methods for computing the Mean Squared Error between covariance estimators, fitting the covariance model, computing Mahalanobis distances, and calculating log-likelihood under the estimated Gaussian model. Furthermore, it includes options for setting parameters, getting metadata routing, and requesting metadata for the score method. The Minimum Covariance Determinant (MCD) is a robust estimator of covariance that is suitable for Gaussian-distributed data, but can also be used with unimodal, symmetric distributions. It is not recommended for multi-modal data. The MCD estimator can be used with or without storing the estimated precision, and it allows for specifying the proportion of points to be included in the support of the raw estimate. The algorithm used in MCD estimation is the FastMCD algorithm.\n",
      "\n",
      "sklearn.covariance: seen during fit. Other related functions include outlier detection, maximum likelihood covariance estimation, sparse inverse covariance estimation, LedoitWolf Estimator, Minimum Covariance Determinant, and covariance estimator with shrinkage. The shrinkage formulation implemented in the regularized covariance differs from the original article by omitting a small value operation for large feature numbers. References are provided for further reading. The estimator also includes functions for computing Mean Squared Error between covariance estimators, fitting the model to data, getting metadata routing, retrieving parameters, computing Mahalanobis distances, computing log-likelihood, and setting parameters. Metadata routing options are available for passing metadata to the score method. The functions provided in the list are related to computing different types of covariance estimators. These include the Maximum Likelihood covariance estimator, the Ledoit-Wolf covariance matrix estimator, and the Oracle Approximating Shrinkage estimator. Each function allows for the computation of covariance estimates from data, with options to center the data before computation. The shrunk covariance is calculated using a convex combination formula, with specific coefficients and formulas provided in the notes. The Oracle Approximating Shrinkage estimator uses a different shrinkage formulation compared to the original article referenced. Overall, these functions provide tools for estimating covariance matrices with different approaches and optimizations. The Maximum Likelihood Covariance Estimator is a method used to estimate the covariance matrix, pseudo-inverse matrix, and mean of a dataset. It can be used with or without centering the data before computation. Other related functions include Sparse Inverse Covariance Estimation, LedoitWolf Estimator, Minimum Covariance Determinant, Oracle Approximating Shrinkage Estimator, and Covariance Estimator with Shrinkage. The Mean Squared Error function allows for comparison between two covariance estimators. Additional functions include computing Mahalanobis distances, log-likelihood of test data under a Gaussian model, setting parameters of the estimator, and requesting metadata routing. These functions provide a comprehensive set of tools for covariance estimation and analysis. The GraphicalLassoCV function is used for sparse inverse covariance estimation with a cross-validated choice of the l1 penalty. It allows for the\n",
      "\n",
      "sklearn.covariance: The MCD estimator provides estimates for the robust location, covariance matrix, and pseudo-inverse matrix. It also computes Mahalanobis distances for the training set observations. Other related estimators include the Maximum Likelihood Covariance Estimator, Sparse Inverse Covariance Estimators, and Oracle Approximating Shrinkage Estimator.   There are methods available to correct raw MCD estimates, compute Mean Squared Error between covariance estimators, re-weight observations, and compute the log-likelihood of test data under the estimated Gaussian model. Additionally, there are methods for setting parameters of the estimator and requesting metadata routing.   Overall, the MCD estimator is a powerful tool for robust covariance estimation, particularly for Gaussian-distributed data, with various options and methods available for customization and analysis. The Covariance estimator with shrinkage allows for the computation of the estimated covariance matrix, estimated mean, and estimated pseudo inverse matrix. It also includes the option to store the estimated precision. The shrinkage coefficient is used in a convex combination for the computation of the shrunk estimate. Other related functions include the Maximum Likelihood Covariance Estimator, Sparse Inverse Covariance Estimation with an l1-penalized estimator, and the Minimum Covariance Determinant (robust estimator of covariance). The Oracle Approximating Shrinkage Estimator is also available. The regularized covariance is calculated using a specific formula involving the shrinkage coefficient and the covariance matrix. Additionally, functions are provided for computing the Mean Squared Error between two covariance estimators, fitting the shrunk covariance model to data, computing Mahalanobis distances, and calculating the log-likelihood of test data under the estimated Gaussian model. The precision matrix can be retrieved using a getter function, and parameters can be set using a specific method. Metadata routing is available for certain parameters, allowing for more flexibility in handling metadata.\n",
      "\n",
      " sklearn.covariance; empirical covariance; X; y; Mahalanobis distances; log-likelihood; GraphicalLassoCV; sparse inverse covariance estimation; l1 penalty; penalization parameter; precision matrix; mean squared error; convergence tolerance; solver type; metadata routing; Mean Squared Error; covariance model fitting; ill-conditioned systems; shrinkage formulation; Maximum Likelihood Covariance Estimator; LedoitWolf Estimator; Minimum Covariance Determinant; Oracle Approximating Shrinkage estimator; centering data; robust estimator; Gaussian-distributed data; shrinkage coefficient; pseudo-inverse matrix; re-weight observations; customization; analysis; precision matrix retrieval; getter function; metadata handling.\n",
      "sklearn.covariance#defaults: Compute the Maximum likelihood covariance estimator. Data from which to compute the covariance estimate. If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data will be centered before computation. Empirical covariance (Maximum Likelihood Estimator).\n",
      "\n",
      "sklearn.covariance#defaults: Estimate covariance with the Oracle Approximating Shrinkage. Read more in the User Guide. Data from which to compute the covariance estimate. If True, data will not be centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data will be centered before computation. Shrunk covariance. Coefficient in the convex combination used for the computation of the shrunk estimate. Notes The regularised covariance is: (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features), where mu = trace(cov) / n_features and shrinkage is given by the OAS formula (see [1]). The shrinkage formulation implemented here differs from Eq. 23 in [1]. In the original article, formula (23) states that 2/p (p being the number of features) is multiplied by Trace(cov*cov) in both the numerator and denominator, but this operation is omitted because for a large p, the value of 2/p is so small that it doesn’t affect the value of the estimator. References “Shrinkage algorithms for MMSE covariance estimation.”, Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O. IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.\n",
      "\n",
      "sklearn.covariance#defaults: Calculate covariance matrices shrunk on the diagonal. Read more in the User Guide. Covariance matrices to be shrunk, at least 2D ndarray. Coefficient in the convex combination used for the computation of the shrunk estimate. Range is [0, 1]. Shrunk covariance matrices. Notes The regularized (shrunk) covariance is given by: where mu = trace(cov) / n_features.\n",
      "\n",
      "sklearn.covariance#defaults: Estimate the shrunk Ledoit-Wolf covariance matrix. Read more in the User Guide. Data from which to compute the covariance estimate. If True, data will not be centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data will be centered before computation. Size of blocks into which the covariance matrix will be split. This is purely a memory optimization and does not affect results. Shrunk covariance. Coefficient in the convex combination used for the computation of the shrunk estimate. Notes The regularized (shrunk) covariance is: (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features) where mu = trace(cov) / n_features\n",
      "\n",
      "sklearn.covariance#defaults: Covariance estimator with shrinkage. Read more in the User Guide. Specify if the estimated precision is stored. If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data will be centered before computation. Coefficient in the convex combination used for the computation of the shrunk estimate. Range is [0, 1]. Estimated covariance matrix Estimated location, i.e. the estimated mean. Estimated pseudo inverse matrix. (stored only if store_precision is True) Number of features seen during fit. Added in version 0.24. Names of features seen during fit. Defined only when X has feature names that are all strings. Added in version 1.0. See also An object for detecting outliers in a Gaussian distributed dataset. Maximum likelihood covariance estimator. Sparse inverse covariance estimation with an l1-penalized estimator. Sparse inverse covariance with cross-validated choice of the l1 penalty. LedoitWolf Estimator. Minimum Covariance Determinant (robust estimator of covariance). Oracle Approximating Shrinkage Estimator. Notes The regularized covariance is given by: (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features) where mu = trace(cov) / n_features  Compute the Mean Squared Error between two covariance estimators. The covariance to compare with. The type of norm used to compute the error. Available error types: - ‘frobenius’ (default): sqrt(tr(A^t.A)) - ‘spectral’: sqrt(max(eigenvalues(A^t.A)) where A is the error (comp_cov - self.covariance_). If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled. Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned. The Mean Squared Error (in the sense of the Frobenius norm) between self and comp_cov covariance estimators. Fit the shrunk covariance model to X. Training data, where n_samples is the number of samples and n_features is the number of features. Not used, present for API consistency by convention. Returns the instance itself. Get metadata routing of this object. Please check User Guide on how the routing mechanism works. A MetadataRequest encapsulating routing information. Get parameters for this estimator. If True, will return the parameters for this estimator and contained subobjects that are estimators. Parameter names mapped to their values. Getter for the precision matrix. The precision matrix associated to the current covariance object. Compute the squared Mahalanobis distances of given observations. The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit. Squared Mahalanobis distances of the observations. Compute the log-likelihood of X_test under the estimated Gaussian model. The Gaussian model is defined by its mean and covariance matrix which are represented respectively by self.location_ and self.covariance_. Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering). Not used, present for API consistency by convention. The log-likelihood of X_test with self.location_ and self.covariance_ as estimators of the Gaussian model mean and covariance matrix respectively. Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object. Estimator parameters. Estimator instance. Request metadata passed to the score method. Note that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config). Please see User Guide on how the routing mechanism works. The options for each parameter are: True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to score. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a Pipeline. Otherwise it has no effect. Metadata routing for X_test parameter in score. The updated object.\n",
      "\n",
      "\n",
      "sklearn.covariance#defaults\n",
      "{'$or': [{'trail': {'$eq': ' pseudo-inverse matrix-->defaults'}}, {'trail': {'$eq': ' ill-conditioned systems-->defaults'}}, {'trail': {'$eq': ' metadata routing-->defaults'}}, {'trail': {'$eq': ' Mean Squared Error-->defaults'}}, {'trail': {'$eq': ' l1 penalty-->defaults'}}, {'trail': {'$eq': ' mean squared error-->defaults'}}, {'trail': {'$eq': ' Mahalanobis distances-->defaults'}}, {'trail': {'$eq': ' shrinkage formulation-->defaults'}}, {'trail': {'$eq': ' empirical covariance-->defaults'}}, {'trail': {'$eq': ' shrinkage coefficient-->defaults'}}, {'trail': {'$eq': ' customization-->defaults'}}, {'trail': {'$eq': ' solver type-->defaults'}}, {'trail': {'$eq': ' Gaussian-distributed data-->defaults'}}, {'trail': {'$eq': ' sparse inverse covariance estimation-->defaults'}}, {'trail': {'$eq': ' X-->defaults'}}, {'trail': {'$eq': ' Oracle Approximating Shrinkage estimator-->defaults'}}, {'trail': {'$eq': ' getter function-->defaults'}}, {'trail': {'$eq': ' centering data-->defaults'}}, {'trail': {'$eq': ' GraphicalLassoCV-->defaults'}}, {'trail': {'$eq': ' log-likelihood-->defaults'}}, {'trail': {'$eq': ' convergence tolerance-->defaults'}}, {'trail': {'$eq': ' Minimum Covariance Determinant-->defaults'}}, {'trail': {'$eq': 'sklearn.covariance-->defaults'}}, {'trail': {'$eq': ' robust estimator-->defaults'}}, {'trail': {'$eq': ' re-weight observations-->defaults'}}, {'trail': {'$eq': ' precision matrix-->defaults'}}, {'trail': {'$eq': ' Maximum Likelihood Covariance Estimator-->defaults'}}, {'trail': {'$eq': ' precision matrix retrieval-->defaults'}}, {'trail': {'$eq': ' penalization parameter-->defaults'}}, {'trail': {'$eq': ' covariance model fitting-->defaults'}}, {'trail': {'$eq': ' analysis-->defaults'}}, {'trail': {'$eq': ' LedoitWolf Estimator-->defaults'}}, {'trail': {'$eq': ' metadata handling.-->defaults'}}, {'trail': {'$eq': ' y-->defaults'}}]}\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "def generate_pairs(list1, list2):\n",
    "    pairs = []\n",
    "    for l1 in list1:\n",
    "        for l2 in list2:\n",
    "            curr_trail = l1\n",
    "            curr_trail += f\"-->{l2}\"\n",
    "            pairs.append(curr_trail)\n",
    "    return [pairs]\n",
    "\n",
    "\n",
    "def generate_pairs_recursive(trail_list):\n",
    "    if len(trail_list) == 1:\n",
    "        return trail_list[0]\n",
    "    curr_pairs = generate_pairs(trail_list[-2], trail_list[-1])\n",
    "    modified_trail_list = trail_list[:-2] + curr_pairs\n",
    "    return generate_pairs_recursive(modified_trail_list)\n",
    "\n",
    "\n",
    "def get_trail_list_pairs(trail_list_pairs, metadata_name=\"trail\"):\n",
    "    if len(trail_list_pairs) == 1:\n",
    "        trail_where_clause = {metadata_name: {\"$eq\": trail_list_pairs[0]}}\n",
    "    elif len(trail_list_pairs) > 1:\n",
    "        trail_where_clause = {\n",
    "            \"$or\": [{metadata_name: {\"$eq\": t}} for t in trail_list_pairs]\n",
    "        }\n",
    "    return trail_where_clause\n",
    "\n",
    "class FirstSecondLevel(dspy.Signature):\n",
    "    \"You are given a list of keys and values separated by semicolon.\"\n",
    "    \"Based on the query, you have to output the key that is most relevant to the question separated by semicolon.\"\n",
    "    \"Be precise and output only the relevant key or keys from the provided keys only.\"\n",
    "    \"Don't include any other information\"\n",
    "\n",
    "    query = dspy.InputField(prefix=\"Query which you need to classify: \", format=str)\n",
    "    keys_values = dspy.InputField(prefix=\"Keys and Values: \", format=str)\n",
    "    output = dspy.OutputField(\n",
    "        prefix=\"Relevant Key(s): \",\n",
    "        format=str,\n",
    "        desc=\"relevant keys separated by semicolon\",\n",
    "    )\n",
    "llm = dspy.OpenAI(model=\"gpt-3.5-turbo-0125\",max_tokens=512)\n",
    "dspy.settings.configure(lm=llm)\n",
    "\n",
    "class SklearnAgentChroma(dspy.Module):\n",
    "    def __init__(self, collection):\n",
    "        super().__init__()\n",
    "        self.collection = collection\n",
    "        self.firstSecondLevel = dspy.Predict(FirstSecondLevel)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return super().__call__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, query: str):\n",
    "        query_emb = emb_fn([query])[0]\n",
    "\n",
    "        # Parent level querying\n",
    "        parent_level = self.collection.query(\n",
    "            query_embeddings=query_emb,\n",
    "            where={\n",
    "                \"type\": {\"$eq\": \"parent_node\"},\n",
    "            },\n",
    "            n_results=3,\n",
    "        )\n",
    "        parent_level_str = \"\"\n",
    "        for parent_level_docs,parent_level_metadata in zip(parent_level['documents'][0],parent_level[\"metadatas\"][0]):\n",
    "            parent_level_str += f\"{parent_level_metadata['name']}: {parent_level_docs}\\n\\n\"\n",
    "\n",
    "        parent_level_answer = self.firstSecondLevel(\n",
    "            query=query, keys_values=parent_level_str\n",
    "        ).output\n",
    "        print(parent_level_str, parent_level_answer)\n",
    "        trail_list = [parent_level_answer.split(\";\")]\n",
    "        trail_list = list(set(trail_list[0]))\n",
    "        trail_list_pairs = generate_pairs_recursive([trail_list])\n",
    "\n",
    "        trail_where_clause = get_trail_list_pairs(trail_list_pairs)\n",
    "\n",
    "        sub_level = self.collection.query(\n",
    "            query_embeddings=query_emb,\n",
    "            where={\n",
    "                \"$and\": [\n",
    "                    trail_where_clause,\n",
    "                    {\"type\": {\"$eq\": \"sub_level_node\"}},\n",
    "                ]\n",
    "            },\n",
    "            n_results=5,\n",
    "        )\n",
    "\n",
    "        sub_level_str = \"\"\n",
    "        for sub_level_docs,function_level_metadata in zip(sub_level['documents'][0],sub_level[\"metadatas\"][0]):\n",
    "            sub_level_str += f\"{function_level_metadata['name']}: {sub_level_docs}\\n\\n\"\n",
    "        print(sub_level_str)\n",
    "        sub_level_answer = self.firstSecondLevel(\n",
    "            query=query, keys_values=sub_level_str\n",
    "        ).output\n",
    "        print(sub_level_answer)\n",
    "        sub_level_list = [sla.split(\"#\")[-1] for sla in sub_level_answer.split(\";\")]\n",
    "        sub_level_list = list(set(sub_level_list))\n",
    "        function_list = generate_pairs_recursive([trail_list_pairs,sub_level_list])\n",
    "        function_where_clause = get_trail_list_pairs(function_list)\n",
    "        print(function_where_clause)\n",
    "        functions = self.collection.query(\n",
    "            query_embeddings=query_emb,\n",
    "            where={\n",
    "                \"$and\": [\n",
    "                    function_where_clause,\n",
    "                    {\"type\": {\"$eq\": \"function_node\"}},\n",
    "                ]\n",
    "            },\n",
    "            n_results=1\n",
    "        )\n",
    "        return functions\n",
    "sklearn_chroma = SklearnAgentChroma(sklearn_collection)\n",
    "funcs = sklearn_chroma(\"How to do empirical covariance of X and y?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'name': 'empirical_covariance', 'descriptions': 'Compute the Maximum likelihood covariance estimator. Data from which to compute the covariance estimate. If True, data will not be centered before computation.\\\\nUseful when working with data whose mean is almost, but not exactly\\\\nzero.\\\\nIf False, data will be centered before computation. Empirical covariance (Maximum Likelihood Estimator). Examples', 'parameters': {'type': 'object', 'properties': {'X': {'type': 'array', 'description': 'ndarray of shape (n_samples, n_features). Data from which to compute the covariance estimate.\\\\n'}, 'assume_centered': {'type': 'boolean', 'description': 'bool, default=False. If True, data will not be centered before computation.\\\\nUseful when working with data whose mean is almost, but not exactly\\\\nzero.\\\\nIf False, data will be centered before computation.\\\\n'}}, 'required': ['X']}}\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs['metadatas'][0][0]['function_calling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.model_selection: On the other hand, the Leave-P-Out cross-validator also provides train/test indices to split data into train/test sets, resulting in testing on all distinct samples of size p while the remaining n - p samples form the training set in each iteration. It is important to note that LeavePOut(p) is not equivalent to KFold(n_splits=n_samples // p) as it creates non-overlapping test sets. Similar to Leave-One-Out, this method can be costly for large datasets, and it is advised to use KFold, StratifiedKFold, or ShuffleSplit instead.\n",
      "sklearn.neighbors: be specified, such as 'ball_tree', 'kd_tree', or 'brute'. The weight function used in prediction can be set to 'uniform', 'distance', or a user-defined function. Additional parameters like leaf size, power parameter for the Minkowski metric, and metric for distance computation can also be adjusted. The model can be fitted with training data and used to predict target values. The coefficient of determination (R^2) can be calculated to evaluate the model's performance. The model parameters can be set and metadata routing can be requested for the estimator. Overall, the Radius Neighbors Regressor provides a flexible and customizable approach to regression based on k-nearest neighbors within a specified radius.\n",
      "sklearn.metrics: also supports sample weights and different types of configurations, such as 'ovr' (One-vs-rest) and 'ovo' (One-vs-one). The Gini Coefficient, which is a summary measure of the ranking ability of binary classifiers, can be calculated using the ROC-AUC score. References for further reading on ROC analysis and the Gini coefficient are provided. The summary covers two main functions: Top-k Accuracy classification score and \\(D^2\\) regression score function.\n",
      "sklearn.utils: function can preserve the dtype of the input or convert it to a specified type. It also provides options for memory layout, copying behavior, handling of special values like np.inf and np.nan, and enforcing minimum dimensions and features in the input data. Additionally, it allows including the estimator name in warning messages and customizing the error message construction. The function returns the converted and validated array after performing all the necessary checks and conversions. The functions provided in the list cover a range of functionalities.\n",
      "sklearn.compose: clones the regressor and transformer before fitting, with the option to use LinearRegression if no regressor is specified. The target y is converted into a 2-dimensional array internally, and reshaped during prediction. The coefficient of determination \\(R^2\\) is calculated for the prediction, with a score of 1.0 indicating the best possible fit. The method also allows for setting parameters, predicting values, and requesting metadata for scoring. Additionally, the method supports metadata routing for sample weights and returns the updated object.\n",
      " sklearn.compose; sklearn.linear_model; sklearn.metrics\n",
      "sklearn.compose#defaults: Create a callable to select columns to be used with ColumnTransformer. make_column_selector can select columns based on datatype or the columns name with a regex. When using multiple selection criteria, all criteria must match for a column to be selected. For an example of how to use make_column_selector within a ColumnTransformer to select columns based on data type (i.e. dtype), refer to Column Transformer with Mixed Types. Name of columns containing this regex pattern will be included. If None, column selection will not be selected based on pattern. A selection of dtypes to include. For more details, see pandas.DataFrame.select_dtypes. A selection of dtypes to exclude. For more details, see pandas.DataFrame.select_dtypes. Callable for column selection to be used by a ColumnTransformer. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space.  Callable for column selection to be used by a ColumnTransformer. DataFrame to select columns from.\n",
      "sklearn.compose#defaults: Meta-estimator to regress on a transformed target. Useful for applying a non-linear transformation to the target y in regression problems. This transformation can be given as a Transformer such as the QuantileTransformer or as a function and its inverse such as np.log and np.exp. The computation during fit is: or: The computation during predict is: or: Read more in the User Guide. Added in version 0.20. Regressor object such as derived from RegressorMixin. This regressor will automatically be cloned each time prior to fitting. If regressor is None, LinearRegression is created and used. Estimator object such as derived from TransformerMixin. Cannot be set at the same time as func and inverse_func. If transformer is None as well as func and inverse_func, the transformer will be an identity transformer. Note that the transformer will be cloned during fitting. Also, the transformer is restricting y to be a numpy array. Function to apply to y before passing to fit. Cannot be set at the same time as transformer. If func is None, the function used will be the identity function. If func is set, inverse_func also needs to be provided. The function needs to return a 2-dimensional array. Function to apply to the prediction of the regressor. Cannot be set at the same time as transformer. The inverse function is used to return predictions to the same space of the original training labels. If inverse_func is set, func also needs to be provided. The inverse function needs to return a 2-dimensional array. Whether to check that transform followed by inverse_transform or func followed by inverse_func leads to the original targets. Fitted regressor. Transformer used in fit and predict. Number of features seen during fit. Names of features seen during fit. Defined only when X has feature names that are all strings. Added in version 1.0. See also Construct a transformer from an arbitrary callable. Notes Internally, the target y is always converted into a 2-dimensional array to be used by scikit-learn transformers. At the time of prediction, the output will be reshaped to a have the same number of dimensions as y.  For a more detailed example use case refer to Effect of transforming the targets in regression model. Fit the model according to the given training data. Training vector, where n_samples is the number of samples and n_features is the number of features. Target values. Parameters passed to the fit method of the underlying regressor. Fitted estimator. Raise NotImplementedError. This estimator does not support metadata routing yet. Get parameters for this estimator. If True, will return the parameters for this estimator and contained subobjects that are estimators. Parameter names mapped to their values. Number of features seen during fit. Predict using the base regressor, applying inverse. The regressor is used to predict and the inverse_func or inverse_transform is applied before returning the prediction. Samples. Parameters passed to the predict method of the underlying regressor. Predicted values. Return the coefficient of determination of the prediction. The coefficient of determination \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares ((y_true - y_pred)** 2).sum() and \\(v\\) is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \\(R^2\\) score of 0.0. Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator. True values for X. Sample weights. \\(R^2\\) of self.predict(X) w.r.t. y. Notes The \\(R^2\\) score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of r2_score. This influences the score method of all the multioutput regressors (except for MultiOutputRegressor). Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object. Estimator parameters. Estimator instance. Request metadata passed to the score method. Note that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config). Please see User Guide on how the routing mechanism works. The options for each parameter are: True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to score. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a Pipeline. Otherwise it has no effect. Metadata routing for sample_weight parameter in score. The updated object.\n",
      "sklearn.compose#defaults: Construct a ColumnTransformer from the given transformers. This is a shorthand for the ColumnTransformer constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting with transformer_weights. Read more in the User Guide. Tuples of the form (transformer, columns) specifying the transformer objects to be applied to subsets of the data. Estimator must support fit and transform. Special-cased strings ‘drop’ and ‘passthrough’ are accepted as well, to indicate to drop the columns or to pass them through untransformed, respectively. Indexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where transformer expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data X and can return any of the above. To select multiple columns by name or dtype, you can use make_column_selector. By default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of 'drop'). By specifying remainder='passthrough', all remaining columns that were not specified in transformers will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting remainder to be an estimator, the remaining non-specified columns will use the remainder estimator. The estimator must support fit and transform. If the transformed output consists of a mix of sparse and dense data, it will be stacked as a sparse matrix if the density is lower than this value. Use sparse_threshold=0 to always return dense. When the transformed output consists of all sparse or all dense data, the stacked result will be sparse or dense, respectively, and this keyword will be ignored. Number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details. If True, the time elapsed while fitting each transformer will be printed as it is completed. If True, ColumnTransformer.get_feature_names_out will prefix all feature names with the name of the transformer that generated that feature. If False, ColumnTransformer.get_feature_names_out will not prefix any feature names and will error if feature names are not unique. Added in version 1.0. Force the columns of the last entry of transformers_, which corresponds to the “remainder” transformer, to always be stored as indices (int) rather than column names (str). See description of the ColumnTransformer.transformers_ attribute for details. Note If you do not access the list of columns for the remainder columns in the ColumnTransformer.transformers_ fitted attribute, you do not need to set this parameter. Added in version 1.5. Changed in version 1.7: The default value for force_int_remainder_cols will change from True to False in version 1.7. Returns a ColumnTransformer object. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space.\n",
      "\n",
      "{'$or': [{'trail': {'$eq': 'sklearn.compose-->defaults'}}, {'trail': {'$eq': ' sklearn.metrics-->defaults'}}, {'trail': {'$eq': ' sklearn.linear_model-->defaults'}}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.schema import Document\n",
    "\n",
    "class FirstSecondLevel_(dspy.Signature):\n",
    "    \"You are given a list of keys and values separated by semicolon.\"\n",
    "    \"Based on the query, you have to output the key that is most relevant to the question separated by semicolon.\"\n",
    "    \"Be precise and output only the relevant key or keys from the provided keys only.\"\n",
    "    \"Don't include any other information and don't answer None and N/A\"\n",
    "\n",
    "    query = dspy.InputField(prefix=\"Query which you need to classify: \", format=str)\n",
    "    keys_values = dspy.InputField(prefix=\"Keys and Values: \", format=str)\n",
    "    output = dspy.OutputField(\n",
    "        prefix=\"Relevant Key(s): \",\n",
    "        format=str,\n",
    "        desc=\"relevant keys separated by semicolon\",\n",
    "    )\n",
    "\n",
    "class SklearnAgentBM25(dspy.Module):\n",
    "    def __init__(self, collection):\n",
    "        super().__init__()\n",
    "        self.collection = collection\n",
    "        self.firstSecondLevel = dspy.Predict(FirstSecondLevel_)\n",
    "        self.parent_langchain_docs = []\n",
    "        parent_level = self.collection.get(\n",
    "            where={\n",
    "                \"type\": {\"$eq\": \"parent_node\"},\n",
    "            }\n",
    "        )\n",
    "        for doc,metadata in zip(parent_level['documents'],parent_level['metadatas']):\n",
    "            self.parent_langchain_docs.append(Document(page_content=doc,metadata=metadata))\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return super().__call__(*args, **kwargs)\n",
    "    \n",
    "    def BM25RetrieverLangchain(self,query:str,node_type:str='parent_node',trail_where_clause:dict={}):\n",
    "\n",
    "        assert node_type in ['parent_node','function_node','sub_level_node'], \"type must be 'parent_node' or 'function_node' or 'sub_level_node'\"\n",
    "        if node_type != 'parent_node' and trail_where_clause=={}:\n",
    "            raise ValueError(\"trail_where_clause must be a dict for function type\")\n",
    "        \n",
    "        if node_type == 'parent_node':\n",
    "            bm25_retriever = BM25Retriever.from_documents(\n",
    "                self.parent_langchain_docs, k=5, preprocess_func=(lambda x: x.lower())\n",
    "                )\n",
    "            parent_bm25_docs = bm25_retriever.invoke(query.lower())\n",
    "            return parent_bm25_docs\n",
    "        else:\n",
    "            function_level = self.collection.get(\n",
    "            where={\n",
    "                \"$and\": [\n",
    "                    trail_where_clause,\n",
    "                    {\"type\": {\"$eq\": node_type}},\n",
    "                ]\n",
    "            },\n",
    "             )\n",
    "            function_langchain_docs = []\n",
    "            for doc,metadata in zip(function_level['documents'],function_level['metadatas']):\n",
    "                function_langchain_docs.append(Document(page_content=doc,metadata=metadata))\n",
    "            bm25_retriever = BM25Retriever.from_documents(\n",
    "                function_langchain_docs, k=3, preprocess_func=(lambda x: x.lower())\n",
    "            )\n",
    "            bm25_docs = bm25_retriever.invoke(query.lower())\n",
    "            return bm25_docs\n",
    "       \n",
    "    def forward(self, query: str):\n",
    "        parent_bm25_docs = self.BM25RetrieverLangchain(query,node_type=\"parent_node\")\n",
    "        parent_level_str = \"\"\n",
    "        for parent_doc in parent_bm25_docs:\n",
    "            parent_level_str+=f\"{parent_doc.metadata['name']}: {parent_doc.page_content}\\n\"\n",
    "        \n",
    "        parent_level_answer = self.firstSecondLevel(\n",
    "            query=query, keys_values=parent_level_str\n",
    "        ).output\n",
    "        print(parent_level_str,parent_level_answer)\n",
    "        trail_list = [parent_level_answer.split(\";\")]\n",
    "        trail_list = list(set(trail_list[0]))\n",
    "        trail_list_pairs = generate_pairs_recursive([trail_list])\n",
    "\n",
    "        trail_where_clause = get_trail_list_pairs(trail_list_pairs)\n",
    "\n",
    "        sub_level_docs = self.BM25RetrieverLangchain(query,node_type='sub_level_node',trail_where_clause=trail_where_clause)\n",
    "        sub_level_str = \"\"\n",
    "        for function_doc in sub_level_docs:\n",
    "            sub_level_str+=f\"{function_doc.metadata['name']}: {function_doc.page_content}\\n\"\n",
    "        print(sub_level_str)\n",
    "        sub_level_answer = self.firstSecondLevel(\n",
    "            query=query, keys_values=sub_level_str\n",
    "        ).output\n",
    "        \n",
    "        sub_level_list = [sla.split(\"#\")[-1] for sla in sub_level_answer.split(\";\")]\n",
    "        sub_level_list = list(set(sub_level_list))\n",
    "        function_list = generate_pairs_recursive([trail_list_pairs,sub_level_list])\n",
    "        function_where_clause = get_trail_list_pairs(function_list)\n",
    "        print(function_where_clause)\n",
    "        functions = self.BM25RetrieverLangchain(query,node_type=\"function_node\",trail_where_clause=function_where_clause)\n",
    "        return functions\n",
    "sklearn_bm25 = SklearnAgentBM25(sklearn_collection)\n",
    "\n",
    "funcs = sklearn_bm25(\"I want to linear discriminant analysis with X and y?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Create a callable to select columns to be used with ColumnTransformer. make_column_selector can select columns based on datatype or the columns name with a regex. When using multiple selection criteria, all criteria must match for a column to be selected. For an example of how to use make_column_selector within a ColumnTransformer to select columns based on data type (i.e. dtype), refer to Column Transformer with Mixed Types. Name of columns containing this regex pattern will be included. If None, column selection will not be selected based on pattern. A selection of dtypes to include. For more details, see pandas.DataFrame.select_dtypes. A selection of dtypes to exclude. For more details, see pandas.DataFrame.select_dtypes. Callable for column selection to be used by a ColumnTransformer. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space. Examples Callable for column selection to be used by a ColumnTransformer. DataFrame to select columns from.', metadata={'full_function': 'class sklearn.compose.make_column_selector(pattern=None, *, dtype_include=None, dtype_exclude=None)', 'function_calling': \"{'name': 'make_column_selector', 'descriptions': 'Create a callable to select columns to be used with\\\\nColumnTransformer. make_column_selector can select columns based on datatype or the\\\\ncolumns name with a regex. When using multiple selection criteria, all\\\\ncriteria must match for a column to be selected. For an example of how to use make_column_selector within a\\\\nColumnTransformer to select columns based on data type (i.e.\\\\ndtype), refer to\\\\nColumn Transformer with Mixed Types. Name of columns containing this regex pattern will be included. If\\\\nNone, column selection will not be selected based on pattern. A selection of dtypes to include. For more details, see\\\\npandas.DataFrame.select_dtypes. A selection of dtypes to exclude. For more details, see\\\\npandas.DataFrame.select_dtypes. Callable for column selection to be used by a\\\\nColumnTransformer. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space. Examples Callable for column selection to be used by a\\\\nColumnTransformer. DataFrame to select columns from.', 'parameters': {'type': 'object', 'properties': {'df': {'type': 'dataframe of shape (n_features, n_samples)', 'description': 'dataframe of shape (n_features, n_samples). DataFrame to select columns from.\\\\n'}}, 'required': ['pattern=None']}}\", 'function_name': 'make_column_selector', 'function_text': 'Create a callable to select columns to be used with\\nColumnTransformer. make_column_selector can select columns based on datatype or the\\ncolumns name with a regex. When using multiple selection criteria, all\\ncriteria must match for a column to be selected. For an example of how to use make_column_selector within a\\nColumnTransformer to select columns based on data type (i.e.\\ndtype), refer to\\nColumn Transformer with Mixed Types. Name of columns containing this regex pattern will be included. If\\nNone, column selection will not be selected based on pattern. A selection of dtypes to include. For more details, see\\npandas.DataFrame.select_dtypes. A selection of dtypes to exclude. For more details, see\\npandas.DataFrame.select_dtypes. Callable for column selection to be used by a\\nColumnTransformer. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space. Examples Callable for column selection to be used by a\\nColumnTransformer. DataFrame to select columns from.', 'name': 'make_column_selector', 'parameter_names_desc': \"[{'param_name': 'df', 'param_type': 'dataframe of shape (n_features, n_samples)', 'param_desc': 'DataFrame to select columns from.\\\\n'}]\", 'trail': 'sklearn.compose-->defaults', 'type': 'function_node', 'url': 'https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html#sklearn.compose.make_column_selector'}),\n",
       " Document(page_content=\"Meta-estimator to regress on a transformed target. Useful for applying a non-linear transformation to the target y in regression problems. This transformation can be given as a Transformer such as the QuantileTransformer or as a function and its inverse such as np.log and np.exp. The computation during fit is: or: The computation during predict is: or: Read more in the User Guide. Added in version 0.20. Regressor object such as derived from RegressorMixin. This regressor will automatically be cloned each time prior to fitting. If regressor is None, LinearRegression is created and used. Estimator object such as derived from TransformerMixin. Cannot be set at the same time as func and inverse_func. If transformer is None as well as func and inverse_func, the transformer will be an identity transformer. Note that the transformer will be cloned during fitting. Also, the transformer is restricting y to be a numpy array. Function to apply to y before passing to fit. Cannot be set at the same time as transformer. If func is None, the function used will be the identity function. If func is set, inverse_func also needs to be provided. The function needs to return a 2-dimensional array. Function to apply to the prediction of the regressor. Cannot be set at the same time as transformer. The inverse function is used to return predictions to the same space of the original training labels. If inverse_func is set, func also needs to be provided. The inverse function needs to return a 2-dimensional array. Whether to check that transform followed by inverse_transform or func followed by inverse_func leads to the original targets. Fitted regressor. Transformer used in fit and predict. Number of features seen during fit. Names of features seen during fit. Defined only when X has feature names that are all strings. Added in version 1.0. See also Construct a transformer from an arbitrary callable. Notes Internally, the target y is always converted into a 2-dimensional array to be used by scikit-learn transformers. At the time of prediction, the output will be reshaped to a have the same number of dimensions as y. Examples For a more detailed example use case refer to Effect of transforming the targets in regression model. Fit the model according to the given training data. Training vector, where n_samples is the number of samples and n_features is the number of features. Target values. Parameters passed to the fit method of the underlying regressor. Fitted estimator. Raise NotImplementedError. This estimator does not support metadata routing yet. Get parameters for this estimator. If True, will return the parameters for this estimator and contained subobjects that are estimators. Parameter names mapped to their values. Number of features seen during fit. Predict using the base regressor, applying inverse. The regressor is used to predict and the inverse_func or inverse_transform is applied before returning the prediction. Samples. Parameters passed to the predict method of the underlying regressor. Predicted values. Return the coefficient of determination of the prediction. The coefficient of determination \\\\(R^2\\\\) is defined as \\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual sum of squares ((y_true - y_pred)** 2).sum() and \\\\(v\\\\) is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0. Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator. True values for X. Sample weights. \\\\(R^2\\\\) of self.predict(X) w.r.t. y. Notes The \\\\(R^2\\\\) score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of r2_score. This influences the score method of all the multioutput regressors (except for MultiOutputRegressor). Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object. Estimator parameters. Estimator instance. Request metadata passed to the score method. Note that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config). Please see User Guide on how the routing mechanism works. The options for each parameter are: True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to score. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a Pipeline. Otherwise it has no effect. Metadata routing for sample_weight parameter in score. The updated object.\", metadata={'full_function': 'class sklearn.compose.TransformedTargetRegressor(regressor=None, *, transformer=None, func=None, inverse_func=None, check_inverse=True)', 'function_calling': '{\\'name\\': \\'TransformedTargetRegressor\\', \\'descriptions\\': \"Meta-estimator to regress on a transformed target. Useful for applying a non-linear transformation to the target y in\\\\nregression problems. This transformation can be given as a Transformer\\\\nsuch as the QuantileTransformer or as a\\\\nfunction and its inverse such as np.log and np.exp. The computation during fit is: or: The computation during predict is: or: Read more in the User Guide. Added in version 0.20. Regressor object such as derived from\\\\nRegressorMixin. This regressor will\\\\nautomatically be cloned each time prior to fitting. If regressor is\\\\nNone, LinearRegression is created and used. Estimator object such as derived from\\\\nTransformerMixin. Cannot be set at the same time\\\\nas func and inverse_func. If transformer is None as well as\\\\nfunc and inverse_func, the transformer will be an identity\\\\ntransformer. Note that the transformer will be cloned during fitting.\\\\nAlso, the transformer is restricting y to be a numpy array. Function to apply to y before passing to fit. Cannot be set\\\\nat the same time as transformer. If func is None, the function used will be\\\\nthe identity function. If func is set, inverse_func also needs to be\\\\nprovided. The function needs to return a 2-dimensional array. Function to apply to the prediction of the regressor. Cannot be set at\\\\nthe same time as transformer. The inverse function is used to return\\\\npredictions to the same space of the original training labels. If\\\\ninverse_func is set, func also needs to be provided. The inverse\\\\nfunction needs to return a 2-dimensional array. Whether to check that transform followed by inverse_transform\\\\nor func followed by inverse_func leads to the original targets. Fitted regressor. Transformer used in fit and predict. Number of features seen during fit. Names of features seen during fit. Defined only when X\\\\nhas feature names that are all strings. Added in version 1.0. See also Construct a transformer from an arbitrary callable. Notes Internally, the target y is always converted into a 2-dimensional array\\\\nto be used by scikit-learn transformers. At the time of prediction, the\\\\noutput will be reshaped to a have the same number of dimensions as y. Examples For a more detailed example use case refer to\\\\nEffect of transforming the targets in regression model. Fit the model according to the given training data. Training vector, where n_samples is the number of samples and\\\\nn_features is the number of features. Target values. Parameters passed to the fit method of the underlying\\\\nregressor. Fitted estimator. Raise NotImplementedError. This estimator does not support metadata routing yet. Get parameters for this estimator. If True, will return the parameters for this estimator and\\\\ncontained subobjects that are estimators. Parameter names mapped to their values. Number of features seen during fit. Predict using the base regressor, applying inverse. The regressor is used to predict and the inverse_func or\\\\ninverse_transform is applied before returning the prediction. Samples. Parameters passed to the predict method of the underlying\\\\nregressor. Predicted values. Return the coefficient of determination of the prediction. The coefficient of determination \\\\\\\\(R^2\\\\\\\\) is defined as\\\\n\\\\\\\\((1 - \\\\\\\\frac{u}{v})\\\\\\\\), where \\\\\\\\(u\\\\\\\\) is the residual\\\\nsum of squares ((y_true - y_pred)** 2).sum() and \\\\\\\\(v\\\\\\\\)\\\\nis the total sum of squares ((y_true - y_true.mean()) ** 2).sum().\\\\nThe best possible score is 1.0 and it can be negative (because the\\\\nmodel can be arbitrarily worse). A constant model that always predicts\\\\nthe expected value of y, disregarding the input features, would get\\\\na \\\\\\\\(R^2\\\\\\\\) score of 0.0. Test samples. For some estimators this may be a precomputed\\\\nkernel matrix or a list of generic objects instead with shape\\\\n(n_samples, n_samples_fitted), where n_samples_fitted\\\\nis the number of samples used in the fitting for the estimator. True values for X. Sample weights. \\\\\\\\(R^2\\\\\\\\) of self.predict(X) w.r.t. y. Notes The \\\\\\\\(R^2\\\\\\\\) score used when calling score on a regressor uses\\\\nmultioutput=\\'uniform_average\\' from version 0.23 to keep consistent\\\\nwith default value of r2_score.\\\\nThis influences the score method of all the multioutput\\\\nregressors (except for\\\\nMultiOutputRegressor). Set the parameters of this estimator. The method works on simple estimators as well as on nested objects\\\\n(such as Pipeline). The latter have\\\\nparameters of the form <component>__<parameter> so that it’s\\\\npossible to update each component of a nested object. Estimator parameters. Estimator instance. Request metadata passed to the score method. Note that this method is only relevant if\\\\nenable_metadata_routing=True (see sklearn.set_config).\\\\nPlease see User Guide on how the routing\\\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to score. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\\\nexisting request. This allows you to change the request for some\\\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\\\nsub-estimator of a meta-estimator, e.g. used inside a\\\\nPipeline. Otherwise it has no effect. Metadata routing for sample_weight parameter in score. The updated object.\", \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'sample_weight\\': {\\'type\\': \\'string\\', \\'description\\': \\'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\\\n\\'}}, \\'required\\': [\\'regressor=None\\']}}', 'function_name': 'TransformedTargetRegressor', 'function_text': \"Meta-estimator to regress on a transformed target. Useful for applying a non-linear transformation to the target y in\\nregression problems. This transformation can be given as a Transformer\\nsuch as the QuantileTransformer or as a\\nfunction and its inverse such as np.log and np.exp. The computation during fit is: or: The computation during predict is: or: Read more in the User Guide. Added in version 0.20. Regressor object such as derived from\\nRegressorMixin. This regressor will\\nautomatically be cloned each time prior to fitting. If regressor is\\nNone, LinearRegression is created and used. Estimator object such as derived from\\nTransformerMixin. Cannot be set at the same time\\nas func and inverse_func. If transformer is None as well as\\nfunc and inverse_func, the transformer will be an identity\\ntransformer. Note that the transformer will be cloned during fitting.\\nAlso, the transformer is restricting y to be a numpy array. Function to apply to y before passing to fit. Cannot be set\\nat the same time as transformer. If func is None, the function used will be\\nthe identity function. If func is set, inverse_func also needs to be\\nprovided. The function needs to return a 2-dimensional array. Function to apply to the prediction of the regressor. Cannot be set at\\nthe same time as transformer. The inverse function is used to return\\npredictions to the same space of the original training labels. If\\ninverse_func is set, func also needs to be provided. The inverse\\nfunction needs to return a 2-dimensional array. Whether to check that transform followed by inverse_transform\\nor func followed by inverse_func leads to the original targets. Fitted regressor. Transformer used in fit and predict. Number of features seen during fit. Names of features seen during fit. Defined only when X\\nhas feature names that are all strings. Added in version 1.0. See also Construct a transformer from an arbitrary callable. Notes Internally, the target y is always converted into a 2-dimensional array\\nto be used by scikit-learn transformers. At the time of prediction, the\\noutput will be reshaped to a have the same number of dimensions as y. Examples For a more detailed example use case refer to\\nEffect of transforming the targets in regression model. Fit the model according to the given training data. Training vector, where n_samples is the number of samples and\\nn_features is the number of features. Target values. Parameters passed to the fit method of the underlying\\nregressor. Fitted estimator. Raise NotImplementedError. This estimator does not support metadata routing yet. Get parameters for this estimator. If True, will return the parameters for this estimator and\\ncontained subobjects that are estimators. Parameter names mapped to their values. Number of features seen during fit. Predict using the base regressor, applying inverse. The regressor is used to predict and the inverse_func or\\ninverse_transform is applied before returning the prediction. Samples. Parameters passed to the predict method of the underlying\\nregressor. Predicted values. Return the coefficient of determination of the prediction. The coefficient of determination \\\\(R^2\\\\) is defined as\\n\\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual\\nsum of squares ((y_true - y_pred)** 2).sum() and \\\\(v\\\\)\\nis the total sum of squares ((y_true - y_true.mean()) ** 2).sum().\\nThe best possible score is 1.0 and it can be negative (because the\\nmodel can be arbitrarily worse). A constant model that always predicts\\nthe expected value of y, disregarding the input features, would get\\na \\\\(R^2\\\\) score of 0.0. Test samples. For some estimators this may be a precomputed\\nkernel matrix or a list of generic objects instead with shape\\n(n_samples, n_samples_fitted), where n_samples_fitted\\nis the number of samples used in the fitting for the estimator. True values for X. Sample weights. \\\\(R^2\\\\) of self.predict(X) w.r.t. y. Notes The \\\\(R^2\\\\) score used when calling score on a regressor uses\\nmultioutput='uniform_average' from version 0.23 to keep consistent\\nwith default value of r2_score.\\nThis influences the score method of all the multioutput\\nregressors (except for\\nMultiOutputRegressor). Set the parameters of this estimator. The method works on simple estimators as well as on nested objects\\n(such as Pipeline). The latter have\\nparameters of the form <component>__<parameter> so that it’s\\npossible to update each component of a nested object. Estimator parameters. Estimator instance. Request metadata passed to the score method. Note that this method is only relevant if\\nenable_metadata_routing=True (see sklearn.set_config).\\nPlease see User Guide on how the routing\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to score. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\nexisting request. This allows you to change the request for some\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\nsub-estimator of a meta-estimator, e.g. used inside a\\nPipeline. Otherwise it has no effect. Metadata routing for sample_weight parameter in score. The updated object.\", 'name': 'TransformedTargetRegressor', 'parameter_names_desc': \"[{'param_name': 'sample_weight', 'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED', 'param_desc': 'Metadata routing for sample_weight parameter in score.\\\\n'}]\", 'trail': 'sklearn.compose-->defaults', 'type': 'function_node', 'url': 'https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html#sklearn.compose.TransformedTargetRegressor'}),\n",
       " Document(page_content=\"Construct a ColumnTransformer from the given transformers. This is a shorthand for the ColumnTransformer constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting with transformer_weights. Read more in the User Guide. Tuples of the form (transformer, columns) specifying the transformer objects to be applied to subsets of the data. Estimator must support fit and transform. Special-cased strings ‘drop’ and ‘passthrough’ are accepted as well, to indicate to drop the columns or to pass them through untransformed, respectively. Indexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where transformer expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data X and can return any of the above. To select multiple columns by name or dtype, you can use make_column_selector. By default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of 'drop'). By specifying remainder='passthrough', all remaining columns that were not specified in transformers will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting remainder to be an estimator, the remaining non-specified columns will use the remainder estimator. The estimator must support fit and transform. If the transformed output consists of a mix of sparse and dense data, it will be stacked as a sparse matrix if the density is lower than this value. Use sparse_threshold=0 to always return dense. When the transformed output consists of all sparse or all dense data, the stacked result will be sparse or dense, respectively, and this keyword will be ignored. Number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details. If True, the time elapsed while fitting each transformer will be printed as it is completed. If True, ColumnTransformer.get_feature_names_out will prefix all feature names with the name of the transformer that generated that feature. If False, ColumnTransformer.get_feature_names_out will not prefix any feature names and will error if feature names are not unique. Added in version 1.0. Force the columns of the last entry of transformers_, which corresponds to the “remainder” transformer, to always be stored as indices (int) rather than column names (str). See description of the ColumnTransformer.transformers_ attribute for details. Note If you do not access the list of columns for the remainder columns in the ColumnTransformer.transformers_ fitted attribute, you do not need to set this parameter. Added in version 1.5. Changed in version 1.7: The default value for force_int_remainder_cols will change from True to False in version 1.7. Returns a ColumnTransformer object. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space. Examples\", metadata={'full_function': \"sklearn.compose.make_column_transformer(*transformers, remainder='drop', sparse_threshold=0.3, n_jobs=None, verbose=False, verbose_feature_names_out=True, force_int_remainder_cols=True)\", 'function_calling': '{\\'name\\': \\'make_column_transformer\\', \\'descriptions\\': \"Construct a ColumnTransformer from the given transformers. This is a shorthand for the ColumnTransformer constructor; it does not\\\\nrequire, and does not permit, naming the transformers. Instead, they will\\\\nbe given names automatically based on their types. It also does not allow\\\\nweighting with transformer_weights. Read more in the User Guide. Tuples of the form (transformer, columns) specifying the\\\\ntransformer objects to be applied to subsets of the data. Estimator must support fit and transform.\\\\nSpecial-cased strings ‘drop’ and ‘passthrough’ are accepted as\\\\nwell, to indicate to drop the columns or to pass them through\\\\nuntransformed, respectively. Indexes the data on its second axis. Integers are interpreted as\\\\npositional columns, while strings can reference DataFrame columns\\\\nby name. A scalar string or int should be used where\\\\ntransformer expects X to be a 1d array-like (vector),\\\\notherwise a 2d array will be passed to the transformer.\\\\nA callable is passed the input data X and can return any of the\\\\nabove. To select multiple columns by name or dtype, you can use\\\\nmake_column_selector. By default, only the specified columns in transformers are\\\\ntransformed and combined in the output, and the non-specified\\\\ncolumns are dropped. (default of \\'drop\\').\\\\nBy specifying remainder=\\'passthrough\\', all remaining columns that\\\\nwere not specified in transformers will be automatically passed\\\\nthrough. This subset of columns is concatenated with the output of\\\\nthe transformers.\\\\nBy setting remainder to be an estimator, the remaining\\\\nnon-specified columns will use the remainder estimator. The\\\\nestimator must support fit and transform. If the transformed output consists of a mix of sparse and dense data,\\\\nit will be stacked as a sparse matrix if the density is lower than this\\\\nvalue. Use sparse_threshold=0 to always return dense.\\\\nWhen the transformed output consists of all sparse or all dense data,\\\\nthe stacked result will be sparse or dense, respectively, and this\\\\nkeyword will be ignored. Number of jobs to run in parallel.\\\\nNone means 1 unless in a joblib.parallel_backend context.\\\\n-1 means using all processors. See Glossary\\\\nfor more details. If True, the time elapsed while fitting each transformer will be\\\\nprinted as it is completed. If True, ColumnTransformer.get_feature_names_out will prefix\\\\nall feature names with the name of the transformer that generated that\\\\nfeature.\\\\nIf False, ColumnTransformer.get_feature_names_out will not\\\\nprefix any feature names and will error if feature names are not\\\\nunique. Added in version 1.0. Force the columns of the last entry of transformers_, which\\\\ncorresponds to the “remainder” transformer, to always be stored as\\\\nindices (int) rather than column names (str). See description of the\\\\nColumnTransformer.transformers_ attribute for details. Note If you do not access the list of columns for the remainder columns\\\\nin the ColumnTransformer.transformers_ fitted attribute,\\\\nyou do not need to set this parameter. Added in version 1.5. Changed in version 1.7: The default value for force_int_remainder_cols will change from\\\\nTrue to False in version 1.7. Returns a ColumnTransformer object. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space. Examples\", \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'*transformers\\': {\\'type\\': \\'tuples\\', \\'description\\': \\'tuples. Tuples of the form (transformer, columns) specifying the\\\\ntransformer objects to be applied to subsets of the data.\\\\n\\\\ntransformer{‘drop’, ‘passthrough’} or estimatorEstimator must support fit and transform.\\\\nSpecial-cased strings ‘drop’ and ‘passthrough’ are accepted as\\\\nwell, to indicate to drop the columns or to pass them through\\\\nuntransformed, respectively.\\\\n\\\\ncolumnsstr,  array-like of str, int, array-like of int, slice,                 array-like of bool or callableIndexes the data on its second axis. Integers are interpreted as\\\\npositional columns, while strings can reference DataFrame columns\\\\nby name. A scalar string or int should be used where\\\\ntransformer expects X to be a 1d array-like (vector),\\\\notherwise a 2d array will be passed to the transformer.\\\\nA callable is passed the input data X and can return any of the\\\\nabove. To select multiple columns by name or dtype, you can use\\\\nmake_column_selector.\\\\n\\\\n\\\\n\\'}, \\'remainder\\': {\\'type\\': \\'string\\', \\'enum\\': [\\'drop\\', \\' passthrough\\'], \\'description\\': \"{‘drop’, ‘passthrough’} or estimator, default=’drop’. By default, only the specified columns in transformers are\\\\ntransformed and combined in the output, and the non-specified\\\\ncolumns are dropped. (default of \\'drop\\').\\\\nBy specifying remainder=\\'passthrough\\', all remaining columns that\\\\nwere not specified in transformers will be automatically passed\\\\nthrough. This subset of columns is concatenated with the output of\\\\nthe transformers.\\\\nBy setting remainder to be an estimator, the remaining\\\\nnon-specified columns will use the remainder estimator. The\\\\nestimator must support fit and transform.\\\\n\"}, \\'sparse_threshold\\': {\\'type\\': \\'float\\', \\'description\\': \\'float, default=0.3. If the transformed output consists of a mix of sparse and dense data,\\\\nit will be stacked as a sparse matrix if the density is lower than this\\\\nvalue. Use sparse_threshold=0 to always return dense.\\\\nWhen the transformed output consists of all sparse or all dense data,\\\\nthe stacked result will be sparse or dense, respectively, and this\\\\nkeyword will be ignored.\\\\n\\'}, \\'n_jobs\\': {\\'type\\': \\'integer\\', \\'description\\': \\'int, default=None. Number of jobs to run in parallel.\\\\nNone means 1 unless in a joblib.parallel_backend context.\\\\n-1 means using all processors. See Glossary\\\\nfor more details.\\\\n\\'}, \\'verbose\\': {\\'type\\': \\'boolean\\', \\'description\\': \\'bool, default=False. If True, the time elapsed while fitting each transformer will be\\\\nprinted as it is completed.\\\\n\\'}, \\'verbose_feature_names_out\\': {\\'type\\': \\'boolean\\', \\'description\\': \\'bool, default=True. If True, ColumnTransformer.get_feature_names_out will prefix\\\\nall feature names with the name of the transformer that generated that\\\\nfeature.\\\\nIf False, ColumnTransformer.get_feature_names_out will not\\\\nprefix any feature names and will error if feature names are not\\\\nunique.\\\\n\\\\nAdded in version 1.0.\\\\n\\\\n\\'}, \\'force_int_remainder_cols\\': {\\'type\\': \\'boolean\\', \\'description\\': \\'bool, default=True. Force the columns of the last entry of transformers_, which\\\\ncorresponds to the “remainder” transformer, to always be stored as\\\\nindices (int) rather than column names (str). See description of the\\\\nColumnTransformer.transformers_ attribute for details.\\\\n\\\\nNote\\\\nIf you do not access the list of columns for the remainder columns\\\\nin the ColumnTransformer.transformers_ fitted attribute,\\\\nyou do not need to set this parameter.\\\\n\\\\n\\\\nAdded in version 1.5.\\\\n\\\\n\\\\nChanged in version 1.7: The default value for force_int_remainder_cols will change from\\\\nTrue to False in version 1.7.\\\\n\\\\n\\'}}, \\'required\\': []}}', 'function_name': 'make_column_transformer', 'function_text': \"Construct a ColumnTransformer from the given transformers. This is a shorthand for the ColumnTransformer constructor; it does not\\nrequire, and does not permit, naming the transformers. Instead, they will\\nbe given names automatically based on their types. It also does not allow\\nweighting with transformer_weights. Read more in the User Guide. Tuples of the form (transformer, columns) specifying the\\ntransformer objects to be applied to subsets of the data. Estimator must support fit and transform.\\nSpecial-cased strings ‘drop’ and ‘passthrough’ are accepted as\\nwell, to indicate to drop the columns or to pass them through\\nuntransformed, respectively. Indexes the data on its second axis. Integers are interpreted as\\npositional columns, while strings can reference DataFrame columns\\nby name. A scalar string or int should be used where\\ntransformer expects X to be a 1d array-like (vector),\\notherwise a 2d array will be passed to the transformer.\\nA callable is passed the input data X and can return any of the\\nabove. To select multiple columns by name or dtype, you can use\\nmake_column_selector. By default, only the specified columns in transformers are\\ntransformed and combined in the output, and the non-specified\\ncolumns are dropped. (default of 'drop').\\nBy specifying remainder='passthrough', all remaining columns that\\nwere not specified in transformers will be automatically passed\\nthrough. This subset of columns is concatenated with the output of\\nthe transformers.\\nBy setting remainder to be an estimator, the remaining\\nnon-specified columns will use the remainder estimator. The\\nestimator must support fit and transform. If the transformed output consists of a mix of sparse and dense data,\\nit will be stacked as a sparse matrix if the density is lower than this\\nvalue. Use sparse_threshold=0 to always return dense.\\nWhen the transformed output consists of all sparse or all dense data,\\nthe stacked result will be sparse or dense, respectively, and this\\nkeyword will be ignored. Number of jobs to run in parallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details. If True, the time elapsed while fitting each transformer will be\\nprinted as it is completed. If True, ColumnTransformer.get_feature_names_out will prefix\\nall feature names with the name of the transformer that generated that\\nfeature.\\nIf False, ColumnTransformer.get_feature_names_out will not\\nprefix any feature names and will error if feature names are not\\nunique. Added in version 1.0. Force the columns of the last entry of transformers_, which\\ncorresponds to the “remainder” transformer, to always be stored as\\nindices (int) rather than column names (str). See description of the\\nColumnTransformer.transformers_ attribute for details. Note If you do not access the list of columns for the remainder columns\\nin the ColumnTransformer.transformers_ fitted attribute,\\nyou do not need to set this parameter. Added in version 1.5. Changed in version 1.7: The default value for force_int_remainder_cols will change from\\nTrue to False in version 1.7. Returns a ColumnTransformer object. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space. Examples\", 'name': 'make_column_transformer', 'parameter_names_desc': '[{\\'param_name\\': \\'*transformers\\', \\'param_type\\': \\'tuples\\', \\'param_desc\\': \\'Tuples of the form (transformer, columns) specifying the\\\\ntransformer objects to be applied to subsets of the data.\\\\n\\\\ntransformer{‘drop’, ‘passthrough’} or estimatorEstimator must support fit and transform.\\\\nSpecial-cased strings ‘drop’ and ‘passthrough’ are accepted as\\\\nwell, to indicate to drop the columns or to pass them through\\\\nuntransformed, respectively.\\\\n\\\\ncolumnsstr,  array-like of str, int, array-like of int, slice,                 array-like of bool or callableIndexes the data on its second axis. Integers are interpreted as\\\\npositional columns, while strings can reference DataFrame columns\\\\nby name. A scalar string or int should be used where\\\\ntransformer expects X to be a 1d array-like (vector),\\\\notherwise a 2d array will be passed to the transformer.\\\\nA callable is passed the input data X and can return any of the\\\\nabove. To select multiple columns by name or dtype, you can use\\\\nmake_column_selector.\\\\n\\\\n\\\\n\\'}, {\\'param_name\\': \\'remainder\\', \\'param_type\\': \\'{‘drop’, ‘passthrough’} or estimator, default=’drop’\\', \\'param_desc\\': \"By default, only the specified columns in transformers are\\\\ntransformed and combined in the output, and the non-specified\\\\ncolumns are dropped. (default of \\'drop\\').\\\\nBy specifying remainder=\\'passthrough\\', all remaining columns that\\\\nwere not specified in transformers will be automatically passed\\\\nthrough. This subset of columns is concatenated with the output of\\\\nthe transformers.\\\\nBy setting remainder to be an estimator, the remaining\\\\nnon-specified columns will use the remainder estimator. The\\\\nestimator must support fit and transform.\\\\n\"}, {\\'param_name\\': \\'sparse_threshold\\', \\'param_type\\': \\'float, default=0.3\\', \\'param_desc\\': \\'If the transformed output consists of a mix of sparse and dense data,\\\\nit will be stacked as a sparse matrix if the density is lower than this\\\\nvalue. Use sparse_threshold=0 to always return dense.\\\\nWhen the transformed output consists of all sparse or all dense data,\\\\nthe stacked result will be sparse or dense, respectively, and this\\\\nkeyword will be ignored.\\\\n\\'}, {\\'param_name\\': \\'n_jobs\\', \\'param_type\\': \\'int, default=None\\', \\'param_desc\\': \\'Number of jobs to run in parallel.\\\\nNone means 1 unless in a joblib.parallel_backend context.\\\\n-1 means using all processors. See Glossary\\\\nfor more details.\\\\n\\'}, {\\'param_name\\': \\'verbose\\', \\'param_type\\': \\'bool, default=False\\', \\'param_desc\\': \\'If True, the time elapsed while fitting each transformer will be\\\\nprinted as it is completed.\\\\n\\'}, {\\'param_name\\': \\'verbose_feature_names_out\\', \\'param_type\\': \\'bool, default=True\\', \\'param_desc\\': \\'If True, ColumnTransformer.get_feature_names_out will prefix\\\\nall feature names with the name of the transformer that generated that\\\\nfeature.\\\\nIf False, ColumnTransformer.get_feature_names_out will not\\\\nprefix any feature names and will error if feature names are not\\\\nunique.\\\\n\\\\nAdded in version 1.0.\\\\n\\\\n\\'}, {\\'param_name\\': \\'force_int_remainder_cols\\', \\'param_type\\': \\'bool, default=True\\', \\'param_desc\\': \\'Force the columns of the last entry of transformers_, which\\\\ncorresponds to the “remainder” transformer, to always be stored as\\\\nindices (int) rather than column names (str). See description of the\\\\nColumnTransformer.transformers_ attribute for details.\\\\n\\\\nNote\\\\nIf you do not access the list of columns for the remainder columns\\\\nin the ColumnTransformer.transformers_ fitted attribute,\\\\nyou do not need to set this parameter.\\\\n\\\\n\\\\nAdded in version 1.5.\\\\n\\\\n\\\\nChanged in version 1.7: The default value for force_int_remainder_cols will change from\\\\nTrue to False in version 1.7.\\\\n\\\\n\\'}]', 'trail': 'sklearn.compose-->defaults', 'type': 'function_node', 'url': 'https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html#sklearn.compose.make_column_transformer'})]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
