{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.scrape import scrape_sklearn_website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn_data = scrape_sklearn_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/sklearn_function_openai.json\",\"r\") as jsonfile:\n",
    "    sklearn_data = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.utils import build_no_summary_graph,get_parents_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_graph = build_no_summary_graph(sklearn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_dict = get_parents_dict(sklearn_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K-fold iterator variant with non-overlapping groups. Each group will appear exactly once in the test set across all folds (the\\nnumber of distinct groups has to be at least equal to the number of folds). The folds are approximately balanced in the sense that the number of\\nsamples is approximately the same in each test fold. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of folds. Must be at least 2. Changed in version 0.22: n_splits default value changed from 3 to 5. See also For splitting the data according to explicit domain-specific stratification of the dataset. Takes class information into account to avoid building folds with imbalanced class proportions (for binary or multiclass classification tasks). Notes Groups appear in an arbitrary order throughout the folds.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Request metadata passed to the split method. Note that this method is only relevant if\\nenable_metadata_routing=True (see sklearn.set_config).\\nPlease see User Guide on how the routing\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to split if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to split. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\nexisting request. This allows you to change the request for some\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\nsub-estimator of a meta-estimator, e.g. used inside a\\nPipeline. Otherwise it has no effect. Metadata routing for groups parameter in split. The updated object. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Group labels for the samples used while splitting the dataset into\\ntrain/test set. The training set indices for that split. The testing set indices for that split.',\n",
       " 'K-Fold cross-validator. Provides train/test indices to split data in train/test sets. Split\\ndataset into k consecutive folds (without shuffling by default). Each fold is then used once as a validation while the k - 1 remaining\\nfolds form the training set. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of folds. Must be at least 2. Changed in version 0.22: n_splits default value changed from 3 to 5. Whether to shuffle the data before splitting into batches.\\nNote that the samples within each split will not be shuffled. When shuffle is True, random_state affects the ordering of the\\nindices, which controls the randomness of each fold. Otherwise, this\\nparameter has no effect.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. See also Takes class information into account to avoid building folds with imbalanced class distributions (for binary or multiclass classification tasks). K-fold iterator variant with non-overlapping groups. Repeats K-Fold n times. Notes The first n_samples % n_splits folds have size\\nn_samples // n_splits + 1, other folds have size\\nn_samples // n_splits, where n_samples is the number of samples. Randomized CV splitters may return different results for each call of\\nsplit. You can make the results identical by setting random_state\\nto an integer.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Leave-One-Out cross-validator. Provides train/test indices to split data in train/test sets. Each\\nsample is used once as a test set (singleton) while the remaining\\nsamples form the training set. Note: LeaveOneOut() is equivalent to KFold(n_splits=n) and\\nLeavePOut(p=1) where n is the number of samples. Due to the high number of test sets (which is the same as the\\nnumber of samples) this cross-validation method can be very costly.\\nFor large datasets one should favor KFold, ShuffleSplit\\nor StratifiedKFold. Read more in the User Guide. See also For splitting the data according to explicit, domain-specific stratification of the dataset. K-fold iterator variant with non-overlapping groups.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Training data, where n_samples is the number of samples\\nand n_features is the number of features. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Leave-P-Out cross-validator. Provides train/test indices to split data in train/test sets. This results\\nin testing on all distinct samples of size p, while the remaining n - p\\nsamples form the training set in each iteration. Note: LeavePOut(p) is NOT equivalent to\\nKFold(n_splits=n_samples // p) which creates non-overlapping test sets. Due to the high number of iterations which grows combinatorically with the\\nnumber of samples this cross-validation method can be very costly. For\\nlarge datasets one should favor KFold, StratifiedKFold\\nor ShuffleSplit. Read more in the User Guide. Size of the test sets. Must be strictly less than the number of\\nsamples.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Training data, where n_samples is the number of samples\\nand n_features is the number of features. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Repeated K-Fold cross validator. Repeats K-Fold n times with different randomization in each repetition. Read more in the User Guide. Number of folds. Must be at least 2. Number of times cross-validator needs to be repeated. Controls the randomness of each repeated cross-validation instance.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. See also Repeats Stratified K-Fold n times. Notes Randomized CV splitters may return different results for each call of\\nsplit. You can make the results identical by setting random_state\\nto an integer.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility.\\nnp.zeros(n_samples) may be used as a placeholder. Always ignored, exists for compatibility.\\nnp.zeros(n_samples) may be used as a placeholder. Group labels for the samples used while splitting the dataset into\\ntrain/test set. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Random permutation cross-validator. Yields indices to split data into training and test sets. Note: contrary to other cross-validation strategies, random splits\\ndo not guarantee that all folds will be different, although this is\\nstill very likely for sizeable datasets. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of re-shuffling & splitting iterations. If float, should be between 0.0 and 1.0 and represent the proportion\\nof the dataset to include in the test split. If int, represents the\\nabsolute number of test samples. If None, the value is set to the\\ncomplement of the train size. If train_size is also None, it will\\nbe set to 0.1. If float, should be between 0.0 and 1.0 and represent the\\nproportion of the dataset to include in the train split. If\\nint, represents the absolute number of train samples. If None,\\nthe value is automatically set to the complement of the test size. Controls the randomness of the training and testing indices produced.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Stratified K-Fold cross-validator. Provides train/test indices to split data in train/test sets. This cross-validation object is a variation of KFold that returns\\nstratified folds. The folds are made by preserving the percentage of\\nsamples for each class. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of folds. Must be at least 2. Changed in version 0.22: n_splits default value changed from 3 to 5. Whether to shuffle each class’s samples before splitting into batches.\\nNote that the samples within each split will not be shuffled. When shuffle is True, random_state affects the ordering of the\\nindices, which controls the randomness of each fold for each class.\\nOtherwise, leave random_state as None.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. See also Repeats Stratified K-Fold n times. Notes The implementation is designed to: Generate test sets such that all contain the same distribution of\\nclasses, or as close as possible. Be invariant to class label: relabelling y = [\"Happy\", \"Sad\"] to\\ny = [1, 0] should not change the indices generated. Preserve order dependencies in the dataset ordering, when\\nshuffle=False: all samples from class k in some test set were\\ncontiguous in y, or separated in y by samples from classes other than k. Generate test sets where the smallest and largest differ by at most one\\nsample. Changed in version 0.22: The previous implementation did not follow the last constraint.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. Note that providing y is sufficient to generate the splits and\\nhence np.zeros(n_samples) may be used as a placeholder for\\nX instead of actual training data. The target variable for supervised learning problems.\\nStratification is done based on the y labels. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split. Notes Randomized CV splitters may return different results for each call of\\nsplit. You can make the results identical by setting random_state\\nto an integer.',\n",
       " 'Time Series cross-validator. Provides train/test indices to split time series data samples\\nthat are observed at fixed time intervals, in train/test sets.\\nIn each split, test indices must be higher than before, and thus shuffling\\nin cross validator is inappropriate. This cross-validation object is a variation of KFold.\\nIn the kth split, it returns first k folds as train set and the\\n(k+1)th fold as test set. Note that unlike standard cross-validation methods, successive\\ntraining sets are supersets of those that come before them. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Added in version 0.18. Number of splits. Must be at least 2. Changed in version 0.22: n_splits default value changed from 3 to 5. Maximum size for a single training set. Used to limit the size of the test set. Defaults to\\nn_samples // (n_splits + 1), which is the maximum allowed value\\nwith gap=0. Added in version 0.24. Number of samples to exclude from the end of each train set before\\nthe test set. Added in version 0.24. Notes The training set has size i * n_samples // (n_splits + 1)\\n+ n_samples % (n_splits + 1) in the i th split,\\nwith a test set of size n_samples//(n_splits + 1) by default,\\nwhere n_samples is the number of samples.  For a more extended example see\\nTime-related feature engineering. Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. Always ignored, exists for compatibility. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Split arrays or matrices into random train and test subsets. Quick utility that wraps input validation,\\nnext(ShuffleSplit().split(X, y)), and application to input data\\ninto a single call for splitting (and optionally subsampling) data into a\\none-liner. Read more in the User Guide. Allowed inputs are lists, numpy arrays, scipy-sparse\\nmatrices or pandas dataframes. If float, should be between 0.0 and 1.0 and represent the proportion\\nof the dataset to include in the test split. If int, represents the\\nabsolute number of test samples. If None, the value is set to the\\ncomplement of the train size. If train_size is also None, it will\\nbe set to 0.25. If float, should be between 0.0 and 1.0 and represent the\\nproportion of the dataset to include in the train split. If\\nint, represents the absolute number of train samples. If None,\\nthe value is automatically set to the complement of the test size. Controls the shuffling applied to the data before applying the split.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. Whether or not to shuffle the data before splitting. If shuffle=False\\nthen stratify must be None. If not None, data is split in a stratified fashion, using this as\\nthe class labels.\\nRead more in the User Guide. List containing train-test split of inputs. Added in version 0.16: If the input is sparse, the output will be a\\nscipy.sparse.csr_matrix. Else, output type is the same as the\\ninput type.',\n",
       " 'Shuffle-Group(s)-Out cross-validation iterator. Provides randomized train/test indices to split data according to a\\nthird-party provided group. This group information can be used to encode\\narbitrary domain specific stratifications of the samples as integers. For instance the groups could be the year of collection of the samples\\nand thus allow for cross-validation against time-based splits. The difference between LeavePGroupsOut and GroupShuffleSplit is that\\nthe former generates splits using all subsets of size p unique groups,\\nwhereas GroupShuffleSplit generates a user-determined number of random\\ntest splits, each with a user-determined fraction of unique groups. For example, a less computationally intensive alternative to\\nLeavePGroupsOut(p=10) would be\\nGroupShuffleSplit(test_size=10, n_splits=100). Note: The parameters test_size and train_size refer to groups, and\\nnot to samples, as in ShuffleSplit. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of re-shuffling & splitting iterations. If float, should be between 0.0 and 1.0 and represent the proportion\\nof groups to include in the test split (rounded up). If int,\\nrepresents the absolute number of test groups. If None, the value is\\nset to the complement of the train size.\\nThe default will change in version 0.21. It will remain 0.2 only\\nif train_size is unspecified, otherwise it will complement\\nthe specified train_size. If float, should be between 0.0 and 1.0 and represent the\\nproportion of the groups to include in the train split. If\\nint, represents the absolute number of train groups. If None,\\nthe value is automatically set to the complement of the test size. Controls the randomness of the training and testing indices produced.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. See also Shuffles samples to create independent test/train sets. Train set leaves out all possible subsets of p groups.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Request metadata passed to the split method. Note that this method is only relevant if\\nenable_metadata_routing=True (see sklearn.set_config).\\nPlease see User Guide on how the routing\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to split if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to split. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\nexisting request. This allows you to change the request for some\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\nsub-estimator of a meta-estimator, e.g. used inside a\\nPipeline. Otherwise it has no effect. Metadata routing for groups parameter in split. The updated object. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Group labels for the samples used while splitting the dataset into\\ntrain/test set. The training set indices for that split. The testing set indices for that split. Notes Randomized CV splitters may return different results for each call of\\nsplit. You can make the results identical by setting random_state\\nto an integer.',\n",
       " 'Leave One Group Out cross-validator. Provides train/test indices to split data such that each training set is\\ncomprised of all samples except ones belonging to one specific group.\\nArbitrary domain specific group information is provided an array integers\\nthat encodes the group of each sample. For instance the groups could be the year of collection of the samples\\nand thus allow for cross-validation against time-based splits. Read more in the User Guide. See also K-fold iterator variant with non-overlapping groups. Notes Splits are ordered according to the index of the group left out. The first\\nsplit has testing set consisting of the group whose index in groups is\\nlowest, and so on.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Group labels for the samples used while splitting the dataset into\\ntrain/test set. This ‘groups’ parameter must always be specified to\\ncalculate the number of splits, though the other parameters can be\\nomitted. Returns the number of splitting iterations in the cross-validator. Request metadata passed to the split method. Note that this method is only relevant if\\nenable_metadata_routing=True (see sklearn.set_config).\\nPlease see User Guide on how the routing\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to split if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to split. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\nexisting request. This allows you to change the request for some\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\nsub-estimator of a meta-estimator, e.g. used inside a\\nPipeline. Otherwise it has no effect. Metadata routing for groups parameter in split. The updated object. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Group labels for the samples used while splitting the dataset into\\ntrain/test set. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Leave P Group(s) Out cross-validator. Provides train/test indices to split data according to a third-party\\nprovided group. This group information can be used to encode arbitrary\\ndomain specific stratifications of the samples as integers. For instance the groups could be the year of collection of the samples\\nand thus allow for cross-validation against time-based splits. The difference between LeavePGroupsOut and LeaveOneGroupOut is that\\nthe former builds the test sets with all the samples assigned to\\np different values of the groups while the latter uses samples\\nall assigned the same groups. Read more in the User Guide. Number of groups (p) to leave out in the test split. See also K-fold iterator variant with non-overlapping groups.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Group labels for the samples used while splitting the dataset into\\ntrain/test set. This ‘groups’ parameter must always be specified to\\ncalculate the number of splits, though the other parameters can be\\nomitted. Returns the number of splitting iterations in the cross-validator. Request metadata passed to the split method. Note that this method is only relevant if\\nenable_metadata_routing=True (see sklearn.set_config).\\nPlease see User Guide on how the routing\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to split if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to split. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\nexisting request. This allows you to change the request for some\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\nsub-estimator of a meta-estimator, e.g. used inside a\\nPipeline. Otherwise it has no effect. Metadata routing for groups parameter in split. The updated object. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Group labels for the samples used while splitting the dataset into\\ntrain/test set. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Predefined split cross-validator. Provides train/test indices to split data into train/test sets using a\\npredefined scheme specified by the user with the test_fold parameter. Read more in the User Guide. Added in version 0.16. The entry test_fold[i] represents the index of the test set that\\nsample i belongs to. It is possible to exclude sample i from\\nany test set (i.e. include sample i in every training set) by\\nsetting test_fold[i] equal to -1.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Repeated Stratified K-Fold cross validator. Repeats Stratified K-Fold n times with different randomization in each\\nrepetition. Read more in the User Guide. Number of folds. Must be at least 2. Number of times cross-validator needs to be repeated. Controls the generation of the random states for each repetition.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. See also Repeats K-Fold n times. Notes Randomized CV splitters may return different results for each call of\\nsplit. You can make the results identical by setting random_state\\nto an integer.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility.\\nnp.zeros(n_samples) may be used as a placeholder. Always ignored, exists for compatibility.\\nnp.zeros(n_samples) may be used as a placeholder. Group labels for the samples used while splitting the dataset into\\ntrain/test set. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Stratified K-Fold iterator variant with non-overlapping groups. This cross-validation object is a variation of StratifiedKFold attempts to\\nreturn stratified folds with non-overlapping groups. The folds are made by\\npreserving the percentage of samples for each class. Each group will appear exactly once in the test set across all folds (the\\nnumber of distinct groups has to be at least equal to the number of folds). The difference between GroupKFold\\nand StratifiedGroupKFold is that\\nthe former attempts to create balanced folds such that the number of\\ndistinct groups is approximately the same in each fold, whereas\\nStratifiedGroupKFold attempts to create folds which preserve the\\npercentage of samples for each class as much as possible given the\\nconstraint of non-overlapping groups between splits. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of folds. Must be at least 2. Whether to shuffle each class’s samples before splitting into batches.\\nNote that the samples within each split will not be shuffled.\\nThis implementation can only shuffle groups that have approximately the\\nsame y distribution, no global shuffle will be performed. When shuffle is True, random_state affects the ordering of the\\nindices, which controls the randomness of each fold for each class.\\nOtherwise, leave random_state as None.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. See also Takes class information into account to build folds which retain class distributions (for binary or multiclass classification tasks). K-fold iterator variant with non-overlapping groups. Notes The implementation is designed to: Mimic the behavior of StratifiedKFold as much as possible for trivial\\ngroups (e.g. when each group contains only one sample). Be invariant to class label: relabelling y = [\"Happy\", \"Sad\"] to\\ny = [1, 0] should not change the indices generated. Stratify based on samples as much as possible while keeping\\nnon-overlapping groups constraint. That means that in some cases when\\nthere is a small number of groups containing a large number of samples\\nthe stratification will not be possible and the behavior will be close\\nto GroupKFold.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Request metadata passed to the split method. Note that this method is only relevant if\\nenable_metadata_routing=True (see sklearn.set_config).\\nPlease see User Guide on how the routing\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to split if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to split. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\nexisting request. This allows you to change the request for some\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\nsub-estimator of a meta-estimator, e.g. used inside a\\nPipeline. Otherwise it has no effect. Metadata routing for groups parameter in split. The updated object. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Group labels for the samples used while splitting the dataset into\\ntrain/test set. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Stratified ShuffleSplit cross-validator. Provides train/test indices to split data in train/test sets. This cross-validation object is a merge of StratifiedKFold and\\nShuffleSplit, which returns stratified randomized folds. The folds\\nare made by preserving the percentage of samples for each class. Note: like the ShuffleSplit strategy, stratified random splits\\ndo not guarantee that all folds will be different, although this is\\nstill very likely for sizeable datasets. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of re-shuffling & splitting iterations. If float, should be between 0.0 and 1.0 and represent the proportion\\nof the dataset to include in the test split. If int, represents the\\nabsolute number of test samples. If None, the value is set to the\\ncomplement of the train size. If train_size is also None, it will\\nbe set to 0.1. If float, should be between 0.0 and 1.0 and represent the\\nproportion of the dataset to include in the train split. If\\nint, represents the absolute number of train samples. If None,\\nthe value is automatically set to the complement of the test size. Controls the randomness of the training and testing indices produced.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. Note that providing y is sufficient to generate the splits and\\nhence np.zeros(n_samples) may be used as a placeholder for\\nX instead of actual training data. The target variable for supervised learning problems.\\nStratification is done based on the y labels. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split. Notes Randomized CV splitters may return different results for each call of\\nsplit. You can make the results identical by setting random_state\\nto an integer.',\n",
       " 'Input checker utility for building a cross-validator. Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n- None, to use the default 5-fold cross validation,\\n- integer, to specify the number of folds.\\n- CV splitter,\\n- An iterable that generates (train, test) splits as arrays of indices. For integer/None inputs, if classifier is True and y is either\\nbinary or multiclass, StratifiedKFold is used. In all other\\ncases, KFold is used. Refer User Guide for the various\\ncross-validation strategies that can be used here. Changed in version 0.22: cv default value changed from 3-fold to 5-fold. The target variable for supervised learning problems. Whether the task is a classification task, in which case\\nstratified KFold will be used. The return value is a cross-validator which generates the train/test\\nsplits via the split method.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parents_dict['sklearn.model_selection']['sklearn.model_selection#Splitters']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL TO SUMMARIZE THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = summary_chain.invoke({\"descriptions\":\"\\n\\n\".join(parents_dict['sklearn']['sklearn#defaults'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = await summary_chain.abatch([{\"descriptions\":\"\\n\\n\".join(parents_dict['sklearn']['sklearn#defaults'])}],config={\"max_concurrency\": 5})\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def split_description(parent_text:List[str],MAX_WORDS:int=500):\n",
    "        split_s = []\n",
    "        running_num_words = 0\n",
    "        curr_func_string = \"\"\n",
    "        for txt in parent_text:\n",
    "            num_words = len(txt.split(\" \"))\n",
    "            running_num_words += num_words\n",
    "            if running_num_words > MAX_WORDS:\n",
    "                running_num_words = num_words\n",
    "                split_s.append(curr_func_string)\n",
    "                curr_func_string = txt\n",
    "            else:\n",
    "                curr_func_string += txt + \"\\n\"\n",
    "        if split_s == []:\n",
    "            split_s.append(curr_func_string)\n",
    "        split_s = [s for s in split_s if s!=\"\"]\n",
    "        return split_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_child_dict = {pn:{} for pn in parents_dict}\n",
    "\n",
    "for parent_name,child_nodes in parents_dict.items():\n",
    "    for child_name,child_texts in child_nodes.items():\n",
    "        child_split_list = split_description(child_texts,500)\n",
    "        parent_child_dict[parent_name].update({child_name:child_split_list})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "# from langchain_openai_limiter import LimitAwaitChatOpenAI, ChooseKeyChatOpenAI\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "# chat_model_limit_await = LimitAwaitChatOpenAI(\n",
    "#     chat_openai=model,\n",
    "#     limit_await_timeout=60.0,\n",
    "#     limit_await_sleep=0.1,\n",
    "# )\n",
    "# chat_model_key_choose = ChooseKeyChatOpenAI(\n",
    "#     chat_openai=chat_model_limit_await,\n",
    "#     openai_api_keys=[\n",
    "#         os.environ[\"OPENAI_API_KEY0\"],\n",
    "#         os.environ[\"OPENAI_API_KEY1\"],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are given a list of descriptions of different functions separated by new paragraph.\n",
    "Your task is to summarize all the text into coherent and detailed summary that covers all the functions descriptions.\n",
    "Be very diligent and make sure that no function description is left out of the final summary. \n",
    "\n",
    "Follow the following format\n",
    "\n",
    "List of function descriptions: list of function descriptions to be summarized\n",
    "Summary: summary of all the function descriptions\n",
    "\n",
    "-----\n",
    "\n",
    "List of functions descriptions: {descriptions}\n",
    "\n",
    "Summary: \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "parent_summary_dict = {pn:{} for pn in parents_dict}\n",
    "\n",
    "batch_list = []\n",
    "for parent_name,child_node in parent_child_dict.items():\n",
    "    for child_name,child_texts in child_node.items():\n",
    "        for ctext in child_texts:\n",
    "            batch_list.append({f\"{parent_name}-->{child_name}\":ctext})\n",
    "\n",
    "\n",
    "description_list = [{\"descriptions\":list(desc.values())[0]} for desc in batch_list]\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "start_key = '0'\n",
    "summaries_list = []\n",
    "for start in tqdm(range(0,len(description_list),BATCH_SIZE),desc=\"Embedding rows\"):\n",
    "    end = min(start+BATCH_SIZE,len(description_list))\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",api_key=os.environ[f'OPENAI_API_KEY{start_key}'],max_retries=5)\n",
    "    summary_chain = summary_prompt | model | StrOutputParser()\n",
    "    curr_description_list = description_list[start:end]\n",
    "    curr_summary_list = summary_chain.batch(curr_description_list,config={\"max_concurrency\":5})\n",
    "    summaries_list.extend(curr_summary_list)\n",
    "\n",
    "\n",
    "\n",
    "summary_list = summary_chain.batch(description_list,config={\"max_concurrency\":5})\n",
    "from tqdm import tqdm\n",
    "\n",
    "pbar = tqdm(total=len(batch_list))\n",
    "for sl in summary_list:\n",
    "    bl = batch_list[sl[0]]\n",
    "    parent_name = list(bl.keys())[0].split(\"-->\")[0]\n",
    "    child_name = list(bl.keys())[0].split(\"-->\")[1]\n",
    "    if child_name in parent_summary_dict[parent_name]:\n",
    "        parent_summary_dict[parent_name][child_name].append(sl[1])\n",
    "    else:\n",
    "        parent_summary_dict[parent_name].update({child_name:[sl[1]]})\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353, 353)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description_list),len(batch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = summary_chain.batch_as_completed(description_list,config={\"max_concurrency\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, \"The global scikit-learn configuration offers various options for customization. \\n- The configuration includes settings for validation of finiteness, size of temporary arrays, printing of non-default parameters, display of estimators, row vectors per chunk, accelerated pairwise-distances reduction backend, Array API dispatching, output format of transform and fit_transform, metadata routing, validation of hyper-parameters' types and values, and retrieval of current configuration values.\\n- Users can choose to skip validation for finiteness for faster processing, set a limit for temporary array size, control the printing of non-default parameters, display estimators as diagrams or text, adjust the number of row vectors per chunk, use the accelerated pairwise-distances reduction backend, enable Array API dispatching, configure output formats, enable metadata routing, and disable validation of hyper-parameters' types and values.\\n- These configurations can be customized globally and the values can be retrieved as needed. \\n- It is important to note that changing configurations can impact the performance and behavior of scikit-learn, and caution should be exercised when making modifications.\")\n",
      "(3, \"The ProbabilityCalibration class in scikit-learn offers the functionality of calibrating classifiers using isotonic regression or logistic regression. It uses cross-validation to estimate classifier parameters and then calibrate the classifier. By default, it fits a copy of the base estimator to a training subset and calibrates it using the testing subset. Predicted probabilities are averaged across individual calibrated classifiers. The calibration method can be 'sigmoid' (Platt's method) or 'isotonic', with the latter not recommended for fewer than 1000 calibration samples due to overfitting tendencies. The class allows for various cross-validation splitting strategies, including default 5-fold cross-validation or custom strategies. The number of jobs to run in parallel can be specified, with the option to use all processors. The calibrator can be fitted using training data and calibrated using testing data for each cross-validation fold, resulting in an ensemble of fitted classifier and calibrator pairs. Alternatively, cross-validation can be used to compute unbiased predictions for calibration. The class also provides methods for fitting the calibrated model, predicting class labels and probabilities, and evaluating accuracy. Metadata routing is available for passing metadata to the fit and score methods. Additionally, the class supports setting and getting parameters for the estimator.\")\n",
      "(2, 'The Mixin class for all transformers in scikit-learn provides functionality such as a fit_transform method, a set_output method for specifying output container type, and automatic wrapping of transform and fit_transform methods. OneToOneFeatureMixin and ClassNamePrefixFeaturesOutMixin are useful for defining get_feature_names_out. The Fit method fits transformer to X and y with optional fit parameters and returns a transformed version of X. The set_output API allows configuring output format for transform and fit_transform, with options like \"default\", \"pandas\", \"polars\", or None. The is_classifier method determines if an estimator is a classifier. \\n\\nThe Mixin class for all bicluster estimators in scikit-learn offers properties like biclusters_, methods to get indices, shape, and submatrix of a bicluster. It provides a convenient way to access row and column indicators together, as well as specific information about biclusters like indices, shape, and submatrix.\\n\\nThe Mixin class for all classifiers defines attributes like _estimator_type and methods like score for calculating mean accuracy on test data. It enforces that fit requires passing y through the requires_y tag. \\n\\nThe Mixin class for all density estimators sets _estimator_type to \"DensityEstimator\" and provides a score method for calculating the model\\'s score on test samples. This class maintains API consistency by including a score method even though it defaults to a no-op.')\n",
      "(1, 'The function descriptions cover various mixins and base classes for estimators in scikit-learn. \\n\\nThe base class provides default implementations for setting and getting parameters, textual and HTML representation, serialization, and data validation. Estimators should specify all parameters in their __init__ method as explicit keyword arguments.\\n\\nThe MetadataMixin class provides metadata routing and encapsulates routing information for an object. ParameterMixin class allows getting and setting parameters for an estimator, including nested objects like Pipelines.\\n\\nThe PrefixMixin class is used for transformers that generate their own feature names, prefixing them with the lowercased class name. ClusterMixin class is for cluster estimators, defining _estimator_type as \"clusterer\" and a fit_predict method for clustering and returning cluster labels.\\n\\nThe MetaEstimatorMixin defines mandatory estimator parameters, while the OutlierMixin class is for outlier detection estimators, with _estimator_type as \"outlier_detector\" and a fit_predict method that returns -1 for outliers and 1 for inliers.')\n"
     ]
    }
   ],
   "source": [
    "for i in ans:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",api_key=os.environ['OPENAI_API_KEY'],max_retries=5,model_kwargs={\"batch_size\":20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_graph(sklearn_graph):\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import ast\n",
    "embed_docs = []\n",
    "embed_metadata = []\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \"\\u200b\",  # Zero-width space\n",
    "        \"\\uff0c\",  # Fullwidth comma\n",
    "        \"\\u3001\",  # Ideographic comma\n",
    "        \"\\uff0e\",  # Fullwidth full stop\n",
    "        \"\\u3002\",  # Ideographic full stop\n",
    "        \"\",\n",
    "    ],\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=400)\n",
    "with open(\"parent_summary.json\",\"r\") as jsonfile:\n",
    "    parent_summary_dict = json.load(jsonfile)\n",
    "\n",
    "splits = text_splitter.split_text(parent_summary_dict['sklearn.linear_model'])\n",
    "with open(\"parent_summary.json\",\"r\") as jsonfile:\n",
    "    parent_summary_dict = json.load(jsonfile)\n",
    "\n",
    "def clean_text(s:str):\n",
    "    s = re.sub(\"\\n\",\" \",s)\n",
    "    s = json.dumps(s)\n",
    "    return ast.literal_eval(s)\n",
    "for node,attr in sklearn_graph.nodes(data=True):\n",
    "    type = attr.get('type')\n",
    "    \n",
    "    if type == 'function_node':\n",
    "        ftext = attr['function_text']\n",
    "        if ftext == \"\": continue\n",
    "        embed_docs.append(clean_text(attr['function_text']))\n",
    "        for k,v in attr.items():\n",
    "            if not isinstance(v,str):\n",
    "                attr[k] = str(v)\n",
    "        embed_metadata.append(attr)\n",
    "    elif type == 'parent_node':\n",
    "        parent_summary = parent_summary_dict[node]\n",
    "        parent_summary_text_split = text_splitter.split_text(parent_summary)\n",
    "        for k,v in attr.items():\n",
    "            if not isinstance(v,str):\n",
    "                attr[k] = str(v)\n",
    "        attr.update({\"name\":node})\n",
    "        for parent_splits in parent_summary_text_split:\n",
    "            embed_docs.append(clean_text(parent_splits))\n",
    "            embed_metadata.append(attr)\n",
    "    elif type == 'sub_level_node':\n",
    "        for ctext in attr['child_texts']:\n",
    "            embed_docs.append(clean_text(ctext))\n",
    "            sub_level_attr = deepcopy(attr)\n",
    "            del sub_level_attr['child_texts']\n",
    "            for k,v in sub_level_attr.items():\n",
    "                if not isinstance(v,str):\n",
    "                    sub_level_attr[k] = str(v)\n",
    "            sub_level_attr.update({\"name\":node})\n",
    "            embed_metadata.append(sub_level_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1303, 1303)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed_docs),len(embed_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'sub_level_node',\n",
       " 'trail': 'sklearn.exceptions',\n",
       " 'name': 'sklearn.exceptions#defaults'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_metadata[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "from chromadb.utils.batch_utils import create_batches\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    "    wait_fixed\n",
    ")\n",
    "@retry(wait=wait_random_exponential(min=1,max=60),stop=stop_after_attempt(6))\n",
    "def build_database(offset:int,docs, metadata, api_key):\n",
    "    database_path = \"SKLEARN_DB\"\n",
    "    collection_name = \"sklearn\"\n",
    "    load_dotenv(find_dotenv(), override=True)\n",
    "    emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=api_key, model_name=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    client = chromadb.PersistentClient(path=database_path)\n",
    "    sklearn_collection = client.get_or_create_collection(\n",
    "        name=collection_name, embedding_function=emb_fn\n",
    "    )\n",
    "\n",
    "    sklearn_ids = [f\"id{offset+i}\" for i in range(len(docs))]\n",
    "    all_ids = sklearn_collection.get()['ids']\n",
    "    if all(skids in all_ids for skids in sklearn_ids):\n",
    "        print(f\"All ids already present in database {offset}\")\n",
    "        return \n",
    "    batches = create_batches(\n",
    "        api=client, ids=sklearn_ids, documents=docs, metadatas=metadata\n",
    "    )\n",
    "    for batch in batches:\n",
    "        sklearn_collection.add(ids=batch[0], documents=batch[3], metadatas=batch[2])\n",
    "    return sklearn_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for 0-200/1303\n",
      "Done for 200-400/1303\n",
      "Done for 400-600/1303\n",
      "Done for 600-800/1303\n",
      "Done for 800-1000/1303\n",
      "Done for 1000-1200/1303\n",
      "Done for 1200-1303/1303\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "def create_embeddings(embed_docs,embed_metadata):\n",
    "    BATCH_SIZE = 200\n",
    "    # pbar = tqdm(total=len(embed_docs)//BATCH_SIZE,)\n",
    "    for start in range(0,len(embed_docs),BATCH_SIZE):\n",
    "        end = min(start+BATCH_SIZE,len(embed_docs))\n",
    "        # sklearn_ids = [f\"id{start+i}\" for i in range()]\n",
    "        curr_docs = embed_docs[start:end]\n",
    "        curr_metadata = embed_metadata[start:end]\n",
    "        build_database(start,curr_docs[:len(curr_docs)//2],curr_metadata[:len(curr_metadata)//2],os.environ['OPENAI_API_KEY1'])\n",
    "        build_database(start+BATCH_SIZE//2,curr_docs[len(curr_docs)//2:],curr_metadata[len(curr_metadata)//2:],os.environ['OPENAI_API_KEY1'])\n",
    "        # print(\"Sleeping...\")\n",
    "        # time.sleep(60)\n",
    "        print(f\"Done for {start}-{end}/{len(embed_docs)}\")\n",
    "\n",
    "create_embeddings(embed_docs,embed_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = \"SKLEARN_DB\"\n",
    "emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=os.environ[\"OPENAI_API_KEY0\"], model_name=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "client = chromadb.PersistentClient(path=database_path)\n",
    "sklearn_collection = client.get_collection(\n",
    "        name=\"sklearn\", embedding_function=emb_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = sklearn_collection.get()['ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sklearn.model_selection#Model validation',\n",
       " 'trail': 'sklearn.model_selection',\n",
       " 'type': 'sub_level_node'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "d = sklearn_collection.get(\n",
    "    where = {\"type\":\"sub_level_node\"},\n",
    ")\n",
    "d['metadatas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.covariance: computing Mahalanobis distances, log-likelihood of test data under a Gaussian model, setting parameters of the estimator, and requesting metadata routing. These functions provide a comprehensive set of tools for covariance estimation and analysis. The GraphicalLassoCV function is used for sparse inverse covariance estimation with a cross-validated choice of the l1 penalty. It allows for the selection of the penalization parameter and provides various outputs such as the estimated mean, covariance matrix, precision matrix, and penalization parameter selected. The function uses a grid search approach to find the optimal penalization parameter and can handle ill-conditioned systems. Additionally, it provides options for setting the tolerance for convergence, the solver type, number of iterations, and parallel processing. The function also offers methods for computing the Mean Squared Error between covariance estimators, fitting the covariance model, computing Mahalanobis distances, and calculating log-likelihood under the estimated Gaussian model. Furthermore, it includes options for setting parameters, getting metadata routing, and requesting metadata for the score method. The Minimum Covariance Determinant (MCD) is a robust estimator of covariance that is suitable for Gaussian-distributed data, but can also be used with unimodal, symmetric distributions. It is not recommended for multi-modal data. The MCD estimator can be used with or without storing the estimated precision, and it allows for specifying the proportion of points to be included in the support of the raw estimate. The algorithm used in MCD estimation is the FastMCD algorithm.\n",
      "\n",
      "sklearn.covariance: seen during fit. Other related functions include outlier detection, maximum likelihood covariance estimation, sparse inverse covariance estimation, LedoitWolf Estimator, Minimum Covariance Determinant, and covariance estimator with shrinkage. The shrinkage formulation implemented in the regularized covariance differs from the original article by omitting a small value operation for large feature numbers. References are provided for further reading. The estimator also includes functions for computing Mean Squared Error between covariance estimators, fitting the model to data, getting metadata routing, retrieving parameters, computing Mahalanobis distances, computing log-likelihood, and setting parameters. Metadata routing options are available for passing metadata to the score method. The functions provided in the list are related to computing different types of covariance estimators. These include the Maximum Likelihood covariance estimator, the Ledoit-Wolf covariance matrix estimator, and the Oracle Approximating Shrinkage estimator. Each function allows for the computation of covariance estimates from data, with options to center the data before computation. The shrunk covariance is calculated using a convex combination formula, with specific coefficients and formulas provided in the notes. The Oracle Approximating Shrinkage estimator uses a different shrinkage formulation compared to the original article referenced. Overall, these functions provide tools for estimating covariance matrices with different approaches and optimizations. The Maximum Likelihood Covariance Estimator is a method used to estimate the covariance matrix, pseudo-inverse matrix, and mean of a dataset. It can be used with or without centering the data before computation. Other related functions include Sparse Inverse Covariance Estimation, LedoitWolf Estimator, Minimum Covariance Determinant, Oracle Approximating Shrinkage Estimator, and Covariance Estimator with Shrinkage. The Mean Squared Error function allows for comparison between two covariance estimators. Additional functions include computing Mahalanobis distances, log-likelihood of test data under a Gaussian model, setting parameters of the estimator, and requesting metadata routing. These functions provide a comprehensive set of tools for covariance estimation and analysis. The GraphicalLassoCV function is used for sparse inverse covariance estimation with a cross-validated choice of the l1 penalty. It allows for the\n",
      "\n",
      "sklearn.covariance: The MCD estimator provides estimates for the robust location, covariance matrix, and pseudo-inverse matrix. It also computes Mahalanobis distances for the training set observations. Other related estimators include the Maximum Likelihood Covariance Estimator, Sparse Inverse Covariance Estimators, and Oracle Approximating Shrinkage Estimator.   There are methods available to correct raw MCD estimates, compute Mean Squared Error between covariance estimators, re-weight observations, and compute the log-likelihood of test data under the estimated Gaussian model. Additionally, there are methods for setting parameters of the estimator and requesting metadata routing.   Overall, the MCD estimator is a powerful tool for robust covariance estimation, particularly for Gaussian-distributed data, with various options and methods available for customization and analysis. The Covariance estimator with shrinkage allows for the computation of the estimated covariance matrix, estimated mean, and estimated pseudo inverse matrix. It also includes the option to store the estimated precision. The shrinkage coefficient is used in a convex combination for the computation of the shrunk estimate. Other related functions include the Maximum Likelihood Covariance Estimator, Sparse Inverse Covariance Estimation with an l1-penalized estimator, and the Minimum Covariance Determinant (robust estimator of covariance). The Oracle Approximating Shrinkage Estimator is also available. The regularized covariance is calculated using a specific formula involving the shrinkage coefficient and the covariance matrix. Additionally, functions are provided for computing the Mean Squared Error between two covariance estimators, fitting the shrunk covariance model to data, computing Mahalanobis distances, and calculating the log-likelihood of test data under the estimated Gaussian model. The precision matrix can be retrieved using a getter function, and parameters can be set using a specific method. Metadata routing is available for certain parameters, allowing for more flexibility in handling metadata.\n",
      "\n",
      " sklearn.covariance; empirical covariance; X; y; Mahalanobis distances; log-likelihood; GraphicalLassoCV; sparse inverse covariance estimation; l1 penalty; penalization parameter; precision matrix; mean squared error; convergence tolerance; solver type; metadata routing; Mean Squared Error; covariance model fitting; ill-conditioned systems; shrinkage formulation; Maximum Likelihood Covariance Estimator; LedoitWolf Estimator; Minimum Covariance Determinant; Oracle Approximating Shrinkage estimator; centering data; robust estimator; Gaussian-distributed data; shrinkage coefficient; pseudo-inverse matrix; re-weight observations; customization; analysis; precision matrix retrieval; getter function; metadata handling.\n",
      "sklearn.covariance#defaults: Compute the Maximum likelihood covariance estimator. Data from which to compute the covariance estimate. If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data will be centered before computation. Empirical covariance (Maximum Likelihood Estimator).\n",
      "\n",
      "sklearn.covariance#defaults: Estimate covariance with the Oracle Approximating Shrinkage. Read more in the User Guide. Data from which to compute the covariance estimate. If True, data will not be centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data will be centered before computation. Shrunk covariance. Coefficient in the convex combination used for the computation of the shrunk estimate. Notes The regularised covariance is: (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features), where mu = trace(cov) / n_features and shrinkage is given by the OAS formula (see [1]). The shrinkage formulation implemented here differs from Eq. 23 in [1]. In the original article, formula (23) states that 2/p (p being the number of features) is multiplied by Trace(cov*cov) in both the numerator and denominator, but this operation is omitted because for a large p, the value of 2/p is so small that it doesn’t affect the value of the estimator. References “Shrinkage algorithms for MMSE covariance estimation.”, Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O. IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.\n",
      "\n",
      "sklearn.covariance#defaults: Calculate covariance matrices shrunk on the diagonal. Read more in the User Guide. Covariance matrices to be shrunk, at least 2D ndarray. Coefficient in the convex combination used for the computation of the shrunk estimate. Range is [0, 1]. Shrunk covariance matrices. Notes The regularized (shrunk) covariance is given by: where mu = trace(cov) / n_features.\n",
      "\n",
      "sklearn.covariance#defaults: Estimate the shrunk Ledoit-Wolf covariance matrix. Read more in the User Guide. Data from which to compute the covariance estimate. If True, data will not be centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data will be centered before computation. Size of blocks into which the covariance matrix will be split. This is purely a memory optimization and does not affect results. Shrunk covariance. Coefficient in the convex combination used for the computation of the shrunk estimate. Notes The regularized (shrunk) covariance is: (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features) where mu = trace(cov) / n_features\n",
      "\n",
      "sklearn.covariance#defaults: Covariance estimator with shrinkage. Read more in the User Guide. Specify if the estimated precision is stored. If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data will be centered before computation. Coefficient in the convex combination used for the computation of the shrunk estimate. Range is [0, 1]. Estimated covariance matrix Estimated location, i.e. the estimated mean. Estimated pseudo inverse matrix. (stored only if store_precision is True) Number of features seen during fit. Added in version 0.24. Names of features seen during fit. Defined only when X has feature names that are all strings. Added in version 1.0. See also An object for detecting outliers in a Gaussian distributed dataset. Maximum likelihood covariance estimator. Sparse inverse covariance estimation with an l1-penalized estimator. Sparse inverse covariance with cross-validated choice of the l1 penalty. LedoitWolf Estimator. Minimum Covariance Determinant (robust estimator of covariance). Oracle Approximating Shrinkage Estimator. Notes The regularized covariance is given by: (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features) where mu = trace(cov) / n_features  Compute the Mean Squared Error between two covariance estimators. The covariance to compare with. The type of norm used to compute the error. Available error types: - ‘frobenius’ (default): sqrt(tr(A^t.A)) - ‘spectral’: sqrt(max(eigenvalues(A^t.A)) where A is the error (comp_cov - self.covariance_). If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled. Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned. The Mean Squared Error (in the sense of the Frobenius norm) between self and comp_cov covariance estimators. Fit the shrunk covariance model to X. Training data, where n_samples is the number of samples and n_features is the number of features. Not used, present for API consistency by convention. Returns the instance itself. Get metadata routing of this object. Please check User Guide on how the routing mechanism works. A MetadataRequest encapsulating routing information. Get parameters for this estimator. If True, will return the parameters for this estimator and contained subobjects that are estimators. Parameter names mapped to their values. Getter for the precision matrix. The precision matrix associated to the current covariance object. Compute the squared Mahalanobis distances of given observations. The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit. Squared Mahalanobis distances of the observations. Compute the log-likelihood of X_test under the estimated Gaussian model. The Gaussian model is defined by its mean and covariance matrix which are represented respectively by self.location_ and self.covariance_. Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering). Not used, present for API consistency by convention. The log-likelihood of X_test with self.location_ and self.covariance_ as estimators of the Gaussian model mean and covariance matrix respectively. Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object. Estimator parameters. Estimator instance. Request metadata passed to the score method. Note that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config). Please see User Guide on how the routing mechanism works. The options for each parameter are: True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to score. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a Pipeline. Otherwise it has no effect. Metadata routing for X_test parameter in score. The updated object.\n",
      "\n",
      "\n",
      "sklearn.covariance#defaults\n",
      "{'$or': [{'trail': {'$eq': ' pseudo-inverse matrix-->defaults'}}, {'trail': {'$eq': ' ill-conditioned systems-->defaults'}}, {'trail': {'$eq': ' metadata routing-->defaults'}}, {'trail': {'$eq': ' Mean Squared Error-->defaults'}}, {'trail': {'$eq': ' l1 penalty-->defaults'}}, {'trail': {'$eq': ' mean squared error-->defaults'}}, {'trail': {'$eq': ' Mahalanobis distances-->defaults'}}, {'trail': {'$eq': ' shrinkage formulation-->defaults'}}, {'trail': {'$eq': ' empirical covariance-->defaults'}}, {'trail': {'$eq': ' shrinkage coefficient-->defaults'}}, {'trail': {'$eq': ' customization-->defaults'}}, {'trail': {'$eq': ' solver type-->defaults'}}, {'trail': {'$eq': ' Gaussian-distributed data-->defaults'}}, {'trail': {'$eq': ' sparse inverse covariance estimation-->defaults'}}, {'trail': {'$eq': ' X-->defaults'}}, {'trail': {'$eq': ' Oracle Approximating Shrinkage estimator-->defaults'}}, {'trail': {'$eq': ' getter function-->defaults'}}, {'trail': {'$eq': ' centering data-->defaults'}}, {'trail': {'$eq': ' GraphicalLassoCV-->defaults'}}, {'trail': {'$eq': ' log-likelihood-->defaults'}}, {'trail': {'$eq': ' convergence tolerance-->defaults'}}, {'trail': {'$eq': ' Minimum Covariance Determinant-->defaults'}}, {'trail': {'$eq': 'sklearn.covariance-->defaults'}}, {'trail': {'$eq': ' robust estimator-->defaults'}}, {'trail': {'$eq': ' re-weight observations-->defaults'}}, {'trail': {'$eq': ' precision matrix-->defaults'}}, {'trail': {'$eq': ' Maximum Likelihood Covariance Estimator-->defaults'}}, {'trail': {'$eq': ' precision matrix retrieval-->defaults'}}, {'trail': {'$eq': ' penalization parameter-->defaults'}}, {'trail': {'$eq': ' covariance model fitting-->defaults'}}, {'trail': {'$eq': ' analysis-->defaults'}}, {'trail': {'$eq': ' LedoitWolf Estimator-->defaults'}}, {'trail': {'$eq': ' metadata handling.-->defaults'}}, {'trail': {'$eq': ' y-->defaults'}}]}\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "def generate_pairs(list1, list2):\n",
    "    pairs = []\n",
    "    for l1 in list1:\n",
    "        for l2 in list2:\n",
    "            curr_trail = l1\n",
    "            curr_trail += f\"-->{l2}\"\n",
    "            pairs.append(curr_trail)\n",
    "    return [pairs]\n",
    "\n",
    "\n",
    "def generate_pairs_recursive(trail_list):\n",
    "    if len(trail_list) == 1:\n",
    "        return trail_list[0]\n",
    "    curr_pairs = generate_pairs(trail_list[-2], trail_list[-1])\n",
    "    modified_trail_list = trail_list[:-2] + curr_pairs\n",
    "    return generate_pairs_recursive(modified_trail_list)\n",
    "\n",
    "\n",
    "def get_trail_list_pairs(trail_list_pairs, metadata_name=\"trail\"):\n",
    "    if len(trail_list_pairs) == 1:\n",
    "        trail_where_clause = {metadata_name: {\"$eq\": trail_list_pairs[0]}}\n",
    "    elif len(trail_list_pairs) > 1:\n",
    "        trail_where_clause = {\n",
    "            \"$or\": [{metadata_name: {\"$eq\": t}} for t in trail_list_pairs]\n",
    "        }\n",
    "    return trail_where_clause\n",
    "\n",
    "class FirstSecondLevel(dspy.Signature):\n",
    "    \"You are given a list of keys and values separated by semicolon.\"\n",
    "    \"Based on the query, you have to output the key that is most relevant to the question separated by semicolon.\"\n",
    "    \"Be precise and output only the relevant key or keys from the provided keys only.\"\n",
    "    \"Don't include any other information\"\n",
    "\n",
    "    query = dspy.InputField(prefix=\"Query which you need to classify: \", format=str)\n",
    "    keys_values = dspy.InputField(prefix=\"Keys and Values: \", format=str)\n",
    "    output = dspy.OutputField(\n",
    "        prefix=\"Relevant Key(s): \",\n",
    "        format=str,\n",
    "        desc=\"relevant keys separated by semicolon\",\n",
    "    )\n",
    "llm = dspy.OpenAI(model=\"gpt-3.5-turbo-0125\",max_tokens=512)\n",
    "dspy.settings.configure(lm=llm)\n",
    "\n",
    "class SklearnAgentChroma(dspy.Module):\n",
    "    def __init__(self, collection):\n",
    "        super().__init__()\n",
    "        self.collection = collection\n",
    "        self.firstSecondLevel = dspy.Predict(FirstSecondLevel)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return super().__call__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, query: str):\n",
    "        query_emb = emb_fn([query])[0]\n",
    "\n",
    "        # Parent level querying\n",
    "        parent_level = self.collection.query(\n",
    "            query_embeddings=query_emb,\n",
    "            where={\n",
    "                \"type\": {\"$eq\": \"parent_node\"},\n",
    "            },\n",
    "            n_results=3,\n",
    "        )\n",
    "        parent_level_str = \"\"\n",
    "        for parent_level_docs,parent_level_metadata in zip(parent_level['documents'][0],parent_level[\"metadatas\"][0]):\n",
    "            parent_level_str += f\"{parent_level_metadata['name']}: {parent_level_docs}\\n\\n\"\n",
    "\n",
    "        parent_level_answer = self.firstSecondLevel(\n",
    "            query=query, keys_values=parent_level_str\n",
    "        ).output\n",
    "        print(parent_level_str, parent_level_answer)\n",
    "        trail_list = [parent_level_answer.split(\";\")]\n",
    "        trail_list = list(set(trail_list[0]))\n",
    "        trail_list_pairs = generate_pairs_recursive([trail_list])\n",
    "\n",
    "        trail_where_clause = get_trail_list_pairs(trail_list_pairs)\n",
    "\n",
    "        sub_level = self.collection.query(\n",
    "            query_embeddings=query_emb,\n",
    "            where={\n",
    "                \"$and\": [\n",
    "                    trail_where_clause,\n",
    "                    {\"type\": {\"$eq\": \"sub_level_node\"}},\n",
    "                ]\n",
    "            },\n",
    "            n_results=5,\n",
    "        )\n",
    "\n",
    "        sub_level_str = \"\"\n",
    "        for sub_level_docs,function_level_metadata in zip(sub_level['documents'][0],sub_level[\"metadatas\"][0]):\n",
    "            sub_level_str += f\"{function_level_metadata['name']}: {sub_level_docs}\\n\\n\"\n",
    "        print(sub_level_str)\n",
    "        sub_level_answer = self.firstSecondLevel(\n",
    "            query=query, keys_values=sub_level_str\n",
    "        ).output\n",
    "        print(sub_level_answer)\n",
    "        sub_level_list = [sla.split(\"#\")[-1] for sla in sub_level_answer.split(\";\")]\n",
    "        sub_level_list = list(set(sub_level_list))\n",
    "        function_list = generate_pairs_recursive([trail_list_pairs,sub_level_list])\n",
    "        function_where_clause = get_trail_list_pairs(function_list)\n",
    "        print(function_where_clause)\n",
    "        functions = self.collection.query(\n",
    "            query_embeddings=query_emb,\n",
    "            where={\n",
    "                \"$and\": [\n",
    "                    function_where_clause,\n",
    "                    {\"type\": {\"$eq\": \"function_node\"}},\n",
    "                ]\n",
    "            },\n",
    "            n_results=1\n",
    "        )\n",
    "        return functions\n",
    "sklearn_chroma = SklearnAgentChroma(sklearn_collection)\n",
    "funcs = sklearn_chroma(\"How to do empirical covariance of X and y?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'name': 'empirical_covariance', 'descriptions': 'Compute the Maximum likelihood covariance estimator. Data from which to compute the covariance estimate. If True, data will not be centered before computation.\\\\nUseful when working with data whose mean is almost, but not exactly\\\\nzero.\\\\nIf False, data will be centered before computation. Empirical covariance (Maximum Likelihood Estimator). Examples', 'parameters': {'type': 'object', 'properties': {'X': {'type': 'array', 'description': 'ndarray of shape (n_samples, n_features). Data from which to compute the covariance estimate.\\\\n'}, 'assume_centered': {'type': 'boolean', 'description': 'bool, default=False. If True, data will not be centered before computation.\\\\nUseful when working with data whose mean is almost, but not exactly\\\\nzero.\\\\nIf False, data will be centered before computation.\\\\n'}}, 'required': ['X']}}\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs['metadatas'][0][0]['function_calling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.model_selection: On the other hand, the Leave-P-Out cross-validator also provides train/test indices to split data into train/test sets, resulting in testing on all distinct samples of size p while the remaining n - p samples form the training set in each iteration. It is important to note that LeavePOut(p) is not equivalent to KFold(n_splits=n_samples // p) as it creates non-overlapping test sets. Similar to Leave-One-Out, this method can be costly for large datasets, and it is advised to use KFold, StratifiedKFold, or ShuffleSplit instead.\n",
      "sklearn.neighbors: be specified, such as 'ball_tree', 'kd_tree', or 'brute'. The weight function used in prediction can be set to 'uniform', 'distance', or a user-defined function. Additional parameters like leaf size, power parameter for the Minkowski metric, and metric for distance computation can also be adjusted. The model can be fitted with training data and used to predict target values. The coefficient of determination (R^2) can be calculated to evaluate the model's performance. The model parameters can be set and metadata routing can be requested for the estimator. Overall, the Radius Neighbors Regressor provides a flexible and customizable approach to regression based on k-nearest neighbors within a specified radius.\n",
      "sklearn.metrics: also supports sample weights and different types of configurations, such as 'ovr' (One-vs-rest) and 'ovo' (One-vs-one). The Gini Coefficient, which is a summary measure of the ranking ability of binary classifiers, can be calculated using the ROC-AUC score. References for further reading on ROC analysis and the Gini coefficient are provided. The summary covers two main functions: Top-k Accuracy classification score and \\(D^2\\) regression score function.\n",
      "sklearn.utils: function can preserve the dtype of the input or convert it to a specified type. It also provides options for memory layout, copying behavior, handling of special values like np.inf and np.nan, and enforcing minimum dimensions and features in the input data. Additionally, it allows including the estimator name in warning messages and customizing the error message construction. The function returns the converted and validated array after performing all the necessary checks and conversions. The functions provided in the list cover a range of functionalities.\n",
      "sklearn.compose: clones the regressor and transformer before fitting, with the option to use LinearRegression if no regressor is specified. The target y is converted into a 2-dimensional array internally, and reshaped during prediction. The coefficient of determination \\(R^2\\) is calculated for the prediction, with a score of 1.0 indicating the best possible fit. The method also allows for setting parameters, predicting values, and requesting metadata for scoring. Additionally, the method supports metadata routing for sample weights and returns the updated object.\n",
      " sklearn.compose; sklearn.linear_model; sklearn.metrics\n",
      "sklearn.compose#defaults: Create a callable to select columns to be used with ColumnTransformer. make_column_selector can select columns based on datatype or the columns name with a regex. When using multiple selection criteria, all criteria must match for a column to be selected. For an example of how to use make_column_selector within a ColumnTransformer to select columns based on data type (i.e. dtype), refer to Column Transformer with Mixed Types. Name of columns containing this regex pattern will be included. If None, column selection will not be selected based on pattern. A selection of dtypes to include. For more details, see pandas.DataFrame.select_dtypes. A selection of dtypes to exclude. For more details, see pandas.DataFrame.select_dtypes. Callable for column selection to be used by a ColumnTransformer. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space.  Callable for column selection to be used by a ColumnTransformer. DataFrame to select columns from.\n",
      "sklearn.compose#defaults: Meta-estimator to regress on a transformed target. Useful for applying a non-linear transformation to the target y in regression problems. This transformation can be given as a Transformer such as the QuantileTransformer or as a function and its inverse such as np.log and np.exp. The computation during fit is: or: The computation during predict is: or: Read more in the User Guide. Added in version 0.20. Regressor object such as derived from RegressorMixin. This regressor will automatically be cloned each time prior to fitting. If regressor is None, LinearRegression is created and used. Estimator object such as derived from TransformerMixin. Cannot be set at the same time as func and inverse_func. If transformer is None as well as func and inverse_func, the transformer will be an identity transformer. Note that the transformer will be cloned during fitting. Also, the transformer is restricting y to be a numpy array. Function to apply to y before passing to fit. Cannot be set at the same time as transformer. If func is None, the function used will be the identity function. If func is set, inverse_func also needs to be provided. The function needs to return a 2-dimensional array. Function to apply to the prediction of the regressor. Cannot be set at the same time as transformer. The inverse function is used to return predictions to the same space of the original training labels. If inverse_func is set, func also needs to be provided. The inverse function needs to return a 2-dimensional array. Whether to check that transform followed by inverse_transform or func followed by inverse_func leads to the original targets. Fitted regressor. Transformer used in fit and predict. Number of features seen during fit. Names of features seen during fit. Defined only when X has feature names that are all strings. Added in version 1.0. See also Construct a transformer from an arbitrary callable. Notes Internally, the target y is always converted into a 2-dimensional array to be used by scikit-learn transformers. At the time of prediction, the output will be reshaped to a have the same number of dimensions as y.  For a more detailed example use case refer to Effect of transforming the targets in regression model. Fit the model according to the given training data. Training vector, where n_samples is the number of samples and n_features is the number of features. Target values. Parameters passed to the fit method of the underlying regressor. Fitted estimator. Raise NotImplementedError. This estimator does not support metadata routing yet. Get parameters for this estimator. If True, will return the parameters for this estimator and contained subobjects that are estimators. Parameter names mapped to their values. Number of features seen during fit. Predict using the base regressor, applying inverse. The regressor is used to predict and the inverse_func or inverse_transform is applied before returning the prediction. Samples. Parameters passed to the predict method of the underlying regressor. Predicted values. Return the coefficient of determination of the prediction. The coefficient of determination \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares ((y_true - y_pred)** 2).sum() and \\(v\\) is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \\(R^2\\) score of 0.0. Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator. True values for X. Sample weights. \\(R^2\\) of self.predict(X) w.r.t. y. Notes The \\(R^2\\) score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of r2_score. This influences the score method of all the multioutput regressors (except for MultiOutputRegressor). Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object. Estimator parameters. Estimator instance. Request metadata passed to the score method. Note that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config). Please see User Guide on how the routing mechanism works. The options for each parameter are: True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to score. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a Pipeline. Otherwise it has no effect. Metadata routing for sample_weight parameter in score. The updated object.\n",
      "sklearn.compose#defaults: Construct a ColumnTransformer from the given transformers. This is a shorthand for the ColumnTransformer constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting with transformer_weights. Read more in the User Guide. Tuples of the form (transformer, columns) specifying the transformer objects to be applied to subsets of the data. Estimator must support fit and transform. Special-cased strings ‘drop’ and ‘passthrough’ are accepted as well, to indicate to drop the columns or to pass them through untransformed, respectively. Indexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where transformer expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data X and can return any of the above. To select multiple columns by name or dtype, you can use make_column_selector. By default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of 'drop'). By specifying remainder='passthrough', all remaining columns that were not specified in transformers will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting remainder to be an estimator, the remaining non-specified columns will use the remainder estimator. The estimator must support fit and transform. If the transformed output consists of a mix of sparse and dense data, it will be stacked as a sparse matrix if the density is lower than this value. Use sparse_threshold=0 to always return dense. When the transformed output consists of all sparse or all dense data, the stacked result will be sparse or dense, respectively, and this keyword will be ignored. Number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details. If True, the time elapsed while fitting each transformer will be printed as it is completed. If True, ColumnTransformer.get_feature_names_out will prefix all feature names with the name of the transformer that generated that feature. If False, ColumnTransformer.get_feature_names_out will not prefix any feature names and will error if feature names are not unique. Added in version 1.0. Force the columns of the last entry of transformers_, which corresponds to the “remainder” transformer, to always be stored as indices (int) rather than column names (str). See description of the ColumnTransformer.transformers_ attribute for details. Note If you do not access the list of columns for the remainder columns in the ColumnTransformer.transformers_ fitted attribute, you do not need to set this parameter. Added in version 1.5. Changed in version 1.7: The default value for force_int_remainder_cols will change from True to False in version 1.7. Returns a ColumnTransformer object. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space.\n",
      "\n",
      "{'$or': [{'trail': {'$eq': 'sklearn.compose-->defaults'}}, {'trail': {'$eq': ' sklearn.metrics-->defaults'}}, {'trail': {'$eq': ' sklearn.linear_model-->defaults'}}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.schema import Document\n",
    "\n",
    "class FirstSecondLevel_(dspy.Signature):\n",
    "    \"You are given a list of keys and values separated by semicolon.\"\n",
    "    \"Based on the query, you have to output the key that is most relevant to the question separated by semicolon.\"\n",
    "    \"Be precise and output only the relevant key or keys from the provided keys only.\"\n",
    "    \"Don't include any other information and don't answer None and N/A\"\n",
    "\n",
    "    query = dspy.InputField(prefix=\"Query which you need to classify: \", format=str)\n",
    "    keys_values = dspy.InputField(prefix=\"Keys and Values: \", format=str)\n",
    "    output = dspy.OutputField(\n",
    "        prefix=\"Relevant Key(s): \",\n",
    "        format=str,\n",
    "        desc=\"relevant keys separated by semicolon\",\n",
    "    )\n",
    "\n",
    "class SklearnAgentBM25(dspy.Module):\n",
    "    def __init__(self, collection):\n",
    "        super().__init__()\n",
    "        self.collection = collection\n",
    "        self.firstSecondLevel = dspy.Predict(FirstSecondLevel_)\n",
    "        self.parent_langchain_docs = []\n",
    "        parent_level = self.collection.get(\n",
    "            where={\n",
    "                \"type\": {\"$eq\": \"parent_node\"},\n",
    "            }\n",
    "        )\n",
    "        for doc,metadata in zip(parent_level['documents'],parent_level['metadatas']):\n",
    "            self.parent_langchain_docs.append(Document(page_content=doc,metadata=metadata))\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return super().__call__(*args, **kwargs)\n",
    "    \n",
    "    def BM25RetrieverLangchain(self,query:str,node_type:str='parent_node',trail_where_clause:dict={}):\n",
    "\n",
    "        assert node_type in ['parent_node','function_node','sub_level_node'], \"type must be 'parent_node' or 'function_node' or 'sub_level_node'\"\n",
    "        if node_type != 'parent_node' and trail_where_clause=={}:\n",
    "            raise ValueError(\"trail_where_clause must be a dict for function type\")\n",
    "        \n",
    "        if node_type == 'parent_node':\n",
    "            bm25_retriever = BM25Retriever.from_documents(\n",
    "                self.parent_langchain_docs, k=5, preprocess_func=(lambda x: x.lower())\n",
    "                )\n",
    "            parent_bm25_docs = bm25_retriever.invoke(query.lower())\n",
    "            return parent_bm25_docs\n",
    "        else:\n",
    "            function_level = self.collection.get(\n",
    "            where={\n",
    "                \"$and\": [\n",
    "                    trail_where_clause,\n",
    "                    {\"type\": {\"$eq\": node_type}},\n",
    "                ]\n",
    "            },\n",
    "             )\n",
    "            function_langchain_docs = []\n",
    "            for doc,metadata in zip(function_level['documents'],function_level['metadatas']):\n",
    "                function_langchain_docs.append(Document(page_content=doc,metadata=metadata))\n",
    "            bm25_retriever = BM25Retriever.from_documents(\n",
    "                function_langchain_docs, k=3, preprocess_func=(lambda x: x.lower())\n",
    "            )\n",
    "            bm25_docs = bm25_retriever.invoke(query.lower())\n",
    "            return bm25_docs\n",
    "       \n",
    "    def forward(self, query: str):\n",
    "        parent_bm25_docs = self.BM25RetrieverLangchain(query,node_type=\"parent_node\")\n",
    "        parent_level_str = \"\"\n",
    "        for parent_doc in parent_bm25_docs:\n",
    "            parent_level_str+=f\"{parent_doc.metadata['name']}: {parent_doc.page_content}\\n\"\n",
    "        \n",
    "        parent_level_answer = self.firstSecondLevel(\n",
    "            query=query, keys_values=parent_level_str\n",
    "        ).output\n",
    "        print(parent_level_str,parent_level_answer)\n",
    "        trail_list = [parent_level_answer.split(\";\")]\n",
    "        trail_list = list(set(trail_list[0]))\n",
    "        trail_list_pairs = generate_pairs_recursive([trail_list])\n",
    "\n",
    "        trail_where_clause = get_trail_list_pairs(trail_list_pairs)\n",
    "\n",
    "        sub_level_docs = self.BM25RetrieverLangchain(query,node_type='sub_level_node',trail_where_clause=trail_where_clause)\n",
    "        sub_level_str = \"\"\n",
    "        for function_doc in sub_level_docs:\n",
    "            sub_level_str+=f\"{function_doc.metadata['name']}: {function_doc.page_content}\\n\"\n",
    "        print(sub_level_str)\n",
    "        sub_level_answer = self.firstSecondLevel(\n",
    "            query=query, keys_values=sub_level_str\n",
    "        ).output\n",
    "        \n",
    "        sub_level_list = [sla.split(\"#\")[-1] for sla in sub_level_answer.split(\";\")]\n",
    "        sub_level_list = list(set(sub_level_list))\n",
    "        function_list = generate_pairs_recursive([trail_list_pairs,sub_level_list])\n",
    "        function_where_clause = get_trail_list_pairs(function_list)\n",
    "        print(function_where_clause)\n",
    "        functions = self.BM25RetrieverLangchain(query,node_type=\"function_node\",trail_where_clause=function_where_clause)\n",
    "        return functions\n",
    "sklearn_bm25 = SklearnAgentBM25(sklearn_collection)\n",
    "\n",
    "funcs = sklearn_bm25(\"I want to linear discriminant analysis with X and y?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Create a callable to select columns to be used with ColumnTransformer. make_column_selector can select columns based on datatype or the columns name with a regex. When using multiple selection criteria, all criteria must match for a column to be selected. For an example of how to use make_column_selector within a ColumnTransformer to select columns based on data type (i.e. dtype), refer to Column Transformer with Mixed Types. Name of columns containing this regex pattern will be included. If None, column selection will not be selected based on pattern. A selection of dtypes to include. For more details, see pandas.DataFrame.select_dtypes. A selection of dtypes to exclude. For more details, see pandas.DataFrame.select_dtypes. Callable for column selection to be used by a ColumnTransformer. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space. Examples Callable for column selection to be used by a ColumnTransformer. DataFrame to select columns from.', metadata={'full_function': 'class sklearn.compose.make_column_selector(pattern=None, *, dtype_include=None, dtype_exclude=None)', 'function_calling': \"{'name': 'make_column_selector', 'descriptions': 'Create a callable to select columns to be used with\\\\nColumnTransformer. make_column_selector can select columns based on datatype or the\\\\ncolumns name with a regex. When using multiple selection criteria, all\\\\ncriteria must match for a column to be selected. For an example of how to use make_column_selector within a\\\\nColumnTransformer to select columns based on data type (i.e.\\\\ndtype), refer to\\\\nColumn Transformer with Mixed Types. Name of columns containing this regex pattern will be included. If\\\\nNone, column selection will not be selected based on pattern. A selection of dtypes to include. For more details, see\\\\npandas.DataFrame.select_dtypes. A selection of dtypes to exclude. For more details, see\\\\npandas.DataFrame.select_dtypes. Callable for column selection to be used by a\\\\nColumnTransformer. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space. Examples Callable for column selection to be used by a\\\\nColumnTransformer. DataFrame to select columns from.', 'parameters': {'type': 'object', 'properties': {'df': {'type': 'dataframe of shape (n_features, n_samples)', 'description': 'dataframe of shape (n_features, n_samples). DataFrame to select columns from.\\\\n'}}, 'required': ['pattern=None']}}\", 'function_name': 'make_column_selector', 'function_text': 'Create a callable to select columns to be used with\\nColumnTransformer. make_column_selector can select columns based on datatype or the\\ncolumns name with a regex. When using multiple selection criteria, all\\ncriteria must match for a column to be selected. For an example of how to use make_column_selector within a\\nColumnTransformer to select columns based on data type (i.e.\\ndtype), refer to\\nColumn Transformer with Mixed Types. Name of columns containing this regex pattern will be included. If\\nNone, column selection will not be selected based on pattern. A selection of dtypes to include. For more details, see\\npandas.DataFrame.select_dtypes. A selection of dtypes to exclude. For more details, see\\npandas.DataFrame.select_dtypes. Callable for column selection to be used by a\\nColumnTransformer. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space. Examples Callable for column selection to be used by a\\nColumnTransformer. DataFrame to select columns from.', 'name': 'make_column_selector', 'parameter_names_desc': \"[{'param_name': 'df', 'param_type': 'dataframe of shape (n_features, n_samples)', 'param_desc': 'DataFrame to select columns from.\\\\n'}]\", 'trail': 'sklearn.compose-->defaults', 'type': 'function_node', 'url': 'https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html#sklearn.compose.make_column_selector'}),\n",
       " Document(page_content=\"Meta-estimator to regress on a transformed target. Useful for applying a non-linear transformation to the target y in regression problems. This transformation can be given as a Transformer such as the QuantileTransformer or as a function and its inverse such as np.log and np.exp. The computation during fit is: or: The computation during predict is: or: Read more in the User Guide. Added in version 0.20. Regressor object such as derived from RegressorMixin. This regressor will automatically be cloned each time prior to fitting. If regressor is None, LinearRegression is created and used. Estimator object such as derived from TransformerMixin. Cannot be set at the same time as func and inverse_func. If transformer is None as well as func and inverse_func, the transformer will be an identity transformer. Note that the transformer will be cloned during fitting. Also, the transformer is restricting y to be a numpy array. Function to apply to y before passing to fit. Cannot be set at the same time as transformer. If func is None, the function used will be the identity function. If func is set, inverse_func also needs to be provided. The function needs to return a 2-dimensional array. Function to apply to the prediction of the regressor. Cannot be set at the same time as transformer. The inverse function is used to return predictions to the same space of the original training labels. If inverse_func is set, func also needs to be provided. The inverse function needs to return a 2-dimensional array. Whether to check that transform followed by inverse_transform or func followed by inverse_func leads to the original targets. Fitted regressor. Transformer used in fit and predict. Number of features seen during fit. Names of features seen during fit. Defined only when X has feature names that are all strings. Added in version 1.0. See also Construct a transformer from an arbitrary callable. Notes Internally, the target y is always converted into a 2-dimensional array to be used by scikit-learn transformers. At the time of prediction, the output will be reshaped to a have the same number of dimensions as y. Examples For a more detailed example use case refer to Effect of transforming the targets in regression model. Fit the model according to the given training data. Training vector, where n_samples is the number of samples and n_features is the number of features. Target values. Parameters passed to the fit method of the underlying regressor. Fitted estimator. Raise NotImplementedError. This estimator does not support metadata routing yet. Get parameters for this estimator. If True, will return the parameters for this estimator and contained subobjects that are estimators. Parameter names mapped to their values. Number of features seen during fit. Predict using the base regressor, applying inverse. The regressor is used to predict and the inverse_func or inverse_transform is applied before returning the prediction. Samples. Parameters passed to the predict method of the underlying regressor. Predicted values. Return the coefficient of determination of the prediction. The coefficient of determination \\\\(R^2\\\\) is defined as \\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual sum of squares ((y_true - y_pred)** 2).sum() and \\\\(v\\\\) is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0. Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator. True values for X. Sample weights. \\\\(R^2\\\\) of self.predict(X) w.r.t. y. Notes The \\\\(R^2\\\\) score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of r2_score. This influences the score method of all the multioutput regressors (except for MultiOutputRegressor). Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object. Estimator parameters. Estimator instance. Request metadata passed to the score method. Note that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config). Please see User Guide on how the routing mechanism works. The options for each parameter are: True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to score. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a Pipeline. Otherwise it has no effect. Metadata routing for sample_weight parameter in score. The updated object.\", metadata={'full_function': 'class sklearn.compose.TransformedTargetRegressor(regressor=None, *, transformer=None, func=None, inverse_func=None, check_inverse=True)', 'function_calling': '{\\'name\\': \\'TransformedTargetRegressor\\', \\'descriptions\\': \"Meta-estimator to regress on a transformed target. Useful for applying a non-linear transformation to the target y in\\\\nregression problems. This transformation can be given as a Transformer\\\\nsuch as the QuantileTransformer or as a\\\\nfunction and its inverse such as np.log and np.exp. The computation during fit is: or: The computation during predict is: or: Read more in the User Guide. Added in version 0.20. Regressor object such as derived from\\\\nRegressorMixin. This regressor will\\\\nautomatically be cloned each time prior to fitting. If regressor is\\\\nNone, LinearRegression is created and used. Estimator object such as derived from\\\\nTransformerMixin. Cannot be set at the same time\\\\nas func and inverse_func. If transformer is None as well as\\\\nfunc and inverse_func, the transformer will be an identity\\\\ntransformer. Note that the transformer will be cloned during fitting.\\\\nAlso, the transformer is restricting y to be a numpy array. Function to apply to y before passing to fit. Cannot be set\\\\nat the same time as transformer. If func is None, the function used will be\\\\nthe identity function. If func is set, inverse_func also needs to be\\\\nprovided. The function needs to return a 2-dimensional array. Function to apply to the prediction of the regressor. Cannot be set at\\\\nthe same time as transformer. The inverse function is used to return\\\\npredictions to the same space of the original training labels. If\\\\ninverse_func is set, func also needs to be provided. The inverse\\\\nfunction needs to return a 2-dimensional array. Whether to check that transform followed by inverse_transform\\\\nor func followed by inverse_func leads to the original targets. Fitted regressor. Transformer used in fit and predict. Number of features seen during fit. Names of features seen during fit. Defined only when X\\\\nhas feature names that are all strings. Added in version 1.0. See also Construct a transformer from an arbitrary callable. Notes Internally, the target y is always converted into a 2-dimensional array\\\\nto be used by scikit-learn transformers. At the time of prediction, the\\\\noutput will be reshaped to a have the same number of dimensions as y. Examples For a more detailed example use case refer to\\\\nEffect of transforming the targets in regression model. Fit the model according to the given training data. Training vector, where n_samples is the number of samples and\\\\nn_features is the number of features. Target values. Parameters passed to the fit method of the underlying\\\\nregressor. Fitted estimator. Raise NotImplementedError. This estimator does not support metadata routing yet. Get parameters for this estimator. If True, will return the parameters for this estimator and\\\\ncontained subobjects that are estimators. Parameter names mapped to their values. Number of features seen during fit. Predict using the base regressor, applying inverse. The regressor is used to predict and the inverse_func or\\\\ninverse_transform is applied before returning the prediction. Samples. Parameters passed to the predict method of the underlying\\\\nregressor. Predicted values. Return the coefficient of determination of the prediction. The coefficient of determination \\\\\\\\(R^2\\\\\\\\) is defined as\\\\n\\\\\\\\((1 - \\\\\\\\frac{u}{v})\\\\\\\\), where \\\\\\\\(u\\\\\\\\) is the residual\\\\nsum of squares ((y_true - y_pred)** 2).sum() and \\\\\\\\(v\\\\\\\\)\\\\nis the total sum of squares ((y_true - y_true.mean()) ** 2).sum().\\\\nThe best possible score is 1.0 and it can be negative (because the\\\\nmodel can be arbitrarily worse). A constant model that always predicts\\\\nthe expected value of y, disregarding the input features, would get\\\\na \\\\\\\\(R^2\\\\\\\\) score of 0.0. Test samples. For some estimators this may be a precomputed\\\\nkernel matrix or a list of generic objects instead with shape\\\\n(n_samples, n_samples_fitted), where n_samples_fitted\\\\nis the number of samples used in the fitting for the estimator. True values for X. Sample weights. \\\\\\\\(R^2\\\\\\\\) of self.predict(X) w.r.t. y. Notes The \\\\\\\\(R^2\\\\\\\\) score used when calling score on a regressor uses\\\\nmultioutput=\\'uniform_average\\' from version 0.23 to keep consistent\\\\nwith default value of r2_score.\\\\nThis influences the score method of all the multioutput\\\\nregressors (except for\\\\nMultiOutputRegressor). Set the parameters of this estimator. The method works on simple estimators as well as on nested objects\\\\n(such as Pipeline). The latter have\\\\nparameters of the form <component>__<parameter> so that it’s\\\\npossible to update each component of a nested object. Estimator parameters. Estimator instance. Request metadata passed to the score method. Note that this method is only relevant if\\\\nenable_metadata_routing=True (see sklearn.set_config).\\\\nPlease see User Guide on how the routing\\\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to score. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\\\nexisting request. This allows you to change the request for some\\\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\\\nsub-estimator of a meta-estimator, e.g. used inside a\\\\nPipeline. Otherwise it has no effect. Metadata routing for sample_weight parameter in score. The updated object.\", \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'sample_weight\\': {\\'type\\': \\'string\\', \\'description\\': \\'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED. Metadata routing for sample_weight parameter in score.\\\\n\\'}}, \\'required\\': [\\'regressor=None\\']}}', 'function_name': 'TransformedTargetRegressor', 'function_text': \"Meta-estimator to regress on a transformed target. Useful for applying a non-linear transformation to the target y in\\nregression problems. This transformation can be given as a Transformer\\nsuch as the QuantileTransformer or as a\\nfunction and its inverse such as np.log and np.exp. The computation during fit is: or: The computation during predict is: or: Read more in the User Guide. Added in version 0.20. Regressor object such as derived from\\nRegressorMixin. This regressor will\\nautomatically be cloned each time prior to fitting. If regressor is\\nNone, LinearRegression is created and used. Estimator object such as derived from\\nTransformerMixin. Cannot be set at the same time\\nas func and inverse_func. If transformer is None as well as\\nfunc and inverse_func, the transformer will be an identity\\ntransformer. Note that the transformer will be cloned during fitting.\\nAlso, the transformer is restricting y to be a numpy array. Function to apply to y before passing to fit. Cannot be set\\nat the same time as transformer. If func is None, the function used will be\\nthe identity function. If func is set, inverse_func also needs to be\\nprovided. The function needs to return a 2-dimensional array. Function to apply to the prediction of the regressor. Cannot be set at\\nthe same time as transformer. The inverse function is used to return\\npredictions to the same space of the original training labels. If\\ninverse_func is set, func also needs to be provided. The inverse\\nfunction needs to return a 2-dimensional array. Whether to check that transform followed by inverse_transform\\nor func followed by inverse_func leads to the original targets. Fitted regressor. Transformer used in fit and predict. Number of features seen during fit. Names of features seen during fit. Defined only when X\\nhas feature names that are all strings. Added in version 1.0. See also Construct a transformer from an arbitrary callable. Notes Internally, the target y is always converted into a 2-dimensional array\\nto be used by scikit-learn transformers. At the time of prediction, the\\noutput will be reshaped to a have the same number of dimensions as y. Examples For a more detailed example use case refer to\\nEffect of transforming the targets in regression model. Fit the model according to the given training data. Training vector, where n_samples is the number of samples and\\nn_features is the number of features. Target values. Parameters passed to the fit method of the underlying\\nregressor. Fitted estimator. Raise NotImplementedError. This estimator does not support metadata routing yet. Get parameters for this estimator. If True, will return the parameters for this estimator and\\ncontained subobjects that are estimators. Parameter names mapped to their values. Number of features seen during fit. Predict using the base regressor, applying inverse. The regressor is used to predict and the inverse_func or\\ninverse_transform is applied before returning the prediction. Samples. Parameters passed to the predict method of the underlying\\nregressor. Predicted values. Return the coefficient of determination of the prediction. The coefficient of determination \\\\(R^2\\\\) is defined as\\n\\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual\\nsum of squares ((y_true - y_pred)** 2).sum() and \\\\(v\\\\)\\nis the total sum of squares ((y_true - y_true.mean()) ** 2).sum().\\nThe best possible score is 1.0 and it can be negative (because the\\nmodel can be arbitrarily worse). A constant model that always predicts\\nthe expected value of y, disregarding the input features, would get\\na \\\\(R^2\\\\) score of 0.0. Test samples. For some estimators this may be a precomputed\\nkernel matrix or a list of generic objects instead with shape\\n(n_samples, n_samples_fitted), where n_samples_fitted\\nis the number of samples used in the fitting for the estimator. True values for X. Sample weights. \\\\(R^2\\\\) of self.predict(X) w.r.t. y. Notes The \\\\(R^2\\\\) score used when calling score on a regressor uses\\nmultioutput='uniform_average' from version 0.23 to keep consistent\\nwith default value of r2_score.\\nThis influences the score method of all the multioutput\\nregressors (except for\\nMultiOutputRegressor). Set the parameters of this estimator. The method works on simple estimators as well as on nested objects\\n(such as Pipeline). The latter have\\nparameters of the form <component>__<parameter> so that it’s\\npossible to update each component of a nested object. Estimator parameters. Estimator instance. Request metadata passed to the score method. Note that this method is only relevant if\\nenable_metadata_routing=True (see sklearn.set_config).\\nPlease see User Guide on how the routing\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to score. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\nexisting request. This allows you to change the request for some\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\nsub-estimator of a meta-estimator, e.g. used inside a\\nPipeline. Otherwise it has no effect. Metadata routing for sample_weight parameter in score. The updated object.\", 'name': 'TransformedTargetRegressor', 'parameter_names_desc': \"[{'param_name': 'sample_weight', 'param_type': 'str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED', 'param_desc': 'Metadata routing for sample_weight parameter in score.\\\\n'}]\", 'trail': 'sklearn.compose-->defaults', 'type': 'function_node', 'url': 'https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html#sklearn.compose.TransformedTargetRegressor'}),\n",
       " Document(page_content=\"Construct a ColumnTransformer from the given transformers. This is a shorthand for the ColumnTransformer constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting with transformer_weights. Read more in the User Guide. Tuples of the form (transformer, columns) specifying the transformer objects to be applied to subsets of the data. Estimator must support fit and transform. Special-cased strings ‘drop’ and ‘passthrough’ are accepted as well, to indicate to drop the columns or to pass them through untransformed, respectively. Indexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where transformer expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data X and can return any of the above. To select multiple columns by name or dtype, you can use make_column_selector. By default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of 'drop'). By specifying remainder='passthrough', all remaining columns that were not specified in transformers will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting remainder to be an estimator, the remaining non-specified columns will use the remainder estimator. The estimator must support fit and transform. If the transformed output consists of a mix of sparse and dense data, it will be stacked as a sparse matrix if the density is lower than this value. Use sparse_threshold=0 to always return dense. When the transformed output consists of all sparse or all dense data, the stacked result will be sparse or dense, respectively, and this keyword will be ignored. Number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details. If True, the time elapsed while fitting each transformer will be printed as it is completed. If True, ColumnTransformer.get_feature_names_out will prefix all feature names with the name of the transformer that generated that feature. If False, ColumnTransformer.get_feature_names_out will not prefix any feature names and will error if feature names are not unique. Added in version 1.0. Force the columns of the last entry of transformers_, which corresponds to the “remainder” transformer, to always be stored as indices (int) rather than column names (str). See description of the ColumnTransformer.transformers_ attribute for details. Note If you do not access the list of columns for the remainder columns in the ColumnTransformer.transformers_ fitted attribute, you do not need to set this parameter. Added in version 1.5. Changed in version 1.7: The default value for force_int_remainder_cols will change from True to False in version 1.7. Returns a ColumnTransformer object. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space. Examples\", metadata={'full_function': \"sklearn.compose.make_column_transformer(*transformers, remainder='drop', sparse_threshold=0.3, n_jobs=None, verbose=False, verbose_feature_names_out=True, force_int_remainder_cols=True)\", 'function_calling': '{\\'name\\': \\'make_column_transformer\\', \\'descriptions\\': \"Construct a ColumnTransformer from the given transformers. This is a shorthand for the ColumnTransformer constructor; it does not\\\\nrequire, and does not permit, naming the transformers. Instead, they will\\\\nbe given names automatically based on their types. It also does not allow\\\\nweighting with transformer_weights. Read more in the User Guide. Tuples of the form (transformer, columns) specifying the\\\\ntransformer objects to be applied to subsets of the data. Estimator must support fit and transform.\\\\nSpecial-cased strings ‘drop’ and ‘passthrough’ are accepted as\\\\nwell, to indicate to drop the columns or to pass them through\\\\nuntransformed, respectively. Indexes the data on its second axis. Integers are interpreted as\\\\npositional columns, while strings can reference DataFrame columns\\\\nby name. A scalar string or int should be used where\\\\ntransformer expects X to be a 1d array-like (vector),\\\\notherwise a 2d array will be passed to the transformer.\\\\nA callable is passed the input data X and can return any of the\\\\nabove. To select multiple columns by name or dtype, you can use\\\\nmake_column_selector. By default, only the specified columns in transformers are\\\\ntransformed and combined in the output, and the non-specified\\\\ncolumns are dropped. (default of \\'drop\\').\\\\nBy specifying remainder=\\'passthrough\\', all remaining columns that\\\\nwere not specified in transformers will be automatically passed\\\\nthrough. This subset of columns is concatenated with the output of\\\\nthe transformers.\\\\nBy setting remainder to be an estimator, the remaining\\\\nnon-specified columns will use the remainder estimator. The\\\\nestimator must support fit and transform. If the transformed output consists of a mix of sparse and dense data,\\\\nit will be stacked as a sparse matrix if the density is lower than this\\\\nvalue. Use sparse_threshold=0 to always return dense.\\\\nWhen the transformed output consists of all sparse or all dense data,\\\\nthe stacked result will be sparse or dense, respectively, and this\\\\nkeyword will be ignored. Number of jobs to run in parallel.\\\\nNone means 1 unless in a joblib.parallel_backend context.\\\\n-1 means using all processors. See Glossary\\\\nfor more details. If True, the time elapsed while fitting each transformer will be\\\\nprinted as it is completed. If True, ColumnTransformer.get_feature_names_out will prefix\\\\nall feature names with the name of the transformer that generated that\\\\nfeature.\\\\nIf False, ColumnTransformer.get_feature_names_out will not\\\\nprefix any feature names and will error if feature names are not\\\\nunique. Added in version 1.0. Force the columns of the last entry of transformers_, which\\\\ncorresponds to the “remainder” transformer, to always be stored as\\\\nindices (int) rather than column names (str). See description of the\\\\nColumnTransformer.transformers_ attribute for details. Note If you do not access the list of columns for the remainder columns\\\\nin the ColumnTransformer.transformers_ fitted attribute,\\\\nyou do not need to set this parameter. Added in version 1.5. Changed in version 1.7: The default value for force_int_remainder_cols will change from\\\\nTrue to False in version 1.7. Returns a ColumnTransformer object. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space. Examples\", \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'*transformers\\': {\\'type\\': \\'tuples\\', \\'description\\': \\'tuples. Tuples of the form (transformer, columns) specifying the\\\\ntransformer objects to be applied to subsets of the data.\\\\n\\\\ntransformer{‘drop’, ‘passthrough’} or estimatorEstimator must support fit and transform.\\\\nSpecial-cased strings ‘drop’ and ‘passthrough’ are accepted as\\\\nwell, to indicate to drop the columns or to pass them through\\\\nuntransformed, respectively.\\\\n\\\\ncolumnsstr,  array-like of str, int, array-like of int, slice,                 array-like of bool or callableIndexes the data on its second axis. Integers are interpreted as\\\\npositional columns, while strings can reference DataFrame columns\\\\nby name. A scalar string or int should be used where\\\\ntransformer expects X to be a 1d array-like (vector),\\\\notherwise a 2d array will be passed to the transformer.\\\\nA callable is passed the input data X and can return any of the\\\\nabove. To select multiple columns by name or dtype, you can use\\\\nmake_column_selector.\\\\n\\\\n\\\\n\\'}, \\'remainder\\': {\\'type\\': \\'string\\', \\'enum\\': [\\'drop\\', \\' passthrough\\'], \\'description\\': \"{‘drop’, ‘passthrough’} or estimator, default=’drop’. By default, only the specified columns in transformers are\\\\ntransformed and combined in the output, and the non-specified\\\\ncolumns are dropped. (default of \\'drop\\').\\\\nBy specifying remainder=\\'passthrough\\', all remaining columns that\\\\nwere not specified in transformers will be automatically passed\\\\nthrough. This subset of columns is concatenated with the output of\\\\nthe transformers.\\\\nBy setting remainder to be an estimator, the remaining\\\\nnon-specified columns will use the remainder estimator. The\\\\nestimator must support fit and transform.\\\\n\"}, \\'sparse_threshold\\': {\\'type\\': \\'float\\', \\'description\\': \\'float, default=0.3. If the transformed output consists of a mix of sparse and dense data,\\\\nit will be stacked as a sparse matrix if the density is lower than this\\\\nvalue. Use sparse_threshold=0 to always return dense.\\\\nWhen the transformed output consists of all sparse or all dense data,\\\\nthe stacked result will be sparse or dense, respectively, and this\\\\nkeyword will be ignored.\\\\n\\'}, \\'n_jobs\\': {\\'type\\': \\'integer\\', \\'description\\': \\'int, default=None. Number of jobs to run in parallel.\\\\nNone means 1 unless in a joblib.parallel_backend context.\\\\n-1 means using all processors. See Glossary\\\\nfor more details.\\\\n\\'}, \\'verbose\\': {\\'type\\': \\'boolean\\', \\'description\\': \\'bool, default=False. If True, the time elapsed while fitting each transformer will be\\\\nprinted as it is completed.\\\\n\\'}, \\'verbose_feature_names_out\\': {\\'type\\': \\'boolean\\', \\'description\\': \\'bool, default=True. If True, ColumnTransformer.get_feature_names_out will prefix\\\\nall feature names with the name of the transformer that generated that\\\\nfeature.\\\\nIf False, ColumnTransformer.get_feature_names_out will not\\\\nprefix any feature names and will error if feature names are not\\\\nunique.\\\\n\\\\nAdded in version 1.0.\\\\n\\\\n\\'}, \\'force_int_remainder_cols\\': {\\'type\\': \\'boolean\\', \\'description\\': \\'bool, default=True. Force the columns of the last entry of transformers_, which\\\\ncorresponds to the “remainder” transformer, to always be stored as\\\\nindices (int) rather than column names (str). See description of the\\\\nColumnTransformer.transformers_ attribute for details.\\\\n\\\\nNote\\\\nIf you do not access the list of columns for the remainder columns\\\\nin the ColumnTransformer.transformers_ fitted attribute,\\\\nyou do not need to set this parameter.\\\\n\\\\n\\\\nAdded in version 1.5.\\\\n\\\\n\\\\nChanged in version 1.7: The default value for force_int_remainder_cols will change from\\\\nTrue to False in version 1.7.\\\\n\\\\n\\'}}, \\'required\\': []}}', 'function_name': 'make_column_transformer', 'function_text': \"Construct a ColumnTransformer from the given transformers. This is a shorthand for the ColumnTransformer constructor; it does not\\nrequire, and does not permit, naming the transformers. Instead, they will\\nbe given names automatically based on their types. It also does not allow\\nweighting with transformer_weights. Read more in the User Guide. Tuples of the form (transformer, columns) specifying the\\ntransformer objects to be applied to subsets of the data. Estimator must support fit and transform.\\nSpecial-cased strings ‘drop’ and ‘passthrough’ are accepted as\\nwell, to indicate to drop the columns or to pass them through\\nuntransformed, respectively. Indexes the data on its second axis. Integers are interpreted as\\npositional columns, while strings can reference DataFrame columns\\nby name. A scalar string or int should be used where\\ntransformer expects X to be a 1d array-like (vector),\\notherwise a 2d array will be passed to the transformer.\\nA callable is passed the input data X and can return any of the\\nabove. To select multiple columns by name or dtype, you can use\\nmake_column_selector. By default, only the specified columns in transformers are\\ntransformed and combined in the output, and the non-specified\\ncolumns are dropped. (default of 'drop').\\nBy specifying remainder='passthrough', all remaining columns that\\nwere not specified in transformers will be automatically passed\\nthrough. This subset of columns is concatenated with the output of\\nthe transformers.\\nBy setting remainder to be an estimator, the remaining\\nnon-specified columns will use the remainder estimator. The\\nestimator must support fit and transform. If the transformed output consists of a mix of sparse and dense data,\\nit will be stacked as a sparse matrix if the density is lower than this\\nvalue. Use sparse_threshold=0 to always return dense.\\nWhen the transformed output consists of all sparse or all dense data,\\nthe stacked result will be sparse or dense, respectively, and this\\nkeyword will be ignored. Number of jobs to run in parallel.\\nNone means 1 unless in a joblib.parallel_backend context.\\n-1 means using all processors. See Glossary\\nfor more details. If True, the time elapsed while fitting each transformer will be\\nprinted as it is completed. If True, ColumnTransformer.get_feature_names_out will prefix\\nall feature names with the name of the transformer that generated that\\nfeature.\\nIf False, ColumnTransformer.get_feature_names_out will not\\nprefix any feature names and will error if feature names are not\\nunique. Added in version 1.0. Force the columns of the last entry of transformers_, which\\ncorresponds to the “remainder” transformer, to always be stored as\\nindices (int) rather than column names (str). See description of the\\nColumnTransformer.transformers_ attribute for details. Note If you do not access the list of columns for the remainder columns\\nin the ColumnTransformer.transformers_ fitted attribute,\\nyou do not need to set this parameter. Added in version 1.5. Changed in version 1.7: The default value for force_int_remainder_cols will change from\\nTrue to False in version 1.7. Returns a ColumnTransformer object. See also Class that allows combining the outputs of multiple transformer objects used on column subsets of the data into a single feature space. Examples\", 'name': 'make_column_transformer', 'parameter_names_desc': '[{\\'param_name\\': \\'*transformers\\', \\'param_type\\': \\'tuples\\', \\'param_desc\\': \\'Tuples of the form (transformer, columns) specifying the\\\\ntransformer objects to be applied to subsets of the data.\\\\n\\\\ntransformer{‘drop’, ‘passthrough’} or estimatorEstimator must support fit and transform.\\\\nSpecial-cased strings ‘drop’ and ‘passthrough’ are accepted as\\\\nwell, to indicate to drop the columns or to pass them through\\\\nuntransformed, respectively.\\\\n\\\\ncolumnsstr,  array-like of str, int, array-like of int, slice,                 array-like of bool or callableIndexes the data on its second axis. Integers are interpreted as\\\\npositional columns, while strings can reference DataFrame columns\\\\nby name. A scalar string or int should be used where\\\\ntransformer expects X to be a 1d array-like (vector),\\\\notherwise a 2d array will be passed to the transformer.\\\\nA callable is passed the input data X and can return any of the\\\\nabove. To select multiple columns by name or dtype, you can use\\\\nmake_column_selector.\\\\n\\\\n\\\\n\\'}, {\\'param_name\\': \\'remainder\\', \\'param_type\\': \\'{‘drop’, ‘passthrough’} or estimator, default=’drop’\\', \\'param_desc\\': \"By default, only the specified columns in transformers are\\\\ntransformed and combined in the output, and the non-specified\\\\ncolumns are dropped. (default of \\'drop\\').\\\\nBy specifying remainder=\\'passthrough\\', all remaining columns that\\\\nwere not specified in transformers will be automatically passed\\\\nthrough. This subset of columns is concatenated with the output of\\\\nthe transformers.\\\\nBy setting remainder to be an estimator, the remaining\\\\nnon-specified columns will use the remainder estimator. The\\\\nestimator must support fit and transform.\\\\n\"}, {\\'param_name\\': \\'sparse_threshold\\', \\'param_type\\': \\'float, default=0.3\\', \\'param_desc\\': \\'If the transformed output consists of a mix of sparse and dense data,\\\\nit will be stacked as a sparse matrix if the density is lower than this\\\\nvalue. Use sparse_threshold=0 to always return dense.\\\\nWhen the transformed output consists of all sparse or all dense data,\\\\nthe stacked result will be sparse or dense, respectively, and this\\\\nkeyword will be ignored.\\\\n\\'}, {\\'param_name\\': \\'n_jobs\\', \\'param_type\\': \\'int, default=None\\', \\'param_desc\\': \\'Number of jobs to run in parallel.\\\\nNone means 1 unless in a joblib.parallel_backend context.\\\\n-1 means using all processors. See Glossary\\\\nfor more details.\\\\n\\'}, {\\'param_name\\': \\'verbose\\', \\'param_type\\': \\'bool, default=False\\', \\'param_desc\\': \\'If True, the time elapsed while fitting each transformer will be\\\\nprinted as it is completed.\\\\n\\'}, {\\'param_name\\': \\'verbose_feature_names_out\\', \\'param_type\\': \\'bool, default=True\\', \\'param_desc\\': \\'If True, ColumnTransformer.get_feature_names_out will prefix\\\\nall feature names with the name of the transformer that generated that\\\\nfeature.\\\\nIf False, ColumnTransformer.get_feature_names_out will not\\\\nprefix any feature names and will error if feature names are not\\\\nunique.\\\\n\\\\nAdded in version 1.0.\\\\n\\\\n\\'}, {\\'param_name\\': \\'force_int_remainder_cols\\', \\'param_type\\': \\'bool, default=True\\', \\'param_desc\\': \\'Force the columns of the last entry of transformers_, which\\\\ncorresponds to the “remainder” transformer, to always be stored as\\\\nindices (int) rather than column names (str). See description of the\\\\nColumnTransformer.transformers_ attribute for details.\\\\n\\\\nNote\\\\nIf you do not access the list of columns for the remainder columns\\\\nin the ColumnTransformer.transformers_ fitted attribute,\\\\nyou do not need to set this parameter.\\\\n\\\\n\\\\nAdded in version 1.5.\\\\n\\\\n\\\\nChanged in version 1.7: The default value for force_int_remainder_cols will change from\\\\nTrue to False in version 1.7.\\\\n\\\\n\\'}]', 'trail': 'sklearn.compose-->defaults', 'type': 'function_node', 'url': 'https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html#sklearn.compose.make_column_transformer'})]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
