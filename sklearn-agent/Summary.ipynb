{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.scrape import scrape_sklearn_website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn_data = scrape_sklearn_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/sklearn_function_openai.json\",\"r\") as jsonfile:\n",
    "    sklearn_data = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.utils import build_no_summary_graph,get_parents_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_graph = build_no_summary_graph(sklearn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_dict = get_parents_dict(sklearn_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K-fold iterator variant with non-overlapping groups. Each group will appear exactly once in the test set across all folds (the\\nnumber of distinct groups has to be at least equal to the number of folds). The folds are approximately balanced in the sense that the number of\\nsamples is approximately the same in each test fold. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of folds. Must be at least 2. Changed in version 0.22: n_splits default value changed from 3 to 5. See also For splitting the data according to explicit domain-specific stratification of the dataset. Takes class information into account to avoid building folds with imbalanced class proportions (for binary or multiclass classification tasks). Notes Groups appear in an arbitrary order throughout the folds.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Request metadata passed to the split method. Note that this method is only relevant if\\nenable_metadata_routing=True (see sklearn.set_config).\\nPlease see User Guide on how the routing\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to split if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to split. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\nexisting request. This allows you to change the request for some\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\nsub-estimator of a meta-estimator, e.g. used inside a\\nPipeline. Otherwise it has no effect. Metadata routing for groups parameter in split. The updated object. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Group labels for the samples used while splitting the dataset into\\ntrain/test set. The training set indices for that split. The testing set indices for that split.',\n",
       " 'K-Fold cross-validator. Provides train/test indices to split data in train/test sets. Split\\ndataset into k consecutive folds (without shuffling by default). Each fold is then used once as a validation while the k - 1 remaining\\nfolds form the training set. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of folds. Must be at least 2. Changed in version 0.22: n_splits default value changed from 3 to 5. Whether to shuffle the data before splitting into batches.\\nNote that the samples within each split will not be shuffled. When shuffle is True, random_state affects the ordering of the\\nindices, which controls the randomness of each fold. Otherwise, this\\nparameter has no effect.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. See also Takes class information into account to avoid building folds with imbalanced class distributions (for binary or multiclass classification tasks). K-fold iterator variant with non-overlapping groups. Repeats K-Fold n times. Notes The first n_samples % n_splits folds have size\\nn_samples // n_splits + 1, other folds have size\\nn_samples // n_splits, where n_samples is the number of samples. Randomized CV splitters may return different results for each call of\\nsplit. You can make the results identical by setting random_state\\nto an integer.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Leave-One-Out cross-validator. Provides train/test indices to split data in train/test sets. Each\\nsample is used once as a test set (singleton) while the remaining\\nsamples form the training set. Note: LeaveOneOut() is equivalent to KFold(n_splits=n) and\\nLeavePOut(p=1) where n is the number of samples. Due to the high number of test sets (which is the same as the\\nnumber of samples) this cross-validation method can be very costly.\\nFor large datasets one should favor KFold, ShuffleSplit\\nor StratifiedKFold. Read more in the User Guide. See also For splitting the data according to explicit, domain-specific stratification of the dataset. K-fold iterator variant with non-overlapping groups.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Training data, where n_samples is the number of samples\\nand n_features is the number of features. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Leave-P-Out cross-validator. Provides train/test indices to split data in train/test sets. This results\\nin testing on all distinct samples of size p, while the remaining n - p\\nsamples form the training set in each iteration. Note: LeavePOut(p) is NOT equivalent to\\nKFold(n_splits=n_samples // p) which creates non-overlapping test sets. Due to the high number of iterations which grows combinatorically with the\\nnumber of samples this cross-validation method can be very costly. For\\nlarge datasets one should favor KFold, StratifiedKFold\\nor ShuffleSplit. Read more in the User Guide. Size of the test sets. Must be strictly less than the number of\\nsamples.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Training data, where n_samples is the number of samples\\nand n_features is the number of features. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Repeated K-Fold cross validator. Repeats K-Fold n times with different randomization in each repetition. Read more in the User Guide. Number of folds. Must be at least 2. Number of times cross-validator needs to be repeated. Controls the randomness of each repeated cross-validation instance.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. See also Repeats Stratified K-Fold n times. Notes Randomized CV splitters may return different results for each call of\\nsplit. You can make the results identical by setting random_state\\nto an integer.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility.\\nnp.zeros(n_samples) may be used as a placeholder. Always ignored, exists for compatibility.\\nnp.zeros(n_samples) may be used as a placeholder. Group labels for the samples used while splitting the dataset into\\ntrain/test set. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Random permutation cross-validator. Yields indices to split data into training and test sets. Note: contrary to other cross-validation strategies, random splits\\ndo not guarantee that all folds will be different, although this is\\nstill very likely for sizeable datasets. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of re-shuffling & splitting iterations. If float, should be between 0.0 and 1.0 and represent the proportion\\nof the dataset to include in the test split. If int, represents the\\nabsolute number of test samples. If None, the value is set to the\\ncomplement of the train size. If train_size is also None, it will\\nbe set to 0.1. If float, should be between 0.0 and 1.0 and represent the\\nproportion of the dataset to include in the train split. If\\nint, represents the absolute number of train samples. If None,\\nthe value is automatically set to the complement of the test size. Controls the randomness of the training and testing indices produced.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Stratified K-Fold cross-validator. Provides train/test indices to split data in train/test sets. This cross-validation object is a variation of KFold that returns\\nstratified folds. The folds are made by preserving the percentage of\\nsamples for each class. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of folds. Must be at least 2. Changed in version 0.22: n_splits default value changed from 3 to 5. Whether to shuffle each class’s samples before splitting into batches.\\nNote that the samples within each split will not be shuffled. When shuffle is True, random_state affects the ordering of the\\nindices, which controls the randomness of each fold for each class.\\nOtherwise, leave random_state as None.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. See also Repeats Stratified K-Fold n times. Notes The implementation is designed to: Generate test sets such that all contain the same distribution of\\nclasses, or as close as possible. Be invariant to class label: relabelling y = [\"Happy\", \"Sad\"] to\\ny = [1, 0] should not change the indices generated. Preserve order dependencies in the dataset ordering, when\\nshuffle=False: all samples from class k in some test set were\\ncontiguous in y, or separated in y by samples from classes other than k. Generate test sets where the smallest and largest differ by at most one\\nsample. Changed in version 0.22: The previous implementation did not follow the last constraint.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. Note that providing y is sufficient to generate the splits and\\nhence np.zeros(n_samples) may be used as a placeholder for\\nX instead of actual training data. The target variable for supervised learning problems.\\nStratification is done based on the y labels. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split. Notes Randomized CV splitters may return different results for each call of\\nsplit. You can make the results identical by setting random_state\\nto an integer.',\n",
       " 'Time Series cross-validator. Provides train/test indices to split time series data samples\\nthat are observed at fixed time intervals, in train/test sets.\\nIn each split, test indices must be higher than before, and thus shuffling\\nin cross validator is inappropriate. This cross-validation object is a variation of KFold.\\nIn the kth split, it returns first k folds as train set and the\\n(k+1)th fold as test set. Note that unlike standard cross-validation methods, successive\\ntraining sets are supersets of those that come before them. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Added in version 0.18. Number of splits. Must be at least 2. Changed in version 0.22: n_splits default value changed from 3 to 5. Maximum size for a single training set. Used to limit the size of the test set. Defaults to\\nn_samples // (n_splits + 1), which is the maximum allowed value\\nwith gap=0. Added in version 0.24. Number of samples to exclude from the end of each train set before\\nthe test set. Added in version 0.24. Notes The training set has size i * n_samples // (n_splits + 1)\\n+ n_samples % (n_splits + 1) in the i th split,\\nwith a test set of size n_samples//(n_splits + 1) by default,\\nwhere n_samples is the number of samples.  For a more extended example see\\nTime-related feature engineering. Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. Always ignored, exists for compatibility. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Split arrays or matrices into random train and test subsets. Quick utility that wraps input validation,\\nnext(ShuffleSplit().split(X, y)), and application to input data\\ninto a single call for splitting (and optionally subsampling) data into a\\none-liner. Read more in the User Guide. Allowed inputs are lists, numpy arrays, scipy-sparse\\nmatrices or pandas dataframes. If float, should be between 0.0 and 1.0 and represent the proportion\\nof the dataset to include in the test split. If int, represents the\\nabsolute number of test samples. If None, the value is set to the\\ncomplement of the train size. If train_size is also None, it will\\nbe set to 0.25. If float, should be between 0.0 and 1.0 and represent the\\nproportion of the dataset to include in the train split. If\\nint, represents the absolute number of train samples. If None,\\nthe value is automatically set to the complement of the test size. Controls the shuffling applied to the data before applying the split.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. Whether or not to shuffle the data before splitting. If shuffle=False\\nthen stratify must be None. If not None, data is split in a stratified fashion, using this as\\nthe class labels.\\nRead more in the User Guide. List containing train-test split of inputs. Added in version 0.16: If the input is sparse, the output will be a\\nscipy.sparse.csr_matrix. Else, output type is the same as the\\ninput type.',\n",
       " 'Shuffle-Group(s)-Out cross-validation iterator. Provides randomized train/test indices to split data according to a\\nthird-party provided group. This group information can be used to encode\\narbitrary domain specific stratifications of the samples as integers. For instance the groups could be the year of collection of the samples\\nand thus allow for cross-validation against time-based splits. The difference between LeavePGroupsOut and GroupShuffleSplit is that\\nthe former generates splits using all subsets of size p unique groups,\\nwhereas GroupShuffleSplit generates a user-determined number of random\\ntest splits, each with a user-determined fraction of unique groups. For example, a less computationally intensive alternative to\\nLeavePGroupsOut(p=10) would be\\nGroupShuffleSplit(test_size=10, n_splits=100). Note: The parameters test_size and train_size refer to groups, and\\nnot to samples, as in ShuffleSplit. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of re-shuffling & splitting iterations. If float, should be between 0.0 and 1.0 and represent the proportion\\nof groups to include in the test split (rounded up). If int,\\nrepresents the absolute number of test groups. If None, the value is\\nset to the complement of the train size.\\nThe default will change in version 0.21. It will remain 0.2 only\\nif train_size is unspecified, otherwise it will complement\\nthe specified train_size. If float, should be between 0.0 and 1.0 and represent the\\nproportion of the groups to include in the train split. If\\nint, represents the absolute number of train groups. If None,\\nthe value is automatically set to the complement of the test size. Controls the randomness of the training and testing indices produced.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. See also Shuffles samples to create independent test/train sets. Train set leaves out all possible subsets of p groups.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Request metadata passed to the split method. Note that this method is only relevant if\\nenable_metadata_routing=True (see sklearn.set_config).\\nPlease see User Guide on how the routing\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to split if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to split. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\nexisting request. This allows you to change the request for some\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\nsub-estimator of a meta-estimator, e.g. used inside a\\nPipeline. Otherwise it has no effect. Metadata routing for groups parameter in split. The updated object. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Group labels for the samples used while splitting the dataset into\\ntrain/test set. The training set indices for that split. The testing set indices for that split. Notes Randomized CV splitters may return different results for each call of\\nsplit. You can make the results identical by setting random_state\\nto an integer.',\n",
       " 'Leave One Group Out cross-validator. Provides train/test indices to split data such that each training set is\\ncomprised of all samples except ones belonging to one specific group.\\nArbitrary domain specific group information is provided an array integers\\nthat encodes the group of each sample. For instance the groups could be the year of collection of the samples\\nand thus allow for cross-validation against time-based splits. Read more in the User Guide. See also K-fold iterator variant with non-overlapping groups. Notes Splits are ordered according to the index of the group left out. The first\\nsplit has testing set consisting of the group whose index in groups is\\nlowest, and so on.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Group labels for the samples used while splitting the dataset into\\ntrain/test set. This ‘groups’ parameter must always be specified to\\ncalculate the number of splits, though the other parameters can be\\nomitted. Returns the number of splitting iterations in the cross-validator. Request metadata passed to the split method. Note that this method is only relevant if\\nenable_metadata_routing=True (see sklearn.set_config).\\nPlease see User Guide on how the routing\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to split if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to split. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\nexisting request. This allows you to change the request for some\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\nsub-estimator of a meta-estimator, e.g. used inside a\\nPipeline. Otherwise it has no effect. Metadata routing for groups parameter in split. The updated object. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Group labels for the samples used while splitting the dataset into\\ntrain/test set. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Leave P Group(s) Out cross-validator. Provides train/test indices to split data according to a third-party\\nprovided group. This group information can be used to encode arbitrary\\ndomain specific stratifications of the samples as integers. For instance the groups could be the year of collection of the samples\\nand thus allow for cross-validation against time-based splits. The difference between LeavePGroupsOut and LeaveOneGroupOut is that\\nthe former builds the test sets with all the samples assigned to\\np different values of the groups while the latter uses samples\\nall assigned the same groups. Read more in the User Guide. Number of groups (p) to leave out in the test split. See also K-fold iterator variant with non-overlapping groups.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Group labels for the samples used while splitting the dataset into\\ntrain/test set. This ‘groups’ parameter must always be specified to\\ncalculate the number of splits, though the other parameters can be\\nomitted. Returns the number of splitting iterations in the cross-validator. Request metadata passed to the split method. Note that this method is only relevant if\\nenable_metadata_routing=True (see sklearn.set_config).\\nPlease see User Guide on how the routing\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to split if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to split. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\nexisting request. This allows you to change the request for some\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\nsub-estimator of a meta-estimator, e.g. used inside a\\nPipeline. Otherwise it has no effect. Metadata routing for groups parameter in split. The updated object. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Group labels for the samples used while splitting the dataset into\\ntrain/test set. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Predefined split cross-validator. Provides train/test indices to split data into train/test sets using a\\npredefined scheme specified by the user with the test_fold parameter. Read more in the User Guide. Added in version 0.16. The entry test_fold[i] represents the index of the test set that\\nsample i belongs to. It is possible to exclude sample i from\\nany test set (i.e. include sample i in every training set) by\\nsetting test_fold[i] equal to -1.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Repeated Stratified K-Fold cross validator. Repeats Stratified K-Fold n times with different randomization in each\\nrepetition. Read more in the User Guide. Number of folds. Must be at least 2. Number of times cross-validator needs to be repeated. Controls the generation of the random states for each repetition.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. See also Repeats K-Fold n times. Notes Randomized CV splitters may return different results for each call of\\nsplit. You can make the results identical by setting random_state\\nto an integer.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility.\\nnp.zeros(n_samples) may be used as a placeholder. Always ignored, exists for compatibility.\\nnp.zeros(n_samples) may be used as a placeholder. Group labels for the samples used while splitting the dataset into\\ntrain/test set. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Stratified K-Fold iterator variant with non-overlapping groups. This cross-validation object is a variation of StratifiedKFold attempts to\\nreturn stratified folds with non-overlapping groups. The folds are made by\\npreserving the percentage of samples for each class. Each group will appear exactly once in the test set across all folds (the\\nnumber of distinct groups has to be at least equal to the number of folds). The difference between GroupKFold\\nand StratifiedGroupKFold is that\\nthe former attempts to create balanced folds such that the number of\\ndistinct groups is approximately the same in each fold, whereas\\nStratifiedGroupKFold attempts to create folds which preserve the\\npercentage of samples for each class as much as possible given the\\nconstraint of non-overlapping groups between splits. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of folds. Must be at least 2. Whether to shuffle each class’s samples before splitting into batches.\\nNote that the samples within each split will not be shuffled.\\nThis implementation can only shuffle groups that have approximately the\\nsame y distribution, no global shuffle will be performed. When shuffle is True, random_state affects the ordering of the\\nindices, which controls the randomness of each fold for each class.\\nOtherwise, leave random_state as None.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary. See also Takes class information into account to build folds which retain class distributions (for binary or multiclass classification tasks). K-fold iterator variant with non-overlapping groups. Notes The implementation is designed to: Mimic the behavior of StratifiedKFold as much as possible for trivial\\ngroups (e.g. when each group contains only one sample). Be invariant to class label: relabelling y = [\"Happy\", \"Sad\"] to\\ny = [1, 0] should not change the indices generated. Stratify based on samples as much as possible while keeping\\nnon-overlapping groups constraint. That means that in some cases when\\nthere is a small number of groups containing a large number of samples\\nthe stratification will not be possible and the behavior will be close\\nto GroupKFold.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Request metadata passed to the split method. Note that this method is only relevant if\\nenable_metadata_routing=True (see sklearn.set_config).\\nPlease see User Guide on how the routing\\nmechanism works. The options for each parameter are: True: metadata is requested, and passed to split if provided. The request is ignored if metadata is not provided. False: metadata is not requested and the meta-estimator will not pass it to split. None: metadata is not requested, and the meta-estimator will raise an error if the user provides it. str: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (sklearn.utils.metadata_routing.UNCHANGED) retains the\\nexisting request. This allows you to change the request for some\\nparameters and not others. Added in version 1.3. Note This method is only relevant if this estimator is used as a\\nsub-estimator of a meta-estimator, e.g. used inside a\\nPipeline. Otherwise it has no effect. Metadata routing for groups parameter in split. The updated object. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. The target variable for supervised learning problems. Group labels for the samples used while splitting the dataset into\\ntrain/test set. The training set indices for that split. The testing set indices for that split.',\n",
       " 'Stratified ShuffleSplit cross-validator. Provides train/test indices to split data in train/test sets. This cross-validation object is a merge of StratifiedKFold and\\nShuffleSplit, which returns stratified randomized folds. The folds\\nare made by preserving the percentage of samples for each class. Note: like the ShuffleSplit strategy, stratified random splits\\ndo not guarantee that all folds will be different, although this is\\nstill very likely for sizeable datasets. Read more in the User Guide. For visualisation of cross-validation behaviour and\\ncomparison between common scikit-learn split methods\\nrefer to Visualizing cross-validation behavior in scikit-learn Number of re-shuffling & splitting iterations. If float, should be between 0.0 and 1.0 and represent the proportion\\nof the dataset to include in the test split. If int, represents the\\nabsolute number of test samples. If None, the value is set to the\\ncomplement of the train size. If train_size is also None, it will\\nbe set to 0.1. If float, should be between 0.0 and 1.0 and represent the\\nproportion of the dataset to include in the train split. If\\nint, represents the absolute number of train samples. If None,\\nthe value is automatically set to the complement of the test size. Controls the randomness of the training and testing indices produced.\\nPass an int for reproducible output across multiple function calls.\\nSee Glossary.  Get metadata routing of this object. Please check User Guide on how the routing\\nmechanism works. A MetadataRequest encapsulating\\nrouting information. Returns the number of splitting iterations in the cross-validator. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Always ignored, exists for compatibility. Returns the number of splitting iterations in the cross-validator. Generate indices to split data into training and test set. Training data, where n_samples is the number of samples\\nand n_features is the number of features. Note that providing y is sufficient to generate the splits and\\nhence np.zeros(n_samples) may be used as a placeholder for\\nX instead of actual training data. The target variable for supervised learning problems.\\nStratification is done based on the y labels. Always ignored, exists for compatibility. The training set indices for that split. The testing set indices for that split. Notes Randomized CV splitters may return different results for each call of\\nsplit. You can make the results identical by setting random_state\\nto an integer.',\n",
       " 'Input checker utility for building a cross-validator. Determines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n- None, to use the default 5-fold cross validation,\\n- integer, to specify the number of folds.\\n- CV splitter,\\n- An iterable that generates (train, test) splits as arrays of indices. For integer/None inputs, if classifier is True and y is either\\nbinary or multiclass, StratifiedKFold is used. In all other\\ncases, KFold is used. Refer User Guide for the various\\ncross-validation strategies that can be used here. Changed in version 0.22: cv default value changed from 3-fold to 5-fold. The target variable for supervised learning problems. Whether the task is a classification task, in which case\\nstratified KFold will be used. The return value is a cross-validator which generates the train/test\\nsplits via the split method.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parents_dict['sklearn.model_selection']['sklearn.model_selection#Splitters']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL TO SUMMARIZE THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = summary_chain.invoke({\"descriptions\":\"\\n\\n\".join(parents_dict['sklearn']['sklearn#defaults'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = await summary_chain.abatch([{\"descriptions\":\"\\n\\n\".join(parents_dict['sklearn']['sklearn#defaults'])}],config={\"max_concurrency\": 5})\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def split_description(parent_text:List[str],MAX_WORDS:int=500):\n",
    "        split_s = []\n",
    "        running_num_words = 0\n",
    "        curr_func_string = \"\"\n",
    "        for txt in parent_text:\n",
    "            num_words = len(txt.split(\" \"))\n",
    "            running_num_words += num_words\n",
    "            if running_num_words > MAX_WORDS:\n",
    "                running_num_words = num_words\n",
    "                split_s.append(curr_func_string)\n",
    "                curr_func_string = txt\n",
    "            else:\n",
    "                curr_func_string += txt + \"\\n\"\n",
    "        if split_s == []:\n",
    "            split_s.append(curr_func_string)\n",
    "        split_s = [s for s in split_s if s!=\"\"]\n",
    "        return split_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_child_dict = {pn:{} for pn in parents_dict}\n",
    "\n",
    "for parent_name,child_nodes in parents_dict.items():\n",
    "    for child_name,child_texts in child_nodes.items():\n",
    "        child_split_list = split_description(child_texts,500)\n",
    "        parent_child_dict[parent_name].update({child_name:child_split_list})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "# from langchain_openai_limiter import LimitAwaitChatOpenAI, ChooseKeyChatOpenAI\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "# chat_model_limit_await = LimitAwaitChatOpenAI(\n",
    "#     chat_openai=model,\n",
    "#     limit_await_timeout=60.0,\n",
    "#     limit_await_sleep=0.1,\n",
    "# )\n",
    "# chat_model_key_choose = ChooseKeyChatOpenAI(\n",
    "#     chat_openai=chat_model_limit_await,\n",
    "#     openai_api_keys=[\n",
    "#         os.environ[\"OPENAI_API_KEY0\"],\n",
    "#         os.environ[\"OPENAI_API_KEY1\"],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are given a list of descriptions of different functions separated by new paragraph.\n",
    "Your task is to summarize all the text into coherent and detailed summary that covers all the functions descriptions.\n",
    "Be very diligent and make sure that no function description is left out of the final summary. \n",
    "\n",
    "Follow the following format\n",
    "\n",
    "List of function descriptions: list of function descriptions to be summarized\n",
    "Summary: summary of all the function descriptions\n",
    "\n",
    "-----\n",
    "\n",
    "List of functions descriptions: {descriptions}\n",
    "\n",
    "Summary: \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "parent_summary_dict = {pn:{} for pn in parents_dict}\n",
    "\n",
    "batch_list = []\n",
    "for parent_name,child_node in parent_child_dict.items():\n",
    "    for child_name,child_texts in child_node.items():\n",
    "        for ctext in child_texts:\n",
    "            batch_list.append({f\"{parent_name}-->{child_name}\":ctext})\n",
    "\n",
    "\n",
    "description_list = [{\"descriptions\":list(desc.values())[0]} for desc in batch_list]\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "start_key = '0'\n",
    "summaries_list = []\n",
    "for start in tqdm(range(0,len(description_list),BATCH_SIZE),desc=\"Embedding rows\"):\n",
    "    end = min(start+BATCH_SIZE,len(description_list))\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",api_key=os.environ[f'OPENAI_API_KEY{start_key}'],max_retries=5)\n",
    "    summary_chain = summary_prompt | model | StrOutputParser()\n",
    "    curr_description_list = description_list[start:end]\n",
    "    curr_summary_list = summary_chain.batch(curr_description_list,config={\"max_concurrency\":5})\n",
    "    summaries_list.extend(curr_summary_list)\n",
    "\n",
    "\n",
    "\n",
    "summary_list = summary_chain.batch(description_list,config={\"max_concurrency\":5})\n",
    "from tqdm import tqdm\n",
    "\n",
    "pbar = tqdm(total=len(batch_list))\n",
    "for sl in summary_list:\n",
    "    bl = batch_list[sl[0]]\n",
    "    parent_name = list(bl.keys())[0].split(\"-->\")[0]\n",
    "    child_name = list(bl.keys())[0].split(\"-->\")[1]\n",
    "    if child_name in parent_summary_dict[parent_name]:\n",
    "        parent_summary_dict[parent_name][child_name].append(sl[1])\n",
    "    else:\n",
    "        parent_summary_dict[parent_name].update({child_name:[sl[1]]})\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353, 353)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description_list),len(batch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = summary_chain.batch_as_completed(description_list,config={\"max_concurrency\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, \"The global scikit-learn configuration offers various options for customization. \\n- The configuration includes settings for validation of finiteness, size of temporary arrays, printing of non-default parameters, display of estimators, row vectors per chunk, accelerated pairwise-distances reduction backend, Array API dispatching, output format of transform and fit_transform, metadata routing, validation of hyper-parameters' types and values, and retrieval of current configuration values.\\n- Users can choose to skip validation for finiteness for faster processing, set a limit for temporary array size, control the printing of non-default parameters, display estimators as diagrams or text, adjust the number of row vectors per chunk, use the accelerated pairwise-distances reduction backend, enable Array API dispatching, configure output formats, enable metadata routing, and disable validation of hyper-parameters' types and values.\\n- These configurations can be customized globally and the values can be retrieved as needed. \\n- It is important to note that changing configurations can impact the performance and behavior of scikit-learn, and caution should be exercised when making modifications.\")\n",
      "(3, \"The ProbabilityCalibration class in scikit-learn offers the functionality of calibrating classifiers using isotonic regression or logistic regression. It uses cross-validation to estimate classifier parameters and then calibrate the classifier. By default, it fits a copy of the base estimator to a training subset and calibrates it using the testing subset. Predicted probabilities are averaged across individual calibrated classifiers. The calibration method can be 'sigmoid' (Platt's method) or 'isotonic', with the latter not recommended for fewer than 1000 calibration samples due to overfitting tendencies. The class allows for various cross-validation splitting strategies, including default 5-fold cross-validation or custom strategies. The number of jobs to run in parallel can be specified, with the option to use all processors. The calibrator can be fitted using training data and calibrated using testing data for each cross-validation fold, resulting in an ensemble of fitted classifier and calibrator pairs. Alternatively, cross-validation can be used to compute unbiased predictions for calibration. The class also provides methods for fitting the calibrated model, predicting class labels and probabilities, and evaluating accuracy. Metadata routing is available for passing metadata to the fit and score methods. Additionally, the class supports setting and getting parameters for the estimator.\")\n",
      "(2, 'The Mixin class for all transformers in scikit-learn provides functionality such as a fit_transform method, a set_output method for specifying output container type, and automatic wrapping of transform and fit_transform methods. OneToOneFeatureMixin and ClassNamePrefixFeaturesOutMixin are useful for defining get_feature_names_out. The Fit method fits transformer to X and y with optional fit parameters and returns a transformed version of X. The set_output API allows configuring output format for transform and fit_transform, with options like \"default\", \"pandas\", \"polars\", or None. The is_classifier method determines if an estimator is a classifier. \\n\\nThe Mixin class for all bicluster estimators in scikit-learn offers properties like biclusters_, methods to get indices, shape, and submatrix of a bicluster. It provides a convenient way to access row and column indicators together, as well as specific information about biclusters like indices, shape, and submatrix.\\n\\nThe Mixin class for all classifiers defines attributes like _estimator_type and methods like score for calculating mean accuracy on test data. It enforces that fit requires passing y through the requires_y tag. \\n\\nThe Mixin class for all density estimators sets _estimator_type to \"DensityEstimator\" and provides a score method for calculating the model\\'s score on test samples. This class maintains API consistency by including a score method even though it defaults to a no-op.')\n",
      "(1, 'The function descriptions cover various mixins and base classes for estimators in scikit-learn. \\n\\nThe base class provides default implementations for setting and getting parameters, textual and HTML representation, serialization, and data validation. Estimators should specify all parameters in their __init__ method as explicit keyword arguments.\\n\\nThe MetadataMixin class provides metadata routing and encapsulates routing information for an object. ParameterMixin class allows getting and setting parameters for an estimator, including nested objects like Pipelines.\\n\\nThe PrefixMixin class is used for transformers that generate their own feature names, prefixing them with the lowercased class name. ClusterMixin class is for cluster estimators, defining _estimator_type as \"clusterer\" and a fit_predict method for clustering and returning cluster labels.\\n\\nThe MetaEstimatorMixin defines mandatory estimator parameters, while the OutlierMixin class is for outlier detection estimators, with _estimator_type as \"outlier_detector\" and a fit_predict method that returns -1 for outliers and 1 for inliers.')\n"
     ]
    }
   ],
   "source": [
    "for i in ans:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",api_key=os.environ['OPENAI_API_KEY'],max_retries=5,model_kwargs={\"batch_size\":20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 20}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_graph(sklearn_graph):\n",
    "from copy import deepcopy\n",
    "import re\n",
    "embed_docs = []\n",
    "embed_metadata = []\n",
    "\n",
    "with open(\"parent_summary.json\",\"r\") as jsonfile:\n",
    "    parent_summary_dict = json.load(jsonfile)\n",
    "\n",
    "def clean_text(s:str):\n",
    "    s = re.sub(\"\\n\",\" \",s)\n",
    "    s = json.dumps(s)\n",
    "    return s\n",
    "for node,attr in sklearn_graph.nodes(data=True):\n",
    "    type = attr.get('type')\n",
    "    \n",
    "    if type == 'function_node':\n",
    "        ftext = attr['function_text']\n",
    "        if ftext == \"\": continue\n",
    "        embed_docs.append(clean_text(attr['function_text']))\n",
    "        for k,v in attr.items():\n",
    "            if not isinstance(v,str):\n",
    "                attr[k] = str(v)\n",
    "        embed_metadata.append(attr)\n",
    "    elif type == 'parent_node':\n",
    "        embed_docs.append(clean_text(parent_summary_dict[node]))\n",
    "        for k,v in attr.items():\n",
    "            if not isinstance(v,str):\n",
    "                attr[k] = str(v)\n",
    "        embed_metadata.append(attr)\n",
    "    elif type == 'sub_level_node':\n",
    "        for ctext in attr['child_texts']:\n",
    "            embed_docs.append(clean_text(ctext))\n",
    "            sub_level_attr = deepcopy(attr)\n",
    "            del sub_level_attr['child_texts']\n",
    "            for k,v in sub_level_attr.items():\n",
    "                if not isinstance(v,str):\n",
    "                    sub_level_attr[k] = str(v)\n",
    "            embed_metadata.append(sub_level_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "from chromadb.utils.batch_utils import create_batches\n",
    "\n",
    "def build_database(docs, metadata, api_key):\n",
    "    database_path = \"Sklearn\"\n",
    "    collection_name = \"sklearn_docs\"\n",
    "    load_dotenv(find_dotenv(), override=True)\n",
    "    emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=api_key, model_name=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    client = chromadb.PersistentClient(path=database_path)\n",
    "    sklearn_collection = client.get_or_create_collection(\n",
    "        name=collection_name, embedding_function=emb_fn\n",
    "    )\n",
    "\n",
    "    sklearn_ids = [f\"id{i}\" for i in range(len(docs))]\n",
    "    batches = create_batches(\n",
    "        api=client, ids=sklearn_ids, documents=docs, metadatas=metadata\n",
    "    )\n",
    "    for batch in batches:\n",
    "        sklearn_collection.add(ids=batch[0], documents=batch[3], metadatas=batch[2])\n",
    "    return sklearn_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1154"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed_docs[:len(embed_docs)//2]) + len(embed_docs[len(embed_docs)//2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 8304 tokens (8304 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m curr_docs \u001b[38;5;241m=\u001b[39m embed_docs[start:end]\n\u001b[1;32m      9\u001b[0m curr_metadata \u001b[38;5;241m=\u001b[39m embed_metadata[start:end]\n\u001b[0;32m---> 10\u001b[0m sklearn_collection \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_docs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcurr_docs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcurr_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcurr_metadata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOPENAI_API_KEY0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m sklearn_collection \u001b[38;5;241m=\u001b[39m build_database(curr_docs[\u001b[38;5;28mlen\u001b[39m(curr_docs)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:],curr_metadata[\u001b[38;5;28mlen\u001b[39m(curr_metadata)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:],os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleeping...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[63], line 23\u001b[0m, in \u001b[0;36mbuild_database\u001b[0;34m(docs, metadata, api_key)\u001b[0m\n\u001b[1;32m     19\u001b[0m batches \u001b[38;5;241m=\u001b[39m create_batches(\n\u001b[1;32m     20\u001b[0m     api\u001b[38;5;241m=\u001b[39mclient, ids\u001b[38;5;241m=\u001b[39msklearn_ids, documents\u001b[38;5;241m=\u001b[39mdocs, metadatas\u001b[38;5;241m=\u001b[39mmetadata\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m---> 23\u001b[0m     \u001b[43msklearn_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sklearn_collection\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/chromadb/api/models/Collection.py:154\u001b[0m, in \u001b[0;36mCollection.add\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mimages)\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/chromadb/api/models/Collection.py:633\u001b[0m, in \u001b[0;36mCollection._embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide an embedding function to compute embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.trychroma.com/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    632\u001b[0m     )\n\u001b[0;32m--> 633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/chromadb/api/types.py:193\u001b[0m, in \u001b[0;36mEmbeddingFunction.__init_subclass__.<locals>.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m: EmbeddingFunction[D], \u001b[38;5;28minput\u001b[39m: D) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[0;32m--> 193\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_embeddings(maybe_cast_one_to_many_embedding(result))\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/chromadb/utils/embedding_functions.py:188\u001b[0m, in \u001b[0;36mOpenAIEmbeddingFunction.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Call the OpenAI Embedding API\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v1:\n\u001b[0;32m--> 188\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deployment_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_name\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Sort resulting embeddings by index\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     sorted_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(embeddings, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m e: e\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/resources/embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Function Calling/openbb-env/lib/python3.10/site-packages/openai/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1028\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 8304 tokens (8304 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "import os\n",
    "import time\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "BATCH_SIZE = 200\n",
    "for start in range(0,len(embed_docs),BATCH_SIZE):\n",
    "    end = min(start+BATCH_SIZE,len(embed_docs))\n",
    "    curr_docs = embed_docs[start:end]\n",
    "    curr_metadata = embed_metadata[start:end]\n",
    "    sklearn_collection = build_database(curr_docs[:len(curr_docs)//2],curr_metadata[:len(curr_metadata)//2],os.environ['OPENAI_API_KEY0'])\n",
    "    sklearn_collection = build_database(curr_docs[len(curr_docs)//2:],curr_metadata[len(curr_metadata)//2:],os.environ['OPENAI_API_KEY1'])\n",
    "    print(\"Sleeping...\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
