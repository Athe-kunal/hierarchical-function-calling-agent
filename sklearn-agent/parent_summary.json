{
    "sklearn": "The global scikit-learn configuration context manager allows for various settings to be adjusted. These settings include skipping validation for finiteness, limiting the size of temporary arrays, printing only non-default parameters, displaying estimators as diagrams or text, configuring pairwise distances reduction, using Array API dispatching, setting output formats for transformations, enabling metadata routing, and disabling validation of hyper-parameters' types and values. These settings can be adjusted to optimize performance, memory usage, and output formats. Additionally, all settings return to their previous values when the context manager is exited. ",
    "sklearn.base": "The base class for all estimators in scikit-learn provides default implementations for setting and getting parameters, textual and HTML representation, serialization, and data validation. Estimators should specify all parameters in their __init__ method. The mixin classes provide functionality for transformers, cluster estimators, meta estimators, and outlier detection estimators. The cluster estimators have a fit_predict method for clustering data, while the outlier detection estimators have a fit_predict method that returns labels for outliers and inliers. The mixin classes also define required parameters and the type of estimator. The transformer mixin generates feature names and prefixes them with the class name. The mixin classes in scikit-learn provide additional functionality for different types of estimators. The OneToOneFeatureMixin and ClassNamePrefixFeaturesOutMixin help define the get_feature_names_out method. The BiclusterEstimatorMixin provides methods for working with bicluster estimators, such as getting row and column indicators, indices, shape, and submatrix. The ClassifierMixin provides functionality for classifiers, including setting the estimator type and calculating the mean accuracy on test data. The DensityEstimatorMixin provides functionality for density estimators, including setting the estimator type and returning the score of the model on the data. Each mixin class adds specific methods and attributes to the estimators they are applied to, enhancing their capabilities. ",
    "sklearn.calibration": "1. Calculate the square root of a number\n2. Find the maximum value in a list of numbers\n3. Sort a list of strings in alphabetical order\n4. Check if a number is prime\n\nSummary:\nThe functions provided include calculating the square root of a number, finding the maximum value in a list of numbers, sorting a list of strings in alphabetical order, and checking if a number is prime. These functions cover a range of mathematical and sorting operations. The ProbabilityCalibration class in scikit-learn allows for probability calibration with isotonic regression or logistic regression. It uses cross-validation to estimate the parameters of a classifier and subsequently calibrate it. By default, an ensemble approach is used where a copy of the base estimator is fitted to the training subset and calibrated using the testing subset for each cross-validation split. Predicted probabilities are then averaged across these individual calibrated classifiers. Alternatively, cross-validation can be used to obtain unbiased predictions, which are then used for calibration. The calibration can be based on the decision_function method of the estimator or predict_proba if the former does not exist. It is recommended to avoid using isotonic calibration with too few samples as it may lead to overfitting. The method for calibration can be 'sigmoid' (Platt's method) or 'isotonic'. The number of cross-validation folds, the number of jobs to run in parallel, and the fitting strategy for the calibrator can also be specified. The class provides methods for fitting the calibrated model, predicting the target of new samples, obtaining calibrated probabilities of classification, and calculating the mean accuracy on test data. Additionally, metadata routing can be enabled to pass metadata to the fit and score methods. This class is useful for calibrating the output of classifiers to provide more accurate predict_proba outputs. The function computes true and predicted probabilities for a calibration curve, assuming inputs from a binary classifier and discretizing the [0, 1] interval into bins. Calibration curves, also known as reliability diagrams, are generated. It requires true targets, probabilities of the positive class, the label of the positive class, and the number of bins to discretize the interval. A bigger number of bins requires more data. Bins with no samples will not be returned. The strategy used to define the widths of the bins can be either identical widths or based on the number of samples and y_prob. The function also calculates the fraction of positives and the mean predicted probability in each bin. References to relevant research are provided for further reading. ",
    "sklearn.cluster": "1. Calculate the square root of a number\n2. Find the maximum value in a list of numbers\n3. Sort a list of strings in alphabetical order\n4. Check if a number is prime\n\nSummary:\nThe functions provided include calculating the square root of a number, finding the maximum value in a list of numbers, sorting a list of strings in alphabetical order, and checking if a number is prime. These functions cover a range of mathematical and sorting operations. The Affinity Propagation Clustering function allows for clustering of data by passing messages between data points. It includes parameters such as damping factor, maximum number of iterations, preferences for each point, type of affinity to use, and whether to be verbose. The function also provides options for setting a pseudo-random number generator and controlling the starting state. It supports different types of clustering methods such as K-Means, Mini-Batch K-Means, Mean Shift, and applying clustering to a projection of the normalized Laplacian. The algorithmic complexity of affinity propagation is quadratic in the number of points. It is important to note that when the algorithm does not converge, caution should be taken when interpreting the results. The function also allows for fitting clustering from features or affinity matrix, returning cluster labels, predicting the closest cluster for new data, and getting metadata routing information. The BIRCH clustering algorithm is a memory-efficient, online-learning algorithm that constructs a tree data structure with cluster centroids read off the leaf. The algorithm allows for the setting of a threshold radius for subclusters and a maximum number of subclusters in each node. It also provides options for the final number of clusters, fitting a model, computing labels, and making copies of data. The tree structure consists of nodes with subclusters, each maintaining linear and squared sums, as well as the number of samples. New points entering the root are merged with the closest subcluster recursively. The algorithm also supports incremental updates of center positions using mini-batches. Additionally, there are references to alternative implementations and detailed information on the structure and properties of the tree. The algorithm allows for online learning to prevent rebuilding the tree from scratch and provides methods for predicting data using subcluster centroids and transforming data into subcluster centroids dimensions. The DBSCAN clustering algorithm is used to find core samples of high density and expand clusters from them, making it suitable for data with clusters of similar density. It has a worst-case memory complexity of \\(O({n}^2)\\) due to the eps parameter and min_samples values. The algorithm computes pointwise distances and finds nearest neighbors using the NearestNeighbors module. The Minkowski metric is used to calculate distances between points, with the default being the Euclidean distance. The algorithm can handle sparse graphs and precomputed distance matrices. The cluster labels, core sample indices, and number of features seen during fitting are provided as outputs. The implementation is optimized for memory usage, but it may attract higher memory complexity when querying nearest neighborhoods. Other clustering algorithms like OPTICS provide similar results with lower memory usage. References to the original DBSCAN algorithm and related research are also provided. The algorithm can be used to cluster training instances or distances between instances, with the option to provide sample weights. Metadata routing and parameter settings are available for customization. HDBSCAN is a hierarchical density-based clustering algorithm that performs DBSCAN over varying epsilon values to find the best stability. It can find clusters of varying densities and is more robust to parameter selection compared to DBSCAN. The algorithm includes parameters such as minimum cluster size, core point neighborhood size, distance threshold, and cluster merging threshold. It also allows for selecting cluster centers as centroids or medoids. Outliers are labeled differently, and the algorithm provides probabilities for sample clustering. HDBSCAN can handle large datasets with options for parallel processing and different cluster selection methods. The algorithm also provides cluster centroids and medoids, and can be used for online learning. References for the algorithm are provided, and it can return cluster labels based on the input data. Additionally, it offers options for metadata routing and parameter retrieval and setting. Mean shift clustering is a centroid-based algorithm that aims to discover \"blobs\" in a smooth density of samples by updating candidates for centroids to be the mean of the points within a given region. The candidates are then filtered to eliminate near-duplicates to form the final set of centroids. Seeding is performed using a binning technique for scalability. The bandwidth used in the flat kernel can be estimated using sklearn.cluster.estimate_bandwidth. Seeds can be used to initialize kernels, and the initial kernel locations can be set to the discretized version of points for faster processing. Orphans that are not within any kernel can be assigned to the nearest kernel or given a cluster label of -1. The algorithm can be parallelized for tasks such as searching for nearest neighbors and label assignments. The maximum number of iterations per seed point can be specified, as well as the number of jobs to use for computation. The complexity of the algorithm tends towards O(T*n*log(n)) in lower dimensions and O(T*n^2) in higher dimensions. The algorithm can be further optimized by using fewer seeds. The mean shift clustering algorithm is less scalable than the estimate_bandwidth function. References for the algorithm are available in the literature. The algorithm can perform clustering on input data and return cluster labels. It also provides methods for predicting cluster labels for new data and setting parameters for the estimator. The OPTICS (Ordering Points To Identify the Clustering Structure) function estimates clustering structure from a vector array. It is closely related to DBSCAN and finds core samples of high density to expand clusters from them. Unlike DBSCAN, OPTICS keeps cluster hierarchy for a variable neighborhood radius, making it better suited for large datasets. Clusters can be extracted using a DBSCAN-like method or an automatic technique. The implementation deviates from the original OPTICS by first performing k-nearest-neighborhood searches on all points to identify core sizes. The function allows for specifying parameters such as the number of samples in a neighborhood, the maximum distance between two samples, the metric for distance computation, and additional keyword arguments for the metric function. The extraction method used to extract clusters can be specified as \"xi\" or \"dbscan\". The function also allows for correcting clusters according to predecessors calculated by OPTICS. Other parameters include the algorithm used to compute nearest neighbors, leaf size, caching, and the number of parallel jobs to run for neighbor search. The function provides cluster labels for each point in the dataset and information on reachability distances, core distances, and predecessors. The list of clusters is ordered according to size, with larger clusters encompassing smaller ones. The function also provides the number of features seen during fit and names of features seen during fit. Additionally, the function allows for performing OPTICS clustering, extracting an ordered list of points and reachability distances, and performing initial clustering using a specified maximum distance. The function returns a fitted instance of itself and cluster labels for the input data. Spectral Clustering is a useful method for clustering when the structure of the individual clusters is highly non-convex or when a measure of the center and spread of the cluster is not a suitable description of the complete cluster, such as nested circles on the 2D plane. It can be used to find normalized graph cuts when the affinity matrix is the adjacency matrix of a graph. The method involves constructing an affinity matrix using a kernel function like the Gaussian kernel or a k-nearest neighbors connectivity matrix. Alternatively, a user-provided affinity matrix can be specified. The number of eigenvectors to use for the spectral embedding can be specified, along with the kernel coefficient for different types of kernels. The stopping criterion for eigen decomposition of the Laplacian matrix can also be set. Different strategies for assigning labels in the embedding space are available, including k-means, discretization, and the cluster_qr method. The function also allows for setting the degree of the polynomial kernel, the number of parallel jobs to run, and the verbosity mode. The affinity matrix used for clustering, labels of each point, number of features seen during fit, and names of features seen during fit are also available. The algorithm can be applied to perform spectral clustering from features or an affinity matrix, returning cluster labels. Additionally, metadata routing information, parameters for the estimator, and the ability to set parameters of the estimator are provided. The Affinity Propagation Clustering function performs clustering of data based on a matrix of similarities between points and preferences for each point. The number of clusters is influenced by the input preferences value, with options to set preferences to the median of similarities or the minimum value for fewer clusters. Additional parameters include the number of iterations for convergence, maximum iterations, damping factor, and verbosity level. The function can return cluster centers, labels for each point, and the number of iterations run. When the algorithm does not converge, it still provides cluster center indices and labels, but caution is advised. The Xi-steep method is used to automatically extract clusters, with reachability distances and predecessors calculated by OPTICS. Parameters such as min_samples, steepness, and correct clusters are used to define cluster boundaries and assign labels to samples.Clusters are represented as lists of indices, ordered by size. The DBSCAN clustering algorithm is used to group samples based on their proximity to each other. It requires parameters such as the maximum distance between samples to be considered neighbors, the number of samples in a neighborhood for a point to be a core point, the metric for calculating distances, and the number of parallel jobs to run. The algorithm assigns cluster labels to each point, with noisy samples labeled as -1. The implementation optimizes memory usage but may have higher memory complexity when querying nearest neighborhoods. To reduce memory and computation time, pre-compute sparse neighborhoods or remove duplicate points. OPTICS is an alternative clustering algorithm with lower memory usage. References include papers by Ester et al. (1996) and Schubert et al. (2017) on the benefits of using DBSCAN. The K-means clustering algorithm is used to cluster observations into a specified number of clusters with a given number of centroids. The algorithm can be initialized using 'k-means++' or 'random' methods, with the option to run multiple times for the best results. The maximum number of iterations and convergence tolerance can be specified. Different algorithms like \"lloyd\" and \"elkan\" can be used, with \"elkan\" being more memory intensive but efficient on certain datasets. The final output includes the centroids, labels for each observation, inertia criterion, and number of iterations for the best results. The data can be pre-processed for numerical accuracy, and options for random number generation are available. The function performs mean shift clustering of data using a flat kernel. The bandwidth can be specified or determined using a heuristic based on the median of all pairwise distances. Initial kernel locations can be set or determined based on bin seeding. The algorithm can be sped up by setting certain options. Orphans, points not within any kernel, can be assigned to the nearest kernel or given a cluster label of -1. The maximum number of iterations per seed point can be specified. Parallelization can be used for nearest neighbor search and label assignments. The function returns the coordinates of cluster centers and cluster labels for each point. For more details and an example, refer to the User Guide and examples/cluster/plot_mean_shift.py. The Ward clustering algorithm is used to recursively merge clusters in a way that minimally increases within-cluster variance. It operates on a feature matrix and can take into account topological structure between samples using a connectivity matrix. The algorithm can stop early at a specified number of clusters to save computation time. It can return various outputs such as distances between clusters, the structure of the tree, the number of connected components, and the parent of each node. The distances between the centers of nodes are calculated using a weighted Euclidean distance formula. This algorithm is also known as the incremental algorithm. Agglomerative Clustering is a function that recursively merges pairs of clusters of sample data using a linkage distance. The number of clusters to find must be specified, and the metric used to compute the linkage can be \"euclidean\", \"l1\", \"l2\", \"manhattan\", \"cosine\", or \"precomputed\". The function also allows for caching the output of the computation of the tree and defining a connectivity matrix. It provides options to stop early the construction of the tree at a certain number of clusters and specifies which linkage criterion to use, such as 'ward', 'average', 'complete', 'maximum', or 'single'. The function also includes a linkage distance threshold, computes distances between clusters, and returns the number of clusters found by the algorithm, cluster labels, number of leaves in the hierarchical tree, and the estimated number of connected components in the graph. Additionally, it provides information on the children of each non-leaf node, distances between nodes, and allows for hierarchical clustering with ward linkage. The function can fit the hierarchical clustering from features or a distance matrix, return the result of each sample's clustering assignment, get metadata routing, get parameters for the estimator, and set the parameters of the estimator. The Bisecting K-Means clustering function is used to form clusters and generate centroids. It allows for different methods of initialization, such as 'k-means++' and 'random'. The algorithm runs multiple times with different centroid seeds to produce the best output. Various parameters can be set, such as the number of iterations, verbosity mode, and convergence tolerance. The function also provides information on cluster centers, labels, and cluster distances. Additionally, it offers methods for predicting cluster indices and setting output containers. The function supports metadata routing and parameter configuration. It is important to note that the function may be inefficient when the number of clusters is less than 3. The Agglomerate features function recursively merges pairs of clusters of features based on a specified number of clusters, metric for linkage computation, and connectivity matrix. It allows for early stopping of tree construction and specifies the linkage criterion to use. Additionally, it computes distances between clusters and provides cluster labels, number of leaves in the hierarchical tree, and estimated number of connected components. The function also returns the transformer and allows for fitting and transforming data, as well as setting output containers. It provides methods for getting output feature names, metadata routing, parameters, and inverse transformation. The function can assign values to each cluster of samples and set parameters for the estimator. Finally, it allows for transforming a new matrix using the built clustering and obtaining pooled values for each feature cluster. The K-Means clustering algorithm is a method for grouping data points into a specified number of clusters. It involves selecting the number of clusters and centroids, as well as choosing an initialization method such as 'k-means++' or 'random'. The algorithm can be run multiple times with different centroid seeds to find the best output. The maximum number of iterations and convergence tolerance can also be specified. The algorithm can use either the \"lloyd\" or \"elkan\" variation, with the latter being more memory intensive but potentially more efficient. The algorithm computes cluster centers, assigns labels to each point, and calculates the sum of squared distances to the closest cluster center. It is recommended to restart the algorithm several times to avoid local minima. Various examples and demonstrations are provided for using K-Means clustering, such as color quantization, clustering text documents, and comparing K-Means with MiniBatchKMeans. Additional methods are available for transforming data, setting parameters, and requesting metadata. Mini-Batch K-Means clustering is a method for clustering data into a specified number of clusters with a specified number of centroids. The algorithm uses methods for initialization such as 'k-means++' and 'random'. It also allows for control over the maximum number of iterations, batch size, and early stopping criteria. The algorithm can handle sparse high-dimensional problems and offers options for the number of random initializations to try. The inertia criterion is used to evaluate the quality of the clustering. The algorithm can compute cluster centers, predict cluster indices, and transform data into cluster-distance space. It also provides options for metadata routing and setting output containers. The algorithm can be updated and configured with specific parameters. Overall, Mini-Batch K-Means clustering is a versatile clustering algorithm with various options for customization and optimization. Spectral biclustering is a method that partitions rows and columns of data under the assumption of an underlying checkerboard structure. The number of row and column clusters in the checkerboard structure can be specified, along with the method of normalizing and converting singular vectors into biclusters. Options for normalization include 'scale', 'bistochastic', or 'log', with 'log' being recommended by the authors. However, if the data is sparse, 'bistochastic' is the default choice. The algorithm for finding singular vectors can be 'randomized' or 'arpack', with 'randomized' being faster for large matrices and 'arpack' being more accurate. Mini-batch k-means can be used for faster results, with options for initialization and number of random initializations. Results of the clustering include row and column partition labels, as well as indicators for rows and columns. Additional features such as metadata routing and parameter retrieval are also available. The method works with sparse matrices and allows for setting parameters of the estimator. The Spectral Co-Clustering algorithm, introduced by Dhillon in 2001, clusters rows and columns of an array X to solve the relaxed normalized cut of the bipartite graph created from X. The resulting bicluster structure is block-diagonal, with each row and column belonging to exactly one bicluster. This algorithm supports sparse matrices as long as they are nonnegative. Users can specify the number of biclusters to find and select the algorithm for finding singular vectors, which can be either 'randomized' or 'arpack'. The number of vectors to use in calculating the SVD can also be specified. Additionally, users can choose to use mini-batch k-means for faster results, specify the initialization method for the k-means algorithm, and set the number of random initializations to try. The results of the clustering, including row and column indicators, as well as the bicluster labels of each row and column, are available after fitting the model. The algorithm also provides a convenient way to get row and column indicators together. Users can access metadata routing information and get parameters for the estimator. The algorithm works with sparse matrices and allows users to set parameters. The functions provided include performing DBSCAN extraction with an arbitrary epsilon, extracting clusters in linear time, calculating reachability distances using OPTICS, determining core distances, ordering point indices, setting the DBSCAN epsilon parameter to be less than the maximum epsilon, and obtaining estimated labels. It is important to note that setting eps close to max_eps will result in labels similar to DBSCAN with similar settings. The OPTICS algorithm is used to compute the reachability graph, with options for defining the neighborhood size, maximum distance between samples, and distance metric. Various metrics can be used, including those from scikit-learn and scipy.spatial.distance. The algorithm can use different methods for computing nearest neighbors, such as BallTree or KDTree. Additional parameters like leaf size and number of parallel jobs can be specified. The output includes a cluster-ordered list of sample indices, core distances, reachability distances, and predecessor points. For more details, refer to the documentation by Ankerst et al. (1999). The function estimates the bandwidth to use with the mean-shift algorithm, which takes at least quadratic time in n_samples. For large datasets, subsampling can be done by setting n_samples or setting the bandwidth parameter to a small value without estimation. The input points should be between [0, 1], with 0.5 representing the median of all pairwise distances. The number of samples to use can be specified, with all samples being used if not given. The function also allows for setting the number of parallel jobs for neighbors search, initializing n_clusters seeds according to k-means++, and specifying the weights for each observation in X. Additionally, it determines the squared Euclidean norm of each data point and the number of seeding trials for each center. The function selects initial cluster centers for k-means clustering in a smart way to speed up convergence. ",
    "sklearn.compose": "1. Calculate the square root of a number\n2. Find the maximum value in a list of numbers\n3. Sort a list of strings in alphabetical order\n4. Check if a number is prime\n\nSummary:\nThe functions provided include calculating the square root of a number, finding the maximum value in a list of numbers, sorting a list of strings in alphabetical order, and checking if a number is prime. These functions cover a range of mathematical and sorting operations. The ColumnTransformer class in scikit-learn allows for the application of different transformers to columns of an array or pandas DataFrame. This is useful for combining various feature extraction mechanisms or transformations into a single transformer, especially for heterogeneous or columnar data. The transformers are specified as a list of (name, transformer, columns) tuples, allowing for flexibility in setting parameters and searching in grid search. Special strings like 'drop' and 'passthrough' can be used to indicate dropping columns or passing them through untransformed. The data can be indexed on its second axis, with integers representing positional columns and strings referencing DataFrame columns by name. The output of the transformers can be weighted multiplicatively, and the transformed output can be either sparse or dense depending on the individual transformers and the sparse_threshold keyword. The ColumnTransformer also provides methods for accessing fitted transformers by name, getting feature names, and configuring the output format. Additionally, it supports metadata routing and allows for setting parameters of the estimator and contained subobjects. Overall, the ColumnTransformer class in scikit-learn is a powerful tool for applying multiple transformers to different columns of a dataset and concatenating the results. The make_column_selector function allows for the selection of columns to be used with ColumnTransformer based on data type or column name using regex. Multiple selection criteria can be used, with all criteria needing to match for a column to be selected. The function can be used within a ColumnTransformer to select columns based on data type. The function also allows for the inclusion or exclusion of specific data types. Overall, make_column_selector is a callable that helps in selecting columns from a DataFrame to be used in a ColumnTransformer. The Meta-estimator is used to regress on a transformed target, allowing for non-linear transformations such as QuantileTransformer or functions like np.log and np.exp. It automatically clones the regressor and transformer before fitting, with the option to use LinearRegression if no regressor is specified. The target y is converted into a 2-dimensional array internally, and reshaped during prediction. The coefficient of determination \\(R^2\\) is calculated for the prediction, with a score of 1.0 indicating the best possible fit. The method also allows for setting parameters, predicting values, and requesting metadata for scoring. Additionally, the method supports metadata routing for sample weights and returns the updated object. ",
    "sklearn.covariance": "1. Calculate the square root of a number\n2. Find the maximum value in a list of numbers\n3. Sort a list of strings in alphabetical order\n4. Check if a number is prime\n\nSummary:\nThe functions provided include calculating the square root of a number, finding the maximum value in a list of numbers, sorting a list of strings in alphabetical order, and checking if a number is prime. These functions cover a range of mathematical and sorting operations. The Minimum Covariance Determinant (MCD) estimator is used for detecting outliers in a Gaussian distributed dataset. It can estimate the precision and compute the support of robust location and covariance estimates. The algorithm can work with data whose mean is close to zero but not exactly zero. The proportion of points to be included in the support of the raw MCD estimate can be specified, as well as the amount of contamination in the dataset. The decision function is defined based on the raw scores and an offset. The estimator also provides information on the raw robust estimated location and covariance before correction and re-weighting. Mahalanobis distances of the training set observations can be computed, along with the number of features seen during fit. The estimator also includes features such as Sparse inverse covariance estimation, LedoitWolf Estimator, and Oracle Approximating Shrinkage Estimator. Outlier detection from covariance estimation may not perform well in high-dimensional settings. Corrections to raw MCD estimates can be applied using empirical correction factors. The Mean Squared Error between two covariance estimators can be computed, and the decision function of given observations can be calculated. The EllipticEnvelope model can be fitted to the data, and labels for inliers and outliers can be predicted. The precision matrix, squared Mahalanobis distances, and negative Mahalanobis distances can also be computed. Re-weighting of raw MCD estimates is possible using Rousseeuw's method. The mean accuracy on test data and labels can be calculated, and parameters of the estimator can be set. Metadata routing and sample weight parameters can also be requested. The GraphicalLasso function is used for sparse inverse covariance estimation with an l1-penalized estimator. The regularization parameter alpha controls the sparsity of the inverse covariance matrix. Users can choose between coordinate descent or LARS as the Lasso solver. The function can handle precomputed covariance matrices or estimate empirical covariances from the data. Other parameters include tolerance for convergence, maximum number of iterations, and machine-precision regularization. The function can return estimated mean, covariance matrix, pseudo-inverse matrix, number of iterations, and objective function values. It also provides options for cross-validated choice of l1 penalty, computing Mean Squared Error between covariance estimators, and computing Mahalanobis distances. Users can compute log-likelihood under the estimated Gaussian model and set parameters of the estimator. Metadata routing and parameter retrieval are also available features. The Ledoit-Wolf Estimator is a form of shrinkage used for large-dimensional covariance matrices. It computes the shrinkage coefficient using a specific formula by Ledoit and Wolf. The estimator can store the estimated precision and split the covariance matrix into blocks for memory optimization. It provides estimates for the covariance matrix, mean, pseudo-inverse matrix, and coefficient for the shrunk estimate. Additionally, it offers features for outlier detection, maximum likelihood covariance estimation, sparse inverse covariance estimation, robust covariance estimation, and shrinkage covariance estimation. The regularized covariance is calculated using a formula that includes shrinkage and the covariance matrix. The estimator also allows for computing the Mean Squared Error between two covariance estimators. Other functionalities include fitting the Ledoit-Wolf shrunk covariance model, computing Mahalanobis distances, log-likelihood of test data under a Gaussian model, setting parameters, and requesting metadata routing. The Oracle Approximating Shrinkage Estimator is a method used for covariance estimation, which can store the estimated precision if specified. It is useful when working with data that has a mean close to zero. The estimator provides various outputs such as the estimated covariance matrix, location (mean), pseudo inverse matrix, and coefficient in the convex combination. Additionally, it includes features like the number of features seen during fit and names of features seen during fit. Other related functions include outlier detection, maximum likelihood covariance estimation, sparse inverse covariance estimation, LedoitWolf Estimator, Minimum Covariance Determinant, and covariance estimator with shrinkage. The shrinkage formulation implemented in the regularized covariance differs from the original article by omitting a small value operation for large feature numbers. References are provided for further reading. The estimator also includes functions for computing Mean Squared Error between covariance estimators, fitting the model to data, getting metadata routing, retrieving parameters, computing Mahalanobis distances, computing log-likelihood, and setting parameters. Metadata routing options are available for passing metadata to the score method. The functions provided in the list are related to computing different types of covariance estimators. These include the Maximum Likelihood covariance estimator, the Ledoit-Wolf covariance matrix estimator, and the Oracle Approximating Shrinkage estimator. Each function allows for the computation of covariance estimates from data, with options to center the data before computation. The shrunk covariance is calculated using a convex combination formula, with specific coefficients and formulas provided in the notes. The Oracle Approximating Shrinkage estimator uses a different shrinkage formulation compared to the original article referenced. Overall, these functions provide tools for estimating covariance matrices with different approaches and optimizations. The Maximum Likelihood Covariance Estimator is a method used to estimate the covariance matrix, pseudo-inverse matrix, and mean of a dataset. It can be used with or without centering the data before computation. Other related functions include Sparse Inverse Covariance Estimation, LedoitWolf Estimator, Minimum Covariance Determinant, Oracle Approximating Shrinkage Estimator, and Covariance Estimator with Shrinkage. The Mean Squared Error function allows for comparison between two covariance estimators. Additional functions include computing Mahalanobis distances, log-likelihood of test data under a Gaussian model, setting parameters of the estimator, and requesting metadata routing. These functions provide a comprehensive set of tools for covariance estimation and analysis. The GraphicalLassoCV function is used for sparse inverse covariance estimation with a cross-validated choice of the l1 penalty. It allows for the selection of the penalization parameter and provides various outputs such as the estimated mean, covariance matrix, precision matrix, and penalization parameter selected. The function uses a grid search approach to find the optimal penalization parameter and can handle ill-conditioned systems. Additionally, it provides options for setting the tolerance for convergence, the solver type, number of iterations, and parallel processing. The function also offers methods for computing the Mean Squared Error between covariance estimators, fitting the covariance model, computing Mahalanobis distances, and calculating log-likelihood under the estimated Gaussian model. Furthermore, it includes options for setting parameters, getting metadata routing, and requesting metadata for the score method. The Minimum Covariance Determinant (MCD) is a robust estimator of covariance that is suitable for Gaussian-distributed data, but can also be used with unimodal, symmetric distributions. It is not recommended for multi-modal data. The MCD estimator can be used with or without storing the estimated precision, and it allows for specifying the proportion of points to be included in the support of the raw estimate. The algorithm used in MCD estimation is the FastMCD algorithm. \n\nThe MCD estimator provides estimates for the robust location, covariance matrix, and pseudo-inverse matrix. It also computes Mahalanobis distances for the training set observations. Other related estimators include the Maximum Likelihood Covariance Estimator, Sparse Inverse Covariance Estimators, and Oracle Approximating Shrinkage Estimator. \n\nThere are methods available to correct raw MCD estimates, compute Mean Squared Error between covariance estimators, re-weight observations, and compute the log-likelihood of test data under the estimated Gaussian model. Additionally, there are methods for setting parameters of the estimator and requesting metadata routing. \n\nOverall, the MCD estimator is a powerful tool for robust covariance estimation, particularly for Gaussian-distributed data, with various options and methods available for customization and analysis. The Covariance estimator with shrinkage allows for the computation of the estimated covariance matrix, estimated mean, and estimated pseudo inverse matrix. It also includes the option to store the estimated precision. The shrinkage coefficient is used in a convex combination for the computation of the shrunk estimate. Other related functions include the Maximum Likelihood Covariance Estimator, Sparse Inverse Covariance Estimation with an l1-penalized estimator, and the Minimum Covariance Determinant (robust estimator of covariance). The Oracle Approximating Shrinkage Estimator is also available. The regularized covariance is calculated using a specific formula involving the shrinkage coefficient and the covariance matrix. Additionally, functions are provided for computing the Mean Squared Error between two covariance estimators, fitting the shrunk covariance model to data, computing Mahalanobis distances, and calculating the log-likelihood of test data under the estimated Gaussian model. The precision matrix can be retrieved using a getter function, and parameters can be set using a specific method. Metadata routing is available for certain parameters, allowing for more flexibility in handling metadata. ",
    "sklearn.cross_decomposition": "1. Calculate the square root of a number\n2. Find the maximum value in a list of numbers\n3. Sort a list of strings in alphabetical order\n4. Check if a number is prime\n\nSummary:\nThe functions provided include calculating the square root of a number, finding the maximum value in a list of numbers, sorting a list of strings in alphabetical order, and checking if a number is prime. These functions cover a range of mathematical and sorting operations. Canonical Correlation Analysis, also known as \u201cMode B\u201d PLS, is a method for comparing cross decomposition algorithms. It involves determining the number of components to keep, scaling X and Y, setting the maximum number of iterations, and defining convergence criteria. The algorithm can copy X and Y before applying centering and scaling, or perform these operations in place. It calculates the left and right singular vectors of cross-covariance matrices, loadings of X and Y, projection matrices, coefficients, and intercepts of the linear model. The method also includes the number of iterations for each component, fitting the model to data, transforming data back to its original space, predicting targets, and calculating the coefficient of determination. Additional features include setting output containers, configuring output formats, setting parameters, and requesting metadata. The method is used for dimension reduction, transforming data, and applying the dimension reduction process. PLS regression, also known as PLS2 or PLS1, is a cross decomposition algorithm that allows for dimension reduction. It involves keeping a specified number of components and scaling the input and output data. The algorithm iterates through the power method until convergence criteria are met. The model fits the training data by transforming the samples and targets using projection matrices. The coefficients of the linear model are used to approximate the targets. The algorithm can also transform data back to its original space and predict targets for new samples. The coefficient of determination \\(R^2\\) is used to evaluate the prediction performance. Additionally, the algorithm provides options for setting output formats and configuring the output of transformations. Metadata routing can be used to request additional information during prediction and scoring. Overall, PLS regression is a versatile algorithm for dimension reduction and prediction tasks. The Partial Least Squares (PLS) transformer and regressor is a method for dimensionality reduction and regression. It is used to estimate the first singular vectors of the cross-covariance matrix, with options for scaling, number of components to keep, and the algorithm used. The PLS algorithm includes features such as the left and right singular vectors, loadings of X and Y, projection matrices, coefficients of the linear model, and intercepts. It also allows for fitting the model to data, transforming data back to its original space, predicting targets, and calculating the coefficient of determination. Additionally, there are options for setting output containers, configuring output formats, setting parameters, and requesting metadata. The method supports metadata routing for various parameters and provides options for copying data or performing in-place normalization. It is important to note that the 'Y' parameter is deprecated in version 1.5 and will be removed in version 1.7, with 'y' being the recommended alternative. ",
    "sklearn.datasets": "The function allows for the deletion of all content in the data home cache and specifies the path to the scikit-learn data directory. It loads filenames and data from the 20 newsgroups dataset for classification, downloading it if necessary. Users can specify a download and cache folder for datasets, select the dataset to load (train, test, or all), and choose whether or not to shuffle the data. The function also allows for the removal of certain types of text from newsgroup posts to prevent overfitting on metadata. Additionally, it provides options for handling HTTP errors, specifying the number of retries and seconds between retries. The function returns a dictionary-like object with attributes such as data list, target labels, dataset description, target class names, and ndarrays containing samples and features. The California housing dataset is loaded for regression analysis, containing 20640 samples with 8 dimensions and real features ranging from 0.15 to 5. The data can be stored in a specified folder, and can be returned as a pandas DataFrame with appropriate data types. The target values are also returned as a pandas DataFrame or Series. Additional parameters include the number of retries and seconds between retries for HTTP errors. The dataset includes 9 features, with each row corresponding to the 8 feature values in order. The average house value is in units of 100,000. The dataset can be returned as a DataFrame with data and target, or as a tuple of two ndarrays representing samples and target values. The function allows for loading the kddcup99 dataset for classification purposes. It provides options to return specific subsets of the dataset or the entire dataset. Users can specify a different download and cache folder for the datasets. Additionally, there are options to shuffle the dataset, load only 10 percent of the data, and control the number of retries and seconds between retries when encountering HTTP errors. The function returns a dictionary-like object with attributes such as the data matrix, regression target, dataset description, column names, and the data and target arrays. Users can also choose to return a pandas DataFrame for the data and target objects. The function loads the Labeled Faces in the Wild (LFW) people dataset for classification purposes. It allows for downloading and caching the dataset, specifying a custom download folder, using the funneled variant of the dataset, resizing face pictures, setting a minimum number of pictures per person, keeping RGB channels, extracting a custom 2D slice from jpeg files, handling data availability, and returning data and target as a tuple. The dataset consists of 5749 classes with a total of 13233 samples and 5828 features. Each row corresponds to a ravelled face image of size 62 x 47 pixels, with labels ranging from 0-5748 representing person IDs. The function returns a tuple of two ndarrays, one containing the samples and the other containing the target samples. The function fetches datasets from openml by name or dataset id. Datasets can be uniquely identified by an integer ID or a combination of name and version. It is recommended to specify an exact version when retrieving a dataset as there may be multiple active versions that are fundamentally different. The function allows for specifying a download and cache folder for the datasets, as well as the column name to use as the target. It also provides options for caching downloaded datasets, returning data and target as pandas DataFrames, and specifying the number of retries and seconds between retries for HTTP errors or network timeouts. The function supports different parsers for loading ARFF files, with options for pandas or liac-arff. Additional keyword arguments can be passed to pandas.read_csv when using the pandas parser. The function returns a dictionary-like object with attributes such as the feature matrix, regression target or classification labels, dataset description, column names, target column names, and metadata from OpenML. Missing values in the data and target are represented as NaNs or None. The function notes differences in data types between the \"pandas\" and \"liac-arff\" parsers, particularly in how categorical features and numerical features are encoded. The function is a loader for the species distribution dataset from Phillips et. al. (2006). It allows for specifying a different download and cache folder for the datasets, as well as setting options for handling missing data and HTTP errors. The dataset includes 14 features measured at each point on a map grid, with latitude/longitude values. It also provides training and test points for two species: \"Bradypus variegatus\" and \"Microryzomys minutus\". The dataset represents the geographic distribution of these species and can be used with scikit-learn for modeling. References to the original study and an example of using the dataset are provided. The breast cancer Wisconsin dataset is a classic and easy binary classification dataset with 569 samples and 30 features. It can be loaded and returned as a pandas DataFrame with appropriate data types. The dataset includes data matrix, classification target, dataset columns names, target classes names, and full description. The data is represented as a 2D ndarray with each row as a sample and each column as a feature. Additionally, specific samples can be selected by their index to retrieve their class name. The function loads and returns the digits dataset for classification purposes. Each datapoint is an 8x8 image of a digit, with 10 classes and approximately 180 samples in total. The dataset has 64 features represented as integers from 0 to 16. It is a copy of the test set of the UCI ML hand-written digits datasets. The function allows for returning the data and target as pandas DataFrames if specified. It provides a dictionary-like object with attributes such as the flattened data matrix, classification target, dataset columns names, and target classes names. Additionally, it includes the raw image data and a full description of the dataset. The function also allows for loading the data and visualizing the images. The iris dataset is a classic and easy multi-class classification dataset with 3 classes, 50 samples per class, and a total of 150 samples. It has 4 features that are real and positive. The dataset can be loaded and returned as a pandas DataFrame with appropriate dtypes for the columns. The target can also be returned as a pandas DataFrame or Series. The dataset includes the data matrix, classification target, column names, target class names, and a full description. The data can be returned as a tuple of two ndarrays, one containing the 2D array of samples and features, and the other containing the target samples. There have been updates to the dataset to fix incorrect data points. Additionally, there is a function to load a numpy array of a single sample image, providing the name of the image and the image itself as a numpy array with dimensions of height x width x color. The function allows for loading datasets in the svmlight/libsvm format into sparse CSR matrix. This text-based format is suitable for sparse datasets and does not store zero-valued features. It can be used to predict a target variable and is recommended to be wrapped with joblib.Memory.cache for faster loading of subsequent calls. Pairwise preference constraints can be ignored unless the query_id parameter is set to True. The implementation is written in Cython and is reasonably fast. The function accepts various parameters such as the file path, number of features, data type, and whether column indices are zero-based or one-based. It can also return the query_id array for each file. Additionally, it allows for ignoring offset bytes and stopping reading data after a certain threshold. The function returns the data matrix, target, and query_id when applicable. The wine dataset is a classic and easy multi-class classification dataset with 3 classes and 178 total samples. It has 13 real, positive features. The dataset can be loaded and returned as a pandas DataFrame with appropriate dtypes for both data and target. The dataset also includes the names of dataset columns and target classes. Additionally, the dataset can be dumped in svmlight/libsvm file format for sparse datasets. It supports multilabel datasets and pairwise preference constraints. The function provided allows for the loading and vectorization of the 20 newsgroups dataset for classification purposes. It can be used to download the dataset if necessary and transform it using the default settings for CountVectorizer. More advanced usage, such as stopword filtering and n-gram extraction, can be achieved by combining fetch_20newsgroups with a custom CountVectorizer, HashingVectorizer, TfidfTransformer, or TfidfVectorizer. The resulting counts are normalized unless specified otherwise. The dataset can be selected to load either the training set, test set, or both, with shuffled ordering. Different subsets of text, such as headers, footers, and quotes, can be removed to prevent overfitting on metadata. Various options are available for specifying download and cache folders for the datasets. Additional features include normalizing each document's feature vector to unit norm, returning data as a pandas DataFrame, and specifying the number of retries and seconds between retries for HTTP errors. The function returns a dictionary-like object with attributes such as the input data matrix, target labels, target classes, and a full description of the dataset. The covertype dataset is a classification dataset with 7 classes and a total of 581012 samples with 54 features. The data can be loaded and cached in a specified folder. Random number generation can be determined for dataset shuffling for reproducible output. The dataset can be shuffled and returned as data and target arrays. The data can be returned as pandas DataFrames with appropriate dtypes. There are options for number of retries and seconds between retries for HTTP errors. The dataset contains 54 features corresponding to 7 forest covertypes. The dataset columns and target columns are named. The dataset can be returned as a tuple of two arrays, one for features and one for target samples. The Labeled Faces in the Wild (LFW) pairs dataset is a classification dataset with a total of 13233 samples and 2 classes. The dimensionality of the dataset is 5828 features, which are real values between 0 and 255. The original images in the dataset are 250 x 250 pixels, but they are resized to 62 x 47 pixels by default. The dataset can be loaded for training, testing, or for official evaluation with 10-folds cross validation. The dataset can be downloaded and cached in a custom folder. The funneled variant of the dataset can be used, and the color channels can be kept separate instead of averaging them. Custom 2D slices can be provided to extract specific parts of the images. HTTP errors encountered during download can be retried with a specified number of retries and time between retries. The dataset consists of pairs of face images, each corresponding to the same or different person from a total of 5749 people. The labels associated with each pair indicate whether the images are of the same person or different persons, with 0 representing \"Different person\" and 1 representing \"Same person\". The functions provided are for loading two different datasets: the Olivetti faces dataset and the RCV1 multilabel dataset. \n\nFor the Olivetti faces dataset, it contains images of faces from 40 different subjects, with each image being a ravelled face image of size 64 x 64 pixels. The dataset also includes labels ranging from 0-39 corresponding to the subject IDs. The function allows for specifying a different download and cache folder, shuffling the dataset, and determining random number generation for reproducible output. It also provides options for retrying HTTP errors and specifying the number of seconds between retries. The function returns a tuple with the data and target objects.\n\nOn the other hand, the RCV1 multilabel dataset includes samples with identification numbers and topics as targets. The dataset is in CSR format with a percentage of non-zero values. Similar to the Olivetti faces dataset function, this function allows for specifying a different download and cache folder, shuffling the dataset, and determining random number generation for reproducible output. It also provides options for retrying HTTP errors and specifying the number of seconds between retries. The function returns a dictionary-like object with attributes related to the dataset. The scikit-learn library provides a function to return the path of the data directory, which is used by certain dataset loaders to avoid redundant downloads. By default, the data directory is set to a folder named 'scikit_learn_data' in the user's home folder, but it can be customized using an environment variable or programmatically. The library also offers a function to load and return the diabetes dataset for regression analysis, containing 442 samples with 10 features. The feature names may not be clear, but the documentation provides information based on scientific literature. The dataset can be returned as a pandas DataFrame with mean-centered and scaled feature variables. The function returns a dictionary-like object with attributes such as the data matrix, regression target, and dataset columns. Additionally, the function returns a tuple of two arrays representing samples and features/targets. The function allows for loading text files with categories as subfolder names. It does not extract features into a numpy array or scipy sparse matrix. If load_content is set to true, the text encoding must be specified. Shuffling the data can be important for certain models. The function can also handle different file extensions and provides a dictionary-like object with raw text data, target labels, target classes, and dataset description. It is recommended to build feature extraction transformers for different types of unstructured data inputs like images, audio, and video. The functions described include loading and returning the physical exercise Linnerud dataset, which is suitable for multi-output regression tasks. The dataset contains 20 samples with a dimensionality of 3 for both data and target features. It can return either a Bunch object or (data, target) if True. The data can be a pandas DataFrame with appropriate column types, and the target can be a DataFrame or Series depending on the number of target columns. The dataset also includes attributes such as the names of dataset and target columns, as well as the path to the data and target locations. Additionally, there is a function to load sample images for image manipulation, specifically the china and flower images. The images can be visualized after loading. The function `load_svmlight_files` allows for loading datasets from multiple files in SVMlight format. It concatenates the results into a single list and ensures that all sample vectors have the same number of features. Pairwise preference constraints can be ignored unless the `query_id` parameter is set to True. The function accepts paths of files to load, can infer the number of features, and specifies the data type of the dataset. It also handles zero-based or one-based column indices and can return the query_id array for each file. Additionally, it provides the option to ignore offset bytes and stop reading data after a certain threshold. The function returns (Xi, yi) pairs or (Xi, yi, qi) triplets if `query_id` is set to True. It is important to note that when fitting a model, the matrices X_train and X_test must have the same number of features. The functions described generate arrays with block diagonal or block checkerboard structures for biclustering. They allow for the creation of datasets with specified shapes, numbers of biclusters, standard deviations of noise, and values within biclusters. The functions also provide options for shuffling samples and controlling random number generation for reproducible output. Additionally, they return arrays with indicators for cluster membership of rows and columns. References to related work in the field of biclustering are also provided for further reading. The function generates a random n-class classification problem by creating clusters of points normally distributed about vertices of a hypercube. It introduces interdependence between features and adds noise to the data. The features are stacked in a specific order, with informative, redundant, and duplicated features included. The number of samples, total features, informative features, redundant features, duplicated features, classes, clusters per class, class proportions, and other parameters can be specified. The function can shuffle samples and features for reproducibility. The algorithm is adapted from Guyon and was designed to generate the \"Madelon\" dataset. The functions provided include generating the \"Friedman #2\" regression problem with specific inputs and outputs, generating isotropic Gaussian and labeling samples by quantile for a classification dataset, and generating a mostly low rank matrix with bell-shaped singular values. The first function creates a regression dataset based on a specific formula, the second function creates a classification dataset with classes separated by nested concentric multi-dimensional spheres, and the third function generates a matrix with structured signal and noisy components. Each function has specific parameters for customization and allows for reproducible output across multiple function calls. The functions provided include generating a random multilabel classification problem, generating an S curve dataset, and options for controlling the output format such as returning sparse matrices or prior class probabilities. The multilabel classification function involves selecting labels, classes, document lengths, and words using various distributions and rejection sampling to ensure constraints are met. The S curve dataset function generates points on an S curve with the option to add Gaussian noise. Both functions allow for reproducible output by specifying a random number generation seed. The functions provided include generating a sparse symmetric definite positive matrix, generating a random symmetric positive-definite matrix, and generating isotropic Gaussian blobs for clustering. The first function allows for the creation of a matrix with specified characteristics such as sparsity, coefficient values, and output format. The second function generates a random symmetric positive-definite matrix based on specified dimensions. The third function is used for creating Gaussian blobs for clustering, with options for defining the number of points, features, centers, standard deviation, and more. Each function has parameters for controlling random number generation and reproducibility. The functions provided generate various datasets for different purposes. \n\nThe first function creates a large circle containing a smaller circle in 2D, which can be used to visualize clustering and classification algorithms. The number of points generated can be specified, and there is an option to shuffle the samples. Gaussian noise can be added to the data, and a scale factor between the inner and outer circle can be set. The function also generates integer labels for class membership.\n\nThe second function generates the \"Friedman #1\" regression problem, where inputs are independent features uniformly distributed on the interval [0, 1]. Only 5 out of the n_features are used to compute the output, with the remaining features being independent. Gaussian noise can be applied to the output, and references to the original research are provided.\n\nThe third function generates the \"Friedman #3\" regression problem, with inputs being 4 independent features uniformly distributed on specific intervals. The output is created according to a specific formula, and Gaussian noise can be applied to the output. References to the original research are provided.\n\nThe fourth function generates data for binary classification used in a specific example, where the features are standard independent Gaussian and the target is defined by a specific formula. References to related literature are provided. Random number generation can be controlled for reproducible output.\n\nOverall, these functions provide a variety of datasets for different machine learning tasks, including clustering, regression, and classification. The functions provided generate datasets for visualization of clustering and classification algorithms. The first function creates two interleaving half circles with the option to specify the number of points, shuffle samples, and add noise. The second function generates a random regression problem with options to control the number of samples, features, informative features, regression targets, bias term, and noise level. It also allows for the manipulation of the singular values profile. The third function generates a signal as a sparse combination of dictionary elements, with options to specify the number of samples, components in the dictionary, features, and active coefficients. Each function provides the necessary outputs for further analysis and modeling. ",
    "sklearn.decomposition": "1. Calculate the square root of a number\n2. Find the maximum value in a list of numbers\n3. Sort a list of strings in alphabetical order\n4. Check if a number is prime\n\nSummary:\nThe functions provided include calculating the square root of a number, finding the maximum value in a list of numbers, sorting a list of strings in alphabetical order, and checking if a number is prime. These functions cover a range of mathematical and sorting operations. Dictionary learning is a method that finds a set of atoms that encode data sparsely. It involves solving an optimization problem using different algorithms such as least angle regression and coordinate descent. The number of dictionary elements, sparsity controlling parameter, maximum iterations, and tolerance can be specified. The algorithm used to transform the data can also be chosen. Other parameters include the number of nonzero coefficients, parallel jobs, warm restart values, and verbosity control. Additional options include splitting the feature vector, enforcing positivity, and setting maximum iterations. The process involves extracting dictionary atoms, fitting the model, transforming data, and encoding data using the dictionary atoms. The output format can be configured, and parameters can be set or retrieved. The sparse combination of dictionary atoms can be used to encode test data. References to online dictionary learning for sparse coding are provided. FastICA is a fast algorithm for Independent Component Analysis, with the implementation based on a specific reference. It allows the user to specify the number of components to use, the algorithm to use, and the whitening strategy. The default whitening strategy changed in version 1.3. The algorithm involves a functional form of the G function, which can be customized. Other parameters include the maximum number of iterations, the initial un-mixing array, and the solver to use for whitening. The algorithm also involves the use of a linear operator to apply to the data to get the independent sources. The model can be fitted to training data and the sources can be recovered from the data. The sources can also be transformed back to the mixed data using the mixing matrix. Additional functionalities include getting output feature names, metadata routing, setting parameters, and requesting metadata for transformations. The algorithm also supports incremental principal components analysis, kernel principal component analysis, mini-batch sparse principal components analysis, and sparse principal components analysis. References are provided for further reading. Kernel Principal Component Analysis (KPCA) is a non-linear dimensionality reduction technique that uses kernels. It can utilize different LAPACK implementations based on the input data shape and the number of components to extract. KPCA can also employ randomized truncated SVD for efficiency. Users can compare KPCA with PCA and see examples of its application in denoising images. Various parameters such as the number of components, kernel type, and hyperparameters can be adjusted. The eigensolver can be selected based on the number of training samples and components. The method can handle zero eigenvalues and offers options for reproducibility and memory optimization. The model can fit data, transform it, and perform inverse transformations. Additional functionalities include metadata routing, setting output containers, and updating estimator parameters. Overall, KPCA offers a comprehensive approach to dimensionality reduction with kernels. The Mini-batch dictionary learning function finds a dictionary that performs well at sparsely encoding the fitted data by solving an optimization problem using different algorithms such as 'lars' and 'cd'. It allows for controlling parameters such as the number of dictionary elements, sparsity, and maximum number of iterations. The function also includes options for parallel processing, batch size, shuffling samples, and transforming data using various algorithms. Additional features like enforcing positivity, early stopping criteria, and output configuration are available. The function can be used to fit the model from data, transform it, update the model using mini-batches, and encode data as a sparse combination of dictionary atoms. References to Sparse Principal Components Analysis and online dictionary learning are provided. Overall, the function provides a comprehensive tool for dictionary learning and sparse coding of data. Sparse Principal Components Analysis (Sparse PCA) is a method that finds a set of sparse components to reconstruct data optimally. The sparseness level is controlled by the L1 penalty coefficient alpha. The number of sparse atoms to extract can be specified, with higher values of the sparsity controlling parameter leading to sparser components. Ridge shrinkage can be applied to improve conditioning during transformation. The algorithm stops after a certain number of iterations, with the option to specify a callable function to be invoked every five iterations. The data can be shuffled before splitting it into mini-batches, with the number of parallel jobs also configurable. Two optimization methods, LARS and CD, can be used. Early stopping criteria based on dictionary changes or cost function improvement can be set. The output includes the sparse components extracted, the estimated number of components, and the number of iterations run. The model can be fit to data and transformed, with options for output format configuration. The parameters of the estimator can be set, and the data can be transformed back to the original space using the inverse transformation. Ridge regression can be applied for stability, and the test data must have the same number of features as the training data. Principal Component Analysis (PCA) is a linear dimensionality reduction technique that uses Singular Value Decomposition to project data into a lower dimensional space. The input data is centered but not scaled before applying SVD. PCA can use LAPACK or randomized truncated SVD methods depending on the input data shape and number of components to extract. It supports sparse inputs for certain solvers. The number of components to keep can be specified, and various solvers can be used based on the input data characteristics. Whitening can be applied to the transformed signal to improve downstream estimator accuracy. PCA computes principal axes in feature space, representing directions of maximum variance, and the amount of variance explained by each component. It also provides the singular values corresponding to each component. The estimated noise covariance and number of features seen during fit are also available. PCA implements various solvers and methods for dimensionality reduction, and provides options for transforming data back to its original space. Additionally, it offers methods for computing data covariance, precision, and log-likelihood. The output format of the transformation can be configured, and parameters of the estimator can be set and updated. Overall, PCA is a powerful tool for reducing the dimensionality of data while preserving important information. Sparse Principal Components Analysis (SparsePCA) is a function that finds a set of sparse components to reconstruct data optimally. The sparseness level is controlled by the L1 penalty coefficient alpha. The number of sparse atoms to extract can be specified, with higher values of the sparsity controlling parameter leading to sparser components. Ridge shrinkage can be applied to improve conditioning during transformation. The function allows for setting the maximum number of iterations, tolerance for stopping conditions, and the method for optimization (lars or cd). It also supports setting the number of parallel jobs, initial values for loadings and components in warm restart scenarios, and controlling verbosity. The function returns the sparse components extracted from the data, errors at each iteration, estimated number of components, and other relevant information. Additionally, there is a mini-batch variant available for faster but less accurate results. The function can fit the model from input data, transform it, get output feature names, retrieve metadata routing, get parameters, transform data from latent space to original space, set output container, and set parameters of the estimator. It also supports least squares projection onto sparse components with optional regularization via ridge regression. The function requires test data with the same number of features as the training data for transformation. The function solves a dictionary learning matrix factorization problem by finding the best dictionary and corresponding sparse code to approximate the data matrix. It takes into account parameters such as the number of dictionary atoms, sparsity controlling parameter, maximum number of iterations, and tolerance for the stopping condition. The method used can be 'lars' or 'cd', with 'lars' being faster for sparse components. Other parameters include the number of parallel jobs, initial values for dictionary and sparse code, verbosity control, and options for enforcing positivity. The function also provides information on the sparse code factor, dictionary factor, errors at each iteration, and number of iterations run. Additionally, it offers an online version of the algorithm, a faster but less accurate version, Sparse Principal Components Analysis, and Mini-batch Sparse Principal Components Analysis. The level of sparsity of the sparse code can be evaluated by comparing the reconstruction error to the original signal. The Fast Independent Component Analysis function is used to estimate independent components from a data matrix X, where X = AS. The function allows for specifying the number of components to use, the algorithm to use, the whitening strategy, the G function used in the approximation to neg-entropy, the maximum number of iterations, the tolerance for convergence, the initial un-mixing array, the solver for whitening, and more. The function returns the un-mixing matrix, the mixing matrix, the estimated source matrix, and the mean over features. It is possible to estimate less sources than features by setting n_components < n_features. The implementation was originally for data of shape [n_features, n_samples], but now the input is transposed before the algorithm is applied for faster processing. Sparse coding is a method used to find a sparse array code that represents a data matrix against a dictionary matrix. Various algorithms can be used such as 'lars', 'lasso_lars', 'lasso_cd', 'omp', and 'threshold'. The number of nonzero coefficients to target in each column of the solution can be specified. The initialization value of the sparse codes and maximum number of iterations can also be set. The process can be parallelized and verbosity controlled. Additionally, positivity can be enforced in the encoding. Overall, sparse coding aims to find a sparse representation of data from a precomputed dictionary using different algorithms and parameters. Factor Analysis (FA) is a simple linear generative model with Gaussian latent variables. The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. The factors are distributed according to a Gaussian with zero mean and unit covariance, while the noise has an arbitrary diagonal covariance matrix. FactorAnalysis performs a maximum likelihood estimate of the loading matrix using an SVD-based approach. The dimensionality of the latent space, stopping tolerance for log-likelihood increase, making a copy of X, maximum number of iterations, initial guess of noise variance, SVD method, number of iterations for the power method, and rotation options are all parameters that can be specified. The model can be fit to data, transformed, and used for computing data covariance, precision matrix, log-likelihood, and expected mean of the latent variables. Other related models include Principal Component Analysis (PCA) and Independent Component Analysis. The FactorAnalysis class instance can be used to fit the model to data, transform it, compute data covariance, get output feature names, get metadata routing, get parameters, set output container, configure output format, set parameters, and apply dimensionality reduction to the data. Incremental Principal Components Analysis (IPCA) is a linear dimensionality reduction technique that uses Singular Value Decomposition (SVD) to project data into a lower-dimensional space by keeping only the most significant singular vectors. This algorithm is memory efficient, especially for sparse input data, with a constant memory complexity. IPCA allows for the use of np.memmap files without loading the entire file into memory. The algorithm computes the principal axes in feature space, representing the directions of maximum variance in the data. It also provides information on the variance explained by each selected component and the singular values corresponding to each component. Whitening can be applied to the transformed signal to improve predictive accuracy, but it may remove some information. The algorithm implements the incremental PCA model and is an extension of the Sequential Karhunen-Loeve Transform. The technique omits an optimization used in specific situations to reduce algorithmic complexity, which could be considered for future optimization. The algorithm also provides methods for fitting the model, transforming data, computing data covariance, and precision matrix, as well as inverse transforming data back to its original space. Additionally, it offers options for setting output containers, configuring output formats, setting parameters, and requesting metadata. The algorithm can be used for dimensionality reduction by projecting data onto the first principal components extracted from a training set using minibatches. Latent Dirichlet Allocation with online variational Bayes algorithm is implemented based on references [1] and [2]. The number of topics can be specified, and there are options to set the prior of document topic distribution theta and topic word distribution beta. The method used to update components can be chosen between online and batch updates, with the default now being batch. Parameters such as learning rate, downweighting, and number of passes over the data can be adjusted. Perplexity can be evaluated to check convergence, but it may increase training time. Variational parameters for topic word distribution are calculated, and the model can be fit to data using variational Bayes method. The estimator can transform data and calculate perplexity or log-likelihood scores. The output format can be configured, and parameters of the estimator can be set or retrieved. The document topic distribution for a given dataset can also be obtained. Mini-Batch Non-Negative Matrix Factorization (NMF) is a method added in version 1.1 that finds two non-negative matrices (W, H) whose product approximates a non-negative matrix X. This factorization can be used for dimensionality reduction, source separation, or topic extraction. The objective function is minimized with an alternating minimization of W and H. The number of components can be automatically inferred if not set. Different methods can be used to initialize the procedure, such as 'random' or 'nndsvd'. The beta divergence, regularization terms, and rescaling of past information can be controlled. Early stopping and maximum number of iterations can also be set. The transformed data is named W and the components matrix is named H. The model can be updated using partial_fit for out-of-core or online learning. The output format can be configured as 'default', 'pandas', 'polars', or None. The model parameters can be set and the data can be transformed according to the fitted MiniBatchNMF model. Non-Negative Matrix Factorization (NMF) is a technique used to find two non-negative matrices, W and H, whose product approximates a given non-negative matrix X. This factorization can be applied for tasks such as dimensionality reduction, source separation, or topic extraction. The objective function involves minimizing the difference between X and the dot product WH, with options for different beta-divergence losses. The regularization terms for W and H are scaled to balance their impact with respect to the data fit term. The NMF model can be initialized using various methods, such as 'random' or 'nndsvd'. The solver options include Coordinate Descent and Multiplicative Update. Additional parameters like regularization constants, mixing parameters, and verbosity can be adjusted. The model can be trained on data X and transformed efficiently. The output can be configured to different formats like 'pandas' or 'polars'. Overall, NMF is a powerful tool for data analysis and dimensionality reduction. Sparse coding is a method that finds a sparse representation of data against a fixed, precomputed dictionary. The goal is to find a sparse array code such that the dictionary atoms used for sparse coding are normalized to unit norm. Various algorithms can be used to transform the data, such as 'lars', 'lasso_lars', 'lasso_cd', 'omp', and 'threshold'. The number of nonzero coefficients to target in each column of the solution can be specified, as well as whether to split the sparse feature vector into its negative and positive parts. Other parameters include the number of parallel jobs to run, whether to enforce positivity when finding the code, and the maximum number of iterations to perform. The method also includes options for dictionary learning, mini-batch sparse principal components analysis, and sparse principal components analysis. Additionally, there are methods for fitting the transformer to data, getting output feature names, metadata routing, getting parameters for the estimator, setting output container, setting parameters of the estimator, and encoding the data as a sparse combination of dictionary atoms. Dimensionality reduction using truncated SVD, also known as LSA, is a linear dimensionality reduction technique that does not center the data before computing the singular value decomposition, making it efficient for sparse matrices. It works on term count/tf-idf matrices and supports two algorithms: a fast randomized SVD solver and a \"naive\" algorithm using ARPACK. The desired dimensionality of the output data can be specified, along with the SVD solver to use, number of iterations, oversamples, and power iteration normalizer for the randomized SVD solver. The right singular vectors, variance of training samples transformed by projection, percentage of variance explained, and singular values corresponding to selected components are provided. SVD suffers from \"sign indeterminacy\", so fitting the class to data once and keeping the instance for transformations is recommended. Other related techniques include incremental principal components analysis, kernel principal component analysis, and non-negative matrix factorization. The class provides methods for fitting the model on training data, performing dimensionality reduction, transforming data back to its original space, setting output containers, configuring output formats, getting metadata routing, getting parameters, setting parameters, and performing dimensionality reduction on new data. The function solves a dictionary learning matrix factorization problem online by finding the best dictionary and corresponding sparse code to approximate the data matrix X. It iterates over mini-batches of input data, with options to control the number of dictionary atoms, sparsity, maximum iterations, and more. Deprecated parameters include max_iter=None, with the default value now set to 100. Additional options include returning the code U, setting initial dictionary values, controlling verbosity, shuffling data, enforcing positivity, and specifying the number of parallel jobs. Different methods can be used to solve the lasso problem, such as 'lars' or 'cd'. The function also provides options for early stopping based on dictionary changes or cost function improvements. The output includes the sparse code, solutions to the dictionary learning problem, and the number of iterations run. This function is a faster, less accurate version of the dictionary learning algorithm, with options for Sparse Principal Components Analysis and Mini-batch Sparse Principal Components Analysis. The level of sparsity of U can be checked by comparing the reconstruction error of the sparse coded signal to the original signal. ",
    "sklearn.discriminant_analysis": "1. Calculate the square root of a number\n2. Find the maximum value in a list of numbers\n3. Sort a list of strings in alphabetical order\n4. Check if a number is prime\n\nSummary:\nThe functions provided include calculating the square root of a number, finding the maximum value in a list of numbers, sorting a list of strings in alphabetical order, and checking if a number is prime. These functions cover a range of mathematical and sorting operations. Linear Discriminant Analysis (LDA) is a classifier with a linear decision boundary that fits a Gaussian density to each class, assuming they share the same covariance matrix. It can also reduce input dimensionality by projecting it to the most discriminative directions. LDA has different solvers like 'svd', 'lsqr', and 'eigen', each with its own advantages. The class prior probabilities, number of components for dimensionality reduction, and other parameters can be specified. LDA can compute the weighted within-class covariance matrix and estimate the rank of the input data. It also supports a covariance estimator for more accurate covariance matrix estimation. LDA provides information on explained variances, class-wise means, class priors, scaling of features, overall mean, unique class labels, and number of features seen during fit. It can apply a decision function to samples, predict class labels, estimate log probabilities, and probabilities. LDA can be fitted to data and transformed, with options to set output container and configure output format. It also allows for setting and getting parameters, as well as requesting metadata for scoring. Additionally, LDA can project data to maximize class separation, with different output shapes depending on the solver used. ",
    "sklearn.dummy": "1. Calculate the square root of a number\n2. Find the maximum value in a list of numbers\n3. Sort a list of strings in alphabetical order\n4. Check if a number is prime\n\nSummary:\nThe functions provided include calculating the square root of a number, finding the maximum value in a list of numbers, sorting a list of strings in alphabetical order, and checking if a number is prime. These functions cover a range of mathematical and sorting operations. The DummyClassifier is a simple baseline classifier that makes predictions ignoring input features. It has different strategies for generating predictions, such as \"most_frequent\", \"prior\", \"stratified\", \"uniform\", and \"constant\". The default strategy changed to \"prior\" in version 0.24. The classifier can control randomness for \"stratified\" and \"uniform\" strategies by setting the random_state parameter. It can handle multi-output classification problems and provides information about unique class labels, number of labels, frequency of classes, features seen during fit, and more. The classifier can be fitted, used for prediction, return log probability estimates, return probability estimates, calculate mean accuracy, and handle metadata routing. Additionally, it allows for setting and getting parameters, as well as updating parameters for the estimator. The DummyClassifier can also handle metadata routing for sample_weight parameters in fit and score methods. ",
    "sklearn.ensemble": "",
    "sklearn.exceptions": "",
    "sklearn.experimental": "",
    "sklearn.feature_extraction": "",
    "sklearn.feature_selection": "",
    "sklearn.gaussian_process": "",
    "sklearn.impute": "",
    "sklearn.inspection": "",
    "sklearn.isotonic": "",
    "sklearn.kernel_approximation": "",
    "sklearn.kernel_ridge": "",
    "sklearn.linear_model": "",
    "sklearn.manifold": "",
    "sklearn.metrics": "",
    "sklearn.mixture": "",
    "sklearn.model_selection": "",
    "sklearn.multiclass": "",
    "sklearn.multioutput": "",
    "sklearn.naive_bayes": "",
    "sklearn.neighbors": "",
    "sklearn.neural_network": "",
    "sklearn.pipeline": "",
    "sklearn.preprocessing": "",
    "sklearn.random_projection": "",
    "sklearn.semi_supervised": "",
    "sklearn.svm": "",
    "sklearn.tree": "",
    "sklearn.utils": "",
    "Recently Deprecated": ""
}